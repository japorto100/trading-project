VOL 75AMS / MAA TEXTBOOKS
Optimal Control
A Differential Equations Approach
Stewart Johnson
Optimal Control
A Differential Equations Approach
AMS/MAA TEXTBOOKS
VOL 75
Optimal Control
A Differential Equations Approach
Stewart Johnson
MAA Textbooks Editorial Board
William R. Green, Co-Editor
Michael J. McAsey, Co-Editor
Paul T. Allen Mark Bollman Susan Crook
Meredith L. Greer Hugh N. Howards Kelly A. Jabbusch
Michael Janssen William Johnston Ryota Matsuura
Pamela Richardson Stephanie Treneer Erika Ward
Elizabeth Wilcox
2020 Mathematics Subject Classification. Primary 49-01, 34-01, 49K15, 49L12, 49N70, 49N90,
34H05, 35Q93, 37N35.
For additional information and updates on this book, visit
www.ams.org/bookpages/text-75
Library of Congress Cataloging-in-Publication Data
Cataloging-in-Publication Data has been applied for by the AMS.
See http://www.loc.gov/publish/cip/.
Copying and reprinting. Individual readers of this publication, and nonprofit libraries acting for them,
are permitted to make fair use of the material, such as to copy select pages for use in teaching or research.
Permission is granted to quote brief passages from this publication in reviews, provided the customary ac-
knowledgment of the source is given.
Republication, systematic copying, or multiple reproduction of any material in this publication is permit-
ted only under license from the American Mathematical Society. Requests for permission to reuse portions
of AMS publication content are handled by the Copyright Clearance Center. For more information, please
visit www.ams.org/publications/pubpermissions.
Send requests for translation rights and licensed reprints to reprint-permission@ams.org.
Â© 2025 by the American Mathematical Society. All rights reserved.
The American Mathematical Society retains all rights
except those granted to the United States Government.
Printed in the United States of America.
âƒâˆ The paper used in this book is acid-free and falls within the guidelines
established to ensure permanence and durability.
Visit the AMS home page at https://www.ams.org/
10 9 8 7 6 5 4 3 2 1 30 29 28 27 26 25
To Mary, my wonderful wife and partner in life.
Contents
Introduction xi
1 Getting Started 1
1.1 A Simple Game 1
1.2 Terminology 6
1.3 The Difficulty 7
1.4 Costs and Ends 8
Key Points 9
Exercises 9
2 Static Optimization 13
2.1 The Derivative 13
2.2 Differentiation 14
2.3 Approximations 15
2.4 Extreme Values 17
2.5 Optimum Along a Path 18
2.6 Lagrange with One Constraint 20
2.7 Higher Dimensions 23
2.8 Multiple Constraints 24
2.9 Lambda 27
2.10 Hamilton and Lagrange 32
Key Points 33
Exercises 33
3 Control: A Discrete Start 35
3.1 Optimal Two-Step Process Control 35
3.2 Optimal ğ‘-Step Process Control 38
3.3 Deriving Principle 0 43
Key Points 45
Exercises 45
4 First Principle 49
4.1 One Dimension, Fixed Ends 49
4.2 Time Dependence 57
4.3 Can We Solve It? 59
Key Points 63
Exercises 63
vii
viii Contents
5 Unpacking Pontryagin 67
5.1 Hamilton and Pontryagin 67
5.2 The Principle of Optimality 71
5.3 Costates 73
5.4 Minimal Surfaces 75
Key Points 78
Exercises 78
6 Easing the Restrictions 81
6.1 One Dimension, Free Ends 81
6.2 When Things Go Wrong 87
6.3 Proving Pontryagin 89
Key Points 93
Exercises 94
7 Linear-Quadratic Systems 99
7.1 Linear-Quadratic with Fixed Ends 99
7.2 Linear-Quadratic with Free Ends 106
Key Points 110
Exercises 111
8 Two Dimensions 115
8.1 Optimal Control in Two Dimensions 116
8.2 Thrust Programming and Rocket Sleds 116
8.3 Zermelo onna Boat 125
8.4 The Brachistochrone Problem 131
Key Points 134
Exercises 134
9 Targets 139
9.1 Free Ends 139
9.2 Hitting a Curve 145
Key Points 152
Exercises 152
10 Switching Controls and Stationarity 155
10.1 Extreme Controls 155
10.2 Bang-Bang Controls 158
10.3 Rocket Races 160
10.4 Stationarity 163
Key Points 166
Exercises 166
11 Time, Value, and Hamilton-Jacobi-Bellman Equation 173
11.1 Time 173
11.2 Performance 178
11.3 Hamilton-Jacobi-Bellman Equation 181
Key Points 186
Exercises 186
Contents ix
12 Differential Games 189
12.1 Games 189
12.2 Differential Games 190
12.3 War 193
Key Points 199
Exercises 199
13 Calculus of Variations 201
13.1 Euler-Lagrange 201
13.2 Isoperimetric Problems 203
13.3 Conversions 204
Key Points 207
Exercises 207
A Table of Principles 209
B Two-Dimensional Linear Systems 211
C Hints 215
D Solutions 219
Bibliography 223
Index 225
Introduction
Humans are intelligent. We influence processes to achieve favorable outcomes, and we
try to get the very best outcome with just the right influence. We optimize.
We do this when choosing an investment strategy to maximize returns, operating
an engine on a hybrid car to minimize fuel use, designing a dosing regime for a medica-
tion to maximize treatment effect, setting an interest rate for an economy to maximize
growth and minimize inflation, establishing policies to maximize sustainable harvests,
and many other cases.
One set of mathematical tools can address all of these situations, and this is the
subject of this text.
```
Our main focus will be differential equation models ğ‘¥â€² = ğ‘“(ğ‘¥, ğ‘¢) containing a
```
state variable ğ‘¥ and a control parameter ğ‘¢, and for which we have some performance
measure ğ½, typically expressed as an integral, that captures the accumulated net payoff
over some time interval. We will explore the tools developed by Lev Pontryagin that
characterize controls ğ‘¢ that produce maximum values for ğ½ under specified constraints.
The following points distinguish the approach in this text:
â€¢ Designed specifically for undergraduates in mathematics, physics, and economics
programs.
â€¢ Assumes nothing more than a good grasp of calculus, differential equations, and
some basic matrix algebra.
â€¢ Jumps right in with basic control problems and develops the theory gradually and
naturally, starting with familiar optimization methods from calculus.
â€¢ Has a hands-on approach and features examples and problems that can be worked
directly or with the assistance of a computer algebra system.
â€¢ Focuses on understanding the structure of the theory and how to apply the princi-
ples to a wide variety of models.
Examples and exercises include basic problems of controlling vehicle thrust, sav-
ing/spending money, inventory management, fisheries management, optimal consump-
tion of diminishing resources, linear-quadratic systems, and the classic brachistochrone
and Zermelo navigation problems. We will also look at some differential games, in-
cluding guarding a target and the war game of attrition and attack. We also generalize
these techniques to apply to geometric constructs like soap films and isoparametric
problems.
xi
xii Introduction
At heart, this text is about mathematical beauty, examined through a meaning-
ful exploration of the deeper structure of differential dynamics through the lens of
```
optimization: how the offsetting influences of costs and benefits fold together over
```
time to produce tantalizing mathematical landscapes that are fascinating, ultimately
intuitive, and very powerful.
To the Student
The emphasis of this text is understanding optimization tools and how to apply them.
This entails using them: understanding the examples and being able to work the exer-
cises are the keys to success.
Understanding the tools requires a good grasp of basic calculus concepts: deriva-
tives, integrals, and gradients.
Using the tools requires solid skills in working with differential equations: solving
```
first-order equations ğ‘¥â€² = ğ‘“(ğ‘¥, ğ‘¡), working with two-dimensional systems (ğ‘¥â€², ğ‘¦â€²) =
```
```
(ğ‘“(ğ‘¥, ğ‘¦), ğ‘”(ğ‘¥, ğ‘¦)) and their associated phase portraits, understanding linear systems,
```
and solving two-point boundary problems. Some of the work with differential equa-
tions can be challenging, and you are strongly encouraged to make good use of com-
puter algebra systems such as SageMath, SymPy, Mathematica, or various other sym-
bolic differential equation solvers. At the time of this writing, AI platforms are notori-
ously bad at solving differential equations, but hopefully this will be improving.
```
Some exercises, marked with (h), have a hint at the back of the book. If you are
```
```
stuck, see if a hint may help you out. Some exercises, marked with (s), have the solution
```
at the back of the book so you can check your answer.
```
Throughout the text I have a number of prompts such as (âˆ— check this âˆ—) or
```
```
(âˆ— verify âˆ—). These are intended to engage and alert the reader that there is a step here
```
that is not trivial or obvious and that it may be worth pausing to consider how you
would check or verify the step. You may want to work out some of these prompts on
scratch paper in order to fully develop understanding, but Iâ€™m not expecting you to stop
at each and every prompt to work out the step by hand.
I hope this study is meaningful for you and that it helps you become a stronger and
more capable mathematician. More than that, I hope you come out of this study with
deeper insight into mathematical structures and a greater appreciation of mathemati-
cal beauty.
To the instructor
While the main purpose of this text is to explore tools for optimal control, it is very
much written to develop greater understanding of and ability to work with sophisti-
cated mathematical structures. This study will develop student skills in working with
differential equations, broaden their insight into optimization concepts, and develop
their understanding of real analytic structures regardless of whether they have had a
formal course in real analysis.
Introduction xiii
The topic of optimization and optimal control is approached intuitively with many
supporting examples. The treatment is mathematically rigorous at a level appropriate
for undergraduates, with the core concepts having careful proofs that are specifically
designed for an undergraduate reader.
The prerequisites are minimal: the student needs a firm grasp of multivariable cal-
```
culus, some basic matrix algebra (up to eigenvalues and eigenvectors for 2Ã—2 systems),
```
a good foundation in solving differential equations and working with phase portraits
for systems of differential equations. Important concepts in calculus and static opti-
mization will be reviewed, including Lagrange multipliers, and we will develop every-
thing else we need as we go.
I avoid a numerical methods approach to this topic, as I have found that approx-
imation tools distract or even obfuscate the core understanding of the concepts, and,
more often than not, the numerical solvers simply fail. As such I rely mainly on exam-
ples and exercises that can be solved in closed form. Some of these can be challenging
for students. I strongly recommend supplementing the course with support for solving
differential equations using a computer algebra platform of choice, such as SageMath,
SymPy, Mathematica, or any other differential equation solver.
Exercises are ordered to follow the chapter material and to build up from easier
computational to more conceptual problems. The text is also peppered with prompts
for the reader to check or verify steps in computations and derivations, which can make
for good short exercises to pace the students through the material.
This text has grown out of teaching a senior-level undergraduate seminar at
Williams College on dynamic modeling and optimal control for over 30 years. Perhaps
my most gratifying experience as an instructor has been hearing from graduates who
report back on how valuable this course has been to them in their growth as mathemati-
cians and how learning these advanced optimization techniques as an undergraduate
has opened opportunities for them in their varied careers. I hope you have a similar
positive experience with this material.
Roadmap
Chapter 1 lays out the basic problem of optimal control in the discrete case and ex-
plains why it can be challenging to solve. Chapter 2 lays a foundation of calculus-level
```
optimization including a review of Lagrange multipliers; this is a longer chapter and
```
```
is intended to be covered fairly quickly (two to three lectures) but may need more or
```
less time depending on background of the learner. Chapter 3 resolves the issues from
Chapter 1 using dynamic Lagrange multipliers, thereby establishing the fundamental
state-costate structure of optimal control. Chapters 4â€“6 develop Pontryaginâ€™s method
using differential equations in a one-dimensional state space, with Chapter 7 filling
out the ideas in the linear-quadratic case, and Chapter 8 generalizing to higher dimen-
sions. These chapters, 1â€“8, would be a minimal treatment of the topic. Chapters 9â€“12
expand the ideas in several important directions and can be covered more or less at
the instructorâ€™s or learnerâ€™s discretion. Chapter 13 covers calculus of variations. The
text is designed to be covered at a brisk pace in a standard 12-week semester, although
individual instructors may prefer to swap out some of the later topics for their own
preferred expositions.
xiv Introduction
Thanks!
I am very grateful to all the students who have engaged this material and provided
feedback, suggestions, and corrections. I am particularly thankful to students who
have done some significant work on this material, including Joe Fox, Jonathan Geller,
Seha Karabacak, and Noah Reich. I deeply appreciate the sustained support, guid-
ance, enthusiasm, and many helpful suggestions of my editors, William Green, Michael
McAsey, and Arlene Oâ€™Sean.
1
Getting Started
We begin our study with contrived examples of simple processes to establish basic ideas
and terminology for optimal control and to identify the core mathematical challenge
for solving such problems. These are toy problems that illuminate basic concepts. In
these examples, one is influencing the state of a process for a fixed amount of time.
Running costs accrue depending on the state of the process and the amount of control
applied, and a final payoff is awarded depending on the final state of the system. You
want to bring the system to a final state that yields a high payoff without expending too
much along the way. The exercise is to figure out how to influence the process in just
the right way to produce the maximum net gain.
1.1 A Simple Game
The following example is a very simple model of a one-step process with costs and
payoffs. The rules of this game are contrived to make an intuitive example with man-
ageable computation.
Example 1.1: One-Step Bocce
Start with a Bocce ball at given location ğ‘¥0 on the positive ğ‘¥-axis.
We may move the ball ğ‘¢ â‰¥ 0 units to the right at a cost of ğ‘¢2/ğ‘¥0 dollars. The
cost is inversely proportional to distance ğ‘¥0 from the origin, so movement gets
cheaper the further we are down the number line. The cost is also proportional
to the square of the distance ğ‘¢ that we move, so longer moves get really expensive.
1
2 Chapter 1. Getting Started
The final location of the ball is then ğ‘¥1 = ğ‘¥0 + ğ‘¢. The end payoff is equal to
this distance we moved down the number line, so we collect a payoff of ğ‘¥1 dollars.
The net payoff in this game is then
```
ğ½(ğ‘¥0, ğ‘¢) = ğ‘¥1 âˆ’ áµ†2ğ‘¥0
```
```
= (ğ‘¥0 + ğ‘¢) âˆ’ áµ†2ğ‘¥0.
```
So if we start the game at ğ‘¥0 = 2 and move the ball ğ‘¢ = 3 units to the right,
we incur a cost of ğ‘¢2/ğ‘¥0 = 32/2 and collect a payoff of ğ‘¥0 + ğ‘¢ = 2 + 3 = 5, for a
```
net gain of ğ½(2, 3) = 5 âˆ’ 9/2 = 1/2.
```
Can we get a better payoff ? Starting at ğ‘¥0 = 2, what is the most we can gain
from this process? This is a question of maximizing the differentiable function
```
ğ½(ğ‘¥0, ğ‘¢) = ğ‘¥0 + ğ‘¢ âˆ’ ğ‘¢
```
2
ğ‘¥0
```
over the domain ğ‘¢ âˆˆ [0, âˆ) for a fixed ğ‘¥0.
```
First, find places inside the domain with a zero derivative:
0 = ğœ•ğ½ğœ•ğ‘¢ = 1 âˆ’ 2ğ‘¢ğ‘¥
0
âŸ¹ ğ‘¢ = 12 ğ‘¥0.
Using the move ğ‘¢ = ğ‘¥0/2 produces a net payoff of
ğ½ = 54 ğ‘¥0.
This is a local maximum since ğœ•2ğ½ğœ•áµ†2 < 0.
```
The control ğ‘¢ is restricted to the interval [0, âˆ), and we check the endpoints:
```
ğ‘¢ = 0 produces a payoff of ğ½ = ğ‘¥0, and ğ‘¢ â†’ âˆ forces ğ½ â†’ âˆ’âˆ. Therefore,
starting at ğ‘¥0, the optimal move is ğ‘¢ = ğ‘¥0/2 producing ğ½ = 5ğ‘¥0/4. No other move
can produce a higher payoff.
We call the optimal payoff starting at ğ‘¥0 the value of the one-step game:
```
ğ‘‰1(ğ‘¥0) = 5ğ‘¥0/4
```
```
Starting at ğ‘¥0 = 2 we have ğ½(2, ğ‘¢) = 2 + ğ‘¢ âˆ’ ğ‘¢2/2 which attains a maximum
```
```
of ğ½ = 5/2 at ğ‘¢ = 1 (Figure 1.1). So ğ‘‰1(2) = 5/2 is the absolute maximum gain for
```
ğ‘¥0 = 2.
1.1. A Simple Game 3
Figure 1.1. Maximum of ğ½ as a function of ğ‘¢.
The one-step game was solved using techniques from single variable calculus. Now
we take the same example with two steps, which will require multivariable calculus.
Example 1.2: Two-Step Bocce
Start with a Bocce ball at location ğ‘¥0 > 0, and on our first step we move it ğ‘¢0 â‰¥ 0
units to the right at a cost of ğ‘¢20/ğ‘¥0 to a new location ğ‘¥1 = ğ‘¥0 + ğ‘¢0. On the second
step we move the ball ğ‘¢1 â‰¥ 0 units to the right at a cost of ğ‘¢21/ğ‘¥1 to its final location
ğ‘¥2 = ğ‘¥1 + ğ‘¢1 and collect a payoff of ğ‘¥2. Our net payoff is then
```
ğ½(ğ‘¥0, ğ‘¢0, ğ‘¢1) = ğ‘¥2 âˆ’ ğ‘¢
```
20
ğ‘¥0âˆ’
ğ‘¢21
ğ‘¥1
```
= (ğ‘¥0 + ğ‘¢0 + ğ‘¢1) âˆ’ ğ‘¢
```
20
ğ‘¥0âˆ’
ğ‘¢21
ğ‘¥0 + ğ‘¢0.
So if we start at ğ‘¥0 = 2 and implement moves ğ‘¢0 = 3 followed by ğ‘¢1 = 4, we
```
have ğ‘¥1 = 5, ğ‘¥2 = 9 and our net payoff is ğ½(2, 3, 4) = 9 âˆ’ 32/2 âˆ’ 42/5 = 1.3.
```
Can we get a better payoff? Starting at ğ‘¥0 = 2, what is the most we can gain
from this process?
In the two-step problem we are given ğ‘¥0 and have control over ğ‘¢0 and ğ‘¢1, so
we look for maximum net gain
```
ğ½(ğ‘¥0, ğ‘¢0, ğ‘¢1) = (ğ‘¥0 + ğ‘¢0 + ğ‘¢1) âˆ’ ğ‘¢
```
20
ğ‘¥0âˆ’
ğ‘¢21
ğ‘¥0 + ğ‘¢0
4 Chapter 1. Getting Started
by setting the partial derivatives ğœ•ğ½ğœ•áµ†0and ğœ•ğ½ğœ•áµ†1equal to zero to find a single critical
```
point (âˆ— check this âˆ—):
```
ğ‘¢0 = 58 ğ‘¥0,
ğ‘¢1 = 1316 ğ‘¥0.
Using these moves yields a net payoff of
ğ½ = 10564 ğ‘¥0.
Using techniques from multivariable calculus, we check that the discriminant at
this critical point is positive and the two second derivatives are negative, so this
critical point is a local maximum.
```
Both controls are restricted to the interval [0, âˆ). We need to analyze the
```
boundaries ğ‘¢0 = 0, ğ‘¢1 = 0, ğ‘¢0 â†’ âˆ, ğ‘¢1 â†’ âˆ to conclude this is a global
maximum. If either control is zero, the game reduces to the one-step game with
a maximum payoff of ğ½ = 5 ğ‘¥0/4 < 105 ğ‘¥0/64. The payoff will become negative
for large values of either control.
Therefore, starting at ğ‘¥0, the optimal moves produce a payoff of ğ½ = 105 ğ‘¥0/64.
This is the absolute maximum. No other pair of moves can produce a higher pay-
off.
The optimal payoff starting at ğ‘¥0 is called the value of the two-step game:
```
ğ‘‰2(ğ‘¥0) = 105 ğ‘¥0/64.
```
```
We conclude that for two moves starting at ğ‘¥0 = 2 we have ğ½(2, ğ‘¢0, ğ‘¢1) =
```
```
(ğ‘¥0 + ğ‘¢0 + ğ‘¢1) âˆ’ ğ‘¢20/ğ‘¥0 âˆ’ ğ‘¢21/(ğ‘¥0 + ğ‘¢0), which has a maximum of ğ½ = 3.28 . . . at
```
```
ğ‘¢0 = 1.25 and ğ‘¢1 = 1.625 (Figure 1.2).
```
Figure 1.2. Contour lines and maximum point for ğ½ as a func-
tion of ğ‘¢0, ğ‘¢1.
Real-world processes may take place over a large number of steps. The following
is the Bocce example for ğ‘ steps.
1.1. A Simple Game 5
Example 1.3: ğ‘-Step Bocce
To generalize the previous examples to ğ‘ steps, start with a Bocce ball at location
ğ‘¥0 > 0, and on our first step we move it ğ‘¢0 â‰¥ 0 units to the right at a cost of ğ‘¢20/ğ‘¥0
to a new location ğ‘¥1 = ğ‘¥0 + ğ‘¢0. On the second step we move the ball ğ‘¢1 â‰¥ 0 units
to the right at a cost of ğ‘¢21/ğ‘¥1 to its final location ğ‘¥2 = ğ‘¥1 + ğ‘¢1, and so on until we
reach a final location ğ‘¥ğ‘ . We collect an end payoff of ğ‘¥ğ‘ , so our net payoff is
```
ğ½(ğ‘¥0, ğ‘¢0, ğ‘¢1, . . . , ğ‘¢ğ‘âˆ’1) = ğ‘¥ğ‘ âˆ’ ğ‘¢
```
20
ğ‘¥0âˆ’
ğ‘¢21
ğ‘¥1âˆ’ â‹¯ âˆ’
ğ‘¢2ğ‘âˆ’1
ğ‘¥ğ‘âˆ’1.
For ğ‘ = 4 this would expand out to be
```
ğ½(ğ‘¥0, ğ‘¢0, ğ‘¢1, ğ‘¢3) = (ğ‘¥0 + ğ‘¢0 + ğ‘¢1 + ğ‘¢2 + ğ‘¢3) âˆ’ ğ‘¢
```
20
ğ‘¥0
âˆ’ ğ‘¢
21
ğ‘¥0 + ğ‘¢0âˆ’
ğ‘¢22
ğ‘¥0 + ğ‘¢0 + ğ‘¢1âˆ’
ğ‘¢23
ğ‘¥0 + ğ‘¢0 + ğ‘¢1 + ğ‘¢2.
To maximize this, we would compute the partials ğœ•ğ½ğœ•áµ†0, . . . , ğœ•ğ½ğœ•áµ†3, set them equal
to zero, and solve the four equations for the four unknown controls ğ‘¢0, . . . , ğ‘¢3:
0 = ğœ•ğ½ğœ•áµ†0= 1 âˆ’ ğ‘¢0 + áµ†
21
```
(2+áµ†0)2 +
```
áµ†22
```
(2+áµ†0+áµ†1)2 +
```
áµ†23
```
(2+áµ†0+áµ†1+áµ†2)2 ,
```
0 = ğœ•ğ½ğœ•áµ†1= 1 âˆ’ 2áµ†12+áµ†0+ áµ†
22
```
(2+áµ†0+áµ†1)2 +
```
áµ†23
```
(2+áµ†0+áµ†1+áµ†2)2 ,
```
0 = ğœ•ğ½ğœ•áµ†2= 1 âˆ’ 2áµ†22+áµ†0+áµ†1+ áµ†
23
```
(2+áµ†0+áµ†1+áµ†2)2 ,
```
0 = ğœ•ğ½ğœ•áµ†3= 1 âˆ’ 2áµ†32+áµ†0+áµ†1+áµ†2.
This is four nonlinear equations in four unknowns. This can be solved by
hand or by using a symbolic processor to conclude that for ğ‘¥0 = 2 our optimal
choices are ğ‘¢0 = 2.31 . . . , ğ‘¢1 = 3.53 . . . , ğ‘¢2 = 4.91 . . . , and ğ‘¢3 = 6.38 . . . , with a
net payoff of ğ½ = 7.30 . . . .
For ğ‘ = 10 we would have 10 equations in 10 unknowns, and working
through a solution and verifying that it is a maximum starts to become quite in-
volved.
6 Chapter 1. Getting Started
The previous example demonstrates that adding more steps compounds the com-
plexity of using calculus techniques to find a maximum. Using this approach, solving
the ğ‘ = 100 case would create a wall of 100 equations in 100 unknowns.
With complicated controls and a large number of steps the problem simply be-
comes impossible to compute by hand and can overwhelm computer symbolic or nu-
merical methods for locating maxima.
Perhaps more importantly, this brute force approach produces a large number of
equations to be solved but doesnâ€™t yield much insight into what is going on within the
process that would lead to good optimization strategies.
We need better tools.
In the 1950s, Lev Pontryagin and his students developed mathematical structures
for analyzing controls by defining a codynamical system of Lagrange multipliers that
addresses the compounding complexity of multiple steps without requiring the simul-
taneous solution of a large number of equations. They defined a quantity, called a
Hamiltonian, within this dynamic/codynamic structure that creates a global optimum
by choosing a local optimum at every step. Understanding this methodology is the core
topic of this text, as will be developed and explored in the upcoming chapters.
We begin by standardizing our terminology for discrete time control processes and
revisiting the problem of compounded complexity demonstrated in the above exam-
ples.
We then need to carefully reexamine the fundamental mathematical structures of
differentiation and optimization and to review the method of Lagrange multipliers,
which we undertake in Chapter 2. The core idea of Pontryaginâ€™s method is developed
in Chapters 3 through 5, and the concepts are more fully explored in the remaining
chapters.
1.2 Terminology
Optimal control uses terminology from dynamical systems.
In the Bocce ball examples the state of the system is the position of the ball, which
could be anywhere on the positive real line, and we call this the state space. We will
work with continuous state spaces, typically â„ or â„ğ‘›.
The future state of the system is completely determined by the current state, the
location ğ‘¥ of the ball, and the influence we apply, represented by the control variable
ğ‘¢ which in this case is the distance we move the ball. The Bocce ball examples are
discrete dynamics: if the current state is ğ‘¥ğ‘›, the next state is
```
ğ‘¥ğ‘›+1 = ğ‘“(ğ‘¥ğ‘›, ğ‘¢ğ‘›) = ğ‘¥ğ‘› + ğ‘¢ğ‘›.
```
The sequence of states is the trajectory of the system. For two-step Bocce, the trajectory
```
is (ğ‘¥0, ğ‘¥1, ğ‘¥2), with ğ‘¥0 as the initial state and ğ‘¥2 as the final state.
```
The sequence of controls is the control vector or the control function. For two-step
```
Bocce the control vector is (ğ‘¢0, ğ‘¢1).
```
We want to maximize payoff, which we typically think of as being positive, with
negative payoffs being cost.
Payoffs have two components: those that accumulate along the way and those that
come from the final state of the system. For two-step Bocce, costs along the way are
```
ğ‘”(ğ‘¥ğ‘›, ğ‘¢ğ‘›) = âˆ’ğ‘¢2ğ‘›/ğ‘¥ğ‘› and the final payoff is ğº(ğ‘¥2) = ğ‘¥2. The final sum of all payoffs and
```
1.3. The Difficulty 7
costs is referred to as net payoff, which for two-step Bocce would be
```
ğ½ = ğº(ğ‘¥2) âˆ’ ğ‘¢
```
20
ğ‘¥0âˆ’
ğ‘¢21
ğ‘¥1.
The general discrete ğ‘-step optimal control problem can thus be formulated with an
```
initial state ğ‘¥0, a dynamical system ğ‘¥ğ‘›+1 = ğ‘“(ğ‘¥ğ‘›, ğ‘¢ğ‘›), and a control vector (ğ‘¢0, . . . , ğ‘¢ğ‘âˆ’1)
```
```
producing a trajectory (ğ‘¥0, . . . , ğ‘¥ğ‘ ). The net payoff would be
```
```
ğ½(ğ‘¥0, ğ‘¢0, . . . , ğ‘¢ğ‘âˆ’1) = ğº(ğ‘¥ğ‘ ) +
```
ğ‘âˆ’1
âˆ‘
ğ‘›=0
```
ğ‘”(ğ‘¥ğ‘›, ğ‘¢ğ‘›) with ğ‘¥ğ‘›+1 = ğ‘“(ğ‘¥ğ‘›, ğ‘¢ğ‘›).
```
The optimal control problem is then to maximize ğ½ for a given starting position ğ‘¥0
over all possible control vectors ğ‘¢0, . . . , ğ‘¢ğ‘›âˆ’1. This typically involves some restrictions
on the state space and/or controls. In the Bocce examples, the ball could be anywhere
on the positive real line, ğ‘¥ > 0, and our controls have to be nonnegative.
Most problems have a specified starting location. Some problems may specify a
fixed ending location, so only the payoffs or costs along the way are relevant. Other
problems have a free end condition with an associated payoff.
In the Bocce ball examples, we incur costs along the way and receive a payoff at
the end. Other possibilities may be payoffs along the way and a cost at the end. It could
be that everything is a payoff. Or it could be that everything is a cost and we want to
minimize rather than maximize.
1.3 The Difficulty
The calculus techniques used in the previous section donâ€™t work well for additional
steps. Consider the four-step case.
We have a starting position ğ‘¥0, our controls are ğ‘¢0, ğ‘¢1, ğ‘¢2, and ğ‘¢3, and we express
intermediate locations in terms of our controls:
```
ğ‘¥1 = ğ‘“(ğ‘¥0, ğ‘¢0),
```
```
ğ‘¥2 = ğ‘“(ğ‘¥1, ğ‘¢1) = ğ‘“(ğ‘“(ğ‘¥0, ğ‘¢0), ğ‘¢1),
```
```
ğ‘¥3 = ğ‘“(ğ‘¥2, ğ‘¢2) = ğ‘“(ğ‘“(ğ‘“(ğ‘¥0, ğ‘¢0), ğ‘¢1), ğ‘¢2),
```
```
ğ‘¥4 = ğ‘“(ğ‘¥3, ğ‘¢3) = ğ‘“(ğ‘“(ğ‘“(ğ‘“(ğ‘¥0, ğ‘¢0), ğ‘¢1), ğ‘¢2), ğ‘¢3).
```
Our payoff is then
```
ğ½(ğ‘¥0, ğ‘¢0, ğ‘¢1, ğ‘¢2, ğ‘¢3) = ğº(ğ‘“(ğ‘“(ğ‘“(ğ‘“(ğ‘¥0, ğ‘¢0), ğ‘¢1), ğ‘¢2), ğ‘¢3)) + ğ‘”(ğ‘¥0, ğ‘¢0) + ğ‘”(ğ‘“(ğ‘¥0, ğ‘¢0), ğ‘¢1)
```
- ğ‘”(ğ‘“(ğ‘“(ğ‘¥0, ğ‘¢0), ğ‘¢1), ğ‘¢2) + ğ‘”(ğ‘“(ğ‘“(ğ‘“(ğ‘¥0, ğ‘¢0), ğ‘¢1), ğ‘¢2), ğ‘¢3).
Yikes. Our calculus technique would then be to take partials with respect to ğ‘¢0,
ğ‘¢1, ğ‘¢2, ğ‘¢3, set them equal to zero, and so on. Very messy. Chain rule nightmare. This
approach becomes bogged down in complexity and simply becomes intractable as more
steps add more free variables ğ‘¢0, . . . , ğ‘¢ğ‘âˆ’1.
This reveals the core problem of optimal control:
Compounding effects of intermediary controls in multiple-step processes cre-
ate insurmountable difficulties for standard methods of optimization.
8 Chapter 1. Getting Started
We crack this complexity problem by using Lagrange multipliers, which will open
up a whole landscape of beautiful mathematics. The big picture is that for the ğ‘-step
process we will need ğ‘ Lagrange multipliers ğœ†1, . . . , ğœ†ğ‘ . Setting up the optimization
will create a dynamic relationship for these values in a costate space, which tends to
proceed from the last multiplier ğœ†ğ‘ to the first ğœ†1. Understanding such structures is
the heart of optimal control.
Before diving in to this control technique, we need to take a deeper look at opti-
mization techniques from calculus and review the technique of Lagrange multipliers,
which we take up in the next chapter.
1.4 Costs and Ends
We conclude this section with one more example, where the initial and final states
of the system are prescribed and the performance is measured in cost, which is to be
minimized.
Example 1.4
Consider a system that must be moved from starting state ğ‘¥ğ‘ = 0 to ending state
ğ‘¥ğ‘ = 100. The state is moved at each step by applying control ğ‘¢ according to the
```
dynamic ğ‘“(ğ‘¥, ğ‘¢) = ğ‘¥ + ğ‘¢, so ğ‘¥ğ‘›+1 = ğ‘¥ğ‘› + ğ‘¢ğ‘›. The cost for each move is given by
```
```
ğ‘”(ğ‘¥, ğ‘¢) = ğ‘¥ + ğ‘¢2/2. The cost is quadratic in the amount of control applied, and it
```
is linearly more expensive at higher values of ğ‘¥.
Suppose we have ğ‘ = 2 moves. We could try equal steps ğ‘¢0 = ğ‘¢1 = 50
```
creating a trajectory 0 â†’ 50 â†’ 100 at a cost of ğ½ = (0 + 502/2) + (100 + 502/2) =
```
2550. Another approach may be to avoid the higher costs at higher ğ‘¥ values and
take ğ‘¢0 = 0, ğ‘¢1 = 100 creating a trajectory 0 â†’ 0 â†’ 100 at a cost of ğ½ = 1002/2 =
5000. Our first guess was better.
To find the optimal control, we consider control variables ğ‘¢0, ğ‘¢1 that create
trajectory ğ‘¥0, ğ‘¥1 = ğ‘¥0 + ğ‘¢0, ğ‘¥2 = ğ‘¥1 + ğ‘¢1 with cost
```
ğ½ = (ğ‘¥0 + 12 ğ‘¢20) + (ğ‘¥1 + 12 ğ‘¢21) .
```
With ğ‘¥0 = 0 and ğ‘¥1 = ğ‘¢0 this becomes
```
ğ½ = ( 12 ğ‘¢20) + (ğ‘¢0 + 12 ğ‘¢21) .
```
For the fixed end location ğ‘¥2 = ğ‘¢0 + ğ‘¢1 = 100 we have ğ‘¢1 = 100 âˆ’ ğ‘¢0 and can
express the payoff
```
ğ½ = ( 12 ğ‘¢20) + (ğ‘¢0 + 12 (100 âˆ’ ğ‘¢0)2)
```
as a function of a single variable which has a minimum of ğ½ = 2549.75 at ğ‘¢0 =
49.5, making ğ‘¢1 = 50.5. So we shave a tiny bit off the cost of our first solution by
not moving quite as far with our first move, thereby avoiding the higher costs at
higher values of ğ‘¥. Okay, but not very impressive. This is the best we can do with
two moves.
Exercises 9
```
How about three moves? With details left to the reader (âˆ— check these âˆ—), the
```
requirement ğ‘¥0 = 0 and ğ‘¥3 = 100 imposes restriction ğ‘¢0 + ğ‘¢1 + ğ‘¢2 = 100, and
this is used to express ğ½ as a function of two variables:
```
ğ½ = ( 12 ğ‘¢20) + (ğ‘¢0 + 12 ğ‘¢21) + (ğ‘¢0 + ğ‘¢1 + 12 (100 âˆ’ ğ‘¢0 âˆ’ ğ‘¢1)2)
```
which has a minimum value of ğ½ = 1765.67 . . . with ğ‘¢0 = 32.33 . . . , ğ‘¢1 = 33.33 . . . ,
and ğ‘¢2 = 34.33 . . . . We save a considerable amount with an additional step, and
note that we take slightly smaller steps at first to avoid the higher costs at higher
ğ‘¥ values.
We could similarly analyze four moves and get ğ½ = 1397.5 with ğ‘¢0 = 23.5,
ğ‘¢1 = 24.5, ğ‘¢2 = 25.5, and ğ‘¢3 = 26.5.
What happens as we add more moves? How low would ğ½ go? There appears
```
to be a pattern in the controls applied; would the pattern continue with more
```
moves? It would be challenging to address these questions with our current mul-
```
tivariable approach (âˆ— try it âˆ—). Perhaps we need new tools. . . .
```
Key Points
In this chapter we considered contrived games to introduce discrete control processes
and the challenge of optimizing control. Our games involved moving a ball along the
ğ‘¥-axis with costs for each move and either a prescribed end location or a free end loca-
tion with an associated payoff. We used terms from dynamical systems to describe the
processes.
We were able to maximize our net payoff for games with a few moves using basic
calculus. This approach wonâ€™t work with larger number of moves as the compounding
nature of sequential controls leads to intractable computation of the chain rule.
Exercises
Exercise 1.1. A Bocce ball is located at ğ‘¥0 = 10 on the real line and must be moved to
the origin. The cost of moving the ball a distance ğ‘¢ > 0 to the left is ğ‘¥ + ğ‘¢2. Our control
system is
```
ğ‘¥ğ‘›+1 = ğ‘“(ğ‘¥ğ‘›, ğ‘¢ğ‘›) = ğ‘¥ğ‘› âˆ’ ğ‘¢ğ‘›
```
and our cost is
```
ğ‘”(ğ‘¥ğ‘›, ğ‘¢ğ‘›) = ğ‘¥ğ‘› + ğ‘¢2ğ‘›.
```
For example, to move the ball in one move from ğ‘¥0 = 10 to ğ‘¥1 = 0 incurs a cost of
ğ½ = 10 + 102 = 110. To move the ball in two moves from ğ‘¥0 = 10 to ğ‘¥1 = 5 to ğ‘¥2 = 0
```
with moves ğ‘¢0 = ğ‘¢1 = 5 incurs a cost of ğ½ = (10 + 52) + (5 + 52) = 65.
```
```
(a) What is the minimum cost for two moves (assume ğ‘¢1, ğ‘¢2 â‰¥ 0)?
```
```
(b) What is the minimum cost for three moves (assume ğ‘¢ğ‘– â‰¥ 0)?
```
10 Chapter 1. Getting Started
Exercise 1.2. A Bocce ball is located at ğ‘¥0 = 0 on the real line and must be moved to
a prescribed location ğ‘¥ğ‘ = ğµ > 0 in ğ‘ moves. The cost of moving the ball ğ‘¢ units is
```
ğ‘”(ğ‘¢) = ğ‘¢2 dollars. Our control system is
```
```
ğ‘¥ğ‘›+1 = ğ‘“(ğ‘¥ğ‘›, ğ‘¢ğ‘›) = ğ‘¥ğ‘› + ğ‘¢ğ‘›
```
and the total cost is
ğ½ = âˆ‘ğ‘âˆ’1ğ‘›=0 ğ‘¢2ğ‘›.
```
(a) What is the minimal cost for ğ‘ = 1 and ğ‘ = 2 moves?
```
```
(b) In general, would it reasonable to assume ğ‘¢0 = ğ‘¢1 = â‹¯ = ğ‘¢ğ‘ ? With this
```
assumption, what is the minimum for ğ‘ moves?
Exercise 1.3. Repeat the previous exercise where the end location of the ball is not
specified, but at the end of ğ‘ moves you receive a payoff of âˆšğ‘¥ğ‘ dollars. So for a given
ğ‘, the net payoff is
ğ½ = âˆšğ‘¥ğ‘ âˆ’ âˆ‘ğ‘âˆ’1ğ‘›=0 ğ‘¢2ğ‘›.
```
We still have ğ‘¥0 = 0 and control dynamics are ğ‘¥ğ‘›+1 = ğ‘“(ğ‘¥ğ‘›, ğ‘¢ğ‘›) = ğ‘¥ğ‘› + ğ‘¢ğ‘›.
```
```
(a) What is the optimal solution for ğ‘ = 1 and ğ‘ = 2 moves?
```
```
(b) Would it be reasonable to assume ğ‘¢0 = ğ‘¢1 = â‹¯ = ğ‘¢ğ‘ in this case? If so, what
```
is your maximum payoff for ğ‘ moves?
```
Exercise 1.4(hs). A Bocce ball rests at location (ğ‘¥ğ‘, ğ‘¦ğ‘) on the Cartesian plane. Moving
```
```
the ball to location (ğ‘¥ğ‘, ğ‘¦ğ‘) incurs a cost of
```
```
ğ½ = (ğ‘¥ğ‘ âˆ’ ğ‘¥ğ‘)
```
```
2 + (ğ‘¦ğ‘ âˆ’ ğ‘¦ğ‘)2
```
|ğ‘¦ğ‘| + 1 .
Note that moving costs are cheaper at higher values of |ğ‘¦ğ‘|.
```
(a) Suppose you want to move the ball from (0, 0) to (10, 0) in two steps. For ex-
```
```
ample, (0, 0) â†’ (6, 2) â†’ (10, 0) would incur a cost of ğ½ = 62 + 22 + (42 + 22)/3 = 140/3.
```
```
What value for (ğ‘¥1, ğ‘¦1) would incur the minimal cost for (0, 0) â†’ (ğ‘¥1, ğ‘¦1) â†’ (10, 0)?
```
```
(b) How about three moves? Set up the equations and describe what youâ€™d have
```
```
to do to find the minimal cost for (0, 0) â†’ (ğ‘¥1, ğ‘¦1) â†’ (ğ‘¥2, ğ‘¦2) â†’ (10, 0)? Can you
```
approximate the solution?
```
Exercise 1.5(h). Be careful with your assumptions. Nowhere in Example 1.2 did we
```
use the fact that the controls ğ‘¢0 and ğ‘¢1 had to be positive. Show that with ğ‘¥0 = 1 you
can get an arbitrarily large payoff in two moves if you allow negative controls.
This demonstrates that optimization principles will only define locally optimal so-
```
lutions; the solution in Example 1.2 is a local optimum.
```
Exercise 1.6. A Bocce ball is located at ğ‘¥0 = ğ‘ on the real line and must be moved to
the origin ğ‘¥ğ‘ = 0 in ğ‘ moves. The cost of moving the ball a distance ğ‘¢ > 0 to the left
```
is (ğ‘¥ + ğ‘¢)2. Our control system is
```
```
ğ‘¥ğ‘›+1 = ğ‘“(ğ‘¥ğ‘›, ğ‘¢ğ‘›) = ğ‘¥ğ‘› âˆ’ ğ‘¢ğ‘›
```
Exercises 11
and our cost is
```
ğ‘”(ğ‘¥ğ‘›, ğ‘¢ğ‘›) = (ğ‘¥ğ‘› + ğ‘¢ğ‘›)2.
```
```
(a) Verify that the optimal trajectory for three moves starting at ğ‘¥0 = 10 is 10 â†’
```
100/21 â†’ 40/21 â†’ 0.
```
(b) Verify that 100/21 â†’ 40/21 â†’ 0 is the optimal trajectory for two moves start-
```
ing at ğ‘¥0 = 100/21.
```
(c) Argue that, in general, if ğ‘ â†’ ğ‘ â†’ ğ‘ â†’ 0 is the optimal trajectory for three
```
moves starting at ğ‘¥0 = ğ‘, then ğ‘ â†’ ğ‘ â†’ 0 has to be the optimal trajectory for two
moves starting at ğ‘¥0 = ğ‘.
```
Exercise 1.7(hs). Continue with the previous exercise: a control system ğ‘¥ğ‘›+1 = ğ‘¥ğ‘› âˆ’
```
```
ğ‘¢ğ‘› and cost ğ‘”(ğ‘¥ğ‘›, ğ‘¢ğ‘›) = (ğ‘¥ğ‘› + ğ‘¢ğ‘›)2.
```
```
(a) Compute ğ‘‰3(ğ‘) as the minimal cost for moving the ball from ğ‘¥ = ğ‘ to ğ‘¥ = 0 in
```
three moves.
```
(b) Compute ğ‘‰2(ğ‘) as the minimal cost for moving the ball from ğ‘¥ = ğ‘ to ğ‘¥ = 0 in
```
two moves.
```
(c) Verify that
```
```
ğ‘‰3(ğ‘) = mináµ† {(ğ‘ + ğ‘¢)2 + ğ‘‰2(ğ‘ âˆ’ ğ‘¢)}.
```
```
(d) Argue that, in general,
```
```
ğ‘‰ ğ‘+1(ğ‘) = mináµ† {(ğ‘ + ğ‘¢)2 + ğ‘‰ ğ‘ (ğ‘ âˆ’ ğ‘¢)}.
```
Use this to derive ğ‘‰4 from ğ‘‰3.
```
(e) Construct a general formula for ğ‘‰ ğ‘ (ğ‘). Can you evaluate limğ‘â†’âˆ ğ‘‰ ğ‘ (ğ‘)?
```
2
Static Optimization
Understanding dynamic optimization requires a solid understanding of the general
concept of optimization. Static optimization techniques are introduced in the study of
calculus, and we take a fresh look at these ideas with a philosophy of understanding
simple things deeply.
Optimization problems are typically solved by ruling out all the cases where things
are not optimal. A single variable function cannot attain a local maximum at a place
where the derivative is nonzero, so we look at locations where the derivative is zero.
This concept generalizes to higher dimensions, and to do so we must carefully under-
stand what we mean by the derivative of a function in one and several variables.
If an optimization problem in multiple dimensions has constraints, we engage the
method of Lagrange multipliers, and this concept will lead us naturally into methods
for optimizing control processes. Most readers will have some familiarity with the topic
of Lagrange multipliers, as it is covered to some degree in most multivariable calculus
courses. We develop what we need from the topic in this chapter.
```
This is foundation building; it is essential to have a clear understanding of these
```
basic concepts as we begin our journey into optimal control.
2.1 The Derivative
The derivative of a function is a fundamental mathematical concept. It is introduced
in calculus for functions ğ‘“ âˆ¶ â„ â†’ â„ as a limit of a difference quotient,
```
ğ‘“â€²(ğ‘) = limğ‘¥â†’ğ‘ğ‘“(ğ‘¥) âˆ’ ğ‘“(ğ‘)ğ‘¥ âˆ’ ğ‘ .
```
If this limit exists, the function is said to be differentiable at ğ‘¥ = ğ‘ and the derivative is
used to get the slope of a line tangent to the function. This tangent line is a very good
```
approximation to the function near the point of tangency (Figure 2.1). For ğ‘¥ near ğ‘,
```
```
ğ‘“(ğ‘¥) â‰ˆ ğ‘“(ğ‘) + ğ‘“â€²(ğ‘)(ğ‘¥ âˆ’ ğ‘).
```
13
14 Chapter 2. Static Optimization
Figure 2.1. Tangent line approximation.
We could invert this and define the derivative as the slope of a line that would give
the best approximation to the function at a point. This approach has the advantage
of naturally generalizing to higher dimensions. Consider a multivariable function ğ‘” âˆ¶
â„2 â†’ â„. For ğ‘¥, ğ‘¦ near ğ‘, ğ‘,
```
ğ‘”(ğ‘¥, ğ‘¦) â‰ˆ ğ‘”(ğ‘, ğ‘) + ğœ•ğ‘”ğœ•ğ‘¥ (ğ‘, ğ‘) (ğ‘¥ âˆ’ ğ‘) + ğœ•ğ‘”ğœ•ğ‘¦ (ğ‘, ğ‘) (ğ‘¦ âˆ’ ğ‘)
```
```
= ğ‘”(ğ‘, ğ‘) + ( ğœ•ğ‘”ğœ•ğ‘¥ (ğ‘, ğ‘), ğœ•ğ‘”ğœ•ğ‘¦ (ğ‘, ğ‘)) â‹… ((ğ‘¥ âˆ’ ğ‘), (ğ‘¦ âˆ’ ğ‘))
```
```
= ğ‘”(ğ‘, ğ‘) + âˆ‡ğ‘”(ğ‘, ğ‘) â‹… ((ğ‘¥ âˆ’ ğ‘), (ğ‘¦ âˆ’ ğ‘)) .
```
Here the gradient vector
```
âˆ‡ğ‘” = ( ğœ•ğ‘”ğœ•ğ‘¥ , ğœ•ğ‘”ğœ•ğ‘¦ )
```
```
is the derivative of ğ‘” since âˆ‡ğ‘”(ğ‘, ğ‘) is the coefficient for ((ğ‘¥ âˆ’ ğ‘), (ğ‘¦ âˆ’ ğ‘)) that yields the
```
```
best linear approximation to ğ‘” at the point (ğ‘, ğ‘, ğ‘”(ğ‘, ğ‘)). We would not be able to get
```
this from a difference quotient interpretation because we canâ€™t divide vectors.
2.2 Differentiation
It is important to distinguish the derivative of a function from the process of differen-
tiation as an operation applied to a function. This is particularly relevant for partial
derivatives of multivariable functions with dependencies among the variables.
```
The expression ğœ•ğ»ğœ•ğ‘¥ (ğ‘¥, ğ‘¦, ğ‘§) is the derivative of the function ğ» with respect to the
```
free variable ğ‘¥.
```
The expression ğœ•ğœ•ğ‘¥ ğ»(ğ‘¥, ğ‘¦, ğ‘§) means to compute the full derivative of ğ»(ğ‘¥, ğ‘¦, ğ‘§) with
```
respect to ğ‘¥ where there may be dependency between ğ‘¥, ğ‘¦, and ğ‘§, in which case we
need to apply the chain rule. Think of this as the ğœ•ğœ•ğ‘¥ operator applied to the expression
```
ğ»(ğ‘¥, ğ‘¦, ğ‘§).
```
The following example should clarify, and it demonstrates how these notations
will be used in the remainder of the text.
2.3. Approximations 15
Example 2.1
```
Suppose that for ğ»(ğ‘¥, ğ‘¦, ğ‘§) = sin(ğ‘¥ + ğ‘¦ + ğ‘§) we want to compute ğœ•ğœ•ğ‘¥ ğ»(ğ‘¥, ğ‘¦, ğ‘§)
```
where ğ‘¦ = ğ‘¥2 and ğ‘§ is unrelated to ğ‘¥. We have ğ‘‘ğ‘¦ğ‘‘ğ‘¥ = 2ğ‘¥ and, since there is
no dependency between ğ‘§ and ğ‘¥, we have ğ‘‘ğ‘§ğ‘‘ğ‘¥ = 0. Using the chain rule, with
ğœ•ğ»
ğœ•ğ‘¥ =
ğœ•ğ»
ğœ•ğ‘¦ =
ğœ•ğ»
```
ğœ•ğ‘§ = cos(ğ‘¥ + ğ‘¦ + ğ‘§), we derive
```
ğœ•
```
ğœ•ğ‘¥ ğ»(ğ‘¥, ğ‘¦, ğ‘§) =
```
ğœ•ğ»
ğœ•ğ‘¥ +
ğœ•ğ»
ğœ•ğ‘¦
ğ‘‘ğ‘¦
ğ‘‘ğ‘¥ +
ğœ•ğ»
ğœ•ğ‘§
ğ‘‘ğ‘§
ğ‘‘ğ‘¥
```
= cos(ğ‘¥ + ğ‘¦ + ğ‘§) + cos(ğ‘¥ + ğ‘¦ + ğ‘§)(2ğ‘¥) + cos(ğ‘¥ + ğ‘¦ + ğ‘§)(0).
```
```
Substituting ğ‘¦ = ğ‘¥2 this simplifies to (1 + 2ğ‘¥) cos(ğ‘¥ + ğ‘¥2 + ğ‘§).
```
```
We could also solve this problem by substituting ğ‘¦ = ğ‘¥2 first to get ğ»(ğ‘¥, ğ‘¥2, ğ‘§) =
```
```
sin(ğ‘¥ + ğ‘¥2 + ğ‘§) and then differentiating with respect to ğ‘¥ to get the same result.
```
Partial differentiation and the chain rule can be tricky, so it is important to keep
clear on the subtle distinction between a differentiation operator ğœ•ğœ•ğ‘¥ and a derivative
ğœ•ğ»
ğœ•ğ‘¥ .
2.3 Approximations
The concept of linear approximation to a differentiable function
```
ğ‘“(ğ‘¥) â‰ˆ ğ‘“(ğ‘) + ğ‘“â€²(ğ‘)(ğ‘¥ âˆ’ ğ‘) (2.1)
```
```
means that the function ğ‘“(ğ‘¥) is very close to the tangent line ğ‘“(ğ‘) + ğ‘“â€²(ğ‘)(ğ‘¥ âˆ’ ğ‘) for
```
points ğ‘¥ near ğ‘. This intuitive understanding is all we really need for this subject, but a
deeper dive into the proofs covered in later chapters requires more precise nomencla-
ture. Recall that Taylor series allow some functions to be expressed as a power series:
```
ğ‘“(ğ‘¥) = ğ‘“(ğ‘) + ğ‘“â€²(ğ‘)(ğ‘¥ âˆ’ ğ‘) + 12 ğ‘“â€³(ğ‘)(ğ‘¥ âˆ’ ğ‘)2 + 16 ğ‘“â€´(ğ‘)(ğ‘¥ âˆ’ ğ‘)3 + â‹¯ .
```
```
For a polynomial approximation to ğ‘“(ğ‘¥) near ğ‘¥ = ğ‘, we can chop this series off at any
```
```
location and toss the subsequent terms into the remainder ğ‘…(ğ‘¥). A linear approxima-
```
tion would be
```
ğ‘“(ğ‘¥) = ğ‘“(ğ‘) + ğ‘“â€²(ğ‘)(ğ‘¥ âˆ’ ğ‘) + ğ‘…(ğ‘¥)
```
```
where ğ‘…(ğ‘¥) is a catch basin for the sum of the subsequent nonlinear terms, and the size
```
```
of |ğ‘…(ğ‘¥)| is on the order of (ğ‘¥ âˆ’ğ‘)2 or smaller. Specifically, this means there is a positive
```
constant ğ¶ such that
||
|
```
ğ‘…(ğ‘¥)
```
```
(ğ‘¥ âˆ’ ğ‘)2
```
||
| < ğ¶ < âˆ
```
for ğ‘¥ values near the point ğ‘ (with ğ‘¥ â‰  ğ‘).
```
```
This may be conveniently expressed using â€œbig-Oâ€ notation. For functions ğ‘(ğ‘¥),
```
```
ğ‘(ğ‘¥) with ğ‘(ğ‘¥) â†’ 0 and ğ‘(ğ‘¥) â†’ 0 as ğ‘¥ â†’ ğ‘ we say ğ‘(ğ‘¥) = ğ‘‚(ğ‘(ğ‘¥)) if there is a constant
```
```
ğ¶ with |ğ‘(ğ‘¥)/ğ‘(ğ‘¥)| < ğ¶ for ğ‘¥ near ğ‘.
```
16 Chapter 2. Static Optimization
```
For our remainder term in the linear approximation (2.1) we say ğ‘…(ğ‘¥) is ğ‘‚((ğ‘¥âˆ’ğ‘)2),
```
or that
```
ğ‘“(ğ‘¥) = ğ‘“(ğ‘) + ğ‘“â€²(ğ‘)(ğ‘¥ âˆ’ ğ‘) + ğ‘‚((ğ‘¥ âˆ’ ğ‘)2)
```
for ğ‘¥ near ğ‘ and ğ‘¥ â‰  ğ‘.
The big-O notation retains the intuitive and informal feel of the approximation
idea, â€œâ‰ˆâ€, but this notation is mathematically rigorous.
```
We could take this a step further with â€œlittle-oâ€ notation. For functions ğ‘(ğ‘¥), ğ‘(ğ‘¥) >
```
```
0 we say ğ‘(ğ‘¥) = ğ‘œ(ğ‘(ğ‘¥)) as ğ‘¥ â†’ ğ‘ if
```
```
limğ‘¥â†’ğ‘|||ğ‘(ğ‘¥)ğ‘(ğ‘¥)||| = 0.
```
```
So ğ‘(ğ‘¥) = ğ‘‚(ğ‘(ğ‘¥)) if |ğ‘(ğ‘¥)/ğ‘(ğ‘¥)| stays bounded near ğ‘, and ğ‘(ğ‘¥) = ğ‘œ(ğ‘(ğ‘¥)) if |ğ‘(ğ‘¥)/ğ‘(ğ‘¥)|
```
is vanishingly small near ğ‘.
```
The more general idea of a linear approximation (2.1) is then expressed as
```
```
ğ‘“(ğ‘¥) = ğ‘“(ğ‘) + ğ‘“â€²(ğ‘)(ğ‘¥ âˆ’ ğ‘) + ğ‘œ(ğ‘¥ âˆ’ ğ‘).
```
```
This version of our linear approximation can be taken as defining ğ‘“â€²(ğ‘) in an equivalent
```
```
way to the standard difference quotient definition: solving this for ğ‘“â€²(ğ‘) yields
```
```
ğ‘“â€²(ğ‘) = ğ‘“(ğ‘¥) âˆ’ ğ‘“(ğ‘)ğ‘¥ âˆ’ ğ‘ + ğ‘œ(ğ‘¥ âˆ’ ğ‘)ğ‘¥ âˆ’ ğ‘
```
```
and so with limğ‘¥â†’ğ‘ ğ‘œ(ğ‘¥ âˆ’ ğ‘)/(ğ‘¥ âˆ’ ğ‘) = 0, we have
```
```
ğ‘“â€²(ğ‘) = limğ‘¥â†’ğ‘ ( ğ‘“(ğ‘¥)âˆ’ğ‘“(ğ‘)ğ‘¥âˆ’ğ‘ + ğ‘œ(ğ‘¥âˆ’ğ‘)ğ‘¥âˆ’ğ‘ )
```
```
= limğ‘¥â†’ğ‘ğ‘“(ğ‘¥)âˆ’ğ‘“(ğ‘)ğ‘¥âˆ’ğ‘ + limğ‘¥â†’ğ‘ğ‘œ(ğ‘¥âˆ’ğ‘)ğ‘¥âˆ’ğ‘
```
```
= limğ‘¥â†’ğ‘ğ‘“(ğ‘¥)âˆ’ğ‘“(ğ‘)ğ‘¥âˆ’ğ‘ .
```
```
We can say that a function ğ‘“(ğ‘¥) is differentiable at a point ğ‘¥ = ğ‘ with derivative
```
```
ğ‘“â€²(ğ‘) if it is well-approximated by a line near ğ‘¥ = ğ‘ as ğ‘“(ğ‘¥) = ğ‘“(ğ‘) + ğ‘“â€²(ğ‘)(ğ‘¥ âˆ’ ğ‘) +
```
```
ğ‘œ(ğ‘¥ âˆ’ ğ‘).
```
The same reasoning applies to our linear approximations in higher dimensions.
We say that a multivariable function ğ‘“ âˆ¶ â„ğ‘› â†’ â„ is differentiable at a point ğš âˆˆ â„ğ‘› if
```
it is well-approximated by a tangent plane (of dimension ğ‘›) for ğ± near ğš. Specifically,
```
this means that
```
ğ‘“(ğ±) = ğ‘“(ğš) + âˆ‡ğ‘“(ğš) â‹… (ğ± âˆ’ ğš) + ğ‘œ (|ğ± âˆ’ ğš|)
```
where the gradient vector
```
âˆ‡ğ‘“ = ( ğœ•ğ‘“ğœ•ğ‘¥1, . . . , ğœ•ğ‘“ğœ•ğ‘¥ğ‘› )
```
```
is the derivative of ğ‘“ with respect to ğ± = (ğ‘¥1, . . . , ğ‘¥ğ‘š).
```
2.4. Extreme Values 17
The key idea is:
The derivative is the multiplicative coefficient in the linear approximation
of a function.
2.4 Extreme Values
```
A function ğ‘¦ = ğ‘“(ğ‘¥) defined on a closed interval ğ¼ attains a maximum value at a point
```
```
ğ‘ âˆˆ ğ¼ if ğ‘“(ğ‘¥) â‰¤ ğ‘“(ğ‘) for all ğ‘¥ âˆˆ ğ¼, and it attains a minimum value at a point ğ‘ âˆˆ ğ¼
```
```
if ğ‘“(ğ‘¥) â‰¥ ğ‘“(ğ‘) for all ğ‘¥ âˆˆ ğ¼. The Extreme Value Theorem tells us that a continuous
```
```
function on a closed interval must attain a maximum and a minimum value.
```
```
A function ğ‘¦ = ğ‘“(ğ‘¥) attains a local maximum at ğ‘¥ = ğ‘ if ğ‘“(ğ‘¥) â‰¤ ğ‘“(ğ‘) for all ğ‘¥
```
in an open interval containing ğ‘. If the function is differentiable at some point ğ‘¥ = ğ‘
```
interior to ğ¼ and ğ‘“â€²(ğ‘) > 0, then the function is increasing near ğ‘: there are points
```
```
ğ‘¥ > ğ‘ with ğ‘“(ğ‘¥) > ğ‘“(ğ‘) and points ğ‘¥ < ğ‘ with ğ‘“(ğ‘¥) < ğ‘“(ğ‘). We can apply the same
```
```
idea if ğ‘“â€²(ğ‘) < 0, and so if the derivative at ğ‘¥ = ğ‘ exists and is nonzero, there are always
```
points near ğ‘ that produce larger and smaller values for the function. Thus we have
that a differentiable function can only attain a local maximum or minimum value if
the derivative is zero.
These ideas generalize to multivariable functions. If â„› is a closed and bounded
region in â„ğ‘› and ğ‘“ âˆ¶ â„› â†’ â„ is continuous, then ğ‘“ must attain a maximum and a
minimum in â„›. This is the Extreme Value Theorem, and it follows from the abstract
concept of compactness covered in real analysis. As in the one-dimensional case, if ğ‘“
is differentiable and has a nonzero derivative at some point ğš interior to â„›, in this case
```
meaning a nonzero gradient âˆ‡ğ‘“(ğš) â‰  ğŸ, then there are points ğ± near ğš with ğ‘“(ğ±) > ğ‘“(ğš)
```
```
and points with ğ‘“(ğ±) < ğ‘“(ğš).
```
Thus we have the following general key principle:
A function ğ‘“ defined on a region â„› cannot attain a maximum or minimum
value at a point ğ‘¥ interior to â„› where the derivative is nonzero.
With this, the only places we need to look for a maximum or minimum of ğ‘“ over
â„› are:
â€¢ Interior points where the derivative is zero.
â€¢ Places where the derivative does not exist.
â€¢ The boundary of â„›.
Example 2.2
```
Consider ğ‘“(ğ‘¥) = |ğ‘¥2 âˆ’ 1| over the interval âˆ’2 â‰¤ ğ‘¥ â‰¤ 2 (Figure 2.2). This function
```
is not differentiable at ğ‘¥ = Â±1, and it has a zero derivative at ğ‘¥ = 0. So the
only candidates for which it may attain extreme values are ğ‘¥ = 0, Â±1, Â±2. The
```
function attains a maximum of ğ‘¦ = 3 at the endpoints and a minimum of ğ‘¦ = 0
```
at the cusps.
18 Chapter 2. Static Optimization
Figure 2.2. Maximum values at endpoints, minimum at cusps.
In the multivariable case we interpret a zero derivative to mean that the gradient
is the zero vector.
Example 2.3
```
To find the shortest distance from the plane ğ‘§ = ğ‘¥ + 2ğ‘¦ to the point (1, 2, 3) we
```
```
formulate the distance âˆš(ğ‘¥ âˆ’ 1)2 + (ğ‘¦ âˆ’ 2)2 + (ğ‘§ âˆ’ 3)2 as a function of ğ‘¥ and ğ‘¦:
```
```
ğ‘‘(ğ‘¥, ğ‘¦) = âˆš(ğ‘¥ âˆ’ 1)2 + (ğ‘¦ âˆ’ 2)2 + ((ğ‘¥ + 2ğ‘¦) âˆ’ 3)2.
```
The square root is an increasing function, and so it suffices to minimize the
square of the distance:
```
ğ‘”(ğ‘¥, ğ‘¦) = (ğ‘¥ âˆ’ 1)2 + (ğ‘¦ âˆ’ 2)2 + ((ğ‘¥ + 2ğ‘¦) âˆ’ 3)2
```
= 2ğ‘¥2 + 4ğ‘¥ğ‘¦ + 5ğ‘¦2 âˆ’ 8ğ‘¥ âˆ’ 16ğ‘¦ + 14
with
```
âˆ‡ğ‘” = (4ğ‘¥ + 4ğ‘¦ âˆ’ 8, 4ğ‘¥ + 10ğ‘¦ âˆ’ 16).
```
We look for points where the gradient is the zero vector by solving
4ğ‘¥ + 4ğ‘¦ âˆ’ 8 = 0
4ğ‘¥ + 10ğ‘¦ âˆ’ 16 = 0
```
for the single solution (ğ‘¥, ğ‘¦, ğ‘§) = (2/3, 4/3, 10/3) with distance ğ‘‘ = âˆš2/3 =
```
```
0.8165 . . . . (How do we know this is a minimum?)
```
2.5 Optimum Along a Path
```
The maximum/minimum of a function ğ‘”(ğ‘¥, ğ‘¦) along a path ğ‘¥(ğ‘¡), ğ‘¦(ğ‘¡) for ğ‘ â‰¤ ğ‘¡ â‰¤ ğ‘ can
```
```
be handled as a single variable max/min problem applied to ğ‘”(ğ‘¥(ğ‘¡), ğ‘¦(ğ‘¡)) as a function
```
of ğ‘¡. However, it is usually easier to use a multivariable approach as follows. Assuming
everything is differentiable, the optimum must occur at the boundary, ğ‘¡ = ğ‘ or ğ‘¡ = ğ‘,
2.5. Optimum Along a Path 19
or at a point where
```
0 = ğ‘‘ğ‘”ğ‘‘ğ‘¡ (ğ‘¥(ğ‘¡), ğ‘¦(ğ‘¡)) = âˆ‡ğ‘”(ğ‘¥(ğ‘¡), ğ‘¦(ğ‘¡)) â‹… (ğ‘¥â€²(ğ‘¡), ğ‘¦â€²(ğ‘¡)).
```
If the optimum is attained at an interior time ğ‘¡1, with ğ‘ < ğ‘¡1 < ğ‘, then the path must
touch the level curve at optimal height, but not cross it. For nonzero speed, the path
```
is tangent to the level curve of ğ‘”(ğ‘¥, ğ‘¦) at the point ğ‘¥(ğ‘¡1), ğ‘¦(ğ‘¡1), making the gradient of ğ‘”
```
perpendicular to the curve at the maximal point. This is one of the key concepts from
multivariable calculus.
```
The same idea works in any dimension: for ğ± âˆˆ â„ğ‘›, to optimize ğ‘“(ğ±) along a path
```
```
ğ±(ğ‘¡) look for points where
```
```
0 = âˆ‡ğ‘”(ğ±(ğ‘¡)) â‹… ğ± â€²(ğ‘¡).
```
Example 2.4: Saddle
Suppose we want to find the maximum and minimum values of the hyperbolic
paraboloid
```
ğ‘”(ğ‘¥, ğ‘¦) = ğ‘¥2 + 2ğ‘¥ğ‘¦ âˆ’ 3ğ‘¦2 + 8ğ‘¦ âˆ’ 4
```
over the circle of radius 2 centered at the origin.
Figure 2.3. Maxima and minima along a path occur at tangen-
cies to level curves.
The extreme values appear where the circle is tangent to the level curves of
```
the surface (Figure 2.3).
```
```
To solve for the extreme values, we can parameterize the circle by ğ‘¥(ğœƒ) =
```
```
2 cos(ğœƒ), ğ‘¦(ğœƒ) = 2 sin(ğœƒ) and express ğ‘”(ğ‘¥(ğœƒ), ğ‘¦(ğœƒ)) as a function of ğœƒ:
```
```
ğ‘” (2 cos(ğœƒ), 2 sin(ğœƒ))
```
```
= 4 cos2(ğœƒ) + 8 cos(ğœƒ) sin(ğœƒ) âˆ’ 12 sin2(ğœƒ) + 16 sin(ğœƒ) âˆ’ 4.
```
20 Chapter 2. Static Optimization
Figure 2.4. Function values along the path show maxima and
minima.
Optimizing as a function of ğœƒ reveals a maximum of 7.682 . . . at ğœƒ = 0.634 . . . ,
```
and it reveals a minimum of âˆ’32.648 . . . at ğœƒ = 4.873 . . . (Figure 2.4).
```
The constraint curve in the previous example was a circle with a nice parameter-
ization. Other constraints may not be so readily parameterized. Fortunately, there is
a more sophisticated approach that avoids parameterization altogether and, although
more abstract, is actually much more straightforward. This is the Lagrange multiplier
technique introduced in the next section.
2.6 Lagrange with One Constraint
```
If we want to optimize ğ‘”(ğ‘¥, ğ‘¦) constrained to a curve defined by ğ‘“(ğ‘¥, ğ‘¦) = ğ‘, we look for
```
```
where the level curves of ğ‘” are tangent to the curve ğ‘“(ğ‘¥, ğ‘¦) = ğ‘. The idea of Lagrange
```
multipliers is to expand the focus beyond just the level curve at height ğ‘ and to consider,
```
in general, where the level curves of ğ‘“ (the constraint function) are tangent to the level
```
```
curves of ğ‘” (the objective function we want to optimize). This set of points is defined
```
by
âˆ‡ğ‘” âˆ¥ âˆ‡ğ‘“,
and the set of solutions to this condition is often referred to as the set of stationary
points. We expect solutions to the constrained optimization problem to occur at a sta-
tionary point, and so we look for solutions that are both stationary and satisfy the con-
straints.
Example 2.5: Saddle
Returning to Example 2.4, suppose we want to find the maximum and mini-
```
mum values of ğ‘”(ğ‘¥, ğ‘¦) = ğ‘¥2 + 2ğ‘¥ğ‘¦ âˆ’ 3ğ‘¦2 + 8ğ‘¦ âˆ’ 4 subject to the constraint
```
2.6. Lagrange with One Constraint 21
```
ğ‘“(ğ‘¥, ğ‘¦) = ğ‘¥2 + ğ‘¦2 = 4. In that example we examined values of ğ‘”(ğ‘¥, ğ‘¦) on a single
```
```
circle ğ‘“(ğ‘¥, ğ‘¦) = ğ‘¥2 + ğ‘¦2 = 4 and used the first derivative test to find extreme
```
values.
Lagrange multipliers represent a different approach which may seem rather
roundabout, but it has some surprising advantages. Compute the gradients
```
âˆ‡ğ‘“ = (2ğ‘¥, 2ğ‘¦),
```
```
âˆ‡ğ‘” = (2ğ‘¥ + 2ğ‘¦, 2ğ‘¥ âˆ’ 6ğ‘¦ + 8)
```
and find when these are parallel by looking for a zero determinant of the matrix
with âˆ‡ğ‘“ and âˆ‡ğ‘” as rows:
0 = Det [
2ğ‘¥ 2ğ‘¦
2ğ‘¥ + 2ğ‘¦ 2ğ‘¥ âˆ’ 6ğ‘¦ + 8
] = 4ğ‘¥2 âˆ’ 16ğ‘¥ğ‘¦ + 4ğ‘¦2 + 16ğ‘¥.
This amounts to looking at the level curves of the saddle ğ‘” and the level curves of
```
the constraint ğ‘“ (a collection of circles in this case) and seeing where these level
```
curves are tangent. The set of tangent points are the stationary values defined by
```
the hyperbola 0 = 4ğ‘¥2 âˆ’ 16ğ‘¥ğ‘¦ + 4ğ‘¦2 + 16ğ‘¥ (Figure 2.5).
```
Figure 2.5. The set of tangencies between level curves of two
functions is a curve of stationary points.
```
Now we apply the constraint ğ‘“(ğ‘¥, ğ‘¦) = 4 by intersecting this hyperbola of
```
stasis points with the constraint circle ğ‘¥2 + ğ‘¦2 = 4 yielding two equations in two
unknowns,
4 = ğ‘¥2 + ğ‘¦2,
0 = 4ğ‘¥2 âˆ’ 16ğ‘¥ğ‘¦ + 4ğ‘¦2 + 16ğ‘¥,
with the same solution as Example 2.4: four critical points with a maximum of
```
7.682 . . . at (ğ‘¥, ğ‘¦) = (1.611 . . . , 1.185 . . . ) and a minimum of âˆ’32.648 . . . at (ğ‘¥, ğ‘¦) =
```
```
(0.319 . . . , âˆ’1.974 . . . ).
```
22 Chapter 2. Static Optimization
The following example demonstrates that the parallel condition âˆ‡ğ‘” âˆ¥ âˆ‡ğ‘“ isnâ€™t
quite sufficient to cover all cases.
Example 2.6: Parabolic Cylinder
```
Consider finding extreme values of ğ‘§ = ğ‘”(ğ‘¥, ğ‘¦) = (ğ‘¦ âˆ’ ğ‘¥)2 subject to ğ‘¦ + 2ğ‘¥2 = 1.
```
```
We formulate the constraint as ğ‘“(ğ‘¥, ğ‘¦) = ğ‘ with ğ‘“(ğ‘¥, ğ‘¦) = ğ‘¦+2ğ‘¥2. Computing
```
gradients,
```
âˆ‡ğ‘“ = (4ğ‘¥, 1),
```
```
âˆ‡ğ‘” = (âˆ’2(ğ‘¦ âˆ’ ğ‘¥), 2(ğ‘¦ âˆ’ ğ‘¥)),
```
we look for where these are parallel by setting
0 = Det [
4ğ‘¥ 1
```
âˆ’2(ğ‘¦ âˆ’ ğ‘¥) 2(ğ‘¦ âˆ’ ğ‘¥)
```
```
] = 2(4ğ‘¥ + 1)(ğ‘¦ âˆ’ ğ‘¥)
```
and find solutions ğ‘¥ = âˆ’1/4 and ğ‘¦ = ğ‘¥.
```
The solution ğ‘¥ = âˆ’1/4 and the constraint ğ‘¦+2ğ‘¥2 = 1 yields a point (âˆ’1/4, 7/8).
```
This produces a maximum value of ğ‘§ = 81/64 = 1.266 . . . for the function given
the constraint. This maximum is attained at a point where the constraint curve
```
ğ‘¦ + 2ğ‘¥2 = 1 is tangent to the level curves of the function ğ‘”(ğ‘¥, ğ‘¦) = (ğ‘¦ âˆ’ ğ‘¥)2.
```
We also have the solution ğ‘¦ = ğ‘¥ and the constraint ğ‘¦ + 2ğ‘¥2 = 1 yielding
```
points (1/2, 1/2) and (âˆ’1, âˆ’1) where ğ‘”(ğ‘¥, ğ‘¦) attains its minimum value of ğ‘§ = 0.
```
However, the constraint curve ğ‘¦ + 2ğ‘¥2 = 1 is not tangent to the level curves of
```
ğ‘”(ğ‘¥, ğ‘¦) at these points, and it doesnâ€™t look like the gradients would be parallel
```
```
(Figure 2.6). Whatâ€™s going on?
```
```
Figure 2.6. Level curves of (ğ‘¦ âˆ’ ğ‘¥)2 and constraint curve ğ‘¦ + 2ğ‘¥2 = 1.
```
```
The function ğ‘”(ğ‘¥, ğ‘¦) attains its global minimum value of ğ‘§ = 0 along a line of
```
```
critical points ğ‘¦ = ğ‘¥, and the gradient of ğ‘”(ğ‘¥, ğ‘¦) is zero at these critical points. It is
```
2.7. Higher Dimensions 23
```
questionable as to whether we want to consider this as a case where âˆ‡ğ‘” âˆ¥ âˆ‡ğ‘“ (is
```
```
every vector parallel to the zero vector?), but note that our method still detected
```
these points as a zero for the determinant of the gradient matrix, since one of the
gradients was zero.
```
The previous example had a constraint curve ğ‘“(ğ‘¥, ğ‘¦) = ğ‘ passing through a mini-
```
```
mum of the objective function ğ‘”(ğ‘¥, ğ‘¦) where âˆ‡ğ‘” = ğŸ. So when looking for solutions to
```
constrained optimization problems, we should look for where
âˆ‡ğ‘” âˆ¥ âˆ‡ğ‘“ or âˆ‡ğ‘” = ğŸ.
This is accomplished by looking for points where
âˆ‡ğ‘” = ğœ†âˆ‡ğ‘“
for any ğœ†, zero or nonzero. This is the Lagrange multiplier approach, and ğœ† is the La-
grange multiplier.
Example 2.7: Parabolic Cylinder Again
```
Returning to Example 2.6, find the extreme values of ğ‘§ = ğ‘”(ğ‘¥, ğ‘¦) = (ğ‘¦ âˆ’ ğ‘¥)2
```
subject to ğ‘¦ + 2ğ‘¥2 = 1 by solving âˆ‡ğ‘” = ğœ†âˆ‡ğ‘“ for the gradients
```
âˆ‡ğ‘” = (âˆ’2(ğ‘¦ âˆ’ ğ‘¥), 2(ğ‘¦ âˆ’ ğ‘¥)),
```
```
âˆ‡ğ‘“ = (4ğ‘¥, 1).
```
Together with the constraint, this yields three equations
```
âˆ’2(ğ‘¦ âˆ’ ğ‘¥) = ğœ†4ğ‘¥,
```
```
2(ğ‘¦ âˆ’ ğ‘¥) = ğœ†,
```
ğ‘¦ + 2ğ‘¥2 = 1
in three unknowns, ğ‘¥, ğ‘¦, ğœ†. Solving these, we find the same solutions found ear-
```
lier. Adding the first two equations yields 0 = ğœ†(4ğ‘¥+1), and so ğœ† = 0 or ğ‘¥ = âˆ’1/4.
```
If ğœ† = 0, we get two solutions, ğ‘¥ = ğ‘¦ = âˆ’1 and ğ‘¥ = ğ‘¦ = 1/2, both with ğ‘§ = 0. If
ğ‘¥ = âˆ’1/4, we get ğ‘¦ = 7/8 and ğœ† = 9/4, with ğ‘§ = 81/64, the same solutions we
had before.
2.7 Higher Dimensions
Most of our work will involve functions with a small number of variables. However, to
develop the machinery of optimal control, we will need to have a basic understanding
of how Lagrange multipliers operate with a large number of variables. The patterns we
see in the two and three variable cases generalize to any number of variables. We will
focus on maximizing a function subject to constraints, but the same techniques work
for minimizing as well.
```
The general idea is that we want to maximize a function ğ‘”(ğ‘¥1, ğ‘¥2, . . . , ğ‘¥ğ‘›) subject to
```
```
a constraint ğ‘ = ğ‘“(ğ‘¥1, ğ‘¥2, . . . , ğ‘¥ğ‘›). For a basic example, suppose we want to maximize
```
24 Chapter 2. Static Optimization
```
ğ‘”(ğ‘¥, ğ‘¦, ğ‘§) subject to a planar restriction, ğ‘ğ‘¥ + ğ‘ğ‘¦ + ğ‘ğ‘§ = ğ‘‘. One could imagine moving
```
```
around on this plane seeking the highest values for ğ‘”(ğ‘¥, ğ‘¦, ğ‘§). Standing at any point
```
```
(ğ‘¥, ğ‘¦, ğ‘§) on the plane, we would project the gradient âˆ‡ğ‘” onto the plane and increase
```
the value of ğ‘” by moving in that projected direction while still remaining on the plane.
This only fails if âˆ‡ğ‘” projects to the zero vector on the plane. In that case, there is no
direction we could move on the plane that would guarantee an increase in the value
of ğ‘”. So the maximum value of ğ‘” restricted to the plane has to occur at a point where
âˆ‡ğ‘” has zero components in the plane. This is characterized by âˆ‡ğ‘” being parallel to the
```
normal vector âˆ‡ğ‘“ = (ğ‘, ğ‘, ğ‘) to the plane ğ‘“(ğ‘¥, ğ‘¦, ğ‘§) = ğ‘ğ‘¥ + ğ‘ğ‘¦ + ğ‘ğ‘§ = ğ‘‘.
```
Example 2.8
In Example 2.3 we found the shortest distance from the plane ğ‘§ = ğ‘¥ + 2ğ‘¦ to the
```
point (1, 2, 3). We can formulate this as a constrained optimization problem as
```
minimizing
```
ğ‘”(ğ‘¥, ğ‘¦, ğ‘§) = (ğ‘¥ âˆ’ 1)2 + (ğ‘¦ âˆ’ 2)2 + (ğ‘§ âˆ’ 3)2
```
subject to
```
ğ‘“(ğ‘¥, ğ‘¦, ğ‘§) = ğ‘¥ + 2ğ‘¦ âˆ’ ğ‘§ = 0.
```
With
```
âˆ‡ğ‘” = (2(ğ‘¥ âˆ’ 1), 2(ğ‘¦ âˆ’ 2), 2(ğ‘§ âˆ’ 3)),
```
```
âˆ‡ğ‘“ = (1, 2, âˆ’1),
```
we look for points that satisfy âˆ‡ğ‘” = ğœ†âˆ‡ğ‘“ and the constraint by solving four linear
```
equations:
```
```
2(ğ‘¥ âˆ’ 1) = ğœ†,
```
```
2(ğ‘¦ âˆ’ 2) = 2ğœ†,
```
```
2(ğ‘§ âˆ’ 3) = âˆ’ğœ†,
```
ğ‘¥ + 2ğ‘¦ âˆ’ ğ‘§ = 0
for four unknowns to get
ğ‘¥ = 2/3, ğ‘¦ = 4/3, ğ‘§ = 10/3, ğœ† = âˆ’2/3.
```
Note that the vector between the points (2/3, 4/3, 10/3) and (1, 2, 3) is parallel to
```
```
the normal vector (1, 2, âˆ’1) to the plane (âˆ— check this âˆ—). The minimum distance
```
```
to the point (1, 2, 3) is attained when the point is straight up from the plane.
```
2.8 Multiple Constraints
The Lagrange multiplier technique also applies to multiple constraints, where we seek
```
to maximize a function ğ‘”(ğ‘¥1, . . . , ğ‘¥ğ‘›) subject to ğ‘š < ğ‘› constraints:
```
```
0 = ğ‘“1(ğ‘¥1, . . . , ğ‘¥ğ‘›),
```
â‹®
```
0 = ğ‘“ğ‘š(ğ‘¥1, . . . , ğ‘¥ğ‘›).
```
```
(2.2)
```
2.8. Multiple Constraints 25
These maxima can occur only at points where the gradient âˆ‡ğ‘” is in the space
spanned by âˆ‡ğ‘“1, âˆ‡ğ‘“2, . . . , âˆ‡ğ‘“ğ‘š, that is, where there are scalars ğœ†1, . . . , ğœ†ğ‘š with
```
âˆ‡ğ‘” = ğœ†1 âˆ‡ğ‘“1 + â‹¯ + ğœ†ğ‘š âˆ‡ğ‘“ğ‘š for some ğœ†1, . . . , ğœ†ğ‘š. (2.3)
```
```
For example, suppose we want to maximize ğ‘”(ğ‘¥, ğ‘¦, ğ‘§) subject to two planar restric-
```
tions
ğ‘‘1 = ğ‘1ğ‘¥ + ğ‘1ğ‘¦ + ğ‘1ğ‘§,
ğ‘‘2 = ğ‘2ğ‘¥ + ğ‘2ğ‘¦ + ğ‘2ğ‘§.
That is, we are restricted to the line of intersection between these two planes. One could
```
imagine moving along this intersection line seeking the highest values for ğ‘”(ğ‘¥, ğ‘¦, ğ‘§).
```
This movement is necessarily perpendicular to both normals to the planes, ğ¯1 =
```
(ğ‘1, ğ‘1, ğ‘1) and ğ¯2 = (ğ‘2, ğ‘2, ğ‘2). The only location where ğ‘” could have a maximum
```
```
is where âˆ‡ğ‘” is in the span of ğ¯1 and ğ¯2; that is, âˆ‡ğ‘” = ğœ†1ğ¯1 + ğœ†2ğ¯2 for some ğœ†1, ğœ†2. Oth-
```
erwise âˆ‡ğ‘” would have a component which is not in the span of ğ¯1 and ğ¯2, and you are
free to move in the direction of that component along the line of intersection, thereby
increasing the value of ğ‘”.
Example 2.9
```
Consider minimizing ğ‘”(ğ‘¥, ğ‘¦, ğ‘§) = ğ‘¥2 + ğ‘¦2 + ğ‘§2 subject to constraints
```
```
ğ‘“1(ğ‘¥, ğ‘¦, ğ‘§) = ğ‘¥ + ğ‘¦ + ğ‘§ = 1,
```
```
ğ‘“2(ğ‘¥, ğ‘¦, ğ‘§) = ğ‘¥ + ğ‘¦ + 2ğ‘§ = 1.
```
Taking a direct approach, the intersection of the two constraints is ğ‘¥ + ğ‘¦ = 1
```
with ğ‘§ = 0, and the minimum is attained at (1/2, 1/2, 0).
```
Applying Lagrange, we formulate
âˆ‡ğ‘” = ğœ†1âˆ‡ğ‘“1 + ğœ†2âˆ‡ğ‘“2,
```
(2ğ‘¥, 2ğ‘¦, 2ğ‘§) = ğœ†1(1, 1, 1) + ğœ†2(1, 1, 2).
```
This yields five linear equations in five unknowns:
2ğ‘¥ = ğœ†1 + ğœ†2,
2ğ‘¦ = ğœ†1 + ğœ†2,
2ğ‘§ = ğœ†1 + 2ğœ†2,
1 = ğ‘¥ + ğ‘¦ + ğ‘§,
1 = ğ‘¥ + ğ‘¦ + 2ğ‘§.
Solving these yields
ğ‘¥ = 12 , ğ‘¦ = 12 , ğ‘§ = 0, ğœ†1 = 2, ğœ†2 = âˆ’1.
```
In more dimensions, if we want to maximize ğ‘”(ğ‘¥1, ğ‘¥2, . . . , ğ‘¥ğ‘›) in the intersection
```
of ğ‘š < ğ‘› planes with normal vectors ğ¯1, ğ¯2, . . . , ğ¯ğ‘š, we would look for points where
26 Chapter 2. Static Optimization
âˆ‡ğ‘” is in the span of ğ¯ğ‘–â€™s. In this case, the intersection of these planes would generically
```
be an (ğ‘› âˆ’ ğ‘š)-dimensional linear space, and if âˆ‡ğ‘” were not in the span of ğ¯ğ‘–â€™s, then
```
âˆ‡ğ‘” would have a positive component in this intersection space. We could move in the
direction of that component and increase the value of ğ‘”.
Note that for ğ‘š constraints in ğ‘›-dimensional space we generate ğ‘› equations for the
Lagrange multipliers and ğ‘š equations for the constraints, for a total of ğ‘› + ğ‘š equations
for the ğ‘› + ğ‘š unknowns ğ‘¥1, . . . , ğ‘¥ğ‘›, ğœ†1, . . . , ğœ†ğ‘š.
It is hard to visualize this multidimensional result, but it is a direct generalization
of the intersection of two planes in three dimensions. The key idea is that a maximum
of ğ‘” can only occur at a location where any movement in any direction that would
increase ğ‘” is prohibited by the constraints, which forces âˆ‡ğ‘” to be in the span of the
normals.
The same principles apply when the constraints are nonlinear. In three dimen-
```
sions the intersection of two surfaces ğ‘“1(ğ‘¥, ğ‘¦, ğ‘§) = ğ‘1 and ğ‘“2(ğ‘¥, ğ‘¦, ğ‘§) = ğ‘2 is generically
```
a curve. A tangent vector to the curve of intersection is perpendicular to the normals
of the surfaces, and any vector perpendicular to the curve must be in the span of those
normals. An optimal point can only occur when âˆ‡ğ‘” is perpendicular to the curve, and
hence in the span of the normals.
In constrained optimization we always assume there are fewer constraints than
dimensions, ğ‘š < ğ‘›. If we have the same number of constraints as dimensions, ğ‘š = ğ‘›
equations in ğ‘š = ğ‘› unknowns, we generically expect the solution to consist of iso-
lated points, so we canâ€™t really move around in the intersection space. If we have more
constraints than dimensions, we typically have no solutions at all.
Example 2.10
```
The maximum and minimum of height ğ‘”(ğ‘¥, ğ‘¦, ğ‘§) = ğ‘§ subject to ğ‘¥ + ğ‘¦ + ğ‘§ = 12
```
```
and ğ‘§ = ğ‘¥2 + ğ‘¦2 occurs at a place where the gradient of ğ‘”(ğ‘¥, ğ‘¦, ğ‘§) is a linear
```
```
combination of the gradients of ğ‘“1(ğ‘¥, ğ‘¦, ğ‘§) = ğ‘¥ + ğ‘¦ + ğ‘§ âˆ’ 12 and ğ‘“2(ğ‘¥, ğ‘¦, ğ‘§) =
```
ğ‘¥2 + ğ‘¦2 âˆ’ ğ‘§:
âˆ‡ğ‘” = ğœ†1âˆ‡ğ‘“1 + ğœ†2âˆ‡ğ‘“2
```
= ğœ†1(1, 1, 1) + ğœ†2(2ğ‘¥, 2ğ‘¦, âˆ’1).
```
This generates five equations in five unknowns:
0 = ğœ†1 + 2ğœ†2ğ‘¥,
0 = ğœ†1 + 2ğœ†2ğ‘¦,
1 = ğœ†1 âˆ’ ğœ†2,
0 = ğ‘¥ + ğ‘¦ + ğ‘§ âˆ’ 12,
0 = ğ‘¥2 + ğ‘¦2 âˆ’ ğ‘§.
```
From the first three we conclude ğ‘¥ = ğ‘¦. (How can we assume ğœ†2 â‰  0?)
```
```
Combining with the last two equations we solve for points (2, 2, 8) (minimum)
```
```
and (âˆ’3, âˆ’3, 18) (maximum).
```
2.9. Lambda 27
2.9 Lambda
What is ğœ†?
In this work, ğœ† is the Lagrange multiplier, and it serves as a place holding tool for
solving a problem of optimizing with constraints. A closer look at ğœ† reveals interesting
properties that will provide insight when we apply the Lagrange multiplier structure
to dynamic controls.
```
Consider the basic case in Section 2.6 of optimizing ğ‘”(ğ‘¥, ğ‘¦) under a constraint
```
```
ğ‘“(ğ‘¥, ğ‘¦) = ğ‘ where extremal points are characterized by the alignment of the gradients
```
```
of ğ‘”(ğ‘¥, ğ‘¦) and ğ‘“(ğ‘¥, ğ‘¦):
```
âˆ‡ğ‘” = ğœ† âˆ‡ğ‘“.
If âˆ‡ğ‘“ is nonzero, |ğœ†| is the ratio of the lengths of the gradients of ğ‘“ and ğ‘”:
|ğœ†| = â€–âˆ‡ğ‘”â€–â€–âˆ‡ğ‘“â€– .
As such, ğœ† is the rate of change in ğ‘” per rate of change in ğ‘“ in the direction the gradients
are aligned. From this we can conclude that if our constraint, ğ‘“, changes from ğ‘ to ğ‘ + ğ›¿
for some small ğ›¿, we expect our optimal value, ğ‘”, to change by about ğ›¿ğœ†.
The following example demonstrates this numerically.
Example 2.11: Saddle
```
Returning to Example 2.5, we found a maximum value for ğ‘”(ğ‘¥, ğ‘¦) = ğ‘¥2 + 2ğ‘¥ğ‘¦ âˆ’
```
```
3ğ‘¦2 + 8ğ‘¦ âˆ’ 4 to be 7.682 . . . at (ğ‘¥, ğ‘¦) = (1.611 . . . , 1.185 . . . ) for the constraint
```
ğ‘¥2 +ğ‘¦2 = 22 = 4. At this extremal point we calculate ğœ† = â€–âˆ‡ğ‘”â€–/â€–âˆ‡ğ‘“â€– = 1.735 . . . .
Figure 2.7. Increasing the constraint moves the maximum
point along the stationary curve.
Now consider a circle of radius, say, 2.3, and maximize subject to the con-
```
straint ğ‘¥2 + ğ‘¦2 = 2.32 = 5.29 (Figure 2.7). Then we have changed the value of
```
28 Chapter 2. Static Optimization
our constraint by ğ›¿ = 1.29, and we can expect our maximum value to increase
```
by ğ›¿ğœ† = (1.29)(1.735 . . . ) = 2.239 . . . . The actual maximum of ğ‘”(ğ‘¥, ğ‘¦) subject to
```
ğ‘¥2 + ğ‘¦2 = 2.32 is 9.870 . . . , an increase of 2.188 . . . . Pretty close to our estimate.
If we maximize over a circle of radius 2.1, making our constraint ğ‘¥2 + ğ‘¦2 =
2.12 = 4.41, we increase our constraint by ğ›¿ = 0.41 and we expect our maxi-
```
mum to increase by ğ›¿ğœ† = (0.41)(1.735 . . . ) = .7115 . . . . The actual maximum is
```
approximately 8.387 . . . , an increase of .706 . . . . Even closer.
Maximizing over a circle of radius 2.01 would increase our constraint by ğ›¿ =
```
0.0401, our anticipated increase would be ğ›¿ğœ† = (0.0401)(1.735 . . . ) = 0.0659 . . .
```
and our actual increase is 0.0654 . . . . In the limit, the ratio of our anticipated
increase and actual increase converges to one.
It is important to see this concept clearly.
Recall that the gradient âˆ‡ğ‘“ points in the direction of steepest ascent of the function
ğ‘“, and the length |âˆ‡ğ‘“| is the rise over run of that steepest ascent.
```
Suppose the maximum of ğ‘”(ğ‘¥, ğ‘¦) given constraint ğ‘“(ğ‘¥, ğ‘¦) = ğ‘ is at a point (ğ‘¥1, ğ‘¦1)
```
```
with value ğ‘£1 = ğ‘”(ğ‘¥1, ğ‘¦1). Assume the gradients are parallel (pointing in the same
```
```
direction) and nonzero at this point, with ğœ† = â€–âˆ‡ğ‘”â€–/â€–âˆ‡ğ‘“â€–.
```
If our constraint ğ‘“ = ğ‘ is relaxed to a constraint ğ‘“ = ğ‘ + ğ›¿ for some small ğ›¿ > 0,
```
we would want to get a better maximum for ğ‘” by moving from the point (ğ‘¥1, ğ‘¦1) in the
```
direction âˆ‡ğ‘”, the direction of greatest increase in ğ‘”.
How far in this direction can we move?
At this optimal point the gradients âˆ‡ğ‘“ and âˆ‡ğ‘” are aligned. So a better question is
how far in the direction of âˆ‡ğ‘“ can we move if we increase the constraint from ğ‘“ = ğ‘ to
ğ‘“ = ğ‘ + ğ›¿. As â€–âˆ‡ğ‘“â€– is the rise over run of ğ‘“ in this direction, the answer is that we can
move a distance ğœ– in this direction where ğ›¿ = ğœ–â€–âˆ‡ğ‘“â€–, or ğœ– = ğ›¿/â€–âˆ‡ğ‘“â€–, see Figure 2.8.
Figure 2.8. Increase in the value of ğ‘“ in the direction of the gradient.
2.9. Lambda 29
How much of an increase will we get from ğ‘” with such a move?
The length â€–âˆ‡ğ‘”â€– is the rise over run of increase in ğ‘” in the direction âˆ‡ğ‘”. If we move
a distance ğœ– in this direction, we expect an increase of about ğœ–â€–âˆ‡ğ‘”â€–.
So, if we increase our constraint from ğ‘ to ğ‘+ğ›¿, we can move a distance ğœ– = ğ›¿/â€–âˆ‡ğ‘“â€–
and expect to increase our optimal ğ‘” by
ğœ– â€–âˆ‡ğ‘”â€– = ğ›¿ â€–âˆ‡ğ‘”â€–â€–âˆ‡ğ‘“â€– = ğ›¿ğœ†.
That is, increasing our constraint by ğ›¿ increases our maximum by ğœ†ğ›¿. This is what ğœ†
is.
We have the following key idea:
The Lagrange multiplier ğœ† is the rate of change in the optimum value of
```
ğ‘”(ğ‘¥, ğ‘¦) per change in constraint value ğ‘ = ğ‘“(ğ‘¥, ğ‘¦).
```
Conceptually, we have
```
ğœ† = ğ‘‘ğ‘‘ğ‘ (Maximum of ğ‘”(ğ‘¥, ğ‘¦) subject to ğ‘“(ğ‘¥, ğ‘¦) = ğ‘).
```
The previous example demonstrated this principle as a numerical approximation. The
following example shows the idea exactly as a derivative in closed form. The example
also demonstrates that the principle applies equally to minimizing.
Example 2.12: Paraboloid
```
Consider ğ‘”(ğ‘¥, ğ‘¦) = 5ğ‘¥2 + 4ğ‘¥ğ‘¦ + 2ğ‘¦2, and suppose we want to find the minimum
```
value of this function subject to a constraint ğ‘¥ = ğ‘. We expect this minimum to
occur at a point where the level curve of ğ‘” is tangent to the vertical line ğ‘¥ = ğ‘.
```
We will express this minimum value as a function of the constraint ğ½(ğ‘). We
```
do this using Lagrange multipliers, and then we explore how the value of the
```
multiplier ğœ† relates to the rate of change of ğ½(ğ‘).
```
```
Note that we can minimize ğ‘”(ğ‘¥, ğ‘¦) subject to this constraint directly by sub-
```
```
stituting ğ‘¥ = ğ‘ and considering ğ‘”(ğ‘, ğ‘¦) = 5ğ‘2 + 4ğ‘ğ‘¦ + 2ğ‘¦2. This is unbounded, so
```
```
no maximum exists. We can complete the square to get ğ‘”(ğ‘, ğ‘¦) = 2(ğ‘¦ + ğ‘)2 + 3ğ‘2,
```
which has a minimum of 3ğ‘2 at ğ‘¦ = âˆ’ğ‘.
```
For Lagrange, formulate the constraint as ğ‘“(ğ‘¥, ğ‘¦) = ğ‘ where ğ‘“(ğ‘¥, ğ‘¦) = ğ‘¥ to
```
get
```
âˆ‡ğ‘“ = (1, 0),
```
```
âˆ‡ğ‘” = (10ğ‘¥ + 4ğ‘¦, 4ğ‘¥ + 4ğ‘¦).
```
```
These are parallel when 4ğ‘¥ + 4ğ‘¦ = 0, producing a stationary set ğ‘¦ = âˆ’ğ‘¥; see
```
Figure 2.9.
30 Chapter 2. Static Optimization
Figure 2.9. Stationary points and a minimum for the con-
straint value ğ‘ = 1.
```
Setting the constraint ğ‘¥ = ğ‘ yields a minimum value of ğ½(ğ‘) = ğ‘”(ğ‘, âˆ’ğ‘) = 3ğ‘2,
```
as we found earlier.
```
Evaluating gradients at the extremal point (ğ‘, âˆ’ğ‘),
```
```
âˆ‡ğ‘”(ğ‘, âˆ’ğ‘) = (6ğ‘, 0),
```
```
âˆ‡ğ‘“(ğ‘, âˆ’ğ‘) = (1, 0),
```
we see ğœ† = 6ğ‘.
```
This is equal to the derivative of the minimum ğ½(ğ‘) = 3ğ‘2 with respect to the
```
constraint value ğ‘.
The value of the Lagrange multiplier ğœ† is the rate of change of the minimum
with respect to change in constraint value ğ‘.
Some care is needed to interpret the sign of ğœ†. Gradients point in the direction of
greatest increase. If ğœ† > 0, then the gradients are pointing in the same direction, and
if ğœ† < 0, they are pointing in opposite directions.
If ğœ† > 0, then the maximum of ğ‘” will increase with an increase in the constraint
ğ‘ = ğ‘“, and if ğœ† < 0, the maximum will decrease.
This flips when minimizing. If ğœ† < 0, then the minimum of ğ‘” will decrease with
an increase in the constraint ğ‘ = ğ‘“, and if ğœ† < 0, the minimum will increase.
We can generalize this interpretation of ğœ† to the multiple constraint case covered
in Section 2.8. When maximizing ğ‘” with multiple constraints ğ‘“ğ‘– = ğ‘ğ‘–, each associated
ğœ†ğ‘– is the rate of change of the optimal value with respect to a change in the restriction
ğ‘ğ‘–. The following example demonstrates the concept.
2.9. Lambda 31
Example 2.13
```
Returning to the Example 2.9, consider minimizing ğ‘”(ğ‘¥, ğ‘¦, ğ‘§) = ğ‘¥2 + ğ‘¦2 + ğ‘§2
```
subject to the general constraint values:
```
ğ‘“1(ğ‘¥, ğ‘¦, ğ‘§) = ğ‘¥ + ğ‘¦ + ğ‘§ = ğ‘1,
```
```
ğ‘“2(ğ‘¥, ğ‘¦, ğ‘§) = ğ‘¥ + ğ‘¦ + 2ğ‘§ = ğ‘2
```
for given constants ğ‘1, ğ‘2. Setting âˆ‡ğ‘” = ğœ†1âˆ‡ğ‘“1 + ğœ†2âˆ‡ğ‘“2 generates
2ğ‘¥ = ğœ†1 + ğœ†2,
2ğ‘¦ = ğœ†1 + ğœ†2,
2ğ‘§ = ğœ†1 + 2ğœ†2.
Solving these five equations for the five unknowns ğ‘¥, ğ‘¦, ğ‘§, ğœ†1, ğœ†2, produces a unique
solution
ğ‘¥ = ğ‘1 âˆ’ 12 ğ‘2,
ğ‘¦ = ğ‘1 âˆ’ 12 ğ‘2,
ğ‘§ = 0,
ğœ†1 = 6ğ‘1 âˆ’ 4ğ‘2,
ğœ†2 = âˆ’4ğ‘1 + 3ğ‘2.
```
(2.4)
```
The minimum value for ğ‘” under these constraints is a function of the con-
straint values:
```
ğ½(ğ‘2, ğ‘2) = ğ‘” (ğ‘1 âˆ’ 12 ğ‘2, ğ‘1 âˆ’ 12 ğ‘2, 0) = 3ğ‘21 âˆ’ 4ğ‘1ğ‘2 + 32 ğ‘22
```
and we note that the partial derivatives turn out to be equal to the values we found
```
in equations (2.4) for the Lagrange multipliers:
```
ğœ•ğ½
ğœ•ğ‘1= 6ğ‘1 âˆ’ 4ğ‘2 = ğœ†1,
ğœ•ğ½
ğœ•ğ‘2= âˆ’4ğ‘1 + 3ğ‘2 = ğœ†2.
That is, the value of the Lagrange multiplier ğœ†ğ‘– is the rate of change of the mini-
mum with respect to change in constraint value ğ‘ğ‘–.
It is insightful to consider how all this plays out in terms of the units of variables
in our models. For example, suppose we have an economic model of production, mea-
```
sured in number of units produced, as a function ğ‘”(ğ‘¥, ğ‘¦, ğ‘§) of hours of labor ğ‘¥, equip-
```
```
ment run time ğ‘¦, and tons of material ğ‘§. The cost of ğ‘¥, ğ‘¦, ğ‘§ is a function ğ‘“1(ğ‘¥, ğ‘¦, ğ‘§)
```
32 Chapter 2. Static Optimization
```
measured in dollars and we have a budget constraint ğ‘“1(ğ‘¥, ğ‘¦, ğ‘§) = ğ‘1 dollars. The car-
```
```
bon dioxide produced by ğ‘¥, ğ‘¦, ğ‘§ is a function ğ‘“2(ğ‘¥, ğ‘¦, ğ‘§) measure in tons of ğ¶ğ‘‚2 and we
```
```
have a regulatory restriction ğ‘“2(ğ‘¥, ğ‘¦, ğ‘§) = ğ‘2 tons. In the Lagrange multiplier condition
```
âˆ‡ğ‘” = ğœ†1âˆ‡ğ‘“1 + ğœ†2âˆ‡ğ‘“2
we have that ğœ†1 is the change in optimal production per dollar change in budget con-
```
straint (production per dollar), and ğœ†2 is the change in optimal production per ton
```
```
change in ğ¶ğ‘‚2 restriction (production per ton). With âˆ‡ğ‘” in production units, âˆ‡ğ‘“1 in
```
dollars, and âˆ‡ğ‘“2 in tons, this yields the dimensional analysis:
```
production = ( productiondollars ) dollars + ( productiontons ) tons.
```
2.10 Hamilton and Lagrange
Hamiltonians and Lagrangians are universal structures with deep mathematical mean-
ing. Our use of them in this study barely scratches the surface of their importance in
mathematics and physics. We will make significant use of the Hamiltonian in the sub-
sequent chapters.
```
For optimizing ğ‘”(ğ‘¥, ğ‘¦) with respect to a constraint ğ‘“(ğ‘¥, ğ‘¦) = 0 we look for points
```
where âˆ‡ğ‘” = ğœ†âˆ‡ğ‘“. Allowing for changes in the sign of ğœ† and expanding the gradient
operator, these conditions can be reframed as three equations in three unknowns:
```
0 = ğœ•ğ‘”ğœ•ğ‘¥ (ğ‘¥, ğ‘¦) + ğœ† ğœ•ğ‘“ğœ•ğ‘¥ (ğ‘¥, ğ‘¦),
```
```
0 = ğœ•ğ‘”ğœ•ğ‘¦ (ğ‘¥, ğ‘¦) + ğœ† ğœ•ğ‘“ğœ•ğ‘¦ (ğ‘¥, ğ‘¦),
```
```
0 = ğ‘“(ğ‘¥, ğ‘¦).
```
Solutions to this set of equations are critical points of
```
ğ»(ğ‘¥, ğ‘¦, ğœ†) = ğ‘”(ğ‘¥, ğ‘¦) + ğœ† ğ‘“(ğ‘¥, ğ‘¦)
```
as these equations are expansions of 0 = ğœ•ğ»ğœ•ğ‘¥ , 0 = ğœ•ğ»ğœ•ğ‘¦ , and 0 = ğœ•ğ»ğœ•ğœ† , respectively.
This function is called the Hamiltonian and has a pretty widespread reputation for
```
awesomeness. The multiple conditions in equations (2.2) and (2.3) can be written as
```
```
ğ‘› + ğ‘š equations in ğ‘› + ğ‘š unknowns (with sign changes for ğœ†ğ‘–â€™s):
```
```
0 = ğœ•ğ‘”ğœ•ğ‘¥1(ğ‘¥1, . . . , ğ‘¥ğ‘š) + ğœ†1ğœ•ğ‘“ğœ•ğ‘¥1(ğ‘¥1, . . . , ğ‘¥ğ‘š) + â‹¯ + ğœ†ğ‘šğœ•ğ‘“ğœ•ğ‘¥1(ğ‘¥1, . . . , ğ‘¥ğ‘š),
```
```
0 = ğœ•ğ‘”ğœ•ğ‘¥2(ğ‘¥1, . . . , ğ‘¥ğ‘š) + ğœ†1ğœ•ğ‘“ğœ•ğ‘¥2(ğ‘¥1, . . . , ğ‘¥ğ‘š) + â‹¯ + ğœ†ğ‘šğœ•ğ‘“ğœ•ğ‘¥2(ğ‘¥1, . . . , ğ‘¥ğ‘š),
```
â‹®
```
0 = ğœ•ğ‘”ğœ•ğ‘¥ğ‘š(ğ‘¥1, . . . , ğ‘¥ğ‘š) + ğœ†1ğœ•ğ‘“ğœ•ğ‘¥ğ‘š(ğ‘¥1, . . . , ğ‘¥ğ‘š) + â‹¯ + ğœ†ğ‘šğœ•ğ‘“ğœ•ğ‘¥ğ‘š(ğ‘¥1, . . . , ğ‘¥ğ‘š),
```
```
0 = ğ‘“1(ğ‘¥1, . . . , ğ‘¥ğ‘š),
```
```
0 = ğ‘“2(ğ‘¥1, . . . , ğ‘¥ğ‘š),
```
â‹®
```
0 = ğ‘“ğ‘›(ğ‘¥1, . . . , ğ‘¥ğ‘š).
```
Exercises 33
Solutions to this set of equations are critical points of a single Hamiltonian function of
ğ‘› + ğ‘š variables:
```
ğ»(ğ‘¥1, . . . , ğ‘¥ğ‘š, ğœ†1, . . . , ğœ†ğ‘›)
```
```
= ğ‘”(ğ‘¥1, . . . , ğ‘¥ğ‘š) + ğœ†1ğ‘“1(ğ‘¥1, . . . , ğ‘¥ğ‘š) + â‹¯ + ğœ†ğ‘›ğ‘“1(ğ‘¥1, . . . , ğ‘¥ğ‘š).
```
Hamiltonians and Lagrangians are deeply involved in the abstract study of phys-
ical mechanics. A Hamiltonian is formed by the sum of kinetic and potential energy
and is a conserved quantity in mechanical systems. The Lagrangian is the difference
between kinetic and potential energy, and the optimization of the Lagrangian produces
mechanical action. These are intertwined and closely related concepts. We wonâ€™t get
into theoretical mechanics, but these deep principles extend far beyond mechanics.
Much of optimal control works from the Hamiltonian viewpoint. The Lagrangian will
make an appearance in calculus of variations in Chapter 13.
Key Points
In this chapter we reviewed methods of static optimization from single and multivari-
able calculus. This entailed a careful examination of what we mean by the derivative
of a function, which we interpret using linearization. We emphasized that locating a
maximum or minimum is a process of eliminating all the places where a maximum or
minimum cannot occur.
We examined the Lagrange multiplier method, for single and multiple constraints,
with an emphasis of understanding the Lagrange multiplier ğœ† as the change in opti-
mum per change in constraint.
We introduced the idea of a Hamiltonian function, whose critical points are solu-
tions to Lagrange multiplier problems.
Exercises
```
Exercise 2.1(s). Use Lagrange multipliers to find the maximum and minimum of
```
```
ğ‘”(ğ‘¥, ğ‘¦) = ğ‘¥ + ğ‘¦ subject to constraint ğ‘“(ğ‘¥, ğ‘¦) = ğ‘¥2 + 2ğ‘¦2 = 1.
```
```
Exercise 2.2(s). Use Lagrange multipliers to find the minimum of ğ‘”(ğ‘¥, ğ‘¦) = ğ‘¥2 +ğ‘¦2 +ğ‘§2
```
```
subject to constraints ğ‘“1(ğ‘¥, ğ‘¦, ğ‘§) = ğ‘¥ + ğ‘¦ + ğ‘§ = 2 and ğ‘“2(ğ‘¥, ğ‘¦, ğ‘§) = ğ‘¥ + ğ‘¦ âˆ’ ğ‘§ = 3.
```
```
Exercise 2.3. Find the extreme values of ğ‘”(ğ‘¥, ğ‘¦) = 5ğ‘¥2 + 5ğ‘¦2 âˆ’ 6ğ‘¥ âˆ’ 8ğ‘¦ + 1 over the
```
circle ğ‘¥2 + ğ‘¦2 = 2 in two different ways:
```
(a) Parameterize the circle ğ‘¥(ğœƒ) = 2 cos(ğœƒ), ğ‘¦(ğœƒ) = 2 sin(ğœƒ) and optimize over ğœƒ.
```
```
(b) Use Lagrange multipliers. Do you get the same result?
```
```
Exercise 2.4. Maximize and minimize ğ‘”(ğ‘¥, ğ‘¦) = 5ğ‘¥2 + 5ğ‘¦2 âˆ’ 6ğ‘¥ âˆ’ 8ğ‘¦ + 1 over the
```
unit circle ğ‘¥2 + ğ‘¦2 = 1 using Lagrange multipliers. Plot some level curves of ğ‘” and the
constraint curve, and identify the maxima and minima. You should get that a Lagrange
multiplier is ğœ† = 0. Show where this occurs.
34 Chapter 2. Static Optimization
```
Exercise 2.5(s). Continuing with the previous exercises, use Lagrange multipliers to
```
find the minimum value of 5ğ‘¥2 + 5ğ‘¦2 âˆ’ 6ğ‘¥ âˆ’ 8ğ‘¦ + 1 over the circle ğ‘¥2 + ğ‘¦2 = ğœ… as a
```
function ğ½(ğœ…) of ğœ… > 0. Plot this function, and show that it has a minimum at ğœ… = 1.
```
How does this relate to Exercise 2.4?
```
Exercise 2.6. Consider the paraboloid ğ‘”(ğ‘¥, ğ‘¦) = (ğ‘¥ âˆ’ 1)2 + ğ‘¦2.
```
```
(a) Use Lagrange multipliers to find the minimum of ğ‘”(ğ‘¥, ğ‘¦) on the line ğ‘¥ + ğ‘¦ = 2.
```
```
(b) Use Lagrange multipliers to find the minimum ğ½(ğœ…) of ğ‘”(ğ‘¥, ğ‘¦) on the line ğ‘¥ + ğ‘¦
```
= ğœ….
```
(c) Verify that ğ½â€²(ğœ…) equals the value of the Lagrange multiplier.
```
```
Exercise 2.7(hs). Consider ğ‘”(ğ‘¥, ğ‘¦, ğ‘§) = ğ‘¥ğ‘¦ + ğ‘§2.
```
```
(a) Use Lagrange multipliers to find the minimum of ğ‘” subject to constraints 2ğ‘¥ âˆ’
```
ğ‘¦ + 3ğ‘§ = 2 and ğ‘¥ + ğ‘¦ + ğ‘§ = 0.
```
(b) Use Lagrange multipliers to find the minimum of ğ‘” subject to constraints 2ğ‘¥ âˆ’
```
```
ğ‘¦ + 3ğ‘§ = ğ‘ and ğ‘¥ + ğ‘¦ + ğ‘§ = ğ‘ as a function ğ½(ğ‘, ğ‘). Verify that ğœ•ğ½ğœ•ğ‘ and ğœ•ğ½ğœ•ğ‘ are equal to
```
the Lagrange multipliers.
```
Exercise 2.8. The Cobb-Douglas production function ğ‘”(ğ‘¥, ğ‘¦) = ğ‘˜ğ‘¥ğ›¼ğ‘¦ğ›½ for positive ğ‘˜,
```
```
ğ›¼, ğ›½, with ğ›¼ + ğ›½ = 1, is the number of units manufactured by ğ‘¥ units of capital (in
```
```
dollars) and ğ‘¦ units of labor (in hours).
```
Assume labor costs ğ¿ dollars per hour, so total cost is measured as ğ‘¥ + ğ¿ğ‘¦ dollars.
Taking ğ›¼ = 1/5, ğ›½ = 4/5, how can we maximize production while spending ğ‘€
```
dollars (so ğ‘¥ + ğ¿ğ‘¦ = ğ‘€)? How would the maximum change if we could spend ğ‘€ + 1
```
dollars? How does this relate to the Lagrange multiplier?
```
Exercise 2.9(h). For values ğ‘1, . . . , ğ‘ğ‘› and ğ‘1, . . . , ğ‘ğ‘›, prove the Cauchy-Schwarz in-
```
equality for finite sums:
âˆ‘ğ‘ğ‘–ğ‘ğ‘– â‰¤ âˆšâˆ‘ ğ‘2ğ‘– âˆšâˆ‘ ğ‘2ğ‘–
by the following steps:
```
(a) Maximize âˆ‘ ğ‘¥ğ‘–ğ‘¦ğ‘– subject to âˆ‘ ğ‘¥2ğ‘– = 1 and âˆ‘ ğ‘¦2ğ‘– = 1.
```
```
(b) Substitute ğ‘¥ğ‘– = ğ‘ğ‘–/âˆšâˆ‘ ğ‘2ğ‘– and ğ‘¦ğ‘– = ğ‘ğ‘–/âˆšâˆ‘ ğ‘2ğ‘– in your result, and conclude
```
the Cauchy-Schwarz inequality.
```
Exercise 2.10(h). Suppose ğ‘“ is differentiable at ğ‘¥ = ğ‘ with ğ‘“â€²(ğ‘) > 0. Use the defini-
```
```
tion of the derivative to prove that there is a point ğ‘ > ğ‘ with ğ‘“(ğ‘) > ğ‘“(ğ‘).
```
3
```
Control: A Discrete Start
```
In Chapter 1 we optimized the net payoff ğ½ in a simple game of Bocce ball using basic
calculus techniques. This worked well for the one- or two-step game, but the direct
approach quickly becomes untenable as more steps are added. Pontryaginâ€™s method,
which we begin to study in this chapter, uses Lagrange multipliers, covered in Chapter
2, to cut through this complexity and create a format that scales up for any number of
steps. Here is where we start building our core theory.
We consider a process that begins in a state ğ‘¥0, and for ğ‘– = 0, 1, . . . , ğ‘ âˆ’ 1 we
exercise a control ğ‘¢ğ‘– to move the system from state ğ‘¥ğ‘– to the next state ğ‘¥ğ‘–+1 according
```
to a control dynamic ğ‘¥ğ‘–+1 = ğ‘“(ğ‘¥ğ‘–, ğ‘¢ğ‘–) creating a trajectory ğ‘¥0, . . . , ğ‘¥ğ‘ in state space. We
```
want to operate the control to optimize the overall performance of the system, which
```
is measured at each step as ğ‘”(ğ‘¥ğ‘–, ğ‘¢ğ‘–) and at the end state as ğº(ğ‘¥ğ‘ ). We can think of
```
```
these as payoffs (with costs being negative values) and so we want to optimize ğ½ =
```
```
ğº(ğ‘¥ğ‘ ) + âˆ‘ğ‘âˆ’1ğ‘–=0 ğ‘”(ğ‘¥ğ‘–, ğ‘¢ğ‘–). In the Bocce ball examples, we had a cost for each move and
```
a payoff at the end, and in Example 1.4 we had a fixed start and end position, with
costs along the way. In this chapter we will develop a general technique for addressing
problems of this form.
3.1 Optimal Two-Step Process Control
We begin by revisiting the two-step optimization problem using Lagrange techniques.
Example 3.1: Two-Step Bocce Redux
We return to our Bocce ball example, Example 1.2, and solve the problem using
Lagrange multipliers.
From a given starting position ğ‘¥0 we make two moves, ğ‘¢0, ğ‘¢1, with ğ‘¥1 = ğ‘¥0 +
ğ‘¢0 and ğ‘¥2 = ğ‘¥1+ğ‘¢1. Each move costs ğ‘¢2ğ‘– /ğ‘¥ğ‘– and our final payoff is ğ‘¥2. Formulating
this in terms of Lagrange multipliers, we want to find values ğ‘¥1, ğ‘¥2, ğ‘¢0, ğ‘¢1 that
35
36 Chapter 3. Control: A Discrete Start
maximize net payoff
```
ğ½(ğ‘¥1, ğ‘¥2, ğ‘¢0, ğ‘¢1) = ğ‘¥2 âˆ’ ğ‘¢
```
20
ğ‘¥0âˆ’
ğ‘¢21
ğ‘¥1
subject to constraints ğ‘¥1 = ğ‘¥0 + ğ‘¢0 and ğ‘¥2 = ğ‘¥1 + ğ‘¢1, which we formulate as
functions set equal to zero
```
ğ‘“1(ğ‘¥1, ğ‘¥2, ğ‘¢0, ğ‘¢1) = ğ‘¥0 + ğ‘¢0 âˆ’ ğ‘¥1 = 0,
```
```
ğ‘“2(ğ‘¥1, ğ‘¥2, ğ‘¢0, ğ‘¢1) = ğ‘¥1 + ğ‘¢1 âˆ’ ğ‘¥2 = 0.
```
The Lagrange condition is
```
âˆ‡ğ½ = ğœ†1âˆ‡ğ‘“1 + ğœ†2âˆ‡ğ‘“2. (3.1)
```
Expanding the gradients by differentiating our functions with respect to each
```
variable ğ‘¥1, ğ‘¥2, ğ‘¢0, ğ‘¢1 yields (âˆ— check âˆ—)
```
```
âˆ‡ğ½ = ( áµ†
```
21
ğ‘¥21, 1, âˆ’
2áµ†0
ğ‘¥0, âˆ’
2áµ†1
```
ğ‘¥1 ) ,
```
```
âˆ‡ğ‘“1 = (âˆ’1, 0, 1, 0) ,
```
```
âˆ‡ğ‘“2 = (1, âˆ’1, 0, 1) .
```
```
Substituting these into equation (3.1) generates four equations (âˆ— verify âˆ—):
```
áµ†21
ğ‘¥21= âˆ’ğœ†1 + ğœ†2,
1 = âˆ’ğœ†2,
âˆ’ 2áµ†0ğ‘¥0= ğœ†1,
âˆ’ 2áµ†1ğ‘¥1= ğœ†2.
```
(3.2)
```
From here, we solve the last two equations to determine ğ‘¢0 and ğ‘¢1 in terms
of ğ‘¥1, ğ‘¥2, ğœ†1, ğœ†2:
ğ‘¢0 = âˆ’ 12 ğ‘¥0ğœ†1,
ğ‘¢1 = âˆ’ 12 ğ‘¥1ğœ†2.
```
Substituting into the first two equations of (3.2) yields (âˆ— check âˆ—)
```
ğœ†22
4 = âˆ’ğœ†1 + ğœ†2,
1 = âˆ’ğœ†2,
which determines our costates: ğœ†2 = âˆ’1 and ğœ†1 = âˆ’5/4.
So at the first step we should apply control ğ‘¢0 = âˆ’ğ‘¥0ğœ†1/2 = 5ğ‘¥0/8 which
moves the ball to ğ‘¥1 = 13ğ‘¥0/8.
On the second step we apply control ğ‘¢1 = âˆ’ğ‘¥1ğœ†2/2 = 13ğ‘¥0/8 moving the ball
to ğ‘¥2 = 39ğ‘¥0/16 for a net payoff of 105ğ‘¥0/64.
This matches our result from Example 1.2.
3.1. Optimal Two-Step Process Control 37
The Lagrange multiplier solution to this example takes more steps and seems more
complicated than the direct solution in Chapter 1, but the idea is that the Lagrange
multiplier approach will readily generalize to any number of steps whereas the direct
approach will become untenable due to compounding complexity.
Letâ€™s look more carefully at a general two-step process, and then we will generalize
to ğ‘› steps.
Consider a problem where we observe the initial state ğ‘¥0 of a system and apply a
```
control ğ‘¢0 which leads to outcome ğ‘¥1 = ğ‘“(ğ‘¥0, ğ‘¢0). We then apply control ğ‘¢1 which
```
```
leads to a final outcome ğ‘¥2 = ğ‘“(ğ‘¥1, ğ‘¢1). We want to choose controls to optimize a
```
performance function in the form
```
ğ½(ğ‘¥1, ğ‘¥2, ğ‘¢0, ğ‘¢1) = ğº(ğ‘¥2) + ğ‘”(ğ‘¥0, ğ‘¢0) + ğ‘”(ğ‘¥1, ğ‘¢1). (3.3)
```
For using Lagrange multipliers, we express our control dynamics as constraints
```
ğ‘“1(ğ‘¥1, ğ‘¥2, ğ‘¢0, ğ‘¢1) = ğ‘“(ğ‘¥0, ğ‘¢0) âˆ’ ğ‘¥1 = 0,
```
```
ğ‘“2(ğ‘¥1, ğ‘¥2, ğ‘¢0, ğ‘¢1) = ğ‘“(ğ‘¥1, ğ‘¢1) âˆ’ ğ‘¥2 = 0.
```
```
(3.4)
```
For a given starting point ğ‘¥0, we look for optimal values of ğ½ over ğ‘› = 4 variables
ğ‘¥1, ğ‘¥2, ğ‘¢0, ğ‘¢1 subject to ğ‘š = 2 constraints by setting up the Lagrange multipliers
âˆ‡ğ½ = ğœ†1âˆ‡ğ‘“1 + ğœ†2âˆ‡ğ‘“2
which generates four equations, one for each partial derivative with respect to ğ‘¥1, ğ‘¥2,
ğ‘¢0, ğ‘¢1.
ğœ•ğ½
ğœ•ğ‘¥1= ğœ†1
ğœ•ğ‘“1
ğœ•ğ‘¥1+ ğœ†2
ğœ•ğ‘“2
ğœ•ğ‘¥1,
ğœ•ğ½
ğœ•ğ‘¥2= ğœ†1
ğœ•ğ‘“1
ğœ•ğ‘¥2+ ğœ†2
ğœ•ğ‘“2
ğœ•ğ‘¥2,
ğœ•ğ½
ğœ•áµ†0= ğœ†1
ğœ•ğ‘“1
ğœ•áµ†0+ ğœ†2
ğœ•ğ‘“2
ğœ•áµ†0,
ğœ•ğ½
ğœ•áµ†1= ğœ†1
ğœ•ğ‘“1
ğœ•áµ†1+ ğœ†2
ğœ•ğ‘“2
ğœ•áµ†1.
When expanding out these four equations many of the terms are zero. Check this
```
with substitutions from equations (3.3) and (3.4) to derive
```
ğœ•ğ‘”
```
ğœ•ğ‘¥ (ğ‘¥1, ğ‘¢1) = âˆ’ğœ†1 + ğœ†2
```
ğœ•ğ‘“
```
ğœ•ğ‘¥ (ğ‘¥1, ğ‘¢1),
```
ğœ•ğº
```
ğœ•ğ‘¥ (ğ‘¥2) = âˆ’ğœ†2,
```
ğœ•ğ‘”
```
ğœ•áµ† (ğ‘¥0, ğ‘¢0) = ğœ†1
```
ğœ•ğ‘“
```
ğœ•áµ† (ğ‘¥0, ğ‘¢0),
```
ğœ•ğ‘”
```
ğœ•áµ† (ğ‘¥1, ğ‘¢1) = ğœ†2
```
ğœ•ğ‘“
```
ğœ•áµ† (ğ‘¥1, ğ‘¢1).
```
```
(3.5)
```
There is a certain order in resolving these equations that makes for an elegant solu-
tion, which is basically the steps we followed in Example 3.1 and which is a consistent
approach throughout our exploration of optimal control techniques.
38 Chapter 3. Control: A Discrete Start
```
First, solve the last two equations in the system (3.5) to determine the optimal
```
controls ğ‘¢0 and ğ‘¢1 in terms of state variables ğ‘¥0, ğ‘¥1 and Lagrange multipliers ğœ†1, ğœ†2.
Second, substitute the resulting ğ‘¢1 and ğ‘¢2 into the first two equations in system
```
(3.5) and the constraints (3.4) to get
```
ğœ•ğ‘”
```
ğœ•ğ‘¥ (ğ‘¥1, ğ‘¢1) = âˆ’ğœ†1 + ğœ†2
```
ğœ•ğ‘“
```
ğœ•ğ‘¥ (ğ‘¥1, ğ‘¢1),
```
ğœ•ğº
```
ğœ•ğ‘¥ (ğ‘¥2) = âˆ’ğœ†2,
```
```
ğ‘¥1 = ğ‘“(ğ‘¥0, ğ‘¢0),
```
```
ğ‘¥2 = ğ‘“(ğ‘¥1, ğ‘¢1).
```
Third, solve these equations for ğ‘¥1, ğ‘¥2, ğœ†1, and ğœ†2, which will determine controls
ğ‘¢1, ğ‘¢2 and payoff ğ½. The ğœ†ğ‘– will be referred to as costate variables and will become part
of a dynamical system that defines necessary conditions for an optimal control.
This method produces state values ğ‘¥1, ğ‘¥2 and costate values ğœ†1, ğœ†2 and a condition
on control values ğ‘¢0, ğ‘¢1 that must be satisfied by an optimal control.
3.2 Optimal ğ‘-Step Process Control
The above two-step method gives us some idea of how Lagrange multipliers can be used
to solve multistep control problems. The general method for solving the ğ‘-step discrete
optimal control problem extends the two-step method and is summarized in the follow-
ing basic principle. We state the principle, explain how to apply it, and demonstrate
with a couple of examples.
This principle reveals the key elements of Pontryaginâ€™s method and shows the basic
template for the principles developed throughout this text.
OPTIMAL PRINCIPLE 0
Local optimal, fixed duration, time independent, one dimension, discrete
Given a starting point ğ‘¥0, a trajectory ğ‘¥0, ğ‘¥1, . . . , ğ‘¥ğ‘ that optimizes
```
ğ½ = ğº(ğ‘¥ğ‘ ) +
```
ğ‘âˆ’1
âˆ‘
ğ‘–=0
```
ğ‘”(ğ‘¥ğ‘–, ğ‘¢ğ‘–)
```
under the process
```
ğ‘¥ğ‘–+1 = ğ‘“(ğ‘¥ğ‘–, ğ‘¢ğ‘–) for ğ‘– = 0, . . . , ğ‘ âˆ’ 1
```
with costates ğœ†0, ğœ†1, . . . , ğœ†ğ‘ that satisfy the recursion
```
ğœ†ğ‘– = ğœ†ğ‘–+1ğœ•ğ‘“ğœ•ğ‘¥ (ğ‘¥ğ‘–, ğ‘¢ğ‘–) âˆ’ ğœ•ğ‘”ğœ•ğ‘¥ (ğ‘¥ğ‘–, ğ‘¢ğ‘–) for ğ‘– = 0, . . . , ğ‘ âˆ’ 1
```
must have a control vector ğ‘¢0, ğ‘¢1, . . . , ğ‘¢ğ‘âˆ’1 that satisfies
ğœ•ğ‘”
```
ğœ•áµ† (ğ‘¥ğ‘–, ğ‘¢ğ‘–) = ğœ†ğ‘–+1
```
ğœ•ğ‘“
```
ğœ•áµ† (ğ‘¥ğ‘–, ğ‘¢ğ‘–) for ğ‘– = 0, . . . , ğ‘ âˆ’ 1.
```
If the end state ğ‘¥ğ‘ is not specified, then the final costate must satisfy
```
ğœ†ğ‘ = âˆ’ğºâ€²(ğ‘¥ğ‘ ).
```
3.2. Optimal ğ‘-Step Process Control 39
With this principle, we have a recursive condition on the sequence ğœ†0, . . . , ğœ†ğ‘ of
```
costates (basically a sequence of Lagrange multipliers) which combine with the recur-
```
sively defined state sequence ğ‘¥0, . . . , ğ‘¥ğ‘ to define conditions that an optimal control
sequence ğ‘¢0, . . . , ğ‘¢ğ‘âˆ’1 must satisfy.
Applying the principle typically proceeds through three steps:
First Step: Solve
ğœ•ğ‘”
```
ğœ•ğ‘¢ (ğ‘¥ğ‘–, ğ‘¢ğ‘–) = ğœ†ğ‘–+1
```
ğœ•ğ‘“
```
ğœ•ğ‘¢ (ğ‘¥ğ‘–, ğ‘¢ğ‘–)
```
for ğ‘¢ğ‘– in terms of ğ‘¥ğ‘– and ğœ†ğ‘–+1 for ğ‘– = 0, . . . , ğ‘ âˆ’ 1.
Second Step: Substitute these ğ‘¢ğ‘– values to set up two recursions:
```
ğ‘¥ğ‘–+1 = ğ‘“(ğ‘¥ğ‘–, ğ‘¢ğ‘–),
```
```
ğœ†ğ‘– = ğœ†ğ‘–+1ğœ•ğ‘“ğœ•ğ‘¥ (ğ‘¥ğ‘–, ğ‘¢ğ‘–) âˆ’ ğœ•ğ‘”ğœ•ğ‘¥ (ğ‘¥ğ‘–, ğ‘¢ğ‘–)
```
for ğ‘– = 0, 1, . . . , ğ‘ âˆ’ 1. Note that the recursion for ğœ†ğ‘– seems to go backwards in time.
Weird.
Third Step: Solve the recursions. You will need two boundary conditions, one of which
```
is that given by the starting point ğ‘¥0. If ğ‘¥ğ‘ is free, use ğœ†ğ‘ = âˆ’ğºâ€²(ğ‘¥ğ‘ ) as the second
```
```
condition (see Example 3.2). If both ğ‘¥0 and ğ‘¥ğ‘ are specified, use these as endpoint
```
```
values and this will determine the value for ğœ†ğ‘ (see Example 3.3). Note that in most
```
recursions, one thinks of starting at some location and proceeding forward. In this
case, and in many of the Pontryagin solutions, we usually have some form of two-point
boundary problems, where a starting and ending location are specified.
Example 3.2: ğ‘-Step Bocce
```
Generalizing Example 3.1 to ğ‘ steps, we have a controlled process ğ‘¥ğ‘›+1 = ğ‘“(ğ‘¥ğ‘›, ğ‘¢ğ‘›)
```
```
= ğ‘¥ğ‘› + ğ‘¢ğ‘› for ğ‘› = 0, . . . , ğ‘ âˆ’ 1 with running costs ğ‘”(ğ‘¥, ğ‘¢) = ğ‘¢2/ğ‘¥ and end payoff
```
```
ğº(ğ‘¥) = ğ‘¥, for net payoff
```
ğ½ = ğ‘¥ğ‘ âˆ’
ğ‘âˆ’1
âˆ‘
ğ‘–=0
ğ‘¢2ğ‘–
ğ‘¥ğ‘–.
40 Chapter 3. Control: A Discrete Start
Applying Principle 0 we have
```
ğ‘“(ğ‘¥, ğ‘¢) = ğ‘¥ + ğ‘¢,
```
```
ğº(ğ‘¥) = ğ‘¥,
```
```
ğ‘”(ğ‘¥, ğ‘¢) = âˆ’ áµ†2ğ‘¥ .
```
First Step: Solve for ğ‘¢ğ‘– from
ğœ•ğ‘”
```
ğœ•áµ† (ğ‘¥ğ‘–, ğ‘¢ğ‘–) = ğœ†ğ‘–+1
```
ğœ•ğ‘“
```
ğœ•áµ† (ğ‘¥ğ‘–, ğ‘¢ğ‘–),
```
âˆ’ 2áµ†ğ‘–ğ‘¥ğ‘–= ğœ†ğ‘–+1
which determines our control
ğ‘¢ğ‘– = âˆ’ 12 ğœ†ğ‘–+1 ğ‘¥ğ‘–
in terms of state ğ‘¥ğ‘– and costate ğœ†ğ‘–+1.
```
Second Step: Set up the recursions; substitute ğ‘¢ğ‘–:
```
```
ğ‘¥ğ‘–+1 = ğ‘“(ğ‘¥ğ‘–, ğ‘¢ğ‘–)
```
= ğ‘¥ğ‘– + ğ‘¢ğ‘–
= ğ‘¥ğ‘– âˆ’ 12 ğœ†ğ‘–+1ğ‘¥ğ‘–,
```
ğœ†ğ‘– = ğœ†ğ‘–+1ğœ•ğ‘“ğœ•ğ‘¥ (ğ‘¥ğ‘–, ğ‘¢ğ‘–) âˆ’ ğœ•ğ‘”ğœ•ğ‘¥ (ğ‘¥ğ‘–, ğ‘¢ğ‘–)ğ‘š
```
= ğœ†ğ‘–+1 âˆ’ áµ†
2ğ‘–
ğ‘¥2ğ‘–
= ğœ†ğ‘–+1 âˆ’ 14 ğœ†2ğ‘–+1
```
= ğœ†ğ‘–+1 (1 âˆ’ 14 ğœ†ğ‘–+1) .
```
Third Step: Solve the recursions. In this case ğ‘¥ğ‘ is not specified, and we have
```
ğœ†ğ‘ = âˆ’ğºâ€²(ğ‘¥ğ‘ ) = âˆ’1. Since the recursion for ğœ†ğ‘– does not involve ğ‘¥ğ‘–, we now have
```
the costate sequence completely defined for any ğ‘. Start with ğœ†ğ‘ = âˆ’1 and work
```
backwards to get ğœ†ğ‘âˆ’1, . . . , ğœ†0 by applying the recursion ğœ†ğ‘– = ğœ†ğ‘–+1 (1 âˆ’ 14 ğœ†ğ‘–+1):
```
ğœ†ğ‘ = âˆ’1,
```
ğœ†ğ‘âˆ’1 = (âˆ’1) (1 âˆ’ 14 (âˆ’1)) = âˆ’ 54 ,
```
```
ğœ†ğ‘âˆ’2 = (âˆ’ 54 ) (1 âˆ’ 14 (âˆ’ 54 )) = âˆ’ 10564 ,
```
â‹® â‹® â‹®
We then use the ğœ†ğ‘– sequence to determine our controls ğ‘¢ğ‘– and this allows us
to solve for the optimal trajectory ğ‘¥0, ğ‘¥1, . . . , ğ‘¥ğ‘ and control sequence ğ‘¢0, ğ‘¢1, . . . ,
ğ‘¢ğ‘âˆ’1.
3.2. Optimal ğ‘-Step Process Control 41
For ğ‘ = 5 and ğ‘¥0 = 1 we achieve a maximum ğ½ = 6.98 . . . using
ğ‘¥ ğœ† ğ‘¢
0 1.00 âˆ’6.99 . . . 1.83 . . .
1 2.83 . . . âˆ’3.65 . . . 3.27 . . .
2 6.09 . . . âˆ’2.31 . . . 5.00 . . .
3 11.09 . . . âˆ’1.64 . . . 6.93 . . .
4 18.03 . . . âˆ’1.25 . . . 9.01 . . .
5 27.04 . . . âˆ’1.00 âˆ’
Note that it is the state-costate system
ğ‘¥ğ‘–+1 = ğ‘¥ğ‘– âˆ’ 12 ğœ†ğ‘–+1 ğ‘¥ğ‘–,
```
ğœ†ğ‘– = ğœ†ğ‘–+1 (1 âˆ’ 14 ğœ†ğ‘–+1)
```
```
with boundary values ğ‘¥0 = 1 and ğœ†5 = âˆ’1 that drives this solution (Figure 3.1).
```
Figure 3.1. The optimal trajectory for ğ‘ = 5 plotted in state-
costate space.
This is typical for optimal control problems: the optimization principle cre-
ates a dynamic set of Lagrange multipliers, or costates, and it is the state-costate
dynamics fitted to the initial conditions that create an optimal solution. It is of
interest that the costates seem to be working backwards in time.
42 Chapter 3. Control: A Discrete Start
The following applies Principle 0 to the control problem in Example 1.4 and demon-
strates how the principle works when both ğ‘¥0 and ğ‘¥ğ‘ are specified, and we are mini-
mizing a cost.
Example 3.3
Consider an ğ‘-step process with fixed end locations ğ‘¥0 = 0, ğ‘¥ğ‘ = 100, control
```
dynamic ğ‘“(ğ‘¥, ğ‘¢) = ğ‘¥ + ğ‘¢, and running costs ğ‘”(ğ‘¥, ğ‘¢) = ğ‘¥ + ğ‘¢2/2, as in Example
```
1.4.
Take ğ‘ = 10. For equal steps, ğ‘¢0 = ğ‘¢1 = â‹¯ = ğ‘¢9 = 10, we calculate a cost of
ğ½ = 950. We donâ€™t expect this to be optimal, as running costs increase as we move
farther down the number line, so perhaps some adjustments will improve the
outcome. Reviewing Example 1.4, we suspect that we might improve by taking
smaller steps at first to reduce the number of steps we have to take in the more
expensive regions farther down the number line.
Applying Principle 0 to find the optimal solution, the costate recursion is
```
ğœ†ğ‘– = ğœ†ğ‘–+1ğœ•ğ‘“ğœ•ğ‘¥ (ğ‘¥ğ‘–, ğ‘¢ğ‘–) âˆ’ ğœ•ğ‘”ğœ•ğ‘¥ (ğ‘¥ğ‘–, ğ‘¢ğ‘–),
```
ğœ†ğ‘– = ğœ†ğ‘–+1 âˆ’ 1.
An optimal control must satisfy
ğœ•ğ‘”
```
ğœ•áµ† (ğ‘¥ğ‘–, ğ‘¢ğ‘–) = ğœ†ğ‘–+1
```
ğœ•ğ‘“
```
ğœ•áµ† (ğ‘¥ğ‘–, ğ‘¢ğ‘–),
```
ğ‘¢ğ‘– = ğœ†ğ‘–+1.
If we assume a terminal costate value of ğœ†10 = ğ¾, we have
ğ‘¢9 = ğœ†10 = ğ¾,
ğ‘¢8 = ğœ†9 = ğ¾ âˆ’ 1,
â‹® â‹®
ğ‘¢0 = ğœ†1 = ğ¾ âˆ’ 9.
Note that the value of the control increases by one unit each step, and we verify
that the pattern we observed in Example 1.4 continues to apply.
Our endpoint restriction then imposes
100 = ğ‘¢0 + ğ‘¢1 + â‹¯ + ğ‘¢9
```
= (ğ¾ âˆ’ 9) + (ğ¾ âˆ’ 8) + â‹¯ + ğ¾
```
```
= 10ğ¾ âˆ’ (9 + 8 + â‹¯ + 1)
```
= 10ğ¾ âˆ’ 45
making ğ¾ = 14.5 and controls ğ‘¢0 = 5.5, ğ‘¢1 = 6.5, . . . , ğ‘¢9 = 14.5 for a total
computed cost of ğ½ = 908.75. With ğ‘¢ğ‘– = ğœ†ğ‘–, the resulting state-costate trajectory
3.3. Deriving Principle 0 43
```
(ğ‘¥ğ‘–, ğœ†ğ‘–) for ğ‘– = 0, . . . , ğ‘ âˆ’ 1 is defined by
```
ğ‘¥ğ‘–+1 = ğ‘¥ğ‘– + ğœ†ğ‘–,
ğœ†ğ‘– = ğœ†ğ‘–+1 âˆ’ 1
with boundary conditions ğ‘¥0 = 0, ğ‘¥10 = 100 and is shown in Figure 3.2.
Figure 3.2. The optimal trajectory for ğ‘ = 10 plotted in state-
costate space.
Examining these discrete cases carefully reveals the foundational concepts of Pon-
tryaginâ€™s theory and demonstrates how applying Pontryaginâ€™s principle avoids the com-
pounding complexity we encountered in Chapter 1. Going deeper into the discrete
cases would involve a treatment of solving two-point boundary problems in discrete
dynamics. We avoid this, as we want to focus on differential equation models covered
in subsequent chapters.
The next section is a derivation of the principle in the general discrete case, allow-
ing some mathematical insight into the machinery that drives these solutions.
3.3 Deriving Principle 0
To derive Principle 0, consider a system that starts in an initial position ğ‘¥0 and evolves
according to a controlled process
```
ğ‘¥ğ‘–+1 = ğ‘“(ğ‘¥ğ‘–, ğ‘¢ğ‘–) for ğ‘– = 0, 2, . . . , ğ‘ âˆ’ 1
```
for a fixed number ğ‘ steps. We reformulate this control process into ğ‘ constraints that
will work with Lagrange multipliers:
```
ğ‘“ğ‘–(ğ‘¥1, . . . , ğ‘¥ğ‘ , ğ‘¢0, . . . , ğ‘¢ğ‘ ) = ğ‘“(ğ‘¥ğ‘–âˆ’1, ğ‘¢ğ‘–âˆ’1) âˆ’ ğ‘¥ğ‘– = 0 for ğ‘– = 1, 2, . . . , ğ‘.
```
With these constraints, we want to operate the controls to optimize performance
```
ğ½(ğ‘¥1, . . . , ğ‘¥ğ‘ , ğ‘¢0, . . . , ğ‘¢ğ‘âˆ’1) = ğº(ğ‘¥ğ‘ ) + âˆ‘ğ‘âˆ’1ğ‘–=0 ğ‘”(ğ‘¥ğ‘–, ğ‘¢ğ‘–)
```
```
= ğº(ğ‘¥ğ‘ ) + ğ‘”(ğ‘¥0, ğ‘¢0) + â‹¯ + ğ‘”(ğ‘¥ğ‘âˆ’1, ğ‘¢ğ‘âˆ’1).
```
```
(3.6)
```
Note that we have 2ğ‘ variables, ğ‘¥1, . . . , ğ‘¥ğ‘ , ğ‘¢0, . . . , ğ‘¢ğ‘âˆ’1, and ğ‘ constraints for an
ğ‘-step process.
44 Chapter 3. Control: A Discrete Start
The problem is now expressed in a way that we can apply Lagrange multipliers,
and we would look for solutions that satisfy
```
âˆ‡ğ½ = ğœ†1âˆ‡ğ‘“1 + ğœ†2âˆ‡ğ‘“2 + â‹¯ + ğœ†ğ‘ âˆ‡ğ‘“ğ‘ (3.7)
```
where each gradient is the vector of partial derivatives for the full set of 2ğ‘ variables, ğ‘¥1,
. . . , ğ‘¥ğ‘› and ğ‘¢0, . . . , ğ‘¢ğ‘›âˆ’1, and this creates a set of 2ğ‘ equations, each equation resulting
from the partial derivative of one of the variables.
Letâ€™s look at these partial derivatives carefully, separating out the derivatives with
respect to controls and with respect to state. Keep in mind the distinction between a
```
derivative and a differential operator (see Section 2.2).
```
```
First consider the derivatives in equation (3.7) with respect to the controls ğ‘¢0, . . . ,
```
```
ğ‘¢ğ‘âˆ’1. For the left-hand side of (3.7) we differentiate with respect to ğ‘¢ğ‘– and get
```
ğœ•ğ½
ğœ•áµ†ğ‘–=
ğœ•
```
ğœ•áµ†ğ‘–(ğº(ğ‘¥ğ‘ ) + ğ‘”(ğ‘¥0, ğ‘¢0) + â‹¯ + ğ‘”(ğ‘¥ğ‘âˆ’1, ğ‘¢ğ‘âˆ’1))
```
```
= ğœ•ğ‘”ğœ•áµ† (ğ‘¥ğ‘–, ğ‘¢ğ‘–).
```
for each ğ‘– = 0, 1, . . . , ğ‘ âˆ’ 1.
```
For the right-hand side of equation (3.7), note that for the constraint functions ğ‘“1,
```
. . . , ğ‘“ğ‘ , only the constraint ğ‘“ğ‘–+1 depends on ğ‘¢ğ‘–, which is the control we operate to get
from ğ‘¥ğ‘– to ğ‘¥ğ‘–+1. We have
ğœ•
ğœ•áµ†ğ‘–ğ‘“ğ‘–+1 =
ğœ•
```
ğœ•áµ†ğ‘–(ğ‘“(ğ‘¥ğ‘–, ğ‘¢ğ‘–) âˆ’ ğ‘¥ğ‘–+1) =
```
ğœ•ğ‘“
```
ğœ•áµ† (ğ‘¥ğ‘–, ğ‘¢ğ‘–),
```
```
so considering just the partial derivative with respect to ğ‘¢ğ‘– in equation (3.7) we get
```
ğœ•ğ‘”
```
ğœ•áµ† (ğ‘¥ğ‘–, ğ‘¢ğ‘–) = ğœ†ğ‘–+1
```
ğœ•ğ‘“
```
ğœ•áµ† (ğ‘¥ğ‘–, ğ‘¢ğ‘–)
```
which we can solve to determine our control ğ‘¢ğ‘– in terms of ğ‘¥ğ‘– and ğœ†ğ‘–+1 for ğ‘– = 0, . . . ,
ğ‘ âˆ’1. This can tell us how to operate our control in terms of state and costate variables.
```
Second, consider the derivatives in equation (3.7) with respect to the state variables
```
```
ğ‘¥1, . . . , ğ‘¥ğ‘ . For the left-hand side, differentiating equation (3.6) with respect to ğ‘¥ğ‘– yields
```
ğœ•ğ½
ğœ•ğ‘¥ğ‘–=
ğœ•
```
ğœ•ğ‘¥ğ‘–(ğº(ğ‘¥ğ‘ ) + âˆ‘
```
ğ‘âˆ’1
```
ğ‘–=0 ğ‘”(ğ‘¥ğ‘–, ğ‘¢ğ‘–))
```
```
= ğœ•ğ‘”ğœ•ğ‘¥ (ğ‘¥ğ‘–, ğ‘¢ğ‘–)
```
```
for each ğ‘– = 1, . . . , ğ‘ âˆ’ 1. For the right-hand side of equation (3.7) note that for the
```
constraint functions ğ‘“1, . . . , ğ‘“ğ‘ , only the constraints ğ‘“ğ‘– and ğ‘“ğ‘–+1 depend on ğ‘¥ğ‘–, and so
we get
ğœ•
ğœ•ğ‘¥ğ‘–ğ‘“ğ‘– =
ğœ•
```
ğœ•ğ‘¥ğ‘–(ğ‘“(ğ‘¥ğ‘–âˆ’1, ğ‘¢ğ‘–âˆ’1) âˆ’ ğ‘¥ğ‘–) = âˆ’1,
```
ğœ•
ğœ•ğ‘¥ğ‘–ğ‘“ğ‘–+1 =
ğœ•
```
ğœ•ğ‘¥ğ‘–(ğ‘“(ğ‘¥ğ‘–, ğ‘¢ğ‘–) âˆ’ ğ‘¥ğ‘–+1) =
```
ğœ•ğ‘“
```
ğœ•ğ‘¥ (ğ‘¥ğ‘–, ğ‘¢ğ‘–).
```
```
Considering just the partial derivative with respect to ğ‘¥ğ‘– in equation (3.7) we have
```
ğœ•ğ‘”
```
ğœ•ğ‘¥ (ğ‘¥ğ‘–, ğ‘¢ğ‘–) = ğœ†ğ‘–+1
```
ğœ•ğ‘“
```
ğœ•ğ‘¥ (ğ‘¥ğ‘–, ğ‘¢ğ‘–) âˆ’ ğœ†ğ‘–,
```
the solution of which would yield the recursion relation for ğœ†ğ‘–â€™s for ğ‘– = 0, . . . , ğ‘ âˆ’ 1.
Exercises 45
```
Considering where ğ‘¥ğ‘ appears in equation (3.7) and taking the derivative with
```
respect to ğ‘¥ğ‘ yields
```
ğºâ€²(ğ‘¥ğ‘ ) = ğœ•ğœ•ğ‘¥ğ‘ (ğ‘“(ğ‘¥ğ‘âˆ’1, ğ‘¢ğ‘âˆ’1) âˆ’ ğ‘¥ğ‘ ) = âˆ’ğœ†ğ‘ .
```
This gives us one endpoint of the recursion, which we solve backwards to get the re-
maining values ğœ†ğ‘âˆ’1, . . . , ğœ†0.
Third, using the endpoint conditions, we can then solve the recurrence relation-
```
ship for ğ‘¥ğ‘–â€™s (state variables) and ğœ†ğ‘–â€™s (costate variables) and together these will inform
```
us how to operate our control ğ‘¢ğ‘–.
This, in a nutshell, is the fundamental application of Pontryaginâ€™s optimal principle
for the discrete case and sets the template for the technique in the chapters to come.
Keep in mind that this establishes necessary conditions for a control to be optimal.
When applying these techniques we still need to address whether the controls are local
maxima or minima and if the control is a global optimum.
While discrete dynamical systems are the most natural way to introduce these op-
timal control techniques, we will avoid going deeper into discrete systems and differ-
ence equations and move on to our main topic of continuous dynamics and differential
equations.
Key Points
In this chapter we used Lagrange multipliers to solve basic multistep discrete con-
trol problems, thereby working around the difficulty of compounding complexity pre-
sented in Chapter 1. We stated the method as Principle 0, with an outline proof.
This frames the basic concepts and format for solving optimal control problems.
Pontryaginâ€™s technique sets up a costate space of Lagrange multipliers and imposes a
dynamic on these costates that must be satisfied in order for a control to be optimal.
This recasts a one-dimensional control problem in state space into a problem of explor-
ing trajectories in two-dimensional state-costate space.
Exercises
```
Exercise 3.1(s). Suppose you start at location ğ‘¥0 = 100 on the ğ‘¥-axis. You have four
```
turns, and at each turn you have the option of moving a distance ğ‘¢ to the left, making
```
your control dynamic ğ‘¥ğ‘–+1 = ğ‘“(ğ‘¥ğ‘–, ğ‘¢ğ‘–) = ğ‘¥ğ‘– âˆ’ ğ‘¢ğ‘–. On each move, you collect a payoff
```
```
of ğ‘”(ğ‘¥ğ‘–, ğ‘¢ğ‘–) = ğ‘¥ğ‘– âˆ’ ğ‘¢2ğ‘– /2 dollars. At the end of four turns, you have to pay the bank 4ğ‘¥4
```
dollars. So if you donâ€™t move at all, you collect 100 dollars on each of four turns and
then pay the bank 400 dollars, for a break-even game. Can you make money at this
game? What is the optimal strategy?
```
Apply Principle 0 with ğ‘“(ğ‘¥, ğ‘¢) = ğ‘¥ âˆ’ ğ‘¢, ğ‘”(ğ‘¥, ğ‘¢) = ğ‘¥ âˆ’ ğ‘¢2/2, and ğº(ğ‘¥) = âˆ’4ğ‘¥. You
```
have ğ‘¥0 = 100 and ğ‘ = 4. Complete the following steps:
```
(a) Solve ğœ•ğ‘”ğœ•áµ† (ğ‘¥ğ‘–, ğ‘¢ğ‘–) = ğœ†ğ‘–+1ğœ•ğ‘“ğœ•áµ† (ğ‘¥ğ‘–, ğ‘¢ğ‘–) to get ğ‘¢ğ‘– in terms of ğœ†ğ‘–+1.
```
```
(b) Solve ğœ†ğ‘– = ğœ†ğ‘–+1ğœ•ğ‘“ğœ•ğ‘¥ (ğ‘¥ğ‘–, ğ‘¢ğ‘–) âˆ’ ğœ•ğ‘”ğœ•ğ‘¥ (ğ‘¥ğ‘–, ğ‘¢ğ‘–) to get a recursion for ğœ†ğ‘– in terms of ğœ†ğ‘–+1.
```
```
(c) Solve ğœ†ğ‘ = ğºâ€²(ğ‘¥ğ‘ ) to get a value for ğœ†4.
```
46 Chapter 3. Control: A Discrete Start
```
(d) Compute the resulting values for ğœ†1, . . . , ğœ†3, ğ‘¢0, . . . , ğ‘¢3, and ğ‘¥0, . . . , ğ‘¥4
```
```
(e) Compute the net payoff ğ½ = ğº(ğ‘¥4) + âˆ‘3ğ‘–=0 ğ‘”(ğ‘¥ğ‘–, ğ‘¢ğ‘–). Did you make money?
```
```
Exercise 3.2(s). Suppose you start at location ğ‘¥0 on the ğ‘¥-axis. You have ğ‘ moves, and
```
at each move you can multiply your position by an amount ğ‘¢ > 0 for a cost of ğ‘¥ğ‘¢2. So
```
your control dynamic is ğ‘“(ğ‘¥ğ‘–, ğ‘¢ğ‘–) = ğ‘¥ğ‘–ğ‘¢ğ‘– and your running cost (as a negative payoff) is
```
```
ğ‘”(ğ‘¥ğ‘–, ğ‘¢ğ‘–) = âˆ’ğ‘¥ğ‘–ğ‘¢2ğ‘– . At the end of ğ‘ moves you collect ğº(ğ‘¥ğ‘ ) = 4ğ‘¥ğ‘ dollars. Find your
```
optimal strategy using Principle 0 with the following steps:
```
(a) Solve ğœ•ğ‘”ğœ•áµ† (ğ‘¥ğ‘–, ğ‘¢ğ‘–) = ğœ†ğ‘–+1ğœ•ğ‘“ğœ•áµ† (ğ‘¥ğ‘–, ğ‘¢ğ‘–) to get ğ‘¢ğ‘– in terms of ğœ†ğ‘–+1.
```
```
(b) Solve ğœ†ğ‘– = ğœ†ğ‘–+1ğœ•ğ‘“ğœ•ğ‘¥ (ğ‘¥ğ‘–, ğ‘¢ğ‘–) âˆ’ ğœ•ğ‘”ğœ•ğ‘¥ (ğ‘¥ğ‘–, ğ‘¢ğ‘–) to get a recursion for ğœ†ğ‘– in terms of ğœ†ğ‘–+1.
```
```
(c) Solve ğœ†ğ‘ = ğºâ€²(ğ‘¥ğ‘ ) to get a value for ğœ†ğ‘ .
```
```
(d) For ğ‘¥0 = 1 and ğ‘ = 4 compute the values of ğœ†1, . . . , ğœ†4, ğ‘¢0, . . . , ğ‘¢3, and
```
ğ‘¥0, . . . , ğ‘¥4
```
(e) Compute the resulting net payoff ğ½ = ğº(ğ‘¥4) + âˆ‘3ğ‘–=0 ğ‘”(ğ‘¥ğ‘–, ğ‘¢ğ‘–). Can you make
```
more money with larger or smaller ğ‘?
```
Exercise 3.3(h). Repeat Example 3.3 with ğ‘ = 100. That is, we have a process with
```
```
fixed end locations ğ‘¥0 = 0, ğ‘¥ğ‘ = 100, control dynamic ğ‘“(ğ‘¥, ğ‘¢) = ğ‘¥ + ğ‘¢, and running
```
```
costs ğ‘”(ğ‘¥, ğ‘¢) = ğ‘¥ +ğ‘¢2/2 with ğ‘ = 100 steps. Comment on anything unusual you notice
```
about your solution.
```
Exercise 3.4(s). Suppose you have a control dynamic ğ‘“(ğ‘¥, ğ‘¢) = ğ‘¥ + ğ‘¢, running costs
```
```
(negative values) of ğ‘”(ğ‘¥, ğ‘¢) = âˆ’(ğ‘¥+ğ‘¢)2, an end payoff of ğº(ğ‘¥ğ‘ ) = 100ğ‘¥ğ‘ , and a starting
```
value ğ‘¥0 > 0. Your optimal strategy is to first move from ğ‘¥0 to ğ‘¥1 = 0, then stay at ğ‘¥ = 0
until your last move, and then move to ğ‘¥ğ‘ = 50. Apply Principle 0 to derive this result.
Exercise 3.5. In Exercise 1.2, we were moving a Bocce ball
```
ğ‘¥ğ‘›+1 = ğ‘“(ğ‘¥ğ‘›, ğ‘¢ğ‘›) = ğ‘¥ğ‘› + ğ‘¢ğ‘›
```
with endpoint conditions ğ‘¥0 = 0 and ğ‘¥ğ‘ = ğµ for a given ğµ > 0, with running costs
```
ğ‘”(ğ‘¢) = ğ‘¢2.
```
```
(a) Apply Principle 0 to justify our assumption that the optimal control is constant
```
ğ‘¢0 = ğ‘¢1 = â‹¯ = ğ‘¢ğ‘ = ğµ/ğ‘.
```
(b) What is the optimal payoff ğ½ as a function, ğ½(ğµ, ğ‘), of ending location ğµ and
```
number of steps ğ‘? What happens to the cost as ğ‘ â†’ âˆ?
Exercise 3.6. Repeat Exercise 3.5 where the end location of the ball is not specified,
```
but at the end of ğ‘ moves you receive a payoff of âˆšğ‘¥ğ‘ dollars (as in Exercise 1.3). So
```
for a given ğ‘, the net payoff is
âˆšğ‘¥ğ‘ âˆ’
ğ‘âˆ’1
âˆ‘
ğ‘›=0
ğ‘¢2ğ‘›.
Exercises 47
```
We still have ğ‘¥0 = 0 and control dynamics are ğ‘¥ğ‘›+1 = ğ‘“(ğ‘¥ğ‘›, ğ‘¢ğ‘›) = ğ‘¥ğ‘› + ğ‘¢ğ‘›.
```
```
(a) What is the optimal solution? What is the optimal payoff?
```
```
(b) Consider the payoff function ğ½(ğµ, ğ‘) from Exercise 3.5. Show that the solution
```
```
to this exercise corresponds to a maximum of âˆšğµ âˆ’ ğ½(ğµ, ğ‘).
```
Exercise 3.7. What would Principle 0 look like for higher dimensions? What if you
had two controls, maybe one for horizontal movement and one for vertical movement,
as in Exercise 1.4. What would you have to add to Principle 0 to have it apply to this
case?
4
First Principle
In developing Principle 0, we considered a controlled system defined by discrete time
dynamics, explored why these systems are difficult to optimize, and demonstrated how
the application of Lagrange multipliers simplifies the problem by creating a state-costate
system, with Principle 0 specifying conditions that must be satisfied by an optimal con-
trol.
Most control problems involve continuous systems that are modeled with differen-
tial equations, and these continuous time problems will be the main focus of our study.
Building on our understanding of discrete systems, this chapter introduces the foun-
dational ideas for continuous systems and introduces Principles I and II that extend
Principle 0 to continuous systems and represent a more complete statement of Pon-
tryaginâ€™s principle. Principle I addresses autonomous systems and Principle II allows
for time dependence.
These principles all have a similar form: optimization imposes dynamics on a
structure of Lagrange multipliers, creating a state-costate dynamical system that de-
fines conditions necessary for a control to be optimal.
4.1 One Dimension, Fixed Ends
Consider the differential equation
```
ğ‘¥â€² = ğ‘“(ğ‘¥, ğ‘¢)
```
on a time interval ğ‘¡ âˆˆ [0, ğ‘‡] with boundary conditions
```
ğ‘¥(0) = ğ´ and ğ‘¥(ğ‘‡) = ğµ.
```
```
The variable ğ‘¢ is the control; it is a function of time, and we can select any function
```
ğ‘¢ we want from a prescribed set ğ’° of allowed controls. We must operate our control
to attain the given boundary conditions. From the set of allowed controls that attain
these endpoints, we want the one that optimizes
ğ½ = âˆ«
ğ‘‡
0
```
ğ‘”(ğ‘¥, ğ‘¢) ğ‘‘ğ‘¡.
```
49
50 Chapter 4. First Principle
If this is a payoff, we want to maximize it, and if it is a cost, we want to minimize it.
These being equivalent problems, we will just speak of optimizing and refer to ğ½ as the
performance.
Example 4.1: Integrator
```
Consider the simple controlled system ğ‘¥â€² = ğ‘¢ with boundary conditions ğ‘¥(0) =
```
```
âˆ’1 and ğ‘¥(6) = 1 and we wish to find a control that minimizes cost
```
ğ½ = âˆ«
6
0
ğ‘¥2 + ğ‘¢2 ğ‘‘ğ‘¡.
In other words,we want to move a point from position ğ‘¥ = âˆ’1 to position ğ‘¥ = 1
in ğ‘‡ = 6 time units, and we control the velocity of the point. The running cost
is the sum of the squares of position and velocity: it is expensive to be farther
away from ğ‘¥ = 0, and it is expensive to move quickly. We assume ğ‘¢ must be a
piecewise continuous function.
```
A constant velocity control with ğ‘¢ â‰¡ 1/3 creates trajectory ğ‘¥(ğ‘¡) = ğ‘¡/3 âˆ’ 1
```
```
(Figure 4.1) which satisfies the initial and endpoint conditions and would cost
```
```
(âˆ— check this âˆ—)
```
ğ½ = âˆ«
6
0
```
( ğ‘¡3 âˆ’ 1)
```
2
- ( 13 )
2
ğ‘‘ğ‘¡ = 8/3 = 2.666 . . . .
Figure 4.1. State and control plotted against time, constant control.
Itâ€™s expensive to be farther away from zero, so maybe we should stay at ğ‘¥ = 0
```
for a little while (its free!). Perhaps move from ğ‘¥ = âˆ’1 to ğ‘¥ = 0 at constant speed
```
in two time units, then stay at ğ‘¥ = 0 for two time units, and move from ğ‘¥ = 0
```
to ğ‘¥ = 1 at constant speed in the last two time units (Figure 4.2). This would
```
```
cost ğ½ = 7/3 = 2.333 . . . (âˆ— check this âˆ—), so weâ€™ve improved our initial proposed
```
control.
4.1. One Dimension, Fixed Ends 51
Figure 4.2. State and control plotted against time, varying control.
There are many other ways we could operate our control to get from ğ‘¥ = âˆ’1
to ğ‘¥ = 1 in six time units. The question is, out of all such controls which one will
cost the very least?
Example 4.2: King Tinyâ€™s Plan
King Tiny is the beloved king and absolute dictator of Tinyland, and the size of
the current economy of Tinyland, measured in dollars, is exactly one dollar. If the
King of Tinyland reinvests all of his domestic product back into capital stock, his
```
economy will grow by 20% per year: ğ‘¥â€² = .2ğ‘¥ (where ğ‘¥ is the size of the economy
```
```
in dollars). The King wants to take out an amount ğ‘¢ â‰¥ 0 (in dollars per day) and
```
allow his loyal subjects to consume it for their enjoyment, so his resulting growth
is ğ‘¥â€² = .2ğ‘¥ âˆ’ ğ‘¢. Now, consumption is a funny thing: the more you consume the
less enjoyment you seem to get out of each additional unit of consumption, so
letâ€™s say the Kingdomâ€™s pleasure is measured as the concave downward function
âˆšğ‘¢.
The King starts with an economy of one dollar and declares that he will dou-
```
ble it in ten years: ğ‘¥(0) = 1 and ğ‘¥(10) = 2. With 20% growth, he can easily do
```
this, with money to spare. How to best consume the spare money? The ques-
tion is, how can he meet his goal of doubling the economy while maximizing the
Kingdomâ€™s pleasure
ğ½ = âˆ«
10
0
âˆšğ‘¢ ğ‘‘ğ‘¡?
The following is our first version of Pontryaginâ€™s maximal principle for continuous
systems and will help solve these two examples by setting differential conditions that
an optimal solution must satisfy.
52 Chapter 4. First Principle
OPTIMAL PRINCIPLE I
Local optimum, fixed duration, fixed endpoint, time independent, one dimension
Consider the controlled system
```
ğ‘¥â€² = ğ‘“(ğ‘¥, ğ‘¢), ğ‘¥, ğ‘¡ âˆˆ â„, ğ‘¢ âˆˆ ğ’°,
```
with fixed endpoint and end time conditions
```
ğ‘¥(0) = ğ´, ğ‘¥(ğ‘‡) = ğµ, ğ´, ğµ, ğ‘‡ given,
```
and objective function
ğ½ = âˆ«
ğ‘‡
0
```
ğ‘”(ğ‘¥, ğ‘¢) ğ‘‘ğ‘¡.
```
Define the Hamiltonian
```
ğ» = ğ»(ğ‘¥, ğ‘¢, ğœ†) = ğ‘”(ğ‘¥, ğ‘¢) + ğœ†ğ‘“(ğ‘¥, ğ‘¢)
```
and the costate equation
```
ğœ†â€² = âˆ’ ğœ•ğ»ğœ•ğ‘¥ (ğ‘¥, ğ‘¢, ğœ†).
```
Then a locally optimal control ğ‘¢ must satisfy
ğœ•ğ»
```
ğœ•ğ‘¢ (ğ‘¥, ğ‘¢, ğœ†) = 0
```
and the optimal control ğ‘¢ that optimizes ğ½ will optimize ğ» at all times.
Furthermore, ğ» is constant on optimal trajectories.
The principle is not at all intuitive, and the role of the costate variable is quite
mysterious at first, but it follows the ideas developed in the discrete case in Chapter 3.
We begin by working through some examples to build up intuition, and then we will
return to more abstract considerations of the principle.
This principle uses the first derivative, ğœ•ğ»ğœ•áµ† , to specify a locally optimal solution, or
the best among an open set of nearby controls in a set ğ’° of allowable controls. This is
analogous to the first derivative test in calculus to find a local maximum or minimum
in the interior of an interval. We explore this concept more carefully in Section 4.3.3.
Some optimal control problems involve solutions at the very boundary of allowable
controls, as explored later in Chapter 10.
```
Note that an optimal control â€œwill optimize ğ» at all timesâ€ (to maximize ğ½ we max-
```
```
imize ğ», and to minimize ğ½ we minimize ğ»). Thus, to optimize an overall measure of
```
performance ğ½, we must optimize ğ» at each instant. This transforms a problem of opti-
mizing a global measure ğ½ in the state space defined by ğ‘¥, to optimizing a local measure
ğ» in the state-costate space defined by ğ‘¥ and ğœ†. This is the power and beauty of Pon-
tryaginâ€™s optimality principles. We can optimize globally in state space by optimizing
locally in state-costate space.
4.1. One Dimension, Fixed Ends 53
```
Finally, note that ğ» is constant on optimal trajectories; we can explore the set of
```
optimal trajectories by examining the level curves of ğ». This is analogous to the conser-
vation of energy in physical systems where the Hamiltonian represents the total energy
of the system. We return to this idea in Section 5.1
There is much here to unpack, but we will take it a step at a time. For now we
focus on applying the principle. Note how the steps for applying Principle I replicate
those of applying Principle 0 in Section 3.2.
Apply Principle I using the following steps:
```
â€¢ Identify your functions ğ‘“(ğ‘¥, ğ‘¢) and ğ‘”(ğ‘¥, ğ‘¢) and define the Hamiltonian
```
```
ğ»(ğ‘¥, ğ‘¢, ğœ†).
```
```
â€¢ Find the value of ğ‘¢ that optimizes ğ»(ğ‘¥, ğ‘¢, ğœ†) (if we want to maximize ğ½,
```
```
then we maximize ğ», and if we want to minimize ğ½, we minimize ğ»).
```
This defines our control ğ‘¢ in terms of of ğ‘¥ and ğœ†.
```
â€¢ Substitute this form of ğ‘¢ into state ğ‘¥â€² = ğ‘“(ğ‘¥, ğ‘¢) and costate ğœ†â€² =
```
```
âˆ’ ğœ•ğ»ğœ•ğ‘¥ (ğ‘¥, ğ‘¢, ğœ†) equations, yielding a system of two ordinary differential
```
equations.
```
â€¢ Endpoints ğ‘¥(0) = ğ´, ğ‘¥(ğ‘‡) = ğµ make a two-point boundary problem;
```
we want to resolve the two integration constants to match the initial and
terminal positions.
This methodology will lead to solutions in our Integrator and King Tiny examples.
Example 4.3: Integrator
```
We return to Example 4.1 and consider ğ‘¥â€² = ğ‘¢ with ğ‘¥(0) = âˆ’1, ğ‘¥(6) = 1 where
```
we want to minimize
ğ½ = âˆ«
6
0
ğ‘¥2 + ğ‘¢2 ğ‘‘ğ‘¡.
```
Applying Principle I with ğ‘“(ğ‘¥) = ğ‘¢ and ğ‘”(ğ‘¥) = ğ‘¥2 + ğ‘¢2, define
```
ğ» = ğ‘¥2 + ğ‘¢2 + ğœ†ğ‘¢.
Look for a control ğ‘¢ that minimizes ğ» by setting the derivative to zero:
0 = ğœ•ğ»ğœ•ğ‘¢ = 2ğ‘¢ + ğœ†,
implying ğ‘¢ = âˆ’ğœ†/2. Note that ğœ•2ğ»ğœ•áµ†2 = 2 > 0, making this a local minimum.
Our state and costate equations are
ğ‘¥â€² = ğ‘¢,
ğœ†â€² = âˆ’2ğ‘¥.
54 Chapter 4. First Principle
Substituting ğ‘¢ = âˆ’ğœ†/2, our state and costate differential equations are
ğ‘¥â€² = âˆ’ğœ†/2,
ğœ†â€² = âˆ’2ğ‘¥.
This represents an important step in analyzing optimal control problems:
we have reduced necessary conditions for optimality to an exercise in solving
```
differential equations. Working with differential equations is an important skill;
```
some differential equations can be solved readily by hand, while others are best
solved using computer algebra systems. Many differential equations simply do
not admit a closed form solution, and analysis relies on numerical techniques.
In this case we have a two-dimensional linear system. Techniques for solving
such systems are briefly reviewed in Appendix B.
```
We have a two-point boundary problem, ğ‘¥(0) = âˆ’1, ğ‘¥(6) = 1, and express
```
our system in matrix form as
```
(
```
ğ‘¥
ğœ†
```
)
```
â€²
= [
0 âˆ’ 12
âˆ’2 0
```
] (
```
ğ‘¥
ğœ†
```
) .
```
```
With eigenvalues Â±1 and eigenvectors (Â±1, 2) for the coefficient matrix, this sys-
```
tem has general solution
ğ‘¥ = ğ¶1ğ‘’âˆ’ğ‘¡ âˆ’ ğ¶2ğ‘’ğ‘¡,
ğœ† = 2ğ¶1ğ‘’âˆ’ğ‘¡ + 2ğ¶2ğ‘’ğ‘¡.
```
Resolving the integration constants ğ¶1, ğ¶2 with boundary conditions ğ‘¥(0) =
```
```
âˆ’1, ğ‘¥(6) = 1 yields
```
ğ‘¥ = ğ‘’
ğ‘¡ âˆ’ ğ‘’6âˆ’ğ‘¡
ğ‘’6 âˆ’ 1 ,
ğœ† = âˆ’2 ğ‘’
ğ‘¡ + ğ‘’6âˆ’ğ‘¡
ğ‘’6 âˆ’ 1 ,
ğ‘¢ = ğ‘’
ğ‘¡ + ğ‘’6âˆ’ğ‘¡
ğ‘’6 âˆ’ 1 ,
```
with ğ‘¥(ğ‘¡) and ğ‘¢(ğ‘¡) plotted in Figure 4.3. This control carries a cost of
```
ğ½ = 2 ğ‘’
6 + 1
ğ‘’6 âˆ’ 1 = 2.009 . . . .
This is the very best anyone can do.
Note that this control verifies the intuition of Example 4.1 that we want to
linger near ğ‘¥ = 0 where cost is low. But now we have the very best balance of
moving close to zero, slowing down there, and then moving on to our required
end value.
4.1. One Dimension, Fixed Ends 55
Figure 4.3. State and control plotted against time, optimal control.
Example 4.4: King Tinyâ€™s Solution
```
King Tiny (Example 4.2) has ğ‘¥â€² = 0.2 ğ‘¥ âˆ’ ğ‘¢ with ğ‘¥(0) = 1, ğ‘¥(10) = 2, and he
```
wants to maximize
ğ½ = âˆ«
10
0
âˆšğ‘¢ ğ‘‘ğ‘¡.
Define
```
ğ» = âˆšğ‘¢ + ğœ†(0.2ğ‘¥ âˆ’ ğ‘¢).
```
Then
0 = ğœ•ğ»ğœ•ğ‘¢ = 1
2âˆšğ‘¢
âˆ’ ğœ†
```
implies ğ‘¢ = 1/(4ğœ†2), and note that this is a local maximum since ğœ•2ğ»ğœ•áµ†2 < 0.
```
We get the costate equation ğœ†â€² = âˆ’ ğœ•ğ»ğœ•ğ‘¥ = âˆ’.2ğœ†. Our state-costate system is
then
ğ‘¥â€² = 0.2ğ‘¥ âˆ’ 0.25ğœ†âˆ’2,
ğœ†â€² = âˆ’0.2ğœ†.
```
In this case we have a two-point boundary problem, ğ‘¥(0) = 1, ğ‘¥(10) = 2,
```
for a system of two first-order differential equations, one of which is nonlinear.
The second equation, ğœ†â€² = âˆ’.2ğœ†, has an exponential solution ğœ† = ğ¶1ğ‘’âˆ’0.2ğ‘¡, which
we substitute into the first equation to obtain ğ‘¥â€² = 0.2ğ‘¥ âˆ’ 0.25ğ¶âˆ’21 ğ‘’0.4ğ‘¡. This is a
first-order linear equation that we can solve by hand using an integrating factor,
```
or using a computer algebra system, to get general solution (âˆ— verify âˆ—)
```
```
ğ‘¥(ğ‘¡) = âˆ’1.25 ğ¶âˆ’21 ğ‘’0.4ğ‘¡ + ğ¶2ğ‘’0.2ğ‘¡.
```
56 Chapter 4. First Principle
```
Resolving the integration constants ğ¶1, ğ¶2 with boundary conditions ğ‘¥(0) = 1
```
```
and ğ‘¥(10) = 2 yields (âˆ— check this âˆ—)
```
```
ğ‘¥ = (âˆ’0.114 . . . ) ğ‘’.4ğ‘¡ + (1.114 . . . ) ğ‘’.2ğ‘¡,
```
```
ğœ† = (3.309 . . . ) ğ‘’âˆ’0.2ğ‘¡,
```
```
ğ‘¢ = (0.023 . . . ) ğ‘’0.4ğ‘¡
```
with performance ğ½ = 4.827 . . . .
Figure 4.4. Capital and optimal consumption plotted against time.
In Figure 4.4 we see this solution overshoots the end goal of doubling the
economy, spending down the excess towards the end of the time period with a
level of spending that would be unsustainable in the longer run. This is the solu-
tion to the problem as stated, but it may differ from the expectations or intentions
of those formulating the problem. This is not uncommon in optimal control the-
ory.
The Hamiltonian ğ» has a constant value on these optimal trajectories, a fea-
ture that we will explore more carefully in Section 5.1 and Example 5.3.
4.2. Time Dependence 57
4.2 Time Dependence
Principle I readily generalizes to systems that explicitly depend on time ğ‘¡. A statement
of the optimal principle that allows for time dependence is:
OPTIMAL PRINCIPLE II
Local optimum, fixed duration, fixed endpoint, time dependent, one dimension
Consider the controlled system
```
ğ‘¥â€² = ğ‘“(ğ‘¥, ğ‘¢, ğ‘¡), ğ‘¥, ğ‘¡ âˆˆ â„, ğ‘¢ âˆˆ ğ’°,
```
with fixed endpoint and end time conditions
```
ğ‘¥(0) = ğ´, ğ‘¥(ğ‘‡) = ğµ, ğ´, ğµ, ğ‘‡ given,
```
and objective function
ğ½ = âˆ«
ğ‘‡
0
```
ğ‘”(ğ‘¥, ğ‘¢, ğ‘¡) ğ‘‘ğ‘¡.
```
Define the Hamiltonian
```
ğ»(ğ‘¥, ğ‘¢, ğ‘¡, ğœ†) = ğ‘”(ğ‘¥, ğ‘¢, ğ‘¡) + ğœ†ğ‘“(ğ‘¥, ğ‘¢, ğ‘¡)
```
and the costate equation
```
ğœ†â€² = âˆ’ ğœ•ğ»ğœ•ğ‘¥ (ğ‘¥, ğ‘¢, ğ‘¡, ğœ†).
```
Then a locally optimal control ğ‘¢ must satisfy
ğœ•ğ»
```
ğœ•ğ‘¢ (ğ‘¥, ğ‘¢, ğ‘¡, ğœ†) = 0
```
and the control ğ‘¢ that optimizes ğ½ will optimize ğ» at all times.
Furthermore, if ğ‘“ and ğ‘” are independent of time ğ‘¡, then ğ» is constant on op-
timal trajectories.
Note that if ğ‘“ and ğ‘” are dependent on time, we can no longer conclude that ğ» is
constant on trajectories.
Example 4.5: King Tiny with a Discount
```
King Tiny (Example 4.4) faced a very unhappy constituency with his plan. Spend-
```
ing money on fun stuff started at a measly 2.3 cents per year and didnâ€™t increase
much for several years. The people did not want to wait that long and really start
getting their groove on. So King Tiny decides to discount the future enjoyment a
rate of 5% per year.
Our system remains
ğ‘¥â€² = .2ğ‘¥ âˆ’ ğ‘¢
but the payoff is now
ğ½ = âˆ«
10
0
ğ‘’âˆ’.05ğ‘¡âˆšğ‘¢ ğ‘‘ğ‘¡.
```
We still have fixed boundary requirements ğ‘‡ = 10, ğ‘¥(0) = 1, and ğ‘¥(10) = 2.
```
58 Chapter 4. First Principle
Our Hamiltonian is
```
ğ» = ğ‘’âˆ’.05ğ‘¡âˆšğ‘¢ + ğœ†(.2ğ‘¥ âˆ’ ğ‘¢)
```
and setting
0 = ğœ•ğ»ğœ•ğ‘¢ = 12 ğ‘¢âˆ’1/2ğ‘’âˆ’.05ğ‘¡ âˆ’ ğœ†
yields
ğ‘¢ = 14 ğœ†âˆ’2ğ‘’âˆ’0.1 ğ‘¡.
This is a local maximum as ğœ•2ğ»ğœ•áµ†2 < 0.
Thus we have the state-costate system
ğ‘¥â€² = .2ğ‘¥ âˆ’ .25ğœ†âˆ’2ğ‘’âˆ’.1 ğ‘¡,
ğœ†â€² = âˆ’.2ğœ†.
```
Solving this with boundary conditions ğ‘¥(0) = 1 and ğ‘¥(10) = 2 (using steps
```
```
as used in Example 4.4) yields (âˆ— verify this âˆ—)
```
```
ğ‘¥ = (1.425 . . . ) ğ‘’.2ğ‘¡ âˆ’ (0.424 . . . ) ğ‘’.3ğ‘¡,
```
```
ğœ† = (2.427 . . . ) ğ‘’âˆ’.2ğ‘¡,
```
```
ğ‘¢ = (0.042 . . . ) ğ‘’.3ğ‘¡
```
with a net payoff of ğ½ = 3.540 . . . .
Figure 4.5. Capital and optimal consumption plotted against
```
time, discounted future (solid) and nondiscounted (dashed).
```
```
Note the contrast in Figure 4.5 between the discounted solution (solid lines)
```
```
and nondiscounted (dashed lines): more consumption early on, less consump-
```
tion later, and a lower peak of maximum capital. King Tiny has nearly doubled
the initial enjoyment of his people, from 2.3 cents per year to 4.2 cents per year.
The original plan had consumption growing at 40% per year, whereas this plan
4.3. Can We Solve It? 59
has consumption growing at 30% per year, so discounting the future by 5% cost
10 percentage points off of annual growth of consumption.
Substituting the solutions into the Hamiltonian and simplifying would lead
to
```
ğ» = (0.6914 . . . ) âˆ’ (0.1030 . . . ) ğ‘’.1ğ‘¡
```
which is not constant on optimal trajectories.
4.3 Can We Solve It?
Not all optimal control problems have solutions. There are several ways that things
can go wrong. First off, the problem has to be clearly formulated. For our purposes,
we will always assume the dynamics of the problem are modeled by a well-defined
```
function ğ‘“(ğ‘¥, ğ‘¢, ğ‘¡) which is differentiable (or at least continuous) in all its variables.
```
This allows us to employ the theorems of existence and uniqueness from differential
equations.
Solving an optimal control problem starts with affirming that the problem is clearly
formulated, that the endpoint conditions can be attained with allowed controls, and
that we expect that the set of allowed controls ğ’° contains an optimal control.
4.3.1 Attainability. In some cases the structure of the stated problem precludes
any solution, optimal or not. For example, if ğ‘¥â€² = ğ‘¢2, then we cannot satisfy boundary
```
conditions ğ‘¥(0) = 1 and ğ‘¥(1) = âˆ’1 (âˆ— why not? âˆ—).
```
We often have restrictions on our control. For example, we may require ğ‘¢ â‰¥ 0 or
maybe |ğ‘¢| â‰¤ 1. If ğ‘¥â€² = ğ‘¢ with |ğ‘¢| â‰¤ 1, then we cannot satisfy boundary conditions
```
ğ‘¥(0) = 0 and ğ‘¥(1) = 2 (âˆ— why not? âˆ—).
```
Specified boundary conditions are attainable if there is some allowed control ğ‘¢ âˆˆ
ğ’° that can match those boundary conditions. We need attainable solutions before we
can begin to optimize.
Example 4.6: King Tinyâ€™s Restrictions
```
King Tiny (Example 4.4) has ğ‘¥â€² = .2ğ‘¥ âˆ’ ğ‘¢ and must have ğ‘¢ â‰¥ 0; otherwise he
```
would be taking money away from people and they do not like that at all. Suppose
```
that instead of doubling the economy in ten years, he would like ğ‘¥(10) = ğµ.
```
```
Starting with ğ‘¥(0) = 1, which values of ğµ would be attainable?
```
```
With ğ‘¥â€² = .2ğ‘¥ âˆ’ ğ‘¢ â‰¤ .2ğ‘¥, the maximum value for ğ‘¥(10) would be attained by
```
```
a constant ğ‘¢ = 0 yielding ğ‘¥(10) = ğ‘’2 and so he can only attain ğµ â‰¤ ğ‘’2 = 7.389 . . . .
```
Note that, as formulated, there is no prohibition on King Tiny attaining neg-
```
ative values for his economy ğ‘¥, whatever that might mean (bankruptcy?). We can
```
```
attain any ğµ â‰¤ ğ‘’2 and this includes negative values. For example ğ‘¥(10) = âˆ’1,000
```
can be attained by a constant ğ‘¢ = 31.53 . . . . It is important that the formulation
of a problem matches all real-world constraints and expectations of the problem.
60 Chapter 4. First Principle
Example 4.7: Integrator
```
For ğ‘¥â€² = ğ‘¢ with |ğ‘¢| â‰¤ 1, what values for ğ´ and ğµ are attainable with ğ‘¥(0) = ğ´,
```
```
ğ‘¥(ğ‘‡) = ğµ given a fixed time ğ‘‡?
```
```
We have âˆ’1 â‰¤ ğ‘¥â€² â‰¤ 1, and so with ğ‘¥(0) = ğ´ we have
```
```
ğ´ âˆ’ ğ‘¡ â‰¤ ğ‘¥(ğ‘¡) â‰¤ ğ´ + ğ‘¡.
```
```
For a given ğ‘‡ and ğ‘¥(ğ‘‡) = ğµ, the attainable values of ğ´ and ğµ are characterized by
```
ğ´ âˆ’ ğ‘‡ â‰¤ ğµ â‰¤ ğ´ + ğ‘‡ or |ğ´ âˆ’ ğµ| â‰¤ ğ‘‡.
4.3.2 Sequences Going Nowhere. Endpoint conditions might be attainable
given a set of allowable controls ğ’°, but an optimal solution may not exist in ğ’°. There
could be a sequence of allowed controls where each subsequent control has better per-
formance, but the sequence has a limit that is not in the set ğ’° of allowed controls.
Example 4.8: Bitter Medicine
You have to drink a bottle of medicine within a one-hour period, and it tastes
```
really bad. The amount of medicine in the bottle at time ğ‘¡ is ğ‘¥(ğ‘¡) and we require
```
```
ğ‘¥(0) = 1 and ğ‘¥(1) = 0. You drink it at rate ğ‘¢ so ğ‘¥â€² = âˆ’ğ‘¢, and your gastrointestinal
```
fortitude is such that we may assume ğ‘¢ â‰¥ 0. Your displeasure at drinking this
stuff is modeled by the concave downward function âˆšğ‘¢ to capture the saturation
```
effect: drinking it twice as fast isnâ€™t quite twice as bad. Your total displeasure is
```
then ğ½ = âˆ«10 âˆšğ‘¢ ğ‘‘ğ‘¡, which you want to minimize. Suppose you consume it as
```
ğ‘¢ğœ…(ğ‘¡) = {
```
1
ğœ… for 0 â‰¤ ğ‘¡ < ğœ…,
0 for ğœ… â‰¤ ğ‘¡ â‰¤ 1
for 0 < ğœ… â‰¤ 1, where lower values of ğœ… means you drink the medicine more
quickly.
```
This satisfies the boundary conditions ğ‘¥(0) = 1 and ğ‘¥(1) = 0 and has
```
ğ½ = âˆ«
ğœ…
0
âˆš1/ğœ… ğ‘‘ğ‘¡ = âˆšğœ….
Taking the medicine quickly is better, and in fact ğ½ â†’ 0 as ğœ… â†’ 0. So there are
allowable solutions with your displeasure as close to zero as you want.
However, if we assume ğ‘¢ is a piecewise continuous function, then there are
no allowable solutions with zero displeasure: to consume the bottle, you need
```
âˆ«10 ğ‘¢ ğ‘‘ğ‘¡ = 1 which implies that ğ‘¢ is positive on some open interval (or more
```
```
generally, some positive measure set), which forces some displeasure âˆ«10 âˆšğ‘¢ ğ‘‘ğ‘¡
```
> 0.
4.3. Can We Solve It? 61
```
The reader may be familiar with the Dirac delta function, ğ›¿(ğ‘¡), a rather ab-
```
stract concept from real analysis which places a unit mass at ğ‘¡ = 0 and is zero
everywhere else. This can be thought of as a formal limit of the functions ğ‘¢ğœ… as
ğœ… â†’ 0, and it would represent taking the medicine instantly all at once. So if our
set of allowable controls contains Dirac delta functions, we actually can attain
zero displeasure.
In the previous example, we minimized displeasure by drinking the medicine as
quickly as possible. The following example demonstrates the opposite effect when you
want to maximize enjoyment.
Example 4.9: Spending Money
You have $100 to spend in ğ‘‡ time units. Suppose that if you spend at a rate ğ‘¢,
your enjoyment is modeled by
ğ½ = âˆ«
ğ‘‡
0
âˆšğ‘¢ ğ‘‘ğ‘¡.
```
Here again we use the concave downwards function ğ‘”(ğ‘¢) = âˆšğ‘¢ to reflect the
```
diminishing returns of higher consumption rates.
```
Let ğ‘¥(ğ‘¡) be the amount of money you have at time ğ‘¡, so ğ‘¥(0) = 100 and ğ‘¥â€² =
```
```
âˆ’ğ‘¢ where you control the spending rate ğ‘¢(ğ‘¡). It stands to reason that we will want
```
```
to spend all the money, so take ğ‘¥(ğ‘‡) = 0.
```
```
Using Principle I, we can conclude that ğ‘¢ will be a constant (see Exercise
```
```
4.11). Matching the endpoints yields ğ‘¢(ğ‘¡) = 100/ğ‘‡, ğ‘¥(ğ‘¡) = 100(1 âˆ’ ğ‘¡/ğ‘‡), and
```
ğ½ = 10âˆšğ‘‡.
What happens as we are given more and more time to spend our $100? Say,
ğ‘‡ â†’ âˆ? For one thing we would have ğ½ â†’ âˆ. However, our spending rate
would go to zero ğ‘¢ â†’ 0. So in the limit it seems we would spend nothing and
be overjoyed. But this doesnâ€™t work, as in the limit we have ğ‘¢ â‰¡ 0 making ğ½ =
âˆ«âˆ0 0 ğ‘‘ğ‘¡ = 0.
4.3.3 Topology and Existence. In calculus, we know that a continuous function
```
ğ‘“(ğ‘¥) over a closed and bounded interval [ğ‘, ğ‘] must attain a maximum and a minimum
```
value. For a differentiable function such an extreme value must be attained at a point in
the interior of the interval where the derivative is zero or at an endpoint of the interval.
Real analysis extends the concepts of â€œinteriorâ€, â€œclosed and boundedâ€, and â€œlocalâ€
to sets that are more abstract than intervals on the real line. It is beyond the scope of
this text to take a deep dive into these real analytic concepts, but we briefly touch upon
the ideas in this section for the benefit of those who have studied real analysis and to
whet the appetite of those who may do so in the near future.
62 Chapter 4. First Principle
We have a collection of allowable controls ğ’°, and we need some idea of when
two controls are near one another. Since our concern is performance measured by an
integral, a natural approach would be the ğ¿1 metric, which says that two controls ğ‘¢1
and ğ‘¢2 on an interval ğ¼ are within ğœ– > 0 of each other if
âˆ«
ğ¼
```
|ğ‘¢1(ğ‘¡) âˆ’ ğ‘¢2(ğ‘¡)| ğ‘‘ğ‘¡ < ğœ–.
```
We could also use the more intuitive ğ¶0 metric, which states that two functions are ğœ–
close if
```
maxğ‘¡âˆˆğ¼ |ğ‘¢1(ğ‘¡) âˆ’ ğ‘¢2(ğ‘¡)| < ğœ–.
```
For a bounded interval ğ¼, if two functions are close in the ğ¶0 metric, they must be close
in the ğ¿1 metric. The converse is not true, so ğ¶0 induces a stronger topology. Weâ€™ll
simplify things later on by using the stronger ğ¶0 metric in our proofs, but keep in mind
that the ğ¿1 metric is more natural for the theory.
A control ğ‘¢ is in the interior of the allowable set of controls ğ’° if for some ğœ– > 0,
every control within ğœ– of ğ‘¢ is also in ğ’°. So a control ğ‘¢ is locally optimal if it is in the
interior of the allowable set ğ’° and there is an ğœ– > 0 such that no other control within ğœ– of
ğ‘¢ in ğ’° can do better. This concept is central to Eulerâ€™s method for proving Pontryaginâ€™s
```
principles (see Section 6.3 for a proof of Principle III).
```
It is entirely possible that the endpoint conditions are attainable in the set ğ’°, but
there are no locally optimal solutions in ğ’°. This can arise in a way similar to the max-
imum of a function being at the boundary of an open interval, like looking for the
maximum of ğ‘¦ = ğ‘¥2 over the interval 0 < ğ‘¥ < 1: there are points inside the interval
that produce higher and higher values of the function, but no point in the open interval
produces a maximum. The Bitter Medicine and Spending Money examples in the pre-
vious section show sequences of increasingly better controls that fail to converge to an
allowable control, and they are cases where the set ğ’° was not closed in the ğ¿1 topology
and ğ’° does not contain the limits of convergent sequences.
Fortunately, the calculus ideas for the existence of a maximum or minimum extend
to functional spaces in optimal control. The abstract formulation for knowing that a
solution exists is:
If the set ğ’° is closed and bounded in the ğ¿1 topology and if the endpoint
conditions are attainable in ğ’°, then ğ’° must contain a solution to the opti-
mization problem. This solution must be either a locally optimal solution
in the interior of ğ’° or a solution on the boundary of ğ’°.
These real analysis considerations are essential for a deeper understanding of this
material, but we will avoid wading too deep into the abstract underpinnings, so donâ€™t
```
worry (too much) if this is all feeling a bit esoteric.
```
For now, we will focus on techniques to find locally optimal controls in the interior
of the allowable set. Principles Iâ€“VI will be stated in terms of finding necessary condi-
tions for a locally optimal control in the interior of ğ’°. With Principle VII in Chapter
10 we consider controls at the boundary of the allowable set ğ’°.
Exercises 63
The control theory literature contains a variety of more specific theorems that ad-
dress specific conditions and contexts that guarantee the existence of a globally opti-
mum solution. For a broader and more in-depth treatment of sufficiency theorems, see
[9, 23, 24].
Key Points
In this chapter we introduced the basic Pontryagin technique presented as Principles
I and II. This establishes necessary conditions for a control to be optimal in a one-
```
dimensional system ğ‘¥â€² = ğ‘“(ğ‘¥, ğ‘¢) with fixed boundaries and time period ğ‘¥(0) = ğ´,
```
```
ğ‘¥(ğ‘‡) = ğµ and performance function ğ½ = âˆ«ğ‘‡0 ğ‘”(ğ‘¥, ğ‘¢) ğ‘‘ğ‘¡. Following the structure pre-
```
sented in Chapter 3 for the discrete case, we solve this one-dimensional problem by
examining a two-dimensional state-costate system of differential equations formed by
introducing dynamic Lagrange multipliers and solving a two-point boundary problem.
The steps for applying Principles I and II are:
â€¢ Determine whether Principle I or II applies, and identify ğ‘“, ğ‘”, ğ´, ğµ, ğ‘‡, and ğ½.
â€¢ Construct the Hamiltonian, ğ». Solve for the optimal ğ‘¢ in terms of ğœ† and ğ‘¥, and
verify if maximizing or minimizing is appropriate.
â€¢ Substitute the optimal ğ‘¢ to construct the state-costate system of ODEs. Solve the
system to match the boundary values.
â€¢ Reflect and conclude.
Locally optimal controls must be critical points of the Hamiltonian function, and
in fact optimize the Hamiltonian at all times. The Hamiltonian is constant on optimal
solutions in the time-independent case.
We extended the technique to time-dependent systems, such as allowing for dis-
```
counted future payoffs; note that the Hamiltonian may no longer be constant on opti-
```
mal trajectories.
We examined questions of attainability and explored cases where an optimal con-
trol does not exist by taking a real analytic view of our set of controls.
These are the foundational elements of optimal control in continuous systems,
examined in their most basic form, and they set the format of methods in subsequent
chapters.
Exercises
```
Exercise 4.1(s). Maximize âˆ«10 âˆšğ‘¢ ğ‘‘ğ‘¡ for the system ğ‘¥â€² = ğ‘¢âˆ’ğ‘¥ with ğ‘¢ â‰¥ 0 and endpoint
```
```
conditions ğ‘¥(0) = 0 and ğ‘¥(1) = 1.
```
```
Exercise 4.2(s). Consider ğ‘¥â€² = ğ‘¥ + ğ‘¢ with ğ‘¢ > 0. Maximize âˆ«10 ln ğ‘¢ ğ‘‘ğ‘¡ for ğ‘¥(0) = 1
```
```
and ğ‘¥(1) = 3.
```
```
Exercise 4.3(s). Minimize âˆ«10 (ğ‘¥ + ğ‘¢) ğ‘‘ğ‘¡ for the system ğ‘¥â€² = ln(ğ‘¢) with ğ‘¥(0) = 1 and
```
```
ğ‘¥(1) = 2 ln(2). Plug your solutions back into ğ» and show that ğ» is a constant.
```
64 Chapter 4. First Principle
```
Exercise 4.4(s). Minimize âˆ«20 ğ‘¢ğ‘¡ ğ‘‘ğ‘¡ for the system ğ‘¥â€² = ğ‘¢2, ğ‘¢ â‰¥ 0, with ğ‘¥(0) = 1 and
```
```
ğ‘¥(2) = 9. Plug your solutions back into ğ» and show that ğ» is a not constant.
```
```
Exercise 4.5(s). Minimize âˆ«10 ğ‘’2ğ‘¡ğ‘¢2 ğ‘‘ğ‘¡ subject to ğ‘¥â€² = ğ‘¢, ğ‘¥(0) = 0, ğ‘¥(1) = ğµ. Plug your
```
solutions back into ğ» and show that ğ» is not a constant.
```
Exercise 4.6(s). Consider the controlled system ğ‘¥â€² = ğ‘¥ + ğ‘¢ with ğ‘¥(0) = 1.
```
```
(a) What values for ğµ are attainable as ğ‘¥(1) = ğµ assuming ğ‘¢ â‰¥ 0?
```
```
(b) What values for ğµ are attainable as ğ‘¥(1) = ğµ assuming |ğ‘¢| â‰¤ 1?
```
```
Exercise 4.7(s). Consider the controlled system ğ‘¥â€² = ğ‘¥ + ğ‘¢ (as in Exercise 4.2) with
```
```
ğ‘¥(0) = ğ´ and ğ‘¥(1) = ğµ.
```
```
(a) If we require ğ‘¢ > 0, what pairs of endpoint values ğ´, ğµ, with ğ´ â‰¥ 0 and ğµ â‰¥ 0,
```
are possible?
```
(b) Compute the solution that maximizes âˆ«10 ln ğ‘¢ ğ‘‘ğ‘¡ for allowed pairs ğ´, ğµ.
```
Exercise 4.8. Suppose you want to minimize ğ½ = âˆ«10 ğ‘¥ ğ‘¢ ğ‘‘ğ‘¡ with ğ‘¥â€² = ğ‘¢2, endpoints
```
ğ‘¥(0) = 0, ğ‘¥(1) = 1, and ğ‘¢ â‰¥ 0. Verify that the control
```
```
ğ‘¢(ğ‘¡) = {1/âˆšğœ… for 0 â‰¤ ğ‘¡ < ğœ…,0 for ğœ… â‰¤ ğ‘¡ â‰¤ 1
```
will satisfy the endpoint conditions on ğ‘¥ for 0 < ğœ… < 1. Show that ğ½ â†’ 0 as ğœ… â†’ 0.
Argue that there is no control ğ‘¢ â‰¥ 0 that will satisfy the endpoint conditions and have
ğ½ = 0.
```
Exercise 4.9. In the Bitter Medicine case (Example 4.8), the solution is to drink the
```
```
medicine instantly all at once. Evaluate the same example with ğ½ = âˆ«10 ğ‘”(ğ‘¢) ğ‘‘ğ‘¡ and the
```
following payoff functions. In which cases would you still want to drink the medicine
as quickly as possible?
```
(a) ğ‘”(ğ‘¢) = log(1 + ğ‘¢).
```
```
(b) ğ‘”(ğ‘¢) = ğ‘¢.
```
```
(c) ğ‘”(ğ‘¢) = ğ‘¢2.
```
```
Exercise 4.10. In the Bitter Medicine case (Example 4.8), the solution is to drink the
```
medicine instantly all at once, but it really doesnâ€™t matter when you do that. Argue that
in the case where you discount the future ğ½ = âˆ«10 ğ‘’âˆ’ğ›¼ğ‘¡âˆšğ‘¢ ğ‘‘ğ‘¡ for ğ›¼ > 0, you would wait
for the last possible moment to down the stuff.
```
Exercise 4.11(h). Consider the Spending Money case (Example 4.9) where we have
```
```
ğ‘¥â€² = âˆ’ğ‘¢ with ğ‘¥(0) = 100 and ğ‘¥(ğ‘‡) = 0 and we want to maximize ğ½ = âˆ«ğ‘‡0 âˆšğ‘¢ ğ‘‘ğ‘¡.
```
```
(a) Verify that the optimal solution would have ğ‘¢ as a constant and derive ğ½ =
```
10âˆšğ‘‡.
Exercises 65
```
(b) Suppose that instead of the square root function we want to maximize ğ½ =
```
```
âˆ«ğ‘‡0 ln(1 + ğ‘¢)ğ‘‘ğ‘¡. Show that ğ‘¢ would still be a constant. Would ğ½ â†’ âˆ as ğ‘‡ â†’ âˆ?
```
```
(c) Suppose that ğ‘”(ğ‘¢) â‰¥ 0 is any concave downwards function defined on 0 â‰¤
```
```
ğ‘¢ < âˆ with continuous second derivative and ğ‘”(0) = 0, and suppose that you want to
```
```
maximize ğ½ = âˆ«ğ‘‡0 ğ‘”(ğ‘¢) ğ‘‘ğ‘¡. Show that ğ‘¢ must be a constant to maximize ğ½.
```
```
(d) Would it be possible to have a function ğ‘” satisfying conditions from part (c)
```
that would make ğ½ â†’ 0 as ğ‘‡ â†’ âˆ? What can you say about the limit of ğ½ as ğ‘‡ â†’ âˆ?
```
Exercise 4.12(s). Suppose that King Tiny (Example 4.4) decides to use a linear func-
```
tion to model his constituentâ€™s pleasure: it is twice as much fun to spend twice as much
money. So overall fun is now measured as
ğ½ = âˆ«
10
0
ğ‘¢ ğ‘‘ğ‘¡
```
which he wants to maximize with ğ‘¥(0) = 1, ğ‘¥(10) = 2, and ğ‘¥â€² = .2ğ‘¥ âˆ’ ğ‘¢. He plans to
```
do this by saving all the money until towards the end of the ten-year period and then
having a huge blow-out party.
```
Suppose that for some value ğ‘, with 0 â‰¤ ğ‘ < 10, King Tiny takes ğ‘¢ğ‘(ğ‘¡) = 0 for
```
```
0 < ğ‘¡ < ğ‘ and ğ‘¢ğ‘(ğ‘¡) = ğ‘¢party for ğ‘ â‰¤ ğ‘¡ < 10.
```
```
(a) Find the value of ğ‘¢party that will match the end condition ğ‘¥(10) = 2.
```
```
(b) Compute the payoff ğ½(ğ‘) as a function of ğ‘.
```
```
(c) Show that ğ½(ğ‘) is increasing with ğ½(ğ‘) â†’ (ğ‘’2 âˆ’ 2) as ğ‘ â†’ 10.
```
This exercise demonstrates how economic models can fall apart without the satu-
ration effect of a concave utility function.
```
Exercise 4.13(hs). In Example 4.1 we improved our initial proposed solution by spend-
```
ing time at ğ‘¥ = 0. Expand on this idea as follows.
For ğ¾ â‰¥ 1/3 consider the control
```
ğ‘¢(ğ‘¡) =
```
â§
â¨
â©
ğ¾ for 0 â‰¤ ğ‘¡ < 1ğ¾ ,
0 for 13 â‰¤ ğ‘¡ < 6 âˆ’ 1ğ¾ ,
ğ¾ for 6 âˆ’ 1ğ¾ â‰¤ ğ‘¡ â‰¤ 6.
```
(a) Show that this control will match the endpoint conditions for any value ğ¾ â‰¥
```
1/3. What would the resulting plots of ğ‘¥ and ğ‘¢ look like?
```
(b) Compute ğ½ as a function of ğ¾. Show that ğ½ â†’ âˆ as ğ¾ â†’ âˆ.
```
This shows that ğ½ can be made arbitrarily large, and so if we wanted to maximize
instead of minimize, there would be no solution.
```
Exercise 4.14. What if King Tiny (Examples 4.2 and 4.4) were replaced by his evil twin,
```
```
who wanted to minimize the enjoyment of his subjects? What could he do? (Assume
```
```
youâ€™d still have ğ‘¥(0) = 1, ğ‘¥(10) = 2, and ğ‘¢ â‰¥ 0.)
```
66 Chapter 4. First Principle
```
Exercise 4.15(hs). This is a simplified economic model for life-cycle savings.
```
An individual has a predetermined lifespan of ğ‘‡ years, earns fixed wages ğ‘¤, and
has access to an interest rate ğ‘Ÿ for savings. The individual increases their net worth by
working and receiving interest on savings and decreases their net worth by spending.
The individualâ€™s net worth is modeled by
ğ‘¥â€² = ğ‘¤ + ğ‘Ÿğ‘¥ âˆ’ ğ‘¢
where ğ‘¢ is the consumption.
```
Modeling the utility of consumption by the concave downwards function ln(ğ‘¢) to
```
reflect diminished returns of higher levels of consumption, we posit that the individual
wants to maximize
ğ½ = âˆ«
ğ‘‡
0
```
ln(ğ‘¢) ğ‘‘ğ‘¡
```
```
over a lifetime of fixed length ğ‘‡. Assuming no inheritance or bequests, ğ‘¥(0) = ğ‘¥(ğ‘‡)
```
= 0.
These are quite simplified assumptions, but it makes an interesting basic model.
```
(a) Solve for optimal consumption.
```
```
(b) Show that consumption increases in time, with consumption less than wages,
```
ğ‘¢ < ğ‘¤, for early life and consumption greater than wages, ğ‘¢ > ğ‘¤, for later life.
5
Unpacking Pontryagin
Pontryaginâ€™s principle is a huge idea. There is much to unpack. Chapters 1â€“3 laid out
the basic framework of the principle in the discrete case, and Chapter 4 stated the more
complete principles for the continuous case.
In this chapter we fill out our understanding of the continuous case by exploring
three topics: how optimization creates a Hamiltonian system, the fundamental Princi-
ple of Optimality, and the nature of costates. We then conclude with an illuminating
example applying control techniques to a geometric optimization problem.
5.1 Hamilton and Pontryagin
In the last chapter we stated that the Hamiltonian is conserved on optimal trajectories
```
for an autonomous (time-independent) system. In fact, a distinguishing property of
```
an optimal control is that it makes the state-costate system into what is known as a
Hamiltonian system.
```
5.1.1 Hamiltonian Systems. Given a differentiable function ğ»(ğ‘¥, ğ‘¦), consider
```
the system of two ODEs:
```
ğ‘¥â€²(ğ‘¡) = ğœ•ğ»ğœ•ğ‘¦ (ğ‘¥, ğ‘¦),
```
```
ğ‘¦â€²(ğ‘¡) = âˆ’ ğœ•ğ»ğœ•ğ‘¥ (ğ‘¥, ğ‘¦).
```
```
Then any solution (ğ‘¥(ğ‘¡), ğ‘¦(ğ‘¡)) of this system will induce a constant value for
```
```
ğ»(ğ‘¥(ğ‘¡), ğ‘¦(ğ‘¡)):
```
ğ‘‘
ğ‘‘ğ‘¡ ğ» =
ğœ•ğ»
ğœ•ğ‘¥ ğ‘¥â€² +
ğœ•ğ»
ğœ•ğ‘¦ ğ‘¦â€² =
ğœ•ğ»
ğœ•ğ‘¥
ğœ•ğ»
ğœ•ğ‘¦ âˆ’
ğœ•ğ»
ğœ•ğ‘¦
ğœ•ğ»
ğœ•ğ‘¥ = 0.
Thus trajectories of this system are level curves for ğ». Systems with this property are
Hamiltonian systems, and the function ğ» is the Hamiltonian.
67
68 Chapter 5. Unpacking Pontryagin
```
The more general form of a Hamiltonian system for ğ± = (ğ‘¥1, . . . , ğ‘¥ğ‘›), ğ² = (ğ‘¦1, . . . , ğ‘¦ğ‘›)
```
is
```
ğ‘¥â€²ğ‘– = ğœ•ğ»ğœ•ğ‘¦ğ‘–(ğ±, ğ²),
```
```
ğ‘¦â€²ğ‘– = âˆ’ ğœ•ğ»ğœ•ğ‘¥ğ‘–(ğ±, ğ²).
```
This concept is used extensively in physics and dynamical systems and is closely re-
lated to the Hamiltonian idea in Section 2.10. The following are examples based on
conservation of energy.
Example 5.1
In the harmonic oscillator or mass-spring system ğ‘¥â€³ = âˆ’ğ‘¥ we have position ğ‘¥
with velocity ğ‘¦ = ğ‘¥â€² and acceleration ğ‘¦â€² = âˆ’ğ‘¥. This is a Hamiltonian system
with energy as the Hamiltonian function ğ» = 12 ğ‘¦2 + 12 ğ‘¥2:
```
ğ‘¥â€² = ğœ•ğ»ğœ•ğ‘¦ (ğ‘¥, ğ‘¦) = ğ‘¦,
```
```
ğ‘¦â€² = âˆ’ ğœ•ğ»ğœ•ğ‘¥ (ğ‘¥, ğ‘¦) = âˆ’ğ‘¥,
```
and we note that ğ» is conserved by this system:
ğ‘‘
ğ‘‘ğ‘¡ ğ» =
ğœ•ğ»
ğœ•ğ‘¥ ğ‘¥â€² +
ğœ•ğ»
ğœ•ğ‘¦ ğ‘¦â€²
```
= (ğ‘¥)(ğ‘¦) + (ğ‘¦)(âˆ’ğ‘¥)
```
= 0.
Example 5.2
A pendulum of mass ğ‘š on a rod of length ğ‘Ÿ is at rest when ğœƒ = 0. The potential
```
energy is ğ‘ƒ = ğ‘šğ‘”ğ‘Ÿ(1 âˆ’ cos ğœƒ) and the kinetic energy is ğ¾ = 12 ğ‘šğ‘Ÿ (ğœƒâ€²)2. Taking
```
angular velocity ğ‘¦ = ğœƒâ€² and using total energy as the Hamiltonian,
```
ğ¸ = ğ‘ƒ + ğ¾ = ğ‘šğ‘”ğ‘Ÿ(1 âˆ’ cos ğœƒ) + 12 ğ‘šğ‘Ÿğ‘¦2,
```
we can derive the equations of motion for the pendulum:
ğœƒâ€² = ğœ•ğ¸ğœ•ğ‘¦ = ğ‘šğ‘Ÿğ‘¦,
ğ‘¦â€² = âˆ’ ğœ•ğ¸ğœ•ğœƒ = âˆ’ğ‘šğ‘”ğ‘Ÿ sin ğœƒ.
These are standard examples of basic conservative systems. Readers unfamiliar
with these examples may wish to look them up.
5.1.2 Pontryaginâ€™s Hamiltonian. Principle I applies to autonomous cases where
```
ğ‘“ and ğ‘” are independent of time; they are functions only of state and control. This leads
```
to the conclusion that ğ» is constant on optimal trajectories, as follows.
5.1. Hamilton and Pontryagin 69
The state and costate equations are
ğ‘¥â€² = ğœ•ğ»ğœ•ğœ† ,
ğœ†â€² = âˆ’ ğœ•ğ»ğœ•ğ‘¥ .
This kind of looks like a Hamiltonian system, but not quite, because ğ» has ğ‘¢ as an
```
additional independent variable: ğ»(ğ‘¥, ğœ†, ğ‘¢). The magic here is that the choosing of ğ‘¢
```
in terms of ğ‘¥ and ğœ† to make ğœ•ğ»ğœ•áµ† = 0 creates the Hamiltonian system.
```
Take ğ‘¢(ğ‘¥, ğœ†) as our solution for ğœ•ğ»ğœ•áµ† = 0 and consider
```
```
Ëœğ»(ğ‘¥, ğœ†) = ğ»(ğ‘¥, ğœ†, ğ‘¢(ğ‘¥, ğœ†)).
```
Pay attention to the scope of the derivatives in the following. Note the difference in
the derivative of a function with respect to an independent variable, ğœ•ğ»ğœ•ğœ† , and the full
```
derivative using the chain rule to account for dependencies, ğœ•ğœ•ğœ† (ğ»). It can be tricky;
```
see Section 2.2.
With ğœ•ğ»ğœ•áµ† = 0 we have
ğœ• Ëœğ»
```
ğœ•ğœ† (ğ‘¥, ğœ†) =
```
ğœ•
```
ğœ•ğœ† (ğ»(ğ‘¥, ğœ†, ğ‘¢(ğ‘¥, ğœ†)))
```
```
= ğœ•ğ»ğœ•ğœ† (ğ‘¥, ğœ†, ğ‘¢(ğ‘¥, ğœ†)) + ğœ•ğ»ğœ•áµ† (ğ‘¥, ğœ†, ğ‘¢(ğ‘¥, ğœ†)) ğœ•áµ†ğœ•ğœ† (ğ‘¥, ğœ†)
```
```
= ğœ•ğ»ğœ•ğœ† (ğ‘¥, ğœ†, ğ‘¢(ğ‘¥, ğœ†))
```
= ğ‘¥â€²,
ğœ• Ëœğ»
```
ğœ•ğ‘¥ (ğ‘¥, ğœ†) =
```
ğœ•
```
ğœ•ğ‘¥ (ğ»(ğ‘¥, ğœ†, ğ‘¢(ğ‘¥, ğœ†)))
```
```
= ğœ•ğ»ğœ•ğ‘¥ (ğ‘¥, ğœ†, ğ‘¢(ğ‘¥, ğœ†)) + ğœ•ğ»ğœ•áµ† (ğ‘¥, ğœ†, ğ‘¢(ğ‘¥, ğœ†)) ğœ•áµ†ğœ•ğ‘¥ (ğ‘¥, ğœ†)
```
```
= ğœ•ğ»ğœ•ğ‘¥ (ğ‘¥, ğœ†, ğ‘¢(ğ‘¥, ğœ†))
```
= âˆ’ğœ†â€².
It follows that Ëœğ» is constant on optimal trajectories.
ğ‘‘
```
ğ‘‘ğ‘¡ ( Ëœğ»(ğ‘¥(ğ‘¡), ğœ†(ğ‘¡))) =
```
ğœ• Ëœğ»
ğœ•ğ‘¥ ğ‘¥â€² +
ğœ• Ëœğ»
ğœ•ğœ† ğœ†â€²
= ğœ• Ëœğ»ğœ•ğ‘¥ğœ• Ëœğ»ğœ•ğœ† âˆ’ ğœ• Ëœğ»ğœ•ğœ†ğœ• Ëœğ»ğœ•ğ‘¥
= 0.
```
In other words, our optimal control precisely forces ğ»(ğ‘¥, ğœ†, ğ‘¢) to be constant. This
```
```
means that optimal solutions produce curves (ğ‘¥(ğ‘¡), ğœ†(ğ‘¡)) in state-costate space that are
```
level curves of the Hamiltonian, Ëœğ», and this can help use visualize these solutions.
70 Chapter 5. Unpacking Pontryagin
Example 5.3: King Tiny
```
In Example 4.4 we maximized ğ½ = âˆ«100 âˆšğ‘¢ ğ‘‘ğ‘¡ subject to ğ‘¥â€² = .2ğ‘¥âˆ’ğ‘¢ with ğ‘¥(0) = 1,
```
```
ğ‘¥(10) = 2 using optimal control ğ‘¢ = 1/(4ğœ†2) where ğœ†â€² = âˆ’.2ğœ†.
```
```
Substituting the optimal control into the Hamiltonian ğ» = âˆšğ‘¢ + ğœ†(.2ğ‘¥ âˆ’ ğ‘¢)
```
and simplifying yields
ğ» = .2ğœ†ğ‘¥ + .25ğœ†âˆ’1.
This is the Hamiltonian function for the equations
ğ‘¥â€² = .2ğ‘¥ âˆ’ .25 ğœ†âˆ’2,
ğœ†â€² = âˆ’.2ğœ†
```
and is constant on optimal trajectories (Figure 5.1):
```
ğ¶ = .2ğœ†ğ‘¥ + .25ğœ†âˆ’1.
Figure 5.1. Optimal solution in state-costate space, with level
curves of the Hamiltonian.
Note that this can be used as a shortcut for solving the differential equations.
We can solve ğœ†â€² = âˆ’.2ğœ† as ğœ† = ğœ†0ğ‘’âˆ’.2ğ‘¡, and then plug this into ğ¶ = .2ğœ†ğ‘¥ + .25ğœ†âˆ’1
to get directly to the form ğ‘¥ = ğ¶1ğ‘’.2ğ‘¡ +ğ¶2ğ‘’.4ğ‘¡ without having to solve a differential
equation for ğ‘¥.
5.2. The Principle of Optimality 71
Example 5.4: Integrator
In Example 4.3 we minimized ğ½ = âˆ«ğ‘‡0 ğ‘¥2 + ğ‘¢2 ğ‘‘ğ‘¡ subject to ğ‘¥â€² = ğ‘¢ by using a
control ğ‘¢ = âˆ’ğœ†/2 which generated the linear state-costate system
ğ‘¥â€² = âˆ’ğœ†/2,
ğœ†â€² = âˆ’2ğ‘¥.
```
For ğ‘‡ = 6, ğ‘¥(0) = âˆ’1, and ğ‘¥(6) = 1, we derived solutions
```
ğ‘¥ = ğ‘’
ğ‘¡ âˆ’ ğ‘’6âˆ’ğ‘¡
ğ‘’6 âˆ’ 1 ,
ğœ† = âˆ’2 ğ‘’
ğ‘¡ + ğ‘’6âˆ’ğ‘¡
ğ‘’6 âˆ’ 1 .
The Hamiltonian for this example was ğ» = ğ‘¥2 + ğ‘¢2 + ğœ†ğ‘¢. Substituting ğ‘¢ =
âˆ’ğœ†/2 this simplifies to
ğ» = ğ‘¥2 âˆ’ 14 ğœ†2
which is the Hamiltonian function for the state-costate system. In fact, the opti-
mal control âˆ’ğœ†/2 is exactly the control that makes this a Hamiltonian system.
We can verify directly that ğ» is constant on the above trajectory by substitut-
```
ing and simplifying (âˆ— check this âˆ—):
```
```
ğ» = ( ğ‘’
```
ğ‘¡ âˆ’ ğ‘’6âˆ’ğ‘¡
```
ğ‘’6 âˆ’ 1 )
```
2
```
âˆ’ 14 (âˆ’2 ğ‘’
```
ğ‘¡ + ğ‘’6âˆ’ğ‘¡
```
ğ‘’6 âˆ’ 1 )
```
2
= âˆ’4ğ‘’
6
```
(ğ‘’6 âˆ’ 1)2 .
```
Knowing that the Hamiltonian is constant on optimal trajectories leads to an
alternate method for solving these systems. Setting the Hamiltonian equal to a
constant, ğ‘¥2 âˆ’ ğœ†2/4 = ğ¶, and solving for ğœ† = Â±2âˆšğ‘¥2 âˆ’ ğ¶, we can take
ğ‘¥â€² = âˆ’ 12 ğœ† = Â±âˆšğ‘¥2 âˆ’ ğ¶.
```
This is a separable differential equation and can be solved directly for ğ‘¥(ğ‘¡). This
```
approach turns out to be more difficult than solving the linear state-costate sys-
tem, but hey, itâ€™s an option.
5.2 The Principle of Optimality
The Principle of Optimality is:
If the optimal path from A to C goes through B, then it is also the optimal
path from A to B and the optimal path from B to C.
This is actually fairly deep.
72 Chapter 5. Unpacking Pontryagin
As you traverse an optimal path, the path is always optimal from where you are.
At any point and time along the path, the same path continues to be optimal for the
remaining time from that point forward.
It is important to recognize that this principle applies to paths in state-costate
space, and it only applies to autonomous systems. In Chapter 8 we will see how the
principle applies directly to the state space for time optimal systems.
Example 5.5: Integrator
In Examples 4.3 and 5.4 we minimized ğ½ = âˆ«ğ‘‡0 ğ‘¥2 + ğ‘¢2 ğ‘‘ğ‘¡ subject to ğ‘¥â€² = ğ‘¢ by
using a control ğ‘¢ = âˆ’ğœ†/2 which generated the linear state-costate system
ğ‘¥â€² = âˆ’ğœ†/2,
ğœ†â€² = âˆ’2ğ‘¥.
```
We can solve this system for any ğ‘‡ > 0 and any endpoint conditions ğ‘¥(0) = ğ´,
```
```
ğ‘¥(ğ‘‡) = ğµ to get the general solution:
```
```
ğ‘¥ = (ğµğ‘’ğ‘‡ âˆ’ğ´) ğ‘’ğ‘¡+(ğ´ğ‘’ğ‘‡ âˆ’ğµ) ğ‘’ğ‘‡âˆ’ğ‘¡ğ‘’2ğ‘‡ âˆ’1 ,
```
```
ğœ† = âˆ’2 (ğµğ‘’ğ‘‡ âˆ’ğ´) ğ‘’ğ‘¡âˆ’(ğ´ğ‘’ğ‘‡ âˆ’ğµ) ğ‘’ğ‘‡âˆ’ğ‘¡ğ‘’2ğ‘‡ âˆ’1 .
```
```
For example, ğ‘‡ = 2 with ğ‘¥(0) = âˆ’1 and ğ‘¥(2) = 1 yields (with simplification)
```
```
ğ‘¥(ğ‘¡) = (ğ‘’ğ‘¡ âˆ’ ğ‘’2âˆ’ğ‘¡)/(ğ‘’2 âˆ’ 1) and ğœ†(ğ‘¡) = âˆ’2(ğ‘’ğ‘¡ + ğ‘’2âˆ’ğ‘¡)/(ğ‘’2 âˆ’ 1). In Figure 5.2 we
```
plot this trajectory in the state-costate plane as a level curve of the Hamiltonian
ğ» = ğ‘¥2 âˆ’ 14 ğœ†2.
Figure 5.2. Optimal solution in state-costate space, with level
curves of the Hamiltonian.
5.3. Costates 73
```
Note that at the halfway point this trajectory has ğ‘¥(1) = 0. The following are
```
left as an exercise for the reader:
```
(a) Solving this system for ğ‘‡ = 1 with endpoints ğ‘¥(0) = âˆ’1 and ğ‘¥(1) = 0
```
```
produces the same solution ğ‘¥ = (ğ‘’ğ‘¡ âˆ’ ğ‘’2âˆ’ğ‘¡)/(ğ‘’2 âˆ’ 1).
```
```
(b) Solving this system for ğ‘‡ = 1 with endpoints ğ‘¥(0) = 0 and ğ‘¥(1) = 1
```
```
produces ğ‘¥ = (ğ‘’1+ğ‘¡ âˆ’ ğ‘’1âˆ’ğ‘¡)/(ğ‘’2 âˆ’ 1), which is our first solution ğ‘¥ = (ğ‘’ğ‘¡ âˆ’ ğ‘’2âˆ’ğ‘¡)/
```
```
(ğ‘’2 âˆ’ 1) shifted by one time unit.
```
These solutions are seen in Figure 5.3 as segments of our prior solution plot-
ted as level curves of the Hamiltonian.
Figure 5.3. Optimal solution with midway point in state-
costate space.
The Principle of Optimality is a key observation that will be important as we con-
tinue our journey through this material.
5.3 Costates
Costates are mysterious.
```
Abstractly, in a controlled system modeled by ğ‘¥â€² = ğ‘“(ğ‘¥, ğ‘¢) one is trying to find the
```
control function ğ‘¢ to optimize some measure ğ½ of performance.
Pontryaginâ€™s principle creates this curious extra differential equation in a so-called
costate variable, ğœ†, and defines potentially optimizing controls as functions of the
costate.
But what are these costate thingies? They arise from the theory of Lagrange multi-
pliers, and they tell us something about how performance depends on the constraints,
as covered in Section 2.9. Specifically:
The costates are the derivative of performance with respect to position.
Letâ€™s see how this works.
74 Chapter 5. Unpacking Pontryagin
Example 5.6: Integrator
```
Consider the integrator system ğ‘¥â€² = ğ‘¢ with endpoints ğ‘¥(0) = 0, ğ‘¥(1) = ğµ, and
```
we want to minimize
```
ğ½(ğµ) = âˆ«
```
1
0
ğ‘¥2 + ğ‘¢2 ğ‘‘ğ‘¡
which is a function of the prescribed endpoint ğµ. How will ğ½ depend on ğµ? In-
creasing/decreasing? Can you guess concavity?
As in Example 4.3, we have ğ» = ğ‘¥2 + ğ‘¢2 + ğœ†ğ‘¢, 0 = ğœ•ğ»ğœ•áµ† = 2ğ‘¢ + ğœ†, ğ‘¢ = âˆ’ğœ†/2
and so
ğ‘¥â€² = âˆ’ğœ†/2,
ğœ†â€² = âˆ’2ğ‘¥.
```
Solving these with boundary conditions ğ‘¥(0) = 0 and ğ‘¥(1) = ğµ yields
```
ğ‘¥ = ğµ ğ‘’
1+ğ‘¡ âˆ’ ğ‘’1âˆ’ğ‘¡
ğ‘’2 âˆ’ 1 ,
ğœ† = âˆ’2ğµ ğ‘’
1+ğ‘¡ + ğ‘’1âˆ’ğ‘¡
ğ‘’2 âˆ’ 1 ,
ğ‘¢ = ğµ ğ‘’
1+ğ‘¡ + ğ‘’1âˆ’ğ‘¡
ğ‘’2 âˆ’ 1 ,
and performance
```
ğ½(ğµ) = ğµ2 ğ‘’
```
2 + 1
ğ‘’2 âˆ’ 1
```
and we see that ğ½(ğµ) is quadratic in ğµ.
```
In particular, and this is the point, we have
```
ğ½â€²(ğµ) = ğ‘‘ğ½ğ‘‘ğµ = 2ğµ ğ‘’
```
2 + 1
```
ğ‘’2 âˆ’ 1 = âˆ’ğœ†(1).
```
```
That is, ğœ†(1) is the decrease in cost per change in final location ğµ. Note that
```
```
ğœ†(1) < 0 and we are trying to minimize ğ½(ğµ). Consistent with our analysis in
```
Section 2.9, an increase in ğµ will increase the minimum.
Dimensional analysis also applies here. In order to add ğ‘” and ğœ†ğ‘“ in the Hamilton-
ian ğ» = ğ‘” + ğœ†ğ‘“, they must have the same units. The units for ğ‘” are performance-units
per time. The units for ğ‘“ are state-units per time. So ğœ† must be performance-units per
state-units, which is consistent with ğœ† being the change in performance per change in
state.
The whole bit about ğœ† being marginal performance holds at any point along the
path, consistent with the Principle of Optimality. If we operate the optimal control
```
for 0 < ğ‘¡ < 1, and at some time ğ‘¡ = ğ›½ for 0 < ğ›½ < 1 we are at location ğ‘¥(ğ›½) with
```
```
accumulated payoff ğ½ğ›½ = âˆ«ğ›½0 ğ‘”(ğ‘¥, ğ‘¢) ğ‘‘ğ‘¡, then ğœ†(ğ›½) is the rate of change of accumulated
```
payoff with respect to current position under the assumption of optimal control.
5.4. Minimal Surfaces 75
If your head is spinning, you are not alone and it is a good sign. We will delve
into these ideas more as we go along. For now, rest assured that you can totally work
all exercises and solve optimal control problems without having a lot of confidence in
understanding the more abstract context.
5.4 Minimal Surfaces
Pontryaginâ€™s principles are extremely flexible and apply to a wide variety of problems.
The following example demonstrates an application to a geometric problem and is a
case where taking ğ» to be constant on optimal trajectories leads to a more tractable so-
lution to the differential equations. This example also demonstrates a technique where
instead of solving for control ğ‘¢ in terms of costate ğœ†, we formulate the solution by ex-
pressing the costate ğœ† in terms of ğ‘¢.
Example 5.7
```
Consider the system ğ‘…â€²(ğ‘§) = ğ‘¢ with control ğ‘¢ and endpoint conditions ğ‘…(âˆ’ğ¿) =
```
```
ğ‘…(ğ¿) = 1. We want to find a control that minimizes
```
ğ½ = âˆ«
ğ¿
âˆ’ğ¿
ğ‘…âˆš1 + ğ‘¢2 ğ‘‘ğ‘§.
Here ğ‘… is our state variable, and ğ‘§ is taking the place of the time variable.
```
Figure 5.4. A possible solution curve ğ‘¦ = ğ‘…(ğ‘§).
```
Note that the cost function is monotone increasing in ğ‘… and in |ğ‘¢|. So we
want to reduce ğ‘… values without incurring too steep of a slope for ğ‘…, while still
```
hitting endpoints of ğ‘…(Â±ğ¿) = 1. One possible solution is plotted in Figure 5.4.
```
The Hamiltonian is
```
ğ»(ğ‘…, ğ‘¢, ğœ†) = ğ‘…âˆš1 + ğ‘¢2 + ğœ†ğ‘¢.
```
76 Chapter 5. Unpacking Pontryagin
Setting ğœ•ğ»ğœ•áµ† = 0 yields
```
ğ‘…ğ‘¢(1 + ğ‘¢2)âˆ’1/2 + ğœ† = 0.
```
Typically, we would solve this for ğ‘¢ in terms of ğœ†. In this case we will do the
```
opposite and solve for ğœ† = âˆ’ğ‘…ğ‘¢(1 + ğ‘¢2)âˆ’1/2 in terms of ğ‘¢. Substituting this back
```
into the Hamiltonian we get
```
ğ» = ğ‘…(1 + ğ‘¢2)1/2 âˆ’ ğ‘…ğ‘¢2(1 + ğ‘¢2)âˆ’1/2
```
```
= ğ‘…(1 + ğ‘¢2)âˆ’1/2.
```
We know that for an optimal solution, the Hamiltonian must be a constant,
```
ğ» = ğ¶, and we have that ğ‘¢ = ğ‘…â€²(ğ‘§). Combining these produces a differential
```
equation
```
ğ¶ = ğ‘…(1 + (ğ‘…â€²)2)âˆ’1/2.
```
With some challenging finagling, we can find a symmetric solution to this equa-
tion as
```
ğ‘…(ğ‘§) = 12 ğ¶ (ğ‘’ğ¿/ğ¶ + ğ‘’âˆ’ğ¿/ğ¶ ) = ğ¶ cosh(ğ¿/ğ¶).
```
```
To match the boundary conditions, we need ğ‘…(Â±ğ¿) = 1. Solving this condition
```
for ğ¿ yields
```
ğ¿ = ğ¶ ln((1 + âˆš1 âˆ’ ğ¶2)/ğ¶) = ğ¶ coshâˆ’1(1/ğ¶).
```
Hereâ€™s where things get interesting. There are two solutions to this equation if
0 < ğ¿ < .6627 . . . and no solutions if ğ¿ > .6627 . . . , as we can see by plotting
```
the function ğ¿ = ğ¹(ğ¶) = ğ¶ coshâˆ’1(1/ğ¶). This is a concave downward function
```
```
defined on 0 < ğ¶ â‰¤ 1 with a maximum of ğ¿ = .6627 . . . at ğ¶ = 0.5524 . . . (Figure
```
```
5.5).
```
```
Figure 5.5. ğ¹(ğ¶) = ğ¿ has two solutions for ğ¿ < 0.6627 . . . , one
```
solution for ğ¿ = 0.6627 . . . , and no solutions for ğ¿ > .6627 . . . .
Whatâ€™s going on? Are there two minimal solutions for small ğ¿ and no solu-
tions for larger ğ¿? How does that work?
5.4. Minimal Surfaces 77
A catenoid is the shape formed by a soap film spanning two circles held par-
allel and aligned. A property of soap films is that they minimize surface area,
and we just solved for this shape using Pontryaginâ€™s principle. The soap film is
```
modeled as a surface formed by revolving the function ğ‘¦ = ğ‘…(ğ‘§) around the hor-
```
izontal ğ‘§-axis. The area of this surface is âˆ«ğ¿âˆ’ğ¿ ğ‘…âˆš1 + ğ‘¢2 ğ‘‘ğ‘§ and is minimized by
the action of surface tension in the soap film. The unit radius ring boundary is
```
satisfied by ğ‘…(Â±ğ¿) = 1:
```
Why are there two solutions for 0 < ğ¿ < .6627 . . . ? Because there is actually
a third solution to the soap film problem, which is a flat disk of film on each ring.
These three solutions are two stable solutions with a third solution in the middle
that doesnâ€™t know which way to go, like a pendulum balanced in the perfectly
upright position. A film in this middle configuration will stay in that configura-
tion, but any tiny perturbation would cause it to spring back to a stable catenoid
or pinch off to form two disks:
For ğ¿ > .6627 . . . , the two rings are simply too far apart to be spanned by a
stable soap film, and Pontryaginâ€™s approach doesnâ€™t produce a minimizing solu-
tion.
78 Chapter 5. Unpacking Pontryagin
Key Points
The previous chapter introduced the main format for analyzing optimal control prob-
lems in continuous systems. In this chapter we took some time to more carefully ex-
amine some of the ideas of the method:
â€¢ The fact that the Hamiltonian is constant on optimal trajectories for time-indepen-
dent problems means that we can understand trajectories as level curves of the
Hamiltonian function.
â€¢ The Principle of Optimality is a fundamental concept and states that every segment
of an optimal trajectory in state-costate space is itself optimal.
â€¢ Costates represent marginal payoffs.
â€¢ Pontryaginâ€™s principles are flexible and can apply to geometric constructs.
Exercises
Exercise 5.1. Suppose
```
ğ‘¥â€² = ğ‘“(ğ‘¥, ğ‘¦),
```
```
ğ‘¦â€² = ğ‘”(ğ‘¥, ğ‘¦)
```
is a Hamiltonian system. Show that
ğœ•ğ‘“
ğœ•ğ‘¥ = âˆ’
ğœ•ğ‘”
ğœ•ğ‘¦ .
```
Exercise 5.2(h). Under what conditions is the linear system
```
ğ‘¥â€² = ğ‘ğ‘¥ + ğ‘ğ‘¦,
ğ‘¦â€² = ğ‘ğ‘¥ + ğ‘‘ğ‘¦
```
a Hamiltonian system? Are the conditions necessary and sufficient (that is, are your
```
```
conditions â€œif and only ifâ€)? What is the Hamiltonian ğ»(ğ‘¥, ğ‘¦)? How does the shape of
```
```
ğ» determine the behavior of the system near the fixed point (0, 0)?
```
```
Exercise 5.3(hs). Suppose ğ‘¥(ğ‘¡) = ğ›¼ğ‘’ğ‘¡ + ğ›½ğ‘’âˆ’ğ‘¡ is a time minimizing solution for some
```
optimal control problem, where the parameters ğ›¼ and ğ›½ are chosen to match endpoint
conditions.
```
(a) Solve for ğ›¼ and ğ›½ to match ğ‘¥(0) = 0 and ğ‘¥(2) = 1.
```
```
(b) Show that for your solution to (a), you have ğ‘¥(1) = 1/(ğ‘’ + ğ‘’âˆ’1).
```
```
(c) Solve for ğ›¼ and ğ›½ to match ğ‘¥(0) = 1/(ğ‘’ + ğ‘’âˆ’1) and ğ‘¥(1) = 1.
```
```
(d) Show that your solution in part (c) is the same trajectory as your solution in
```
```
part (a), shifted by one time unit.
```
Exercises 79
Exercise 5.4. Consider the system ğ‘¥â€² = ğ‘¥ + ğ‘¢ with performance ğ½ = âˆ«ğ‘‡0 ğ‘¢2/2 ğ‘‘ğ‘¡.
```
(a) Show that the minimizing solution for fixed time ğ‘‡ and endpoints ğ‘¥(0) = ğ´
```
```
and ğ‘¥(ğ‘‡) = ğµ is
```
```
ğ‘¥(ğ‘¡) = ğ‘’
```
âˆ’ğ‘¡
```
ğ‘’2ğ‘‡ âˆ’ 1 (ğµğ‘’
```
```
ğ‘‡ (ğ‘’2ğ‘¡ âˆ’ 1) + ğ´(ğ‘’2ğ‘‡ âˆ’ ğ‘’2ğ‘¡)) .
```
```
(b) Construct the solution for ğ‘‡ = 2, ğ‘¥(0) = 0, and ğ‘¥(2) = ğ‘’ + ğ‘’âˆ’1. Show that this
```
```
solution has ğ‘¥(1) = 1.
```
```
(c) Construct the solution for ğ‘‡ = 1, ğ‘¥(0) = 0, and ğ‘¥(1) = 1. Show that this
```
```
solution is algebraically equivalent to the solution from part (b).
```
```
(d) Construct the solution for ğ‘‡ = 1, ğ‘¥(0) = 1, and ğ‘¥(1) = ğ‘’ + ğ‘’âˆ’1. Show that this
```
```
solution is algebraically equivalent to the solutions from parts (a) and (b), shifted by
```
one time unit.
```
(e) Explain parts (a)â€“(d) using level curves of the Hamiltonian.
```
```
Exercise 5.5(hs). Consider the system ğ‘¥â€² = 2ğ‘¢ with performance ğ½ = âˆ«ğ‘‡012 ğ‘¥2 âˆ’2ğ‘¢2 ğ‘‘ğ‘¡.
```
```
(a) Solve for the control ğ‘¢, in terms of ğ‘¥ and ğœ†, that maximizes ğ½.
```
```
(b) Substitute back into the Hamiltonian and sketch some level curves in the (ğ‘¥, ğœ†)-
```
plane.
```
(c) Solve the ğ‘¥â€², ğœ†â€² system to get the general solution ğ‘¥(ğ‘¡) = ğ›¼ cos ğ‘¡ + ğ›½ sin ğ‘¡ for ğ‘¥.
```
```
(d) Solve for boundary conditions ğ‘¥(0) = âˆ’1, ğ‘¥(ğœ‹/2) = 1. Sketch the trajectory in
```
```
the (ğ‘¥, ğœ†)-plane and verify that ğ‘¥(ğœ‹/4) = 0.
```
```
(e) Solve for the boundary conditions ğ‘¥(0) = âˆ’1 and ğ‘¥(ğœ‹/4) = 0. Show that you
```
```
get the same trajectory as in part (d).
```
```
(f) Solve for boundary conditions ğ‘¥(0) = 0, ğ‘¥(ğœ‹/4) = 1. Show that this produces
```
```
the same trajectory as in part (b), but shifted by ğœ‹/4 time units.
```
```
Exercise 5.6(s). Consider the system ğ‘¥â€² = 2ğ‘¥+ğ‘¢ with performance ğ½ = âˆ«ğ‘‡0 ğ‘¥2âˆ’ 14 ğ‘¢2 ğ‘‘ğ‘¡.
```
```
(a) Solve for the control ğ‘¢, in terms of ğ‘¥ and ğœ†, that maximizes ğ½.
```
```
(b) Substitute back into the Hamiltonian and conclude that ğ‘¥ + ğœ† is constant on
```
optimal trajectories.
```
(c) Substitute for ğ‘¢ in ğ‘¥â€² = 2ğ‘¥ + ğ‘¢ and conclude that ğ‘¥â€² is constant.
```
```
Exercise 5.7(s). Consider the system ğ‘¥â€² = ğ‘¢ğ‘¥ with performance ğ½ = âˆ«ğ‘‡014 ğ‘¥ğ‘¢2 ğ‘‘ğ‘¡.
```
```
(a) Solve for the control ğ‘¢, in terms of ğ‘¥ and ğœ†, that maximizes ğ½.
```
```
(b) Substitute back into the Hamiltonian and sketch some level curves in the (ğ‘¥, ğœ†)-
```
plane.
80 Chapter 5. Unpacking Pontryagin
```
(c) Derive the equation for ğœ†â€² and substitute for ğ‘¢ to get ğœ†â€² = ğœ†2. Solve to conclude
```
```
ğœ† = 1/(ğ¶ âˆ’ ğ‘¡).
```
```
(d) Substitute ğœ† = 1/(ğ¶ âˆ’ ğ‘¡) back into the Hamiltonian, and use the fact that the
```
Hamiltonian is constant on optimal trajectories to solve for ğ‘¥.
Note that you were able to solve for ğ‘¥ without solving the differential equation for
ğ‘¥, as in Examples 5.3 and 5.4.
```
Exercise 5.8(s). Analyze ğ½ = âˆ«ğœ‹/4012 ğ‘¥2 âˆ’2ğ‘¢2 ğ‘‘ğ‘¡ for ğ‘¥â€² = 2ğ‘¢ (same system as in Exercise
```
```
5.5) with ğ‘¥(0) = 0, ğ‘¥(ğœ‹/4) = ğµ.
```
```
(a) For any ğµ, compute the maximum performance ğ½(ğµ).
```
```
(b) Verify ğ½â€²(ğµ) = âˆ’ğœ†(ğœ‹/4).
```
```
Exercise 5.9(s). Consider the system ğ‘¥â€² = ğ‘¥+ğ‘¢ with ğ‘¢ > 0 and performance âˆ«10 ln ğ‘¢ ğ‘‘ğ‘¡,
```
as in Exercises 4.2 and 4.7.
```
(a) For ğ‘¥(0) = ğ´ and ğ‘¥(1) = ğµ, solve for the maximum performance ğ½(ğ´, ğµ).
```
```
(b) Show that ğœ•ğ½ğœ•ğµ = âˆ’ğœ†(1) and ğœ•ğ½ğœ•ğ´ = ğœ†(0).
```
```
Exercise 5.10(s). Consider the controlled system ğ‘¥â€² = ğ‘¢ with performance index ğ½ =
```
```
âˆ«1012 (ğ‘¢2 âˆ’ ğœ‹2ğ‘¥2) ğ‘‘ğ‘¡ and endpoint conditions ğ‘¥(0) = ğ´ and ğ‘¥(1) = ğµ.
```
```
(a) Find the control ğ½(ğ´, ğµ) that minimizes performance.
```
```
(b) Verify ğœ•ğ½ğœ•ğ´ (ğ´, ğµ) = ğœ†(0) and ğœ•ğ½ğœ•ğµ (ğ´, ğµ) = âˆ’ğœ†(1).
```
```
(c) For optimal control ğ‘¢, show that the controlÌƒ ğ‘¢ = ğ‘¢ + ğ›¿ cos(ğ‘›ğœ‹ğ‘¡), where ğ‘› is an
```
```
integer, would still match the endpoint conditions ğ‘¥(0) = ğ´ and ğ‘¥(1) = ğµ.
```
```
(d) Show that ğ½ is strictly greater using any such controlÌƒ ğ‘¢.
```
```
Parts (c) and (d) and some Fourier theory will imply directly that ğ‘¢ is a locally
```
optimal control.
```
Exercise 5.11. Consider the controlled system ğ‘…â€²(ğ‘§) = ğ‘¢ with endpoint conditions
```
```
ğ‘…(ğ‘) = ğ´ and ğ‘…(ğ‘) = ğµ, ğ‘ < ğ‘. Find the control that minimizes ğ½ = âˆ«ğ‘ğ‘ âˆš1 + ğ‘¢2 ğ‘‘ğ‘§.
```
Solve this exercise using steps similar to those in Example 5.7: set up the Hamil-
tonian, set ğœ•áµ†ğ» = 0, solve for ğœ† in terms of ğ‘¢, and substitute back into the Hamiltonian.
Then use the fact that the Hamiltonian must be constant on optimal solutions to con-
clude that ğ‘¢ must be constant.
```
Then use the fact that âˆ«ğ‘ğ‘ âˆš1 + (ğ‘…â€²)2 ğ‘‘ğ‘§ is arclength to argue that the shortest dis-
```
```
tance between two points (ğ‘, ğ´) and (ğ‘, ğµ) on a plane is a straight line.
```
6
Easing the Restrictions
Principles I and II considered fixed endpoints and a fixed time duration. We ease these
restrictions in this chapter and allow for end time and terminal location to be free and
therefore subject to optimization. This typically includes payoff functions based on
terminal location, as in the Bocce ball examples.
Freeing up the endpoint conditions can cause issues. We will explore more cases
like those in Section 4.3 where there is no valid solution to the optimization problem.
We will conclude this chapter with a proof of Pontryaginâ€™s principle. The classic
proof we present requires the free terminal location.
6.1 One Dimension, Free Ends
Consider the controlled system
```
ğ‘¥â€² = ğ‘“(ğ‘¥, ğ‘¢, ğ‘¡)
```
```
where ğ‘¥ is state, ğ‘¢ is control, and ğ‘¡ is time. Suppose we have a starting conditions ğ‘¥(0) =
```
```
ğ´ but may or may not have a specified end time ğ‘‡ and/or end condition ğ‘¥(ğ‘‡) = ğµ. We
```
want to optimize
```
ğ½(ğµ, ğ‘‡, ğ‘¢) = ğº(ğµ, ğ‘‡) + âˆ«
```
ğ‘‡
0
```
ğ‘”(ğ‘¥, ğ‘¢, ğ‘¡) ğ‘‘ğ‘¡,
```
where we allow a payoff/cost ğº for the ending location ğµ and total time ğ‘‡. The follow-
ing more general optimal principle provides necessary conditions for optimization in
this expanded scope.
81
82 Chapter 6. Easing the Restrictions
OPTIMAL PRINCIPLE III
Local optimum, free duration, free endpoint, time dependent, one dimension
Consider the controlled system
```
ğ‘¥â€² = ğ‘“(ğ‘¥, ğ‘¢, ğ‘¡), ğ‘¥, ğ‘¡ âˆˆ â„, ğ‘¢ âˆˆ ğ’°,
```
```
starting at ğ‘¥(0) = ğ´ and objective function
```
```
ğ½ = ğº(ğµ, ğ‘‡) + âˆ«
```
ğ‘‡
0
```
ğ‘”(ğ‘¥, ğ‘¢, ğ‘¡) ğ‘‘ğ‘¡
```
```
where ğµ = ğ‘¥(ğ‘‡).
```
Define the Hamiltonian
```
ğ»(ğ‘¥, ğœ†, ğ‘¢, ğ‘¡) = ğ‘”(ğ‘¥, ğ‘¢, ğ‘¡) + ğœ†ğ‘“(ğ‘¥, ğ‘¢, ğ‘¡)
```
and costate equation
ğœ†â€² = âˆ’ ğœ•ğ»ğœ•ğ‘¥ .
Then a locally optimal control must satisfy
ğœ•ğ»
ğœ•ğ‘¢ = 0
and the control ğ‘¢ that optimizes ğ½ will optimize ğ» at all times.
```
The ending location ğ‘¥(ğ‘‡) = ğµ may be prescribed. Otherwise, the optimal
```
ending location will satisfy
ğœ•ğº
```
ğœ•ğµ (ğµ, ğ‘‡) âˆ’ ğœ†(ğ‘‡) = 0.
```
The ending time ğ‘‡ may be prescribed. Otherwise, the optimal ending time will
satisfy
ğœ•ğº
```
ğœ•ğ‘‡ (ğµ, ğ‘‡) + ğ»(ğ‘‡) = 0.
```
Furthermore, if ğ‘“ and ğ‘” are independent of time ğ‘¡, then ğ» is constant on optimal
trajectories.
This principle allows us to consider time dependence, free endpoint conditions,
free endtime conditions, or any combination thereof.
The next example is a continuous version of the Bocce ball examples, where costs
accrue during the game with a payoff at the end determined by how far down the ğ‘¥-axis
the ball has been moved.
Example 6.1: Continuous Bocce
Consider using control
ğ‘¥â€² = ğ‘¢
to maximize
```
ğ½ = ğ‘¥(ğ‘‡) âˆ’ âˆ«
```
ğ‘‡
0
ğ‘¢2
ğ‘¥ ğ‘‘ğ‘¡
6.1. One Dimension, Free Ends 83
```
with given starting position ğ‘¥(0) = ğ‘¥0 and end time ğ‘‡. We have
```
```
ğ»(ğ‘¥, ğœ†, ğ‘¢) = âˆ’ ğ‘¢
```
2
ğ‘¥ + ğœ† ğ‘¢
and costate equation
ğœ†â€² = âˆ’ ğœ•ğ»ğœ•ğ‘¥ = âˆ’ ğ‘¢
2
ğ‘¥2 .
Setting
ğœ•ğ»
ğœ•ğ‘¢ = âˆ’
2ğ‘¢
ğ‘¥ + ğœ† = 0
leads to ğ‘¢ = 12 ğœ†ğ‘¥. Using this control yields the state-costate system
ğ‘¥â€² = 12 ğœ†ğ‘¥,
ğœ†â€² = âˆ’ 14 ğœ†2.
```
We have one initial condition ğ‘¥(0) = ğ‘¥0, but in order to define a unique tra-
```
jectory in this system we need an additional condition. The endtime ğ‘‡ is speci-
```
fied, but the end location ğ‘¥(ğ‘‡) is free. Therefore we look for a control that satisfies
```
ğœ•ğº
```
ğœ•ğµ (ğµ, ğ‘‡) âˆ’ ğœ†(ğ‘‡) = 0.
```
```
With ğº(ğµ, ğ‘‡) = ğµ this yields ğœ†(ğ‘‡) = 1
```
We solve this system by first solving the second equation ğœ†â€² = âˆ’ğœ†2/2 as a
```
separable equation with endpoint condition ğœ†(ğ‘‡) = 1 to get ğœ† = 4/(4 + ğ‘¡ âˆ’ ğ‘‡)
```
```
(âˆ— verify this âˆ—). Substituting this result into the first equation and solving with
```
```
boundary condition ğ‘¥(0) = ğ‘¥0 produces optimal trajectory (âˆ— check this âˆ—)
```
```
ğ‘¥ = ğ‘¥0 (1 + ğ‘¡4 âˆ’ ğ‘‡ )
```
2
.
```
Substituting ğ‘¥(ğ‘¡) and ğœ†(ğ‘¡) back into the Hamiltonian we can verify that ğ» does
```
```
not depend upon ğ‘¡ (âˆ— verify âˆ—), so trajectories are level curves of ğ» = 14 ğœ†2ğ‘¥.
```
Solving this with ğ‘¥0 = 12 and ğ‘‡ = 2 produces the following trajectories for ğ‘¥
```
and ğ‘¢ in time (Figure 6.1) and for ğ‘¥ and ğœ† in the state-costate space (Figure 6.2).
```
Figure 6.1. State and control plotted against time, optimal control.
84 Chapter 6. Easing the Restrictions
Figure 6.2. Optimal solution in state-costate space, with level
curves of the Hamiltonian.
Example 6.2: Ninety-Nine Bottles of Beer
We have ninety-nine bottles of a favored beverage arranged on shelving along a
wall. Every night one of the bottles mysteriously falls and breaks. No one can
figure out why. It just happens.
```
We have a diminishing resource ğ‘¥(ğ‘¡) that starts at ğ‘¥(0) = 100 that naturally
```
loses one unit per day. We can withdraw ğ‘¢ units per day from this resource and
consume it, making
ğ‘¥â€² = âˆ’1 âˆ’ ğ‘¢.
We want to draw from this resource in such a way to maximize our enjoyment
ğ½ = âˆ«
ğ‘‡
0
âˆšğ‘¢ ğ‘‘ğ‘¡.
Here we use the concave downwards function âˆšğ‘¢ to model the saturation effect
of our enjoyment.
Consuming the resources quickly is not a good strategy because of the di-
minished returns at high levels of consumption. However, if we consume too
slowly, we also miss out because the resources naturally disappear so we donâ€™t
get to consume them. So what is the perfect balance?
Our end time ğ‘‡ is not predetermined but is taken to be when our resources
```
run out, ğ‘¥(ğ‘‡) = 0.
```
The Hamiltonian is
```
ğ» = âˆšğ‘¢ âˆ’ ğœ†(1 + ğ‘¢)
```
and ğœ•ğ»ğœ•ğ‘¥ = 0, so ğœ† is constant.
6.1. One Dimension, Free Ends 85
We require
0 = ğœ•ğ»ğœ•ğ‘¢ = 1
2âˆšğ‘¢
âˆ’ ğœ†
```
and so ğ‘¢ = 1/(4ğœ†2) is also constant. We check that ğœ•2ğ»ğœ•áµ†2 < 0, and maximizing is
```
appropriate.
Knowing that the control is constant, we could solve the problem directly by
```
setting ğ‘¢ = ğ¾, solving for ğ‘‡, computing ğ½, and maximizing over ğ¾ (âˆ— try it âˆ—).
```
```
Sticking to Principle III, we substitute ğ‘¢ = 1/(4ğœ†2) into ğ» and get
```
ğ» = 14ğœ† âˆ’ ğœ†.
```
Our endtime ğ‘‡ is free, so we look for the condition ğ»(ğ‘‡) + ğœ•ğºğœ•ğ‘‡ = 0. In this case
```
ğœ•ğº
```
ğœ•ğ‘‡ = 0, so ğ»(ğ‘‡) = 0.
```
```
We know that ğ» is constant on optimal trajectories, so ğ»(ğ‘‡) = 0 means that
```
ğ» is identically zero, ğ» â‰¡ 0. This implies ğœ† = Â±1/2 and ğ‘¢ = 1. Our optimal
```
trajectory is then ğ‘¥(ğ‘¡) = 100 âˆ’ 2ğ‘¡ with final time ğ‘‡ = 50 days (Figure 6.3).
```
Figure 6.3. State and consumption plotted against time, opti-
mal control.
Should we ever find ourselves in this situation, and having these powerful op-
timization techniques in hand, we can confidently relax and consume one bever-
age every evening in full knowledge that we are optimizing our enjoyment under
the given circumstances.
The previous example resulted in linear consumption. However, if we discount
```
future rewards, as in King Tiny (Example 4.5), we get a slightly different result.
```
86 Chapter 6. Easing the Restrictions
Example 6.3: Ninety-Nine Bottles of Beer, Discounted
We repeat the previous example with a discounted future. Specifically, we have
```
a diminishing resource ğ‘¥(ğ‘¡) that starts at ğ‘¥(0) = 100 that naturally loses one unit
```
per day, and we can draw ğ‘¢ units per day from this resource, making
ğ‘¥â€² = âˆ’1 âˆ’ ğ‘¢.
How can we draw from this resource in such a way as to maximize
ğ½ = âˆ«
ğ‘‡
0
ğ‘’âˆ’ğ›¼ğ‘¡âˆšğ‘¢ ğ‘‘ğ‘¡?
We can assume there will be nothing left of the resource when we are done, mak-
```
ing ğ‘¥(ğ‘‡) = 0.
```
Our Hamiltonian is
```
ğ» = ğ‘’âˆ’ğ›¼ğ‘¡âˆšğ‘¢ âˆ’ ğœ†(1 + ğ‘¢)
```
and ğœ•ğ»ğœ•ğ‘¥ = 0, so ğœ†â€² = âˆ’ ğœ•ğ»ğœ•ğ‘¥ = 0 implying ğœ† is constant.
We require
```
0 = ğœ•ğ»ğœ•ğ‘¢ = ğ‘’âˆ’ğ›¼ğ‘¡/(2âˆšğ‘¢) âˆ’ ğœ†,
```
```
and so ğ‘¢ = ğ‘’âˆ’2ğ›¼ğ‘¡/(4ğœ†2). Substituting this into our system,
```
ğ‘¥â€² = âˆ’1 âˆ’ ğ‘’
âˆ’2ğ›¼ğ‘¡
4ğœ†2
with general solution
ğ‘¥ = âˆ’ğ‘¡ + ğ‘’
âˆ’2ğ›¼ğ‘¡
```
8ğ›¼ğœ†2 + ğ¶. (6.1)
```
We currently have three unknown constants, ğœ†, ğ‘‡, and the integration con-
```
stant ğ¶. We have two endpoint conditions ğ‘¥(0) = 100 and ğ‘¥(ğ‘‡) = 0. We get a
```
```
third condition from ğ»(ğ‘‡) + ğœ•ğºğœ•ğ‘‡ = 0 and since ğœ•ğºğœ•ğ‘‡ = 0, we have ğ»(ğ‘‡) = 0. Note
```
that in this case, ğ» is a function of ğ‘¡ and is not constant on optimal trajectories.
```
Substituting ğ‘¢ = ğ‘’âˆ’2ğ›¼ğ‘¡/(4ğœ†2) into ğ» and solving ğ»(ğ‘‡) = 0 yields ğ‘’âˆ’ğ›¼ğ‘‡ = 2ğœ†.
```
```
So we have equation (6.1) with ğ‘¥(0) = 100, ğ‘¥(ğ‘‡) = 0, and ğ‘’âˆ’ğ›¼ğ‘‡ = 2ğœ†. Elimi-
```
```
nating ğœ† yields ğ‘¥(ğ‘¡) = âˆ’ğ‘¡+ğ‘’2ğ›¼(ğ‘‡âˆ’ğ‘¡)/(2ğ›¼)+ğ¶. Resolving ğ‘¥(0) = 100 yields (âˆ— check
```
```
these steps âˆ—)
```
```
ğ‘¥(ğ‘¡) = 100 âˆ’ ğ‘¡ + 12ğ›¼ (ğ‘’2ğ›¼(ğ‘‡âˆ’ğ‘¡) âˆ’ ğ‘’2ğ›¼ğ‘‡ )
```
```
for 0 â‰¤ ğ‘¡ â‰¤ ğ‘‡. To find the final time, we set ğ‘¥(ğ‘‡) = 0 and solve for ğ‘‡. For
```
example, a 1% discount, ğ›¼ = 0.01, yields a numerically approximated final time
```
of ğ‘‡ = 39.60 . . . days (Figure 6.4).
```
6.2. When Things Go Wrong 87
Figure 6.4. State and consumption plotted against time, opti-
mal control, discounted future.
With a future discount we initially consume at a higher rate, over two bev-
erages an evening, tapering off to little over one beverage an evening when our
supply is depleted at time ğ‘‡ = 39.62 . . . , about 10 days earlier than in the nondis-
counted model.
6.2 When Things Go Wrong
Allowing for a free end time in the previous two examples lead to a unique solution
with finite ğ‘‡ in each case. This doesnâ€™t always happen.
As we have seen in Section 4.3, the set of possible solutions may not be closed.
There may be a sequence of controls that satisfy the properties we want, and the se-
quence may converge, but the limiting control does not satisfy the same properties.
This was the case in Example 4.9 where we had $100 to spend with payoff ğ½ = âˆ«ğ‘‡0 âˆšğ‘¢ ğ‘‘ğ‘¡,
```
which increases as ğ‘‡ increases. So if time ğ‘‡ were free (and there is no future discount),
```
we would wind up not spending any of it.
The following is another example of how this problem arises where ğ‘‡ is free.
Example 6.4: Integrator
For the integrator ğ‘¥â€² = ğ‘¢ we want to minimize
ğ½ = âˆ«
ğ‘‡
0
ğ‘¥2 + ğ‘¢2 ğ‘‘ğ‘¡
```
with endpoint conditions ğ‘¥(0) = âˆ’1, ğ‘¥(ğ‘‡) = 1 and a free endtime ğ‘‡.
```
88 Chapter 6. Easing the Restrictions
```
Solving this for any fixed time ğ‘‡, as in Example 5.5, leads to solutions (âˆ— check
```
```
these âˆ—)
```
```
ğ‘¥ğ‘‡ (ğ‘¡) = ğ‘’ğ‘¡âˆ’ğ‘’ğ‘‡âˆ’ğ‘¡ğ‘’ğ‘‡ âˆ’1 ,
```
```
ğ‘¢ğ‘‡ (ğ‘¡) = ğ‘’ğ‘¡+ğ‘’ğ‘‡âˆ’ğ‘¡ğ‘’ğ‘‡ âˆ’1 .
```
Performance ğ½ as a function of allowed time ğ‘‡ is computed as
```
ğ½(ğ‘‡) = 2 ğ‘’
```
ğ‘‡ + 1
ğ‘’ğ‘‡ âˆ’ 1
and is plotted in Figure 6.5.
Figure 6.5. Plot of performance ğ½ as a function of allowed time ğ‘‡.
Note that ğ½ â†’ âˆ as ğ‘‡ â†’ 0 as it becomes very expensive to move from ğ‘¥ = âˆ’1
to ğ‘¥ = 1 in very short periods of time.
On the other hand, as ğ‘‡ â†’ âˆ this cost function monotonically decreases to
```
ğ½ = 2, and trajectories ğ‘¥ğ‘‡ (ğ‘¡), plotted in Figure 6.6, spend increasingly more time
```
near ğ‘¥ = 0.
```
Figure 6.6. Sequence of optimal solutions ğ‘¥(ğ‘¡) for increasing
```
values of ğ‘‡.
```
The limiting control is ğ‘¢ğ‘‡ (ğ‘¡) â†’ ğ‘’âˆ’ğ‘¡. However, using the control ğ‘¢(ğ‘¡) = ğ‘’âˆ’ğ‘¡
```
```
with ğ‘¥(0) = âˆ’1 produces a trajectory ğ‘¥(ğ‘¡) = âˆ’ğ‘’âˆ’ğ‘¡ < 0, which never attains the
```
required endpoint of ğ‘¥ = 1. It doesnâ€™t even get close.
A sequence of better and better controls and trajectories fails to converge to a
controlled system that will match the endpoint conditions. There is no optimiz-
ing solution for a free endtime.
```
This lack of a solution is the result of there being no upper bound on time;
```
we can always spend a bit more time and save a bit more cost, but the limit of
spending infinite time does not achieve the required end conditions.
6.3. Proving Pontryagin 89
6.3 Proving Pontryagin
We conclude this chapter with a proof of Pontryaginâ€™s method. This classic proof relies
```
on end time and location being free (Principle III). The proof is pretty awesome but
```
may be skipped by those whose primary interest is applications.
We will walk through a basic treatment of the proof, highlighting the key ideas.
For a more in-depth treatment, see [5, 10, 13, 15, 16].
6.3.1 A Basic Case. To introduce the structure of the proof, we start with a very
simplified version of a control problem without time dependence, and therefore with-
out the compounding complexity of multistep controls, and which can be solved di-
rectly with calculus-level optimization methods.
Suppose you have a system you want to control and the state of this system is rep-
resented as a single real variable ğ‘¥. You can influence this system by choosing a value
for a control variable ğ‘¢.
We assume the relationship between the state ğ‘¥ and the control ğ‘¢ is defined im-
```
plicitly by 0 = ğ‘“(ğ‘¥, ğ‘¢). That is, as we operate our control ğ‘¢, the state ğ‘¥ changes to keep
```
```
ğ‘“(ğ‘¥, ğ‘¢) = 0.
```
```
We assume our payoff is given as a performance function ğ‘”(ğ‘¥, ğ‘¢) that we want to
```
maximize. So we want to find a value for ğ‘¢ that gives the highest possible value for
```
ğ‘”(ğ‘¥, ğ‘¢) under the restriction ğ‘“(ğ‘¥, ğ‘¢) = 0.
```
```
This basic control scenario is thus cast as a problem of maximizing ğ‘”(ğ‘¥, ğ‘¢) subject
```
```
to ğ‘“(ğ‘¥, ğ‘¢) = 0, and this is a situation for using Lagrange multipliers. Here is precisely
```
the point where Lagrange multipliers come into play in optimal control. Lagrange
meets Pontryagin. We analyze the problem as in Section 2.10 by looking for critical
points for the Hamiltonian
```
ğ»(ğ‘¥, ğ‘¢, ğœ†) = ğ‘”(ğ‘¥, ğ‘¢) + ğœ†ğ‘“(ğ‘¥, ğ‘¢).
```
That is, we look for points where
0 = ğœ•ğ»ğœ•ğ‘¥ = ğœ•ğ»ğœ•ğ‘¢ = ğœ•ğ»ğœ•ğœ† .
Setting ğœ•ğ»ğœ•ğ‘¥ = 0 yields
0 = ğœ•ğ‘”ğœ•ğ‘¥ + ğœ† ğœ•ğ‘“ğœ•ğ‘¥ and so ğœ† = âˆ’ ğœ•ğ‘”ğœ•ğ‘¥ / ğœ•ğ‘“ğœ•ğ‘¥ .
```
Setting ğœ•ğ»ğœ•ğœ† = 0 recaptures the constraint 0 = ğ‘“(ğ‘¥, ğ‘¢). Differentiating this with respect
```
to ğ‘¢ yields
```
0 = ğ‘‘ğ‘‘áµ† ğ‘“(ğ‘¥, ğ‘¢) = ğœ•ğ‘“ğœ•ğ‘¥ğ‘‘ğ‘¥ğ‘‘áµ† + ğœ•ğ‘“ğœ•áµ†
```
or
ğ‘‘ğ‘¥
ğ‘‘áµ† = âˆ’
ğœ•ğ‘“
ğœ•áµ† /
ğœ•ğ‘“
ğœ•ğ‘¥ .
To be clear, ğ‘‘ğ‘¥ğ‘‘áµ† is the derivative of ğ‘¥ with respect to ğ‘¢ determined by the relation 0 =
```
ğ‘“(ğ‘¥, ğ‘¢). This is an application of implicit differentiation.
```
90 Chapter 6. Easing the Restrictions
Now analyze the rate of change of our payoff ğ‘” per change in ğ‘¢ under this setup.
A key step is from line 2 to line 3 where we exchange the positions of ğœ•ğ‘“ğœ•áµ† and ğœ•ğ‘”ğœ•ğ‘¥ :
ğ‘‘
```
ğ‘‘áµ† ğ‘”(ğ‘¥, ğ‘¢) =
```
ğœ•ğ‘”
ğœ•ğ‘¥
ğ‘‘ğ‘¥
ğ‘‘áµ† +
ğœ•ğ‘”
ğœ•áµ†
```
= ğœ•ğ‘”ğœ•ğ‘¥ (âˆ’ ğœ•ğ‘“ğœ•áµ† / ğœ•ğ‘“ğœ•ğ‘¥ ) + ğœ•ğ‘”ğœ•áµ†
```
```
= ğœ•ğ‘“ğœ•áµ† (âˆ’ ğœ•ğ‘”ğœ•ğ‘¥ / ğœ•ğ‘“ğœ•ğ‘¥ ) + ğœ•ğ‘”ğœ•áµ†
```
= ğœ•ğ‘“ğœ•áµ† ğœ† + ğœ•ğ‘”ğœ•áµ†
```
= ğœ•ğ»ğœ•áµ† (ğ‘¥, ğ‘¢, ğœ†).
```
```
Here we can see the key insight that ğœ•ğ»ğœ•áµ† (ğ‘¥, ğ‘¢, ğœ†) is the rate of change of the payoff
```
```
ğ‘”(ğ‘¥, ğ‘¢) with respect to changes in control ğ‘¢ under the restrictions ğ‘“(ğ‘¥, ğ‘¢) = 0 and
```
ğœ•ğ»
ğœ•ğ‘¥ = 0.
In order for ğ‘¢ to be a local optimum, this rate of change ğœ•ğ»ğœ•áµ† must be zero. Putting
this all together, we have that optimal controls occur at critical points of the Hamilton-
```
ian ğ»(ğ‘¥, ğ‘¢, ğœ†) where ğœ•ğ»ğœ•ğ‘¥ , ğœ•ğ»ğœ•áµ† , and ğœ•ğ»ğœ•ğœ† are all zero. This is pretty cool.
```
Mathematics relies on precise language. The following definition and theorem are
a careful statement of the result for the proof outlined above. The reader is encouraged
to carefully consider the definition and how each assumption in the premise of the
theorem is required for the result.
```
Definition: For open sets ğ‘ƒ, ğ‘„ âŠ‚ â„ and real-valued functions ğ‘“(ğ‘¥, ğ‘¢), ğ‘”(ğ‘¥, ğ‘¢)
```
which are defined and continuous for ğ‘¥ âˆˆ ğ‘ƒ, ğ‘¢ âˆˆ ğ‘„, we say a local maximum for
```
ğ‘”(ğ‘¥, ğ‘¢) subject to ğ‘“(ğ‘¥, ğ‘¢) = 0 occurs at (Ìƒğ‘¥,Ìƒ ğ‘¢) if there are open intervals ğ‘€, ğ‘ with
```
```
Ìƒğ‘¥ âˆˆ ğ‘€ âŠ‚ ğ‘ƒ,Ìƒ ğ‘¢ âˆˆ ğ‘ âŠ‚ ğ‘„ such that ğ‘”(Ìƒğ‘¥,Ìƒ ğ‘¢) â‰¥ ğ‘”(ğ‘¥, ğ‘¢) for all ğ‘¥ âˆˆ ğ‘€, ğ‘¢ âˆˆ ğ‘ that
```
```
satisfy ğ‘“(ğ‘¥, ğ‘¢) = 0.
```
```
Theorem: For open sets ğ‘ƒ, ğ‘„ âŠ‚ â„ and real-valued functions ğ‘“(ğ‘¥, ğ‘¢), ğ‘”(ğ‘¥, ğ‘¢) which
```
```
are defined and differentiable for ğ‘¥ âˆˆ ğ‘ƒ, ğ‘¢ âˆˆ ğ‘„, define ğ»(ğ‘¥, ğ‘¢, ğœ†) = ğ‘”(ğ‘¥, ğ‘¢) +
```
```
ğœ†ğ‘“(ğ‘¥, ğ‘¢) for ğœ† âˆˆ â„. If a local maximum for ğ‘”(ğ‘¥, ğ‘¢) subject to ğ‘“(ğ‘¥, ğ‘¢) = 0 occurs at
```
```
(Ìƒğ‘¥,Ìƒ ğ‘¢), then there existsÌƒ ğœ† âˆˆ â„ such that
```
```
0 = ğœ•ğ»ğœ•ğ‘¥ (Ìƒğ‘¥,Ìƒ ğ‘¢,Ìƒ ğœ†) = ğœ•ğ»ğœ•ğœ† (Ìƒğ‘¥,Ìƒ ğ‘¢,Ìƒ ğœ†) = ğœ•ğ»ğœ•áµ† (Ìƒğ‘¥,Ìƒ ğ‘¢,Ìƒ ğœ†).
```
Note that the condition is necessary, not sufficient, for the control to be a local
maximum. These conditions must hold in order for a local maximum to exist, but
which by themselves do not guarantee a local maximum.
6.3.2 Eulerâ€™s Technique. We sketch the standard proof of Pontryaginâ€™s principle
in the autonomous case of Principle III, using a technique developed by Euler. We
begin with a careful statement of exactly what we are proving. Note how the following
definitions and theorems mirror those of the previous section and how the concepts
are elevated to function spaces.
6.3. Proving Pontryagin 91
```
Definition: Let ğ’« be a set of real-valued functions ğ‘¥(ğ‘¡) that are continuous for
```
ğ‘¡ âˆˆ [0, ğ‘‡]. Then ğ’« is open in the ğ¶0 topology if for allÌƒ ğ‘¥ âˆˆ ğ’« there is an ğœ– > 0 such
```
that every real-valued continuous function ğ‘¥(ğ‘¡) with |Ìƒğ‘¥(ğ‘¡) âˆ’ ğ‘¥(ğ‘¡)| < ğœ– for ğ‘¡ âˆˆ [0, ğ‘‡]
```
is also in ğ’«.
```
Definition: Let ğ’« be a set of real-valued functions that are continuous on [0, ğ‘‡]
```
```
and differentiable on (0, ğ‘‡), and let ğ’¬ be a set of real-valued functions continuous
```
on [0, ğ‘‡]. Suppose that both ğ’«, ğ’¬ are open in the ğ¶0 topology. For real-valued
```
functions ğ‘“(ğ‘¥, ğ‘¢), ğ‘”(ğ‘¥, ğ‘¢), ğº(ğ‘¥) continuous for all values for the functions ğ‘¥ âˆˆ ğ’«,
```
```
ğ‘¢ âˆˆ ğ’¬, we say a local maximum for ğ½(ğ‘¥, ğ‘¢) = ğº(ğ‘¥(ğ‘‡)) + âˆ«ğ‘‡0 ğ‘”(ğ‘¥, ğ‘¢) ğ‘‘ğ‘¡ subject
```
```
to ğ‘¥â€² = ğ‘“(ğ‘¥, ğ‘¢) and ğ‘¥(0) = ğ´ occurs at (Ìƒğ‘¥,Ìƒ ğ‘¢) if there are open sets â„³, ğ’© with
```
```
Ìƒğ‘¥ âˆˆ â„³ âŠ‚ ğ’«,Ìƒ ğ‘¢ âˆˆ ğ’© âŠ‚ ğ’¬ such that ğ½(Ìƒğ‘¥,Ìƒ ğ‘¢) â‰¥ ğ½(ğ‘¥, ğ‘¢) for all ğ‘¥ âˆˆ â„³, ğ‘¢ âˆˆ ğ’© that
```
```
satisfy ğ‘¥â€² = ğ‘“(ğ‘¥, ğ‘¢) and ğ‘¥(0) = ğ´.
```
```
Theorem: Let ğ’« be a set of real-valued functions continuous on [0, ğ‘‡] and differ-
```
```
entiable on (0, ğ‘‡), and let ğ’¬ be a set of real-valued functions continuous on [0, ğ‘‡],
```
```
with both ğ’«, ğ’¬ open in the ğ¶0 topology. For real-valued functions ğ‘“(ğ‘¥, ğ‘¢), ğ‘”(ğ‘¥, ğ‘¢),
```
```
ğº(ğ‘¥) continuous for all values for the functions ğ‘¥ âˆˆ ğ’«, ğ‘¢ âˆˆ ğ’¬, define
```
```
ğ»(ğ‘¥, ğ‘¢, ğœ†) = ğ‘”(ğ‘¥, ğ‘¢) + ğœ†ğ‘“(ğ‘¥, ğ‘¢).
```
If a local maximum for
```
ğ½(ğ‘¥, ğ‘¢) = ğº(ğ‘¥(ğ‘‡)) + âˆ«
```
ğ‘‡
0
```
ğ‘”(ğ‘¥, ğ‘¢) ğ‘‘ğ‘¡
```
```
subject to ğ‘¥â€² = ğ‘“(ğ‘¥, ğ‘¢) and ğ‘¥(0) = ğ´ occurs at (Ìƒğ‘¥,Ìƒ ğ‘¢), then there exists a real-valued
```
```
functionÌƒ ğœ†(ğ‘¡) defined on [0, ğ‘‡] and differentiable on (0, ğ‘‡) such that
```
ğœ•ğ»
```
ğœ•ğœ† (Ìƒğ‘¥(ğ‘¡),Ìƒ ğ‘¢(ğ‘¡),Ìƒ ğœ†(ğ‘¡)) =Ìƒ ğ‘¥â€²(ğ‘¡),
```
ğœ•ğ»
```
ğœ•ğ‘¥ (Ìƒğ‘¥(ğ‘¡),Ìƒ ğ‘¢(ğ‘¡),Ìƒ ğœ†(ğ‘¡)) = âˆ’Ìƒğœ†â€²(ğ‘¡),
```
ğœ•ğ»
```
ğœ•áµ† (Ìƒğ‘¥(ğ‘¡),Ìƒ ğ‘¢(ğ‘¡),Ìƒ ğœ†(ğ‘¡)) = 0
```
```
for all ğ‘¡ âˆˆ (0, ğ‘‡).
```
That was a lot of technical details. It is important in mathematics to be able to state
precisely what you are doing.
```
The steps of the proof are as follows. For the controlled system ğ‘¥â€² = ğ‘“(ğ‘¥, ğ‘¢) with
```
```
initial condition ğ‘¥(0) = ğ´ we want to optimize
```
```
ğ½(ğ‘¢) = ğº(ğ‘¥(ğ‘‡)) + âˆ«
```
ğ‘‡
0
```
ğ‘”(ğ‘¥, ğ‘¢) ğ‘‘ğ‘¡
```
```
over all allowable control functions ğ‘¢ = ğ‘¢(ğ‘¡). We assume the allowable controls is the
```
set of continuous functions on the closed interval [0, ğ‘‡], and weâ€™ll use the ğ¶0 topology:
```
two functions are close together if |ğœ‡(ğ‘¡) âˆ’ ğœ™(ğ‘¡)| is small for all 0 â‰¤ ğ‘¡ â‰¤ ğ‘‡.
```
92 Chapter 6. Easing the Restrictions
We assume there is a local optimum in this allowable set. That is, there is a well-
```
defined continuous control functionÌƒ ğ‘¢(ğ‘¡) that performs better than any other nearby
```
```
control starting from ğ‘¥(0) = ğ´. Weâ€™d like to set up conditions that this local optimal
```
```
control ğ‘¢(ğ‘¡) must satisfy.
```
Motivated by Lagrange multipliers and Hamiltonians, we define
```
ğ»(ğ‘¥, ğ‘¢, ğœ†) = ğ‘”(ğ‘¥, ğ‘¢) + ğœ† ğ‘“(ğ‘¥, ğ‘¢)
```
and consider
```
ğ½âˆ—(ğ‘¥, ğ‘¢, ğœ†) = ğº(ğ‘¥(ğ‘‡)) + âˆ«
```
ğ‘‡
0
```
ğ‘”(ğ‘¥, ğ‘¢) + ğœ† (ğ‘“(ğ‘¥, ğ‘¢) âˆ’ ğ‘¥â€²) ğ‘‘ğ‘¡
```
```
for general functions ğ‘¥ = ğ‘¥(ğ‘¡), ğ‘¢ = ğ‘¢(ğ‘¡), and ğœ† = ğœ†(ğ‘¡). Under the restriction ğ‘¥â€² =
```
```
ğ‘“(ğ‘¥, ğ‘¢) this is exactly the performance function we seek to optimize.
```
Eulerâ€™s method proceeds with integration by parts:
```
ğ½âˆ—(ğ‘¥, ğ‘¢, ğœ†) = ğº(ğ‘¥(ğ‘‡)) + âˆ«
```
ğ‘‡
0
```
ğ‘”(ğ‘¥, ğ‘¢) + ğœ†(ğ‘“(ğ‘¥, ğ‘¢) âˆ’ ğ‘¥â€²) ğ‘‘ğ‘¡
```
```
= ğº(ğ‘¥(ğ‘‡)) + âˆ«
```
ğ‘‡
0
```
ğ»(ğ‘¥, ğ‘¢, ğœ†) ğ‘‘ğ‘¡ âˆ’ âˆ«
```
ğ‘‡
0
ğœ†ğ‘¥â€² ğ‘‘ğ‘¡
```
= ğº(ğ‘¥(ğ‘‡)) + âˆ«
```
ğ‘‡
0
```
ğ»(ğ‘¥, ğ‘¢, ğœ†) ğ‘‘ğ‘¡ âˆ’ ğœ†(ğ‘¡)ğ‘¥(ğ‘¡)||ğ‘‡ğ‘¡=0 + âˆ«
```
ğ‘‡
0
ğœ†â€²ğ‘¥ ğ‘‘ğ‘¡
```
= ğº(ğ‘¥(ğ‘‡)) + âˆ’ğœ†(ğ‘‡)ğ‘¥(ğ‘‡) + ğœ†(0)ğ´ + âˆ«
```
ğ‘‡
0
```
(ğ»(ğ‘¥, ğ‘¢, ğœ†) + ğœ†â€²ğ‘¥) ğ‘‘ğ‘¡.
```
Now we investigate what happens to this expression as we add some variation to
```
the control ğ‘¢(ğ‘¡). That is, we take a very small continuous perturbation ğœ‡1(ğ‘¡) and re-
```
```
place ğ‘¢(ğ‘¡) withÌ‚ ğ‘¢(ğ‘¡) = ğ‘¢(ğ‘¡) + ğœ‡1(ğ‘¡). Under ğ‘¥â€² = ğ‘“(ğ‘¥, ğ‘¢) this will cause a very small
```
```
perturbation in ğ‘¥(ğ‘¡) creating a new trajectoryÌ‚ ğ‘¥(ğ‘¡) = ğ‘¥(ğ‘¡) + ğœ‡2(ğ‘¡). Consider how this
```
may change ğ½âˆ—, where we use the linear approximation ideas from Section 2.3:
```
Î”ğ½âˆ— = ğ½âˆ—(Ì‚ğ‘¥,Ì‚ ğ‘¢, ğœ†) âˆ’ ğ½âˆ—(ğ‘¥, ğ‘¢, ğœ†)
```
```
= ğº(Ì‚ğ‘¥(ğ‘‡)) âˆ’ ğº(ğ‘¥(ğ‘‡)) âˆ’ ğœ†(ğ‘‡)Ì‚ğ‘¥(ğ‘‡) + ğœ†(ğ‘‡)ğ‘¥(ğ‘‡)
```
- âˆ«ğ‘‡0 ğ»(Ì‚ğ‘¥,Ì‚ ğ‘¢, ğœ†) âˆ’ ğ»(ğ‘¥, ğ‘¢, ğœ†) + ğœ†â€²Ì‚ ğ‘¥ âˆ’ ğœ†â€²ğ‘¥ ğ‘‘ğ‘¡
```
= ğºâ€²(ğ‘¥(ğ‘‡))ğœ‡2(ğ‘‡) âˆ’ ğœ†(ğ‘‡)ğœ‡2(ğ‘‡)
```
- âˆ«ğ‘‡0ğœ•ğ»ğœ•áµ† (ğ‘¥, ğ‘¢, ğœ†) ğœ‡1 + ğœ•ğ»ğœ•ğ‘¥ (ğ‘¥, ğ‘¢, ğœ†) ğœ‡2 + ğœ†â€²ğœ‡2 ğ‘‘ğ‘¡
```
+ğ‘œ(|ğœ‡1|, |ğœ‡2|)
```
```
= (ğºâ€²(ğ‘¥(ğ‘‡)) âˆ’ ğœ†(ğ‘‡)) ğœ‡2(ğ‘‡)
```
- âˆ«ğ‘‡0ğœ•ğ»ğœ•áµ† (ğ‘¥, ğ‘¢, ğœ†) ğœ‡1 + ( ğœ•ğ»ğœ•ğ‘¥ (ğ‘¥, ğ‘¢, ğœ†) + ğœ†â€²) ğœ‡2 ğ‘‘ğ‘¡
```
+ğ‘œ(|ğœ‡1|, |ğœ‡2|).
```
```
(6.2)
```
Key Points 93
```
At this point the function ğœ†(ğ‘¡) is arbitrary, and we can arbitrarily restrict our atten-
```
tion to those functions that are solutions to the differential equation
```
ğœ†â€² = âˆ’ ğœ•ğ»ğœ•ğ‘¥ (ğ‘¥, ğ‘¢, ğœ†) with ğœ†(ğ‘‡) = ğºâ€²(ğ‘¥(ğ‘‡)).
```
```
This one weird trick of taking ğœ†â€² = âˆ’ ğœ•ğ»ğœ•ğ‘¥ forces âˆ«ğ‘‡0 ( ğœ•ğœ•ğ‘¥ ğ»(ğ‘¥, ğ‘¢, ğœ†) + ğœ†â€²) ğœ‡2 ğ‘‘ğ‘¡ = 0 and
```
```
ğœ†(ğ‘‡) = ğºâ€²(ğ‘¥(ğ‘‡)) = 0 for all possible ğœ‡2(ğ‘¡). So equation (6.2) becomes
```
Î”ğ½âˆ— = âˆ«
ğ‘‡
0
ğœ•ğ»
```
ğœ•áµ† (ğ‘¥, ğ‘¢, ğœ†) ğœ‡1 ğ‘‘ğ‘¡ + ğ‘œ(|ğœ‡1|, |ğœ‡2|). (6.3)
```
Supposing an optimal control ğ‘¢ and trajectory ğ‘¥, if there were any time at which ğœ•ğ»ğœ•áµ† â‰ 
0, we could exploit this with a carefully chosen function ğœ‡1 that would make Î”ğ½âˆ— â‰  0.
```
Thus, in order for a given pair ğ‘¥(ğ‘¡), ğ‘¢(ğ‘¡) to optimize ğ½âˆ—, under the restrictions ğœ†â€² =
```
```
âˆ’ ğœ•ğ»ğœ•ğ‘¥ (ğ‘¥, ğ‘¢, ğœ†) and ğœ†(ğ‘‡) = ğºâ€²(ğ‘¥(ğ‘‡)) we must have Î”ğ½âˆ— = 0 for all perturbations ğœ‡1(ğ‘¡).
```
This requires
ğœ•ğ»
```
ğœ•áµ† (ğ‘¥, ğ‘¢, ğœ†) = 0
```
```
for all ğ‘¡. This is the condition we use to find the optimal control ğ‘¢(ğ‘¡).
```
The arbitrary assignment of ğœ† in this proof may seem unsatisfying. In fact, a more
careful approach would reveal that for Î”ğ½âˆ— to be zero for all perturbations ğœ‡1 and ğœ‡2
```
forces ğœ†â€² = âˆ’ ğœ•ğ»ğœ•ğ‘¥ and ğœ†(ğ‘‡) = ğºâ€²(ğ‘¥(ğ‘‡)). We take the more direct route of simply assign-
```
ing these restrictions, which suffices to prove the result.
We have omitted the more delicate proof that ğ» must be a local maximum or min-
```
imum on optimal trajectories, but one can see the crux of the argument: equation (6.3)
```
is basically saying
ğœ•
ğœ•ğ‘¢ ğ½ â‰ˆ âˆ«
ğ‘‡
0
ğœ•ğ»
```
ğœ•ğ‘¢ (ğ‘¥, ğ‘¢, ğœ†) ğ‘‘ğ‘¡
```
which a handwaving leap of faith leads to
ğœ•2
ğœ•ğ‘¢2 ğ½ â‰ˆ âˆ«
ğ‘‡
0
ğœ•2ğ»
```
ğœ•ğ‘¢2 (ğ‘¥, ğ‘¢, ğœ†) ğ‘‘ğ‘¡
```
which must be positive for a minimum, negative for a max. This is achieved by ğœ•2ğ»ğœ•áµ†2
being positive for a minimum, negative for a max. There are lots of holes in this part
of the proof, but enough of this for now.
```
We also omit proofs for fixed end times and locations (Principles I and II), which
```
get a bit more involved.
An expanded treatment of proofs associated with optimal control can be found in
[5, 10, 13, 15, 16].
Key Points
In this chapter we generalize the optimal control techniques to allow for a free endpoint
```
ğ‘¥(ğ‘‡) and/or a free time ğ‘‡, with an added payoff function ğº(ğ‘¥(ğ‘‡), ğ‘‡) depending on
```
the end location and time used. For a control to be optimal, Principle III specifies
```
necessary conditions on ğ‘¥(ğ‘‡) and ğ‘‡ in terms of terminal values of the costates and
```
partial derivatives of the Hamiltonian.
94 Chapter 6. Easing the Restrictions
This allows additional ways to view solutions as level curves of the Hamiltonian
and new ways that solutions can fail to exist. It also allows additional insight into
costates as representing marginal payoffs.
The standard proof for Pontryaginâ€™s method applies to the form of Principle III. We
made mathematically careful definitions and stated Principle III as a formal theorem,
and we walked through a proof of the theorem using real analytic concepts.
Exercises
Exercise 6.1. Free B. In Exercise 5.9 we used Principle I to analyze the system ğ‘¥â€² = ğ‘¥+ğ‘¢
```
and performance ğ½ = âˆ«10 ln ğ‘¢ ğ‘‘ğ‘¡ for ğ‘¢ > 0. For ğ‘¥(0) = ğ´ and ğ‘¥(1) = ğµ we derived
```
```
maximal performance ğ½(ğ´, ğµ) = ln(ğµ âˆ’ ğ´ğ‘’) âˆ’ 12
```
```
(a) Use this to conclude that for the system ğ‘¥â€² = ğ‘¥ + ğ‘¢ and performance
```
```
ğ½ = âˆ’ğ‘¥(1) + âˆ«
```
1
0
ln ğ‘¢ ğ‘‘ğ‘¡
```
for prescribed ğ‘¥(0) = 1 and ğ‘¥(1) = ğµ we have maximal performance ğ½(ğµ) = âˆ’ğµ +
```
```
ln(ğµ âˆ’ ğ‘’) âˆ’ 12 .
```
```
(b) Show that this performance attains a maximum at ğµ = ğ‘’ + 1.
```
```
(c) Show that applying Principle III to this problem with free endpoint ğ‘¥(1) = ğµ
```
will produce the same conclusion.
Exercise 6.2. Free B. In Exercise 5.10 we used Principle I to analyze the system ğ‘¥â€² = ğ‘¢
```
and performance ğ½ = âˆ«10 ğ‘¢2 âˆ’ ( ğœ‹2 )2ğ‘¥2 ğ‘‘ğ‘¡ with endpoint conditions ğ‘¥(0) = ğ´ and ğ‘¥(1) =
```
ğµ to get minimal performance ğ½ = âˆ’ğœ‹ğ´ğµ.
```
(a) Use this to conclude that for the system ğ‘¥â€² = ğ‘¢ and performance
```
```
ğ½ = (ğ‘¥(1))2 + âˆ«
```
1
0
ln ğ‘¢ ğ‘‘ğ‘¡
```
for ğ‘¥(0) = ğ´ and ğ‘¥(1) = ğµ we have minimal performance ğ½(ğµ) = ğµ2 âˆ’ ğœ‹ğ´ğµ.
```
```
(b) Show that this performance attains a minimum at ğµ = ğœ‹ğ´/2.
```
```
(c) Show that applying Principle III to this problem with free endpoint ğ‘¥(1) = ğµ
```
will produce the same conclusion.
```
Exercise 6.3. Free T. Consider ğ‘¥â€² = ğ‘¥ + 2ğ‘¢ with ğ‘¥(0) = 0, ğ‘¥(ğ‘‡) = ğµ, ğ‘‡ > 0 is free, with
```
performance
ğ½ = ğ‘‡ + âˆ«
ğ‘‡
0
2ğ‘¥2 + ğ‘¢2 ğ‘‘ğ‘¡.
```
(a) Show that a minimizing solution is of the form ğ‘¥(ğ‘¡) = ğ¾ sinh(3ğ‘¡) for some ğ¾
```
```
(where sinh is the hyperbolic sine).
```
Exercises 95
```
(b) Using Principle III, conclude that ğ» must be constant and equal to âˆ’1. Use
```
```
this to conclude ğœ†(0) = Â±1.
```
```
(c) Use parts (a) and (b) to conclude ğ¾ = 2/3 if ğµ > 0 and ğ¾ = âˆ’2/3 if ğµ < 0.
```
```
(d) Note that we have not used the end location ğ‘¥(ğ‘‡) = ğµ. Use the end location
```
to solve for end time ğ‘‡ in terms of ğµ.
```
(e) Sketch the solution trajectory in the (ğ‘¥, ğœ†)-plane as a level curve of ğ».
```
```
(f) Comment on how this demonstrates the Principle of Optimality covered in
```
Section 5.2.
```
Exercise 6.4(s). Canoe I: Free B. You are practicing for the canoeing competition in the
```
upcoming Summer Olympics, and you hope to bring home a gold medal for the newly
created Optimal Control event. A straight river flows at one unit of distance per one
unit of time. You start at position ğ‘¥ = 0 on the stream in your canoe. You can paddle
your canoe either with or against the current, so your equation of motion is ğ‘¥â€² = 1 + ğ‘¢.
Your goal is to get close to position ğ‘¥ = 2 at exactly time ğ‘‡. Your end location payoff is
```
ğ‘¥(ğ‘‡)(4 âˆ’ ğ‘¥(ğ‘‡)) which has a maximum payoff of 2 at position ğ‘¥ = 2.
```
Paddling a canoe is hard work and incurs a cost of ğ‘¢2. So your net payoff is
```
ğ½ = ğ‘¥(ğ‘‡) (4 âˆ’ ğ‘¥(ğ‘‡)) âˆ’ âˆ«
```
ğ‘‡
0
ğ‘¢2 ğ‘‘ğ‘¡.
```
Your end location ğµ = ğ‘¥(ğ‘‡) is free. Suppose you have a fixed time ğ‘‡ = 1. What is your
```
optimal solution? Where is your endpoint? Suppose your fixed end time is ğ‘‡ = 3. Now
what is your optimal solution and endpoint?
What is the solution and end location for a general fixed end time ğ‘‡? Solve for ğ½ as
a function of ğ‘‡ and plot. What value of ğ‘‡ would produce maximum payoff? Explain
why this makes intuitive sense.
```
Exercise 6.5(s). Canoe II: Free T. You are back in your canoe, with ğ‘¥â€² = 1 + ğ‘¢ and
```
```
ğ½ = ğ‘¥(ğ‘‡) (4 âˆ’ ğ‘¥(ğ‘‡)) âˆ’ âˆ«ğ‘‡0 ğ‘¢2 ğ‘‘ğ‘¡.
```
```
Suppose your starting location ğ‘¥(0) = ğ´ and end location ğ‘¥(ğ‘‡) = ğµ are both fixed,
```
but now ğ‘‡ is free.
What is your optimal control? Consider the three cases ğ´ < ğµ, ğ´ > ğµ, and ğ´ = ğµ.
96 Chapter 6. Easing the Restrictions
```
Exercise 6.6(h). Canoe III: Free B & T. You are in the canoe again, with ğ‘¥â€² = 1 + ğ‘¢
```
```
and ğ½ = ğ‘¥(ğ‘‡) (4 âˆ’ ğ‘¥(ğ‘‡)) âˆ’ âˆ«ğ‘‡0 ğ‘¢2 ğ‘‘ğ‘¡.
```
```
Your starting location is fixed at ğ‘¥(0) = ğ´ but your end location ğµ = ğ‘¥(ğ‘‡) and end
```
time ğ‘‡ are both free.
```
(a) If ğ´ < 2, argue that your optimal control is to drift ğ‘¢ = 0 until ğ‘¥ = 2 and then
```
stop.
```
(b) If ğ´ > 4, argue that your optimal control is to paddle upstream at ğ‘¢ = âˆ’2 until
```
ğ‘¥ = 4 and then stop.
```
(c) What if 2 â‰¤ ğ´ â‰¤ 4? Argue that ğ‘‡ = 0 is a legitimate solution to Principle III
```
and that it performs better than ğ‘¢ = 0 or ğ‘¢ = âˆ’2 when 2 â‰¤ ğ´ â‰¤ 4.
```
(d) Argue that ğ‘‡ = 0 does not perform as well as ğ‘¢ = 0 for ğ´ < 2 and does not
```
perform as well as ğ‘¢ = âˆ’2 for ğ´ > 4.
Exercise 6.7. Time is money, but it doesnâ€™t matter if you collect it at the end or as you
go. The following two performance functions are equivalent. Show that they lead to
the same results in Principle III.
```
ğ½1 = ğº(ğµ, ğ‘‡) + âˆ«ğ‘‡0 1 + ğ‘”(ğ‘¥, ğ‘¢) ğ‘‘ğ‘¡,
```
```
ğ½2 = ğ‘‡ + ğº(ğµ, ğ‘‡) + âˆ«ğ‘‡0 ğ‘”(ğ‘¥, ğ‘¢) ğ‘‘ğ‘¡.
```
```
Exercise 6.8(s). Good equations can sometimes go bad. Suppose you are given ğ‘‡ and
```
```
want to optimize âˆ«ğ‘‡0 ğ‘”(ğ‘¢) ğ‘‘ğ‘¡ for ğ‘¥â€² = ğ‘¢ with ğ‘¥(0) = 0 and ğ‘¥(ğ‘‡) = 1.
```
```
(a) Show that Pontryaginâ€™s principle would require ğ‘¢ to be constant for an optimal
```
control.
```
(b) Take ğ‘”(ğ‘¢) = âˆšğ‘¢ and solve. What happens to ğ½ as ğ‘‡ â†’ 0? What happens to ğ½
```
as ğ‘‡ â†’ âˆ? What is the optimal control if ğ‘‡ is free?
```
(c) Take ğ‘”(ğ‘¢) = ğ‘¢2 and solve. What happens to ğ½ as ğ‘‡ â†’ 0? What happens to ğ½ as
```
ğ‘‡ â†’ âˆ? What is the optimal control if ğ‘‡ is free?
```
(d) Take ğ‘”(ğ‘¢) = ğ‘¢ğ›¼ and analyze. What happens at ğ›¼ = 1?
```
```
Exercise 6.9(hs). Free T. Growth of the population size of an invasive pest is modeled
```
```
by the normalized logistic equation ğ‘¥â€² = ğ‘¥(1 âˆ’ ğ‘¥). You can control this pest by appli-
```
```
cation of a pesticide at rate ğ‘¢ resulting in ğ‘¥â€² = ğ‘¥(1 âˆ’ ğ‘¥) âˆ’ ğ‘¥ğ‘¢. You want to reduce this
```
```
population from ğ‘¥(0) = 0.9 to ğ‘¥(ğ‘‡) = 0.1 while minimizing the environmental impact
```
of the pesticide, measured as ğ½ = âˆ«ğ‘‡012 ğ‘¢2 ğ‘‘ğ‘¡. End time ğ‘‡ is unspecified and should be
chosen as part of the minimization. Solve. Sketch your solution as a level curve of ğ»
```
in the (ğ‘¥, ğœ†)-plane.
```
Exercises 97
```
Exercise 6.10(s). Free B. Consider the life-cycle savings model in Exercise 4.15, ğ‘¥â€² =
```
```
ğ‘¤ + ğ‘Ÿğ‘¥ âˆ’ ğ‘¢, with wages ğ‘¤, interest rate ğ‘Ÿ, lifespan ğ‘‡, and ğ‘¥(0) = 0. Suppose ğ‘¥(ğ‘‡) is free
```
and performance is
```
ğ½ = ğ›½ ln(ğµ) + âˆ«
```
ğ‘‡
0
```
ln(ğ‘¢) ğ‘‘ğ‘¡
```
```
where ğ›½ ln(ğµ) now represents the utility of bequeathing amount ğµ. Solve for the opti-
```
mal ğµ in terms of ğ‘¤, ğ‘Ÿ, ğ‘‡, and ğ›½. Show that ğ‘¢ â†’ 0 as ğ›½ â†’ âˆ.
```
Exercise 6.11(hs). Free B,T. You work in a lab that cultures a beneficial yeast. The
```
yeast culture grows exponentially, and you can influence the growth rate with nutri-
```
ents, temperature, Mozart, etc., resulting in a growth rate of ğ‘¥â€² = (ğ‘Ÿ + ğ‘¢) ğ‘¥ where ğ‘Ÿ is
```
fixed and you control ğ‘¢ â‰¥ 0.
Assume that you start with ğ‘¥0 = 1 unit of yeast and you want to produce ğµ â‰¥ 1
units by time ğ‘‡.
```
Running costs are ğ‘¢2/2 and the end payoff is ğº(ğµ, ğ‘‡), so you want to maximize
```
```
ğ½ = ğº(ğµ, ğ‘‡) âˆ’ âˆ«
```
ğ‘‡
0
1
2 ğ‘¢2 ğ‘‘ğ‘¡.
```
(a) Show that for a solution satisfying Principle III, ğ‘¢ will be a constant.
```
```
(b) Suppose ğº(ğµ, ğ‘‡) = ğµ âˆ’ ğ‘‡ with ğµ fixed, say, ğµ = 10, and ğ‘‡ free. Set ğ‘Ÿ = 1/2
```
and solve for the optimal solution. Repeat for a general fixed value of ğµ and find the
optimal time ğ‘‡ as a function of ğµ.
```
(c) With ğº(ğµ, ğ‘‡) = ğµ âˆ’ ğ‘‡, show that ğ½ can be made arbitrarily large if ğ‘‡ and
```
```
ğ‘¥(ğ‘‡) = ğµ are free.
```
```
Exercise 6.12(s). Free B,T. You are creating a startup to deliver an amount of product
```
```
ğ‘¥(ğ‘‡) in ğ‘‡ time units by expending effort (measured in dollars) ğ‘¢. You start with ğ‘¥(0) =
```
0 and effort has a saturation effect:
ğ‘¥â€² = ğ›¼âˆšğ‘¢.
At the end of ğ‘‡ time units you sell your product for a fixed price ğ‘ƒ, and you have dis-
count rate ğ‘Ÿ, so your performance is measured as
```
ğ½ = ğ‘ƒ ğ‘¥(ğ‘‡)ğ‘’âˆ’ğ‘Ÿğ‘‡ âˆ’ âˆ«
```
ğ‘‡
0
ğ‘¢ğ‘’âˆ’ğ‘Ÿğ‘¡ ğ‘‘ğ‘¡.
```
(a) Suppose ğ‘¥(ğ‘‡) = ğµ is free and ğ‘‡ is fixed. What is your optimal payoff ğ½(ğ‘‡)?
```
Show that this function attains a maximum when ğ‘’ğ‘Ÿğ‘‡ = 2.
```
(b) Suppose ğ‘¥(ğ‘‡) = ğµ and ğ‘‡ are both free. Apply Principle III to show that the
```
optimal stopping time is when ğ‘’ğ‘Ÿğ‘‡ = 2.
7
Linear-Quadratic Systems
Linear-quadratic systems have linear differential models and quadratic costs/payoffs
and are a basic class of optimal control problems that exhibit a wide variety of phenom-
ena. Strict local optimums require convexity, and these are the simplest systems with
the necessary convexity to produce interesting examples. Linear-quadratic systems can
also be used as local approximations of more complicated nonlinear and nonquadratic
problems.
A nice property of these systems is that they generate linear state-costate dynamics,
making them more accessible to study. Our current tools are fully sufficient to explore
these systems, and doing so will help solidify understanding of optimal control.
This section will rely on understanding a two-dimensional system of linear au-
tonomous equations, which is an important topic in differential equations. We worked
```
with such systems in the Integrator examples; see Examples 4.3 and 5.4 in particular.
```
While computer algebra systems handle these cases well, it is important to have a good
theoretical understanding of these systems and the basic matrix algebra involved in
order to work with them efficiently. The reader is encouraged to review the topic as
needed, and Appendix B contains a brief check of techniques.
We start by defining and analyzing the general two-dimensional linear-quadratic
case, and we follow up with several examples and specific solutions.
7.1 Linear-Quadratic with Fixed Ends
Consider the linear case
ğ‘¥â€² = ğ‘šğ‘¥ + ğ‘¢
with quadratic performance
ğ½ = âˆ«
ğ‘‡
0
ğ‘ğ‘¥2 + ğ‘ğ‘¢2 ğ‘‘ğ‘¡.
The coefficients that determine the system are ğ‘š, ğ‘, ğ‘, and ğ‘‡. The coefficient of ğ‘¢
in the dynamic ğ‘¥â€² = ğ‘šğ‘¥ + ğ‘¢ is normalized to one: any nonzero coefficient could be
99
100 Chapter 7. Linear-Quadratic Systems
absorbed into the control, and it would make no sense for the coefficient to be zero.
Other coefficients could be normalized, but this choice will suite our purposes.
Applying Principle III, the Hamiltonian is
```
ğ» = (ğ‘ğ‘¥2 + ğ‘ğ‘¢2) + ğœ†(ğ‘šğ‘¥ + ğ‘¢)
```
and
0 = ğœ•ğ»ğœ•ğ‘¢ = 2ğ‘ğ‘¢ + ğœ†
implies that an optimal control must satisfy
ğ‘¢ = âˆ’ 12ğ‘ ğœ†
```
which reduces the Hamiltonian to (âˆ— check this âˆ—)
```
ğ» = ğ‘ğ‘¥2 + ğ‘šğ‘¥ğœ† âˆ’ 14ğ‘ ğœ†2.
Note ğœ•2ğ»ğœ•áµ†2 = 2ğ‘, and so the sign of ğ‘ determines whether maximizing or minimizing is
appropriate.
With the costate equation ğœ†â€² = âˆ’ ğœ•ğ»ğœ•ğ‘¥ we have
ğ‘¥â€² = ğ‘šğ‘¥ âˆ’ 12ğ‘ ğœ†,
ğœ†â€² = âˆ’2ğ‘ğ‘¥ âˆ’ ğ‘šğœ†.
This is a two-dimensional linear system, and a brief review of working with such
systems can be found in Appendix B. We express the system in matrix form:
```
(
```
ğ‘¥
ğœ†
```
)
```
â€²
= [
ğ‘š âˆ’ 12ğ‘
âˆ’2ğ‘ âˆ’ğ‘š
```
] (
```
ğ‘¥
ğœ†
```
) .
```
The coefficient matrix has zero trace, which is both necessary and sufficient for it
```
to be a Hamiltonian system (see Exercise 5.2), and trajectories are level curves of the
```
quadric surface
ğ» = ğ‘ğ‘¥2 + ğ‘šğ‘¥ğœ† âˆ’ 14ğ‘ ğœ†2.
For positive determinant/discriminant âˆ’ğ‘š2 âˆ’ ğ‘ğ‘ > 0 the system is a center. For neg-
ative determinant/discriminant âˆ’ğ‘š2 âˆ’ ğ‘ğ‘ < 0 the system is a saddle with eigenvalues
Â±âˆšğ‘š2 + ğ‘ğ‘ .
For initial state ğ´, fixed final state ğµ, and fixed end time ğ‘‡, we look for solutions
```
that will satisfy boundary conditions ğ‘¥(0) = ğ´ and ğ‘¥(ğ‘‡) = ğµ.
```
7.1. Linear-Quadratic with Fixed Ends 101
Example 7.1
Consider the controlled exponential growth ğ‘¥â€² = 14 ğ‘¥ + ğ‘¢ and suppose we want a
control which minimizes a quadratic expense âˆ«ğ‘‡0 ğ‘¢2 ğ‘‘ğ‘¡.
The Hamiltonian is
```
ğ» = ğ‘¢2 + ğœ† ( 14 ğ‘¥ + ğ‘¢).
```
Setting 0 = ğœ•ğ»ğœ•áµ† = 2ğ‘¢ + ğœ† yields ğ‘¢ = âˆ’ 12 ğœ†. Check that ğœ•2ğ»ğœ•áµ†2 > 0, so minimizing is
appropriate. With ğœ•ğ»ğœ•ğ‘¥ = 14 ğœ† we have
ğ‘¥â€² = 14 ğ‘¥ âˆ’ 12 ğœ†,
ğœ†â€² = âˆ’ 14 ğœ†
with general solution
ğ‘¥ = ğ¶1 ğ‘’âˆ’ğ‘¡/4 + ğ¶2 ğ‘’ğ‘¡/4,
ğœ† = ğ¶1 ğ‘’âˆ’ğ‘¡/4
```
corresponding to the eigenvalues Â±1/4 and eigenvectors (1, 1) and (1, 0) for the
```
```
coefficient matrix (see Appendix B).
```
Substituting ğ‘¢ = âˆ’ 12 ğœ† into ğ», we have that trajectories are level curves of the
hyperbolic paraboloid
```
ğ» = 14 ğœ†(ğ‘¥ âˆ’ ğœ†).
```
```
We can satisfy the boundary conditions ğ‘¥(0) = ğ´ and ğ‘¥(ğ‘‡) = ğµ with ğ¶1 =
```
```
(ğ´ğ‘’ğ‘‡/4 âˆ’ ğµ)/(ğ‘’ğ‘‡/4 âˆ’ ğ‘’âˆ’ğ‘‡/4) and ğ¶2 = (âˆ’ğ´ğ‘’âˆ’ğ‘‡/4 + ğµ)/(ğ‘’ğ‘‡/4 âˆ’ ğ‘’âˆ’ğ‘‡/4) (âˆ— check this âˆ—).
```
```
For example, ğ‘¥(0) = âˆ’1 and ğ‘¥(4) = 1 yields (with some simplification)
```
ğ‘¥ = âˆ’ ğ‘’1âˆ’ğ‘¡/4ğ‘’âˆ’1 + ğ‘’ğ‘¡/4ğ‘’âˆ’1 ,
ğœ† = âˆ’ ğ‘’1âˆ’ğ‘¡/4ğ‘’âˆ’1 .
```
Figure 7.1 displays trajectories for the following endpoint conditions (âˆ— match
```
```
them âˆ—):
```
ğ´ ğµ ğ‘‡
âˆ’1 1 4
1 âˆ’1 4
0 2 2
1 2 2
1 2 8
102 Chapter 7. Linear-Quadratic Systems
Figure 7.1. Optimal trajectories in state-costate space, with
level curves of the Hamiltonian.
Example 7.2: Integrator
```
The integrator case (Example 4.3) is linear-quadratic: ğ‘¥â€² = ğ‘¢ and cost ğ½ = âˆ«ğ‘‡0 ğ‘¥2 +
```
ğ‘¢2 ğ‘‘ğ‘¡. With optimal control ğ‘¢ = âˆ’ğœ†/2 we get the state-costate system
ğ‘¥â€² = âˆ’ 12 ğœ†,
ğœ†â€² = âˆ’2ğ‘¥
and trajectories are level curves of the hyperbolic paraboloid
```
ğ»(ğ‘¥, ğœ†) = ğ‘¥2 âˆ’ 14 ğœ†2.
```
```
In Example 6.4 we solved this system with endpoints ğ‘¥(0) = âˆ’1 and ğ‘¥(ğ‘‡) = 1 for
```
```
a general allowed time ğ‘‡. We saw that performance ğ½(ğ‘‡) = 2(ğ‘’ğ‘‡ âˆ’ 1)/(ğ‘’ğ‘‡ + 1)
```
```
was a decreasing function of ğ‘‡, with ğ½ â†’ 2 as ğ‘‡ â†’ âˆ and with solutions ğ‘¥(ğ‘¡)
```
spending more and more time near ğ‘¥ = 0 as shown in Figure 6.6.
We can now interpret those results as trajectories of a linear system in two-
dimensional state-costate space that are level curves of ğ», as shown in Figure
7.2.
7.1. Linear-Quadratic with Fixed Ends 103
```
Figure 7.2. Plots of (ğ‘¥(ğ‘¡), ğœ†(ğ‘¡)) as ğ‘‡ â†’ âˆ.
```
```
Allowing increasing total time ğ‘‡ â†’ âˆ the optimal solutions (ğ‘¥(ğ‘¡), ğœ†(ğ‘¡)) ap-
```
```
proach the incoming and outgoing eigendirections in the (ğ‘¥, ğœ†)-plane, spending
```
```
more and more time in the slow-moving region near the origin (0, 0).
```
Example 7.3
```
Consider the controlled system ğ‘¥â€² = ğ‘¢ with endpoint conditions ğ‘¥(0) = 1 and
```
```
ğ‘¥(ğ‘‡) = 2, and suppose we want a control which maximizes
```
ğ½ = âˆ«
ğ‘‡
0
1
```
2 (ğ‘¥2 âˆ’ ğ‘¢2) ğ‘‘ğ‘¡.
```
The Hamiltonian is
```
ğ» = 12 (ğ‘¥2 âˆ’ ğ‘¢2) + ğœ†ğ‘¢.
```
We have 0 = ğœ•ğ»ğœ•áµ† = âˆ’ğ‘¢ + ğœ† making ğ‘¢ = ğœ†. Note ğœ•2ğ»ğœ•áµ†2 < 0, which is consistent
with maximizing. With ğœ•ğ»ğœ•ğ‘¥ = ğ‘¥ we have the state-costate system
ğ‘¥â€² = ğœ†,
ğœ†â€² = âˆ’ğ‘¥
with solutions
```
ğ‘¥ = ğ‘¥0 cos(ğ‘¡) + ğœ†0 sin(ğ‘¡),
```
```
ğœ† = âˆ’ğ‘¥0 sin(ğ‘¡) + ğœ†0 cos(ğ‘¡).
```
Trajectories are level curves of the elliptic paraboloid
```
ğ» = 12 (ğ‘¥2 + ğœ†2).
```
104 Chapter 7. Linear-Quadratic Systems
```
We can satisfy the boundary conditions ğ‘¥(0) = 1 and ğ‘¥(ğ‘‡) = 2 with
```
ğ‘¥0 = 1,
```
ğœ†0 = 2âˆ’cos(ğ‘‡)sin(ğ‘‡) .
```
```
(7.1)
```
```
For instance, if ğ‘‡ = 1/2, we get ğœ†0 = (2 âˆ’ cos(1/2))/ sin(1/2) = 2.341 . . . and
```
the trajectory is on the level curve ğ» = 3.241 . . . as seen in Figure 7.3.
Figure 7.3. Optimal trajectory in state-costate space, with level
curves of the Hamiltonian.
As we did in Example 6.4 we can keep the same initial and final condi-
```
tions, ğ‘¥(0) = 1, ğ‘¥(ğ‘‡) = 2, and consider what the solutions look like for a va-
```
```
riety of terminal times ğ‘‡. From equation (7.1), this is accomplished with ğœ†0 =
```
```
(2 âˆ’ cos(ğ‘‡))/ sin(ğ‘‡) plotted in Figure 7.4.
```
```
Figure 7.4. Initial costate ğœ†0 = (2 âˆ’ cos(ğ‘‡))/ sin(ğ‘‡) as a func-
```
tion of allowed time ğ‘‡.
7.1. Linear-Quadratic with Fixed Ends 105
Representative trajectories for 0 < ğ‘‡ < ğœ‹ are plotted in the state-costate
plane in Figure 7.5.
For small ğ‘‡ > 0 we have very little time to transition from ğ‘¥ = 1 to ğ‘¥ = 2.
```
This is accomplished with very large ğœ†0 so the trajectory in the (ğ‘¥, ğœ†)-plane sweeps
```
from ğ‘¥ = 1 to ğ‘¥ = 2 very quickly. Starting values ğœ†0 decrease to a minimum of
```
ğœ†0 = âˆš3 = 1.732 . . . as ğ‘‡ increases to ğ‘‡ = ğœ‹/3 = 1.047 . . . . The trajectory ğ‘¥(ğ‘¡) is
```
monotone increasing for the region 0 < ğ‘‡ â‰¤ ğœ‹/3.
If we are allowed a bit more time and ğ‘‡ increases past ğ‘‡ = ğœ‹/3, then we get
```
increasing values for ğœ†0 and the trajectory ğ‘¥(ğ‘¡) overshoots the end value ğ‘¥(ğ‘‡) = 2
```
before returning. Initial values ğœ†0 increase back towards âˆ as ğ‘‡ approaches ğœ‹.
Figure 7.5. Optimal solution in state-costate space, with level
curves of the Hamiltonian, for various allowed times ğ‘‡.
With some work, we can calculate performance as a function of allowed time
```
ğ‘‡ (Figure 7.6),
```
```
ğ½(ğ‘‡) = 4 âˆ’ 5 cos(ğ‘‡)2 sin(ğ‘‡) .
```
For small ğ‘‡ > 0 our maximal payoff is extremely negative. For such small al-
lowed time ğ‘‡ we are forced to use large values of ğ‘¢ to attain the required end-
points, and this creates large negative values in our payoff function ğ‘” = ğ‘¥2 âˆ’ ğ‘¢2.
These negative performance values are simply the best we can do under the cir-
cumstances.
The optimal payoff increases with more allowed time, eventually becoming
positive. For ğ‘‡ values just below ğœ‹ we have sufficient time to travel far beyond
our endpoint value ğ‘¥ = 2 before returning, thereby accumulating positive values
in the payoff function ğ‘” = ğ‘¥2 âˆ’ ğ‘¢2. In fact, the payoff increases without bound as
ğ‘‡ increases towards ğœ‹. We could perhaps get an infinite payoff at ğ‘‡ = ğœ‹ by going
all the way to infinity and back, but that probably wouldnâ€™t be allowed. No one
ever lets you go to infinity and back. You might see something.
106 Chapter 7. Linear-Quadratic Systems
```
Figure 7.6. Optimal payoff ğ½ = (4 âˆ’ 5 cos(ğ‘‡))/(2 sin(ğ‘‡)) as a
```
```
function of allowed time ğ‘‡.
```
Having payoffs become unbounded as ğ‘‡ â†’ ğœ‹ brings up some subtle points
regarding the existence of solutions and the topology of the space of solutions,
as we discussed in Section 4.3.3, and we will revisit this in Chapter 10. It turns
out that for any allowed time ğ‘‡ > ğœ‹ we can attain arbitrarily large payoffs, as we
work out later in Exercise 10.8.
7.2 Linear-Quadratic with Free Ends
We can apply Principle III to linear-quadratic systems, allowing for free end conditions.
```
7.2.1 Free Endpoint. If we have a free condition ğ‘¥(ğ‘‡) = ğµ with an associated pay-
```
```
off ğº(ğµ), our performance would be
```
```
ğ½ = ğº(ğµ) + âˆ«
```
ğ‘‡
0
ğ‘ğ‘¥2 + ğ‘ğ‘¢2 ğ‘‘ğ‘¡
as discussed in Section 6.1. This leads to a two-point boundary problem in state and
costate variables according to Principle III:
```
ğ‘¥(0) = ğ´,
```
```
ğœ†(ğ‘‡) = ğœ•ğºğœ•ğµ (ğµ).
```
```
For ğº(ğµ) zero or constant, we have ğœ†(ğ‘‡) = 0.
```
7.2. Linear-Quadratic with Free Ends 107
Example 7.4: Integrator
```
Returning to the Integrator (Example 7.2) we have ğ‘¥â€² = ğ‘¢, cost ğ½ = âˆ«ğ‘‡0 ğ‘¥2 + ğ‘¢2 ğ‘‘ğ‘¡,
```
optimal control ğ‘¢ = âˆ’ğœ†/2, and state-costate system
ğ‘¥â€² = âˆ’ 12 ğœ†,
ğœ†â€² = âˆ’2ğ‘¥
```
with trajectories being level curves of the hyperbolic paraboloid ğ»(ğ‘¥, ğœ†) =
```
ğ‘¥2 âˆ’ 14 ğœ†2.
```
Suppose we have a starting location ğ‘¥(0) = 4 and a fixed time ğ‘‡, but our end
```
```
location ğ‘¥(ğ‘‡) is free.
```
```
Here ğº(ğµ) is the zero function, so our endpoint condition is ğœ†(ğ‘‡) = 0. Solv-
```
```
ing this for ğ‘‡ = 1 we have boundary values ğ‘¥(0) = 4 and ğœ†(1) = 0, and we get
```
```
a solution ğ‘¥(ğ‘¡) = 4 (ğ‘’1âˆ’ğ‘¡ + ğ‘’ğ‘¡âˆ’1)(ğ‘’ + ğ‘’âˆ’1) and ğœ†(ğ‘¡) = âˆ’8 (ğ‘’1âˆ’ğ‘¡ âˆ’ ğ‘’ğ‘¡âˆ’1)(ğ‘’ + ğ‘’âˆ’1).
```
Solutions for ğ‘‡ = 2, ğ‘‡ = 1, and ğ‘‡ = 1/2 are plotted in Figure 7.7.
Figure 7.7. Optimal solution for a free endpoint, with level
curves of the Hamiltonian.
Other values for initial position ğ‘¥0 and fixed end time ğ‘‡ are accommodated
with appropriate starting values for ğœ†0 as
```
ğœ†0(ğ‘‡) = 2ğ‘¥0ğ‘’
```
ğ‘‡ âˆ’ ğ‘’âˆ’ğ‘‡
```
ğ‘’ğ‘‡ + ğ‘’âˆ’ğ‘‡ = 2ğ‘¥0 tanh(ğ‘‡).
```
Note that ğœ†0 â†’ 2ğ‘¥0 as ğ‘‡ â†’ âˆ, with trajectories approaching the incoming
eigendirection. For shorter time periods, ğœ†0 â†’ 0 as ğ‘‡ â†’ 0.
108 Chapter 7. Linear-Quadratic Systems
Example 7.5
```
Returning to Example 7.3 with a free endpoint and terminal cost ğº(ğµ) = âˆ’ğµ2/2,
```
we have performance
```
ğ½ = âˆ’ 12 ğ‘¥(ğ‘‡)2 + âˆ«
```
ğ‘‡
0
1
```
2 (ğ‘¥2 âˆ’ ğ‘¢2) ğ‘‘ğ‘¡.
```
For ğ‘¥â€² = ğ‘¢ our solutions are the same:
```
ğ‘¥ = ğ‘¥0 cos(ğ‘¡) + ğœ†0 sin(ğ‘¡),
```
```
ğœ† = âˆ’ğ‘¥0 sin(ğ‘¡) + ğœ†0 cos(ğ‘¡)
```
```
which are contained in level curves of ğ» = (ğ‘¥2 + ğœ†2)/2.
```
Our endpoint conditions are now an initial value for ğ‘¥, a fixed time ğ‘‡, and a
terminal value for ğœ† given by Principle III:
```
ğœ†(ğ‘‡) = ğºâ€²(ğµ) = âˆ’ğµ = âˆ’ğ‘¥(ğ‘‡).
```
That is, trajectories must terminate on the line ğ‘¥ = âˆ’ğœ†. To find the trajectory for
```
a given ğ‘‡, we solve ğœ†(ğ‘‡) = âˆ’ğ‘¥(ğ‘‡) for ğœ†0.
```
```
Solutions starting at ğ‘¥(0) = 1 for ğ‘‡ = 0.5, ğ‘‡ = 1.0, and ğ‘‡ = 3.0 are depicted
```
in Figure 7.8.
Figure 7.8. Optimal solution for a free endpoint, for various
values of allowed time ğ‘‡.
```
7.2.2 Free End Time. If ğ‘¥(ğ‘‡) is fixed, but ğ‘‡ is free,
```
```
ğ½ = ğº(ğ‘‡) + âˆ«
```
ğ‘‡
0
ğ‘ğ‘¥2 + ğ‘ğ‘¢2 ğ‘‘ğ‘¡,
```
we would look for solutions where ğ»(ğ‘‡) = âˆ’ ğœ•ğºğœ•ğ‘‡ . As ğ» is conserved by the state-costate
```
systems, ğ» is constant at this value. If ğº is zero or constant, solutions are zero-level
curves of ğ».
7.2. Linear-Quadratic with Free Ends 109
Example 7.6
As in Example 7.1, consider controlled exponential growth ğ‘¥â€² = 14 ğ‘¥ + ğ‘¢ with
```
endpoint conditions ğ‘¥(0) = âˆ’1 and ğ‘¥(4) = 1.
```
Suppose that instead of fixing ğ‘‡ = 4, we allow ğ‘‡ to be free and we introduce
a term that imposes a penalty for ğ‘‡ deviating from 4. We seek to minimize:
```
ğ½ = ğ›¼(ğ‘‡ âˆ’ 4)2 + âˆ«
```
ğ‘‡
0
ğ‘¢2 ğ‘‘ğ‘¡.
Here ğ›¼ adjusts the severity of the penalty, and we anticipate that as ğ›¼ â†’ âˆ our
solutions will converge to that of fixing ğ‘‡ = 4.
By Principle III, the optimal end time must satisfy
```
ğ»(ğ‘‡) = âˆ’ğºâ€²(ğ‘‡) = âˆ’2ğ›¼(ğ‘‡ âˆ’ 4).
```
As before, we have ğ‘¢ = âˆ’ 12 ğœ† and state-costate system
ğ‘¥â€² = 14 ğ‘¥ âˆ’ 12 ğœ†,
ğœ†â€² = âˆ’ 14 ğœ†
with solutions
```
ğ‘¥ = ğ‘¥0ğ‘’ğ‘¡/4 âˆ’ ğœ†0(ğ‘’ğ‘¡/4 âˆ’ ğ‘’âˆ’ğ‘¡/4),
```
ğœ† = ğœ†0ğ‘’âˆ’ğ‘¡/4
that are level curves of
```
ğ» = 14 ğœ†(ğ‘¥ âˆ’ ğœ†). (7.2)
```
So how does this all come together? We have three free parameters ğ‘¥0, ğœ†0, ğ‘‡, and
```
three endpoint conditions ğ‘¥(0) = âˆ’1, ğ‘¥(ğ‘‡) = 1, and ğ»(ğ‘‡) = âˆ’2ğ›¼(ğ‘‡ âˆ’ 4).
```
```
First, match ğ‘¥(0) = âˆ’1 and ğ‘¥(ğ‘‡) = 1 to yield (as in Example 7.1, and with
```
```
some simplification)
```
ğœ†0 = âˆ’ ğ‘’
ğ‘‡/4
```
ğ‘’ğ‘‡/4 âˆ’ 1 . (7.3)
```
```
Next, we solve for ğ‘‡ using ğ»(ğ‘‡) = âˆ’2ğ›¼(ğ‘‡ âˆ’ 4). We use the fact that ğ»
```
```
is constant on optimal trajectories (this is a cool trick!), so ğ»(ğ‘‡) = ğ»(0) =1
```
```
4 ğœ†0(ğ‘¥0 âˆ’ ğœ†0) from equation (7.2), and we get
```
1
```
4 ğœ†0(âˆ’1 âˆ’ ğœ†0) = âˆ’2ğ›¼(ğ‘‡ âˆ’ 4).
```
```
Keep in mind that ğœ†0 is dependent on ğ‘‡ (equation (7.3)), and so the full hideous
```
expression is
1
```
4 (âˆ’
```
ğ‘’ğ‘‡/4
```
ğ‘’ğ‘‡/4 âˆ’ 1 ) (âˆ’1 +
```
ğ‘’ğ‘‡/4
```
ğ‘’ğ‘‡/4 âˆ’ 1 ) = âˆ’2ğ›¼(ğ‘‡ âˆ’ 4)
```
which simplifies to
ğ‘’ğ‘‡/4
```
8(ğ‘’ğ‘‡/4 âˆ’ 1)2 = ğ›¼(ğ‘‡ âˆ’ 4).
```
The optimal value for ğ‘‡ is where the transcendental expression on the left
```
side of the equality intersects the line ğ›¼(ğ‘‡ âˆ’ 4) on the right. Solving for ğ‘‡ directly
```
110 Chapter 7. Linear-Quadratic Systems
```
wonâ€™t work (transcendentals and polynomials donâ€™t mix very well), so we numer-
```
ically approximate. For example, when ğ›¼ = 1 the intersection is approximately
```
ğ‘‡ = 4.109 . . . (Figure 7.9).
```
Figure 7.9. Intersection of line and transcendental function,
with close-up.
Here the solution balances the end penalty against the running costs to take
a bit more time and reach the end condition a little later than the optimal ğ‘‡ = 4
in order to save on running costs.
As ğ›¼ â†’ âˆ the penalty increases, the slope of the line increases, the value
for the optimal ğ‘‡ converges to ğ‘‡ = 4, and our solution converges to that found
in Example 7.1. As ğ›¼ â†’ 0 we get ğ‘‡ â†’ âˆ and the limiting trajectory is the
straight line trajectory ğ‘¥ = âˆ’ğ‘’âˆ’ğ‘¡/4, ğœ† = âˆ’ğ‘’âˆ’ğ‘¡/4 that approaches the origin along
```
the incoming eigendirection (see Figure 7.1) and never reaches ğ‘¥ = 1.
```
Key Points
Linear-quadratic systems are the simplest form of control problem to have enough con-
vexity to produce meaningful locally optimal controls and apply to a broad array of
control problems.
These systems produce state-costate dynamics that are linear systems of differen-
tial equations, and thus they allow solutions for hands-on study and geometric inter-
pretations.
Exercises 111
Exercises
```
Exercise 7.1(s). Consider the controlled exponential decay system ğ‘¥â€² = âˆ’ğ‘¥ + ğ‘¢ with
```
quadratic cost on control ğ½ = âˆ«ğ‘‡012 ğ‘¢2 ğ‘‘ğ‘¡.
```
(a) Find the minimum cost for ğ‘¥(0) = 1 and ğ‘¥(1) = âˆ’1. Plot the trajectory as a
```
level curve of ğ» in state-costate space.
```
(b) Find the minimum cost for ğ‘¥(0) = 1 and ğ‘¥(ğ‘‡) = âˆ’1 for a fixed ğ‘‡ > 0. Plot
```
some representative trajectories in state-costate space for various values of ğ‘‡.
```
(c) Find the minimum cost for ğ‘¥(0) = 2 and ğ‘¥(ğ‘‡) = 1 for a fixed ğ‘‡. Plot some
```
representative trajectories in state-costate space. Explain solutions for very small and
very large values of ğ‘‡. Do any solutions have zero cost?
```
Exercise 7.2(s). As in the previous exercise, consider the system ğ‘¥â€² = âˆ’ğ‘¥ + ğ‘¢ with
```
```
running costs ğ‘”(ğ‘¢) = 12 ğ‘¢2. Suppose we have a fixed starting point ğ‘¥(0) = 2, a fixed
```
```
time ğ‘‡ > 0,and free end location ğ‘¥(ğ‘‡).
```
```
(a) For ğ½ = ğ‘¥(ğ‘‡)2 + âˆ«ğ‘‡012 ğ‘¢2 ğ‘‘ğ‘¡, find the minimizing solution for ğ‘‡ = 1. Describe
```
the minimizing solution for very small and very large values of ğ‘‡. Use level curves of
ğ» in the state-costate space to explain your findings.
```
(b) For ğ½ = (ğ‘¥(ğ‘‡) âˆ’ 1)2 + âˆ«ğ‘‡012 ğ‘¢2 ğ‘‘ğ‘¡, find the minimizing solution for ğ‘‡ = 1.
```
Describe the minimizing solution for very small and very large values of ğ‘‡. Use level
curves of ğ» in the state-costate space to explain your findings.
```
Exercise 7.3. Consider the system ğ‘¥â€² = ğ‘¢, starting location ğ‘¥(0) = 1, and performance
```
```
ğ½ = âˆ«ğ‘‡012 (ğ‘¥2 âˆ’ ğ‘¢2) ğ‘‘ğ‘¡ (as in Example 7.3).
```
```
What is the optimal control if ğ‘‡ = 1 is fixed, but ğ‘¥(ğ‘‡) is free? What is the optimal
```
```
control for any fixed ğ‘‡ with ğ‘¥(ğ‘‡) free? Explain using level curves of ğ» in the (ğ‘¥, ğœ†)-
```
plane. Plot representative trajectories for 0 < ğ‘‡ < ğœ‹/2, for ğœ‹/2 < ğ‘‡ < ğœ‹, and for
ğœ‹ < ğ‘‡ < 3ğœ‹/2.
```
Exercise 7.4. Consider the system ğ‘¥â€² = ğ‘¢, starting location ğ‘¥(0) = 1, end location
```
```
ğ‘¥(ğ‘‡) = 2, and performance ğ½ = âˆ«ğ‘‡012 (ğ‘¥2 âˆ’ ğ‘¢2) ğ‘‘ğ‘¡ (as in Example 7.3).
```
```
(a) Verify that ğ½(ğ‘‡) = (4 âˆ’ 5 cos(ğ‘‡))/(2 sin(ğ‘‡)).
```
```
(b) Verify that ğ» = (5 âˆ’ 4 cos(ğ‘‡))/(2 sin2(ğ‘‡)).
```
Now suppose the payoff is
```
ğ½ = âˆ’10(ğ‘‡ âˆ’ 1)2 + âˆ«
```
ğ‘‡
0
1
```
2 (ğ‘¥2 âˆ’ ğ‘¢2) ğ‘‘ğ‘¡.
```
```
(c) Compute ğ½(ğ‘‡) for any given ğ‘‡. Numerically approximate any local maxima for
```
0 < ğ‘‡ < ğœ‹.
```
(d) For ğ‘‡ free, use Principle III to numerically approximate values for ğ‘‡ that pro-
```
```
duce local maxima for 0 < ğ‘‡ < ğœ‹. Does this confirm your result in part (c)?
```
```
(e) Is there a global maximum?
```
112 Chapter 7. Linear-Quadratic Systems
```
Exercise 7.5(s). Consider the system from Example 7.3 with ğ‘¥â€² = ğ‘¢ and performance
```
```
ğ½ = âˆ«ğ‘‡012 (ğ‘¥2 âˆ’ ğ‘¢2)ğ‘‘ğ‘¡. Set the endpoint conditions to be ğ‘¥(0) = âˆ’1, ğ‘¥(ğ‘‡) = 1.
```
```
(a) Find the maximal solution for a given ğ‘‡. Plot ğ½(ğ‘‡) as a function of ğ‘‡. Over
```
what range of values ğ‘‡ > 0 does this approach appear to be valid?
```
(b) The solution in part (a) breaks down for ğ‘‡ = ğœ‹. Show that ğ‘¥(ğ‘¡) = âˆ’ cos(ğ‘¡) +
```
```
ğœ†0 sin(ğ‘¡) with control ğ‘¢ = sin(ğ‘¡) + ğœ†0 cos(ğ‘¡) satisfies the boundary conditions at ğ‘‡ = ğœ‹,
```
satisfies Principle III, and produces ğ½ = 0 for all values of ğœ†0. Plot a few trajectories for
various values of ğœ†0 on the state-costate space. Why did this solution not show up in
```
part (a)?
```
```
Exercise 7.6(h). Consider the system from Example 7.3 with ğ‘¥â€² = ğ‘¢ and performance
```
```
ğ½ = 4ğ‘‡(2âˆ’ğ‘‡)+âˆ«ğ‘‡012 (ğ‘¥2 âˆ’ğ‘¢2)ğ‘‘ğ‘¡. Set the endpoint conditions to be ğ‘¥(0) = âˆ’1, ğ‘¥(ğ‘‡) = 1.
```
```
(a) Solve this assuming a fixed value for ğ‘‡ in the range 0 < ğ‘‡ < ğœ‹ and plot the
```
payoff ğ½ as a function of time ğ‘‡. What is the approximate maximum payoff in this
range?
```
(b) Now assume ğ‘‡ is free. Show that you get the same maximum payoff applying
```
Principle III with free ğ‘‡.
Exercise 7.7. Consider the general linear case ğ‘¥â€² = ğ‘šğ‘¥ + ğ‘¢ with ğ½ = âˆ«ğ‘‡0 ğ‘ğ‘¥2 + ğ‘ğ‘¢2 ğ‘‘ğ‘¡.
```
What happens when ğ‘ = 0? Argue that there are no optimal solutions; ğ½ can be as
```
large, positive or negative, as you want.
```
Exercise 7.8(hs). Consider the case ğ‘¥â€² = ğ‘¢ with ğ½ = ğº(ğ‘‡, ğ‘¥(ğ‘‡))+âˆ«ğ‘‡0 ğ‘¢2 ğ‘‘ğ‘¡ for a general
```
```
differentiable function ğº. Taking ğ‘¥(0) = 0, describe the solution for all combinations
```
```
(fixed or free) of endpoint conditions ğ‘‡, ğ‘¥(ğ‘‡).
```
Exercise 7.9. Show that discounting the future in a linear-quadratic system can be
simplified with a change of variables.
Consider
ğ‘¥â€² = ğ‘šğ‘¥ + ğ‘¢
with performance
ğ½ = âˆ«
ğ‘‡
0
```
ğ‘’âˆ’ğ›¾ğ‘¡(ğ‘ğ‘¥2 + ğ‘ğ‘¢2) ğ‘‘ğ‘¡.
```
```
(a) Introduce variablesÌ‚ ğ‘¥ andÌ‚ ğ‘¢ with
```
Ì‚ğ‘¥ = ğ‘’âˆ’ğ›¾ğ‘¡/2 ğ‘¥,Ì‚
ğ‘¢ = ğ‘’âˆ’ğ›¾ğ‘¡/2 ğ‘¢
Exercises 113
and convert the system to a linear-quadratic system
```
Ì‚ğ‘¥â€² = (ğ‘š âˆ’ ğ›¾2 )Ì‚ğ‘¥ +Ì‚ ğ‘¢
```
with performance
ğ½ = âˆ«
ğ‘‡
0
ğ‘Ì‚ğ‘¥2 + ğ‘Ì‚ğ‘¢2 ğ‘‘ğ‘¡.
```
(b) Is ğ‘š = ğ›¾/2 special? Why?
```
8
Two Dimensions
In this chapter we generalize the optimization principle to higher dimensions, where
the state of the system is described by a vector of state variables, extending the scope of
our tools to a wider variety of models. Our main focus will be two-dimensional state
spaces which allows us to model acceleration caused by physical forces. By generaliz-
ing to two dimensions, it becomes clearer how the theory would apply to any number
of dimensions.
Many problems involve minimizing time, so our first higher-dimensional principle
will allow free endtime. Higher-dimensional state spaces also allow for more general
endpoint conditions, which can be a bit trickier, and this will be addressed in the next
chapter.
115
116 Chapter 8. Two Dimensions
8.1 Optimal Control in Two Dimensions
OPTIMAL PRINCIPLE IV
Local optimum, free duration, fixed endpoint, time dependent, two dimensions
Consider the controlled system
```
( ğ‘¥
```
â€²1
```
ğ‘¥â€²2) = (
```
```
ğ‘“1(ğ‘¥1, ğ‘¥2, ğ‘¢, ğ‘¡)
```
```
ğ‘“2(ğ‘¥1, ğ‘¥2, ğ‘¢, ğ‘¡) ) , ğ‘¥1, ğ‘¥2, ğ‘¡ âˆˆ â„, ğ‘¢ âˆˆ ğ’°,
```
with fixed endpoint conditions
```
ğ‘¥1(0) = ğ´1, ğ‘¥1(ğ‘‡) = ğµ1,
```
```
ğ‘¥2(0) = ğ´2, ğ‘¥2(ğ‘‡) = ğµ2
```
and objective function
```
ğ½ = ğº(ğ‘‡) + âˆ«
```
ğ‘‡
0
```
ğ‘”(ğ‘¥1, ğ‘¥2, ğ‘¢, ğ‘¡) ğ‘‘ğ‘¡.
```
Define the Hamiltonian
```
ğ»(ğ‘¥1, ğ‘¥2, ğœ†1, ğœ†2, ğ‘¢, ğ‘¡) = ğ‘” + ğœ†1ğ‘“1 + ğœ†2ğ‘“2
```
and costate equations
ğœ†â€²1 = âˆ’ ğœ•ğ»ğœ•ğ‘¥1,
ğœ†â€²2 = âˆ’ ğœ•ğ»ğœ•ğ‘¥2.
Then a locally optimal control must satisfy
ğœ•ğ»
ğœ•ğ‘¢ = 0
and the control ğ‘¢ that optimizes ğ½ will optimize ğ» at all times.
The ending time ğ‘‡ may be prescribed. Otherwise, the optimal ending time
will satisfy
ğœ•ğº
```
ğœ•ğ‘‡ (ğ‘‡) + ğ»(ğ‘‡) = 0.
```
Furthermore, if ğ» does not explicitly depend on ğ‘¡, it is constant on optimal trajec-
tories.
This two-dimensional principle is stated using numerical subscripts ğ‘¥1, ğ‘¥2, ğœ†1, ğœ†2
to more clearly see the effect of dimension and how the principle would generalize to
any finite number of dimensions. In practice we will focus on applying the principle
```
in two dimensions. The examples will be phrased in terms of (ğ‘¥, ğ‘¦)-coordinates and
```
corresponding symbolic subscripts ğœ†ğ‘¥, ğœ†ğ‘¦ for ease and clarity.
8.2 Thrust Programming and Rocket Sleds
8.2.1 Thrust Programming. Lots of models incorporate second derivatives, with
accelerations and forces being natural objects for examination. Taking control ğ‘¢ to be
8.2. Thrust Programming and Rocket Sleds 117
thrust, here are some scenarios for control:
```
System: Model for:
```
ğ‘¥â€³ = âˆ’ğ‘Ÿğ‘¥â€² + ğ‘¢ Linear motion
ğ‘¥â€³ = âˆ’ğœ… âˆ’ ğ‘Ÿğ‘¥â€² + ğ‘¢ Vertical motion in constant gravity
ğ‘¥â€³ = âˆ’ğ‘¥ âˆ’ ğ‘Ÿğ‘¥â€² + ğ‘¢ Oscillator
```
Any of these can be taken as frictionless by taking ğ‘Ÿ = 0; otherwise we assume a positive
```
coefficient of friction ğ‘Ÿ > 0.
Typical choices for performance ğ½ = âˆ« ğ‘” ğ‘‘ğ‘¡ include:
ğ : Optimize for:
1 time
|ğ‘¢| net thrust
ğ‘¢2 cost
Performance may also depend on position by augmenting one of the above with a func-
tion of ğ‘¥. For example, ğ‘” = ğ‘¢2 +ğ‘¥2 represents a need to conserve thrust but also remain
close to ğ‘¥ = 0.
Future payoffs can also be discounted by including a time-dependent ğ‘’âˆ’ğ›¼ğ‘¡ term.
One class of problems referred to as â€œsoft-landingâ€ problems are where the task is
to bring the system to rest ğ‘¥â€² = 0 at the origin ğ‘¥ = 0.
Second-order differential equations with state variable ğ‘¥ are analyzed by convert-
ing to a system of two first-order differential equations with the introduction of a ve-
locity variable ğ‘¦ = ğ‘¥â€², making a two-dimensional state space.
8.2.2 Rocket Sleds. We will cover a number of examples and exercises for a sim-
ple thrust control system that we refer to as Rocket Sleds. Acceleration of the sled is
modeled as
ğ‘¥â€³ = âˆ’ğ‘Ÿğ‘¥â€² + ğ‘¢
where our control ğ‘¢ is the thrust. For the Rocket Sled examples, we standardize the
coefficient of friction ğ‘Ÿ = 1, which produces exponential solutions. For exercises we
```
consider the frictionless case ğ‘Ÿ = 0 (Rocket Sled on Ice), which has polynomial solu-
```
```
tions (easier to work with!).
```
We will consider three possibilities for performance: cost of the thrust ğ½ = âˆ«ğ‘‡0 ğ‘¢2 ğ‘‘ğ‘¡,
time of travel ğ½ = ğ‘‡, or a combination of both ğ½ = âˆ«ğ‘‡0 1 + ğ‘¢2 ğ‘‘ğ‘¡. This gives us quite a
few possibilities to consider.
The Rocket Sled problems and exercises will create a template that directly applies
to a wide variety of problems from robotic control to adjusting satellite orbits.
118 Chapter 8. Two Dimensions
Example 8.1: Rocket Sled
We have the one-dimensional thrust system with viscous friction
ğ‘¥â€³ = âˆ’ğ‘Ÿğ‘¥â€² + ğ‘¢
where we control thrust ğ‘¢. In one unit of time, ğ‘‡ = 1, we want to move this
```
system from rest, ğ‘¥â€²(0) = 0, at location ğ‘¥(0) = âˆ’1 to rest, ğ‘¥â€²(1) = 0, at location
```
```
ğ‘¥(1) = 0 while minimizing
```
ğ½ = âˆ«
1
0
ğ‘¢2 ğ‘‘ğ‘¡.
With ğ‘¦ = ğ‘¥â€² and taking ğ‘Ÿ = 1 we have the system
ğ‘¥â€² = ğ‘¦,
ğ‘¦â€² = âˆ’ğ‘¦ + ğ‘¢.
The Hamiltonian is
```
ğ» = ğ‘¢2 + ğœ†ğ‘¥ğ‘¦ + ğœ†ğ‘¦(âˆ’ğ‘¦ + ğ‘¢)
```
which will be constant on optimal trajectories. Optimal control must satisfy
0 = ğœ•ğ»ğœ•ğ‘¢ = 2ğ‘¢ + ğœ†ğ‘¦
making ğ‘¢ = âˆ’ 12 ğœ†ğ‘¦. Note ğœ•2ğ»ğœ•áµ†2 < 0 so minimizing is appropriate. We have the
following set of state and costate equations:
ğ‘¥â€² = ğ‘¦,
ğ‘¦â€² = âˆ’ğ‘¦ âˆ’ 12 ğœ†ğ‘¦,
ğœ†â€²ğ‘¥ = 0,
ğœ†â€²ğ‘¦ = âˆ’ğœ†ğ‘¥ + ğœ†ğ‘¦.
We solve this system of differential equations by first solving for ğœ†ğ‘¥ and ğœ†ğ‘¦, then
```
solving for ğ‘¦, then ğ‘¥. We get (âˆ— verify this âˆ—)
```
ğ‘¥ = âˆ’ 12 ğ‘ğ‘¡ âˆ’ 14 ğ‘ğ‘’ğ‘¡ âˆ’ ğ‘ğ‘’âˆ’ğ‘¡ + ğ‘‘,
ğ‘¦ = âˆ’ 12 ğ‘ âˆ’ 14 ğ‘ğ‘’ğ‘¡ + ğ‘ğ‘’âˆ’ğ‘¡,
ğœ†ğ‘¥ = ğ‘,
ğœ†ğ‘¦ = ğ‘ + ğ‘ğ‘’ğ‘¡
with four integration constants ğ‘, ğ‘, ğ‘, ğ‘‘.
Using boundary conditions
```
ğ‘¥(0) = âˆ’1, ğ‘¦(0) = 0, ğ‘¥(1) = 0, ğ‘¦(1) = 0
```
8.2. Thrust Programming and Rocket Sleds 119
we have four linear equations in four unknowns ğ‘, ğ‘, ğ‘, ğ‘‘:
```
ğ‘¥(0) = âˆ’1 = âˆ’ 14 ğ‘ âˆ’ ğ‘ + ğ‘‘,
```
```
ğ‘¦(0) = 0 = âˆ’ 12 ğ‘ âˆ’ 14 ğ‘ + ğ‘,
```
```
ğ‘¥(1) = 0 = âˆ’ 12 ğ‘ âˆ’ 14 ğ‘ğ‘’ âˆ’ ğ‘ğ‘’âˆ’1 + ğ‘‘,
```
```
ğ‘¦(1) = 0 = âˆ’ 12 ğ‘ âˆ’ 14 ğ‘ğ‘’ + ğ‘ğ‘’âˆ’1.
```
These equations are linear in the unknowns and can be readily solved with
a symbolic processor. It is actually not too bad to solve this system by hand: one
could start by subtracting the second and fourth equations to get ğ‘ = âˆ’4ğ‘/ğ‘’, and
subtracting the third and fourth equations to get ğ‘‘ = 2ğ‘/ğ‘’, substituting these into
```
the first equation to get ğ‘ = âˆ’ğ‘’/(3 âˆ’ ğ‘’), and so on. This will lead to optimal
```
trajectory
ğ‘¥ = ğ‘’
```
1âˆ’ğ‘¡ âˆ’ ğ‘’ğ‘¡ + ğ‘¡(1 + ğ‘’) âˆ’ 2
```
3 âˆ’ ğ‘’
```
at a cost of ğ½ = (1 + ğ‘’)/(3 âˆ’ ğ‘’). Plots of state ğ‘¥(ğ‘¡) and control ğ‘¢(ğ‘¡) are shown in
```
Figure 8.1.
Figure 8.1. State and control for optimal solution plotted against time.
Different values of final time ğ‘‡ produce different solutions, as shown in Fig-
```
ure 8.2 plotted as trajectories (ğ‘¥(ğ‘¡), ğ‘¦(ğ‘¡)) in the position/velocity plane.
```
120 Chapter 8. Two Dimensions
Figure 8.2. Optimal solutions in position-velocity state space
for various values of allowed time ğ‘‡.
For these endpoint conditions, the cost as a function of end time is
```
ğ½(ğ‘‡) = ğ‘’
```
ğ‘¡ + 1
```
ğ‘‡ + 2 + ğ‘’ğ‘‡ (ğ‘‡ âˆ’ 2) = (ğ‘‡ âˆ’ 2 tanh(
```
ğ‘‡
```
2 ))
```
âˆ’1
.
```
This is monotonically decreasing (more time reduces cost) with ğ½(ğ‘‡) â†’ 0 as
```
ğ‘‡ â†’ âˆ. The limiting control would be ğ‘¢ â‰¡ 0, which gets you nowhere. So
if ğ‘‡ is allowed to be free in this example, we wouldnâ€™t actually get a solution,
another case of a sequence of improving controls that do not converge to anything
```
meaningful (as in Example 7.2).
```
Allowing ğ‘‡ to be free in this example creates a meaningless situation, as is often
the case when ğ‘‡ is free and without any additional cost. If we add a cost for time, we
usually get reasonable solutions, as in the following examples.
Example 8.2: Rocket Sled, Free ğ‘‡
Consider the one-dimensional thrust system with viscous friction
ğ‘¥â€³ = âˆ’ğ‘¥â€² + ğ‘¢
and performance
ğ½ = âˆ«
ğ‘‡
0
1 + ğ‘¢2 ğ‘‘ğ‘¡
```
with ğ‘¥(0) = âˆ’1, ğ‘¥â€²(0) = 0, ğ‘¥(ğ‘‡) = 0, ğ‘¥â€²(ğ‘‡) = 0, and ğ‘‡ free.
```
```
Having the same end conditions ğ‘¥(0) = âˆ’1, ğ‘¦(0) = 0, ğ‘¥(ğ‘‡) = 0, and
```
```
ğ‘¦(ğ‘‡) = 0 doesnâ€™t change any of the initial calculations in the previous example
```
8.2. Thrust Programming and Rocket Sleds 121
```
(âˆ— check this âˆ—), so we still have
```
ğ‘¥ = âˆ’ 12 ğ‘ğ‘¡ âˆ’ 14 ğ‘ğ‘’ğ‘¡ âˆ’ ğ‘ğ‘’âˆ’ğ‘¡ + ğ‘‘,
ğ‘¦ = âˆ’ 12 ğ‘ âˆ’ 14 ğ‘ğ‘’ğ‘¡ + ğ‘ğ‘’âˆ’ğ‘¡,
ğœ†ğ‘¥ = ğ‘,
ğœ†ğ‘¦ = ğ‘ + ğ‘ğ‘’ğ‘¡
with four integration constants ğ‘, ğ‘, ğ‘, ğ‘‘.
Final time ğ‘‡ is free, and so we have five free parameters ğ‘, ğ‘, ğ‘, ğ‘‘, ğ‘‡. We have
```
the four constraints ğ‘¥(0) = âˆ’1, ğ‘¦(0) = 0, ğ‘¥(ğ‘‡) = 0, and ğ‘¦(ğ‘‡) = 0. Principle
```
```
IV supplies ğ»(ğ‘‡) = 0 as the fifth constraint, allowing us to solve for the five
```
unknowns.
There are several approaches to implementing this fifth constraint. I think
the following is about as clean as it gets. We know that ğ» is constant on optimal
trajectories, and substituting control and costate into the Hamiltonian produces
```
(âˆ— verify âˆ—)
```
ğ» = 1 âˆ’ 14 ğ‘2 âˆ’ ğ‘ğ‘ = 0.
Our five constraints are then
```
ğ‘¥(0) = âˆ’1 = âˆ’ 14 ğ‘ âˆ’ ğ‘ + ğ‘‘,
```
```
ğ‘¦(0) = 0 = âˆ’ 12 ğ‘ âˆ’ 14 ğ‘ + ğ‘,
```
```
ğ‘¥(ğ‘‡) = 0 = âˆ’ 12 ğ‘ğ‘‡ âˆ’ 14 ğ‘ğ‘’ğ‘‡ âˆ’ ğ‘ğ‘’âˆ’ğ‘‡ + ğ‘‘,
```
```
ğ‘¦(ğ‘‡) = 0 = âˆ’ 12 ğ‘ âˆ’ 14 ğ‘ğ‘’ğ‘‡ + ğ‘ğ‘’âˆ’ğ‘‡ ,
```
ğ» = 0 = 1 âˆ’ 14 ğ‘2 âˆ’ ğ‘ğ‘.
In the previous example, the solution involved solving four equations for four
unknowns ğ‘, ğ‘, ğ‘, ğ‘‘ which, for any given ğ‘‡, were linear in the unknowns and
could be solved directly. In this example ğ‘‡ is free and solving for the optimal ğ‘‡ is
part of the problem. This leads to the above five equations with five unknowns
ğ‘, ğ‘, ğ‘, ğ‘‘, ğ‘‡, which are not linear: we have terms ğ‘2, ğ‘’ğ‘‡ , and ğ‘’âˆ’ğ‘‡ . This significantly
complicates the solution.
To tackle this problem, we recommend using a computer algebra system and
the following steps. Note that the first four equations are linear in ğ‘, ğ‘, ğ‘, ğ‘‘, and
so we solve these four equations for any given ğ‘‡ to get
```
ğ‘ = âˆ’2(ğ‘’
```
```
ğ‘‡ + 1)
```
```
ğ‘‡(ğ‘’ğ‘‡ + 1) âˆ’ 2(ğ‘’ğ‘‡ âˆ’ 1) ,
```
```
ğ‘ = 4ğ‘‡(ğ‘’ğ‘‡ + 1) âˆ’ 2(ğ‘’ğ‘‡ âˆ’ 1) ,
```
ğ‘ = âˆ’ğ‘’
ğ‘‡
```
ğ‘‡(ğ‘’ğ‘‡ + 1) âˆ’ 2(ğ‘’ğ‘‡ âˆ’ 1) ,
```
```
ğ‘‘ = âˆ’ğ‘‡(ğ‘’
```
```
ğ‘‡ + 1) + ğ‘’ğ‘‡ âˆ’ 1
```
```
ğ‘‡(ğ‘’ğ‘‡ + 1) âˆ’ 2(ğ‘’ğ‘‡ âˆ’ 1) .
```
122 Chapter 8. Two Dimensions
```
From these we can derive (âˆ— check these steps âˆ—)
```
ğ‘¢ = âˆ’ 12 ğœ†ğ‘¦ = 1 âˆ’ 2ğ‘’
ğ‘¡ + ğ‘’ğ‘‡
```
ğ‘‡(ğ‘’ğ‘‡ + 1) âˆ’ 2(ğ‘’ğ‘‡ âˆ’ 1)
```
and compute
ğ½ = âˆ«
ğ‘‡
0
1 + ğ‘¢2 ğ‘‘ğ‘¡ = ğ‘’
```
ğ‘‡ (ğ‘‡ âˆ’ 1)2 + (ğ‘‡ + 1)2
```
```
ğ‘’ğ‘‡ (ğ‘‡ âˆ’ 2) + ğ‘‡ + 2 = ğ‘‡ + (ğ‘‡ âˆ’ 2 tanh(
```
ğ‘‡
```
2 ))
```
âˆ’1
,
as plotted in Figure 8.3. This is the cost for any specified end time ğ‘‡.
Figure 8.3. Performance as a function of allowed time ğ‘‡.
We locate the minimum of this function by numerically approximating ğ‘‡ =
2.576 . . . as a solution to ğœ•ğ½ğœ•ğ‘‡ = 0. The minimizing solution is plotted in the posi-
tion/velocity plane in Figure 8.4.
Figure 8.4. Optimal solution for free time ğ‘‡ with associated
cost occurs at ğ‘‡ = 2.576 . . . .
8.2. Thrust Programming and Rocket Sleds 123
It is of interest to note that the condition ğœ•ğ½ğœ•ğ‘‡ = 0 is algebraically equivalent
```
to the fifth condition above, 0 = 1 âˆ’ ğ‘2/4 âˆ’ ğ‘ğ‘ (this takes some work to show).
```
Recall that Pontryaginâ€™s principles establish necessary, not sufficient, conditions
for optimality. In some cases, applying the principle produces several candidate so-
lutions, not all of which may be locally optimal, or even reasonable solutions to the
original problem.
Example 8.3: Rocket Sled, Free ğ‘‡, Multiple Solutions
Continuing with the Rocket Sled ğ‘¥â€³ = âˆ’ğ‘¥â€² + ğ‘¢ and ğ½ = âˆ«ğ‘‡0 1 + ğ‘¢2 ğ‘‘ğ‘¡ with free
```
endtime ğ‘‡ and soft-landing criteria ğ‘¥(0) = ğ‘¦(0) = 0, we consider a more general
```
```
starting point (ğ‘¥0, ğ‘¦0).
```
```
If we take (ğ‘¥0, ğ‘¦0) = (âˆ’2, 3) and use the methods of the previous example to
```
search for solutions that satisfy Principle IV, we find two, as plotted in Figure 8.5.
Figure 8.5. Two locally optimal trajectories in position-
velocity space, one in reverse time.
```
One of the solutions is in backward time (after all, we never required ğ‘‡ > 0).
```
This is the solution on the left in the graph and is actually the solution to traveling
```
from ğ‘¥(0) = 0, ğ‘¦(0) = 0, to ğ‘¥(ğ‘‡) = âˆ’2, ğ‘¦(ğ‘‡) = 3. So the whole loop is the solution
```
```
from the starting point, stopping at (0, 0) and then returning to the starting point.
```
```
If we take (ğ‘¥0, ğ‘¦0) = (âˆ’1, 3) and search for solutions, we find four, plotted in
```
Figure 8.6, one in negative time and three in positive time.
124 Chapter 8. Two Dimensions
Figure 8.6. Four locally optimal trajectories in position-
velocity space, one in reverse time.
We know the optimal solution must satisfy the conditions that generated
these solutions, we know these are all the possible solutions that arise from those
conditions, and so the optimal solution must be one of these. We discard the neg-
ative time solution and then calculate performance for the remaining three solu-
```
tions (see Figure 8.7) and find that the optimal is the third (rightmost) solution.
```
Figure 8.7. Three locally optimal forward time trajectories in
```
position-velocity space; one is globally optimal.
```
How did these multiple solutions arise as we moved ğ‘¥0 from âˆ’2 to âˆ’1? Keep-
ing initial velocity at ğ‘¦0 = 3 and taking ğ‘¥0 closer to 0, it takes ever increasing
```
amounts of fuel to rocket the sled to the origin (0, 0) without changing directions.
```
At about ğ‘¥0 = âˆ’1.150 . . . a more efficient solution emerges that overshoots the
target and then comes back. This solution uses less fuel but takes a bit more time,
for a net savings in performance.
8.3. Zermelo onna Boat 125
The intermediary solution with ğ½ = 6.023 . . . is not a local minimum. This
is significant: the solution satisfies all properties of Principle IV, including the
minimization condition ğœ•2ğ»ğœ•2áµ† > 0, and yet it is not a local minimum for the ğ‘‡ free
case. The Pontryagin conditions are necessary, not sufficient: they must be true
at a local optimum, but do not guarantee a local optimum.
Solving for the optimal solution for a range of fixed end time values ğ‘‡, we
plot the optimal performance ğ½ as a function of ğ‘‡ in Figure 8.8.
Figure 8.8. Performance as a function of allowed time ğ‘‡.
This shows the two local minimum solutions. One is a fast solution that
conserves time and uses a lot of thrust to proceed monotonically to zero with
ğ½ = 5.720 . . . at ğ‘‡ = 0.892 . . . , and one is a slower solution that conserves thrust
but takes more time by overshooting and then returning to zero producing ğ½ =
5.518 . . . at ğ‘‡ = 3.022 . . . .
If ğ½ is a continuous function of ğ‘‡ and has minima at ğ‘‡ = 0.892 . . . and ğ‘‡ =
3.022 . . . , then there must be a local maximum between these times. The local
max in this plot appears at ğ‘‡ = 1.405 . . . with ğ½ = 6.022 . . . and is at a cusp
between prioritizing thrust or prioritizing time. To be clear: if we were to specify
ğ‘‡ = 1.405 . . . , then this is the optimal solution for that fixed value of ğ‘‡. The key is
that we could do better if we were given either a bit more or a bit less time. This is
similar to the multiple solutions we found in the soap film catenoid, Example 5.7,
where we had two clear minimizers with one unstable solution in the middle.
These examples indicate a rich structure of the solution space for Rocket Sled prob-
lems. Exercise 8.1 is formulated to explore this geometry in the easier no-friction case.
8.3 Zermelo onna Boat
Zermelo problems are classic navigational problems formulated as steering a boat un-
der conditions of variable wind and water current. Working with this formulation pro-
vides excellent insight into optimal control that extends well beyond navigational mod-
els. The following is a basic Zermelo navigation problem that leads to the linear tangent
law.
126 Chapter 8. Two Dimensions
Example 8.4: Zermelo
In the gathering dusk, the good Captain Zermelo is piloting a unit-speed boat
with no inertia on a sea with no name. Captain Zermeloâ€™s sagely hand lay upon
```
the tiller, and in his keen eye glints the North Star. (His other eye donâ€™t glint too
```
```
good.)
```
```
The water itself heaves at a shear. At location (ğ‘¥, ğ‘¦) on the map, the water is
```
moving east at speed ğœ…ğ‘¦, shown in Figure 8.9.
Figure 8.9. Zermelo navigation.
Captain Zermelo must navigate this vector field of moving water in a boat
which has constant unit speed relative to the water and can be pointed in any
```
direction he wishes. If Zermelo is at location (ğ‘¥, ğ‘¦) and has his boat turned to an
```
angle ğœƒ from due east, his resulting velocity is then
ğ‘¥â€² = ğœ…ğ‘¦ + cos ğœƒ,
ğ‘¦â€² = sin ğœƒ.
```
The good Captain Zermelo is charged to pilot his craft from location (ğ‘, ğ‘) to
```
```
location (ğ‘, ğ‘‘) in the very least amount of time. Pray tell: how does he do it?
```
```
Note that there is no acceleration in this model; it is a two-dimensional prob-
```
lem involving direction and speed.
```
The angle ğœƒ is our control variable. To minimize time we take ğ‘”(ğ‘¥, ğ‘¦, ğœƒ, ğ‘¡) = 0
```
```
and ğº(ğ‘‡) = ğ‘‡. Our Hamiltonian is then
```
```
ğ» = ğœ†ğ‘¥(ğœ…ğ‘¦ + cos ğœƒ) + ğœ†ğ‘¦ sin ğœƒ.
```
8.3. Zermelo onna Boat 127
Our costate equations are
ğœ†â€²ğ‘¥ = 0,
ğœ†â€²ğ‘¦ = âˆ’ğœ…ğœ†ğ‘¥.
```
Hence ğœ†ğ‘¥ is constant and ğœ†ğ‘¦ = ğœ†ğ‘¥(ğ¶ âˆ’ ğœ…ğ‘¡) for some constant ğ¶ (we chose this
```
```
form to make things come out cleaner later on).
```
The condition
0 = ğœ•ğ»ğœ•ğœƒ = âˆ’ğœ†ğ‘¥ sin ğœƒ + ğœ†ğ‘¦ cos ğœƒ
determines the control variable in terms of the costate variables
tan ğœƒ =
ğœ†ğ‘¦
ğœ†ğ‘¥= ğ¶ âˆ’ ğœ…ğ‘¡.
This is called the linear tangent law, and it shows up frequently in these types of
problems. Using some basic right-angle trigonometry, we get
```
sin(ğœƒ) = ğ¶ âˆ’ ğœ…ğ‘¡
```
```
Â±âˆš1 + (ğ¶ âˆ’ ğœ…ğ‘¡)2
```
```
, cos(ğœƒ) = 1
```
```
Â±âˆš1 + (ğ¶ âˆ’ ğœ…ğ‘¡)2
```
where the sign on the square root is either positive for both equations or negative
for both. We see that for optimal control, the horizontal component of steering,
```
cos(ğœƒ), never changes sign, while the vertical component, sin(ğœƒ), might.
```
The resulting differential equations
```
ğ‘¥â€² = ğœ…ğ‘¦ Â± (1 + (ğ¶ âˆ’ ğœ…ğ‘¡)2)âˆ’1/2,
```
```
ğ‘¦â€² = Â±(ğ¶ âˆ’ ğœ…ğ‘¡)(1 + (ğ¶ âˆ’ ğœ…ğ‘¡)2)âˆ’1/2
```
```
can be solved in closed form (a computer algebra system is recommended!). The
```
solution for positive square roots is
```
ğ‘¥ = ğ´ + ğµğœ…ğ‘¡ + ğœ…2 (ğ¶ âˆ’ ğœ…ğ‘¡)âˆš1 + (ğ¶ âˆ’ ğœ…ğ‘¡)2 + ğœ…âˆ’22 arcsinh(ğ¶ âˆ’ ğœ…ğ‘¡),
```
```
ğ‘¦ = ğµ âˆ’ âˆš1 + (ğ¶ âˆ’ ğœ…ğ‘¡)2
```
and for negative square roots it is
```
ğ‘¥ = ğ´ + ğµğœ…ğ‘¡ âˆ’ ğœ…2 (ğ¶ âˆ’ ğœ…ğ‘¡)âˆš1 + (ğ¶ âˆ’ ğœ…ğ‘¡)2 âˆ’ ğœ…âˆ’22 arcsinh(ğ¶ âˆ’ ğœ…ğ‘¡),
```
```
ğ‘¦ = ğµ + âˆš1 + (ğ¶ âˆ’ ğœ…ğ‘¡)2.
```
The two constants of integration, ğ´, ğµ, together with the unknowns ğ¶ and ğ‘‡
```
can be used to match the endpoint conditions (ğ‘, ğ‘) and (ğ‘, ğ‘‘).
```
```
For example, with ğœ… = 1 the optimal path from (âˆ’1, âˆ’1) to (2, 1) can be solved
```
using the positive square roots. The solution takes ğ‘‡ = 3.178 . . . time units and
is shown as in Figure 8.10.
128 Chapter 8. Two Dimensions
Figure 8.10. Optimal Zermelo trajectory in state space.
```
Another example with ğœ… = 1 is an optimal path from (âˆ’5, 0) to (0, 0) taking
```
ğ‘‡ = 3.572 . . . time units, shown in Figure 8.11.
```
Figure 8.11. Optimal trajectory from (âˆ’5, 0) to (0, 0).
```
The differential equations in the Zermelo example can be quite challenging
to solve. A simplifying change of variables and a more tractable version of the
problem are explored in the Exercises 8.5 and 8.6.
Note the similarity and differences between the Zermelo system and the Rocket
Sled system from Section 8.2.2 with zero friction. Both have the same underlying sys-
tem ğ‘¥â€² = ğ‘¦, ğ‘¦â€² = 0. The Rocket Sled system has a control ğ‘¢ that can be of any magnitude
but only operates in the ğ‘¦-direction, and the goal is to minimize fuel ğ½ = âˆ« ğ‘¢2 ğ‘‘ğ‘¡. The
Zermelo system has a control of fixed magnitude but can be pointed in any direction
ğœƒ, and the goal is to minimize time.
In Section 5.2 we explored how the Principle of Optimality applies in state-costate
space. For time minimizing problems, where our performance function was simply
ğ½ = ğ‘‡, the principle applies directly to state space: if a minimal time path from point ğ´
8.3. Zermelo onna Boat 129
to point ğ¶ passes through point ğµ, then it must be minimal time from point ğ´ to point
ğµ and minimal time from point ğµ to point ğ¶. The following example shows how we
can use this to interpret our state space trajectories.
Example 8.5
In the previous example, we constructed a time optimal trajectory from ğ´ =
```
(âˆ’1, 1) to ğ¶ = (2, 1) which required ğ‘‡ = 3.178 . . . time units. If we take ğµ to
```
```
be any point on this path, say, ğµ = (ğ‘¥(2), ğ‘¦(2)), then the same path is optimal
```
from ğ´ to ğµ and from ğµ to ğ¶.
```
To be clear, computing ğ‘¥(ğ‘¡), ğ‘¦(ğ‘¡) as the optimal trajectory from (âˆ’1, 1) to
```
```
(2, 1), we find ğ‘¥(2) = âˆ’0.2126 . . . and ğ‘¦(2) = 0.7154 . . . , as shown in Figure
```
```
8.12. This (ğ‘¥(ğ‘¡), ğ‘¦(ğ‘¡)) curve is also the optimal path from (âˆ’1, 1) to (âˆ’0.2126 . . . ,
```
```
0.7154 . . . ), which would require 2 time units, and the optimal path from
```
```
(âˆ’0.2126 . . . , 0.7154 . . . ) to (2, 1), which would require ğ‘‡ = 1.178 . . . time units.
```
Figure 8.12. Optimal trajectory from ğ´ to ğ¶ passes through ğµ.
We can also extrapolate this idea backwards in time and create a rather fas-
cinating structure for cases where the terminal location is fixed. For example, fix
```
the terminal location to be the origin. A minimal time trajectory (ğ‘¥(ğ‘¡), ğ‘¦(ğ‘¡)) start-
```
```
ing at (1, 1) will reach (0, 0) in ğ‘‡ = 1.779 . . . time units and is shown in Figure
```
8.13. We can extend this trajectory backwards in time by plugging in negative
values of ğ‘¡, say, ğ‘¡ = âˆ’1. For values ğ‘¡ = âˆ’1, 0, 1.779 . . . we get
```
(ğ‘¥(âˆ’1), ğ‘¦(âˆ’1)) = (âˆ’0.0448 . . . , 1.913 . . . ),
```
```
(ğ‘¥(0), ğ‘¦(0)) = (1, 1),
```
```
(ğ‘¥(1.779 . . . ), ğ‘¦(1.779 . . . )) = (0, 0).
```
130 Chapter 8. Two Dimensions
```
Figure 8.13. Trajectory from (1, 1) to (0, 0) extended back-
```
wards in time.
```
If we were to compute the minimum time trajectory from (âˆ’0.0448 . . . ,
```
```
1.913 . . . ) to (0, 0), we would find that it takes ğ‘‡ = 2.779 . . . time units to reach
```
```
the origin, and this trajectory passes through (1, 1) at time ğ‘¡ = 1. Continuing to
```
extend this trajectory backwards in time creates an infinite curve that contains
the forward minimal time trajectory to the origin for any starting point on the
curve.
```
Fixing the terminal location at the origin, each starting location (ğ‘¥0, ğ‘¦0) gen-
```
```
erates a unique optimal trajectory terminating at (0, 0). The set of all minimum
```
time trajectories for all starting locations generates a set of curves that fills up the
plane and only intersect at the origin, as depicted in Figure 8.14.
```
Figure 8.14. A collection of minimal time trajectories to (0, 0).
```
This is a map Captain Zermelo could use to find the fastest route home from
any point on the plane.
8.4. The Brachistochrone Problem 131
8.4 The Brachistochrone Problem
```
The brachistochrone (in Greek brachistos = shortest, chronos = time) problem has a
```
```
rich mathematical history (check out the Wikipedia entry). In a vertical plane, what is
```
```
the shape of the curve down which a frictionless bead will slide from point (ğ‘¥ğ‘, ğ‘¦ğ‘) to
```
```
point (ğ‘¥ğ‘, ğ‘¦ğ‘) in minimal time?
```
There are several historical approaches to the solution. The earliest mention of
this problem seems to have been by Galileo in 1638, where he incorrectly stated that
it would be the arc of a circle. In June 1696, Johann Bernoulli posed the problem in
Acta Eruditorum. Several mathematicians responded with a solution, including New-
ton and Leibniz who then engaged in an extended quibble over who solved it first.
The brachistochrone concept is so iconic that it even earned a mention in Herman
Melvilleâ€™s Moby Dick as the author described the motion of a piece of soapstone sliding
in a cauldron.
The surprise is that the solution curves are segments of cycloids: paths traced by a
point on a rolling circle. The following is a standard solution using the techniques we
have developed.
Example 8.6: Brachistochrone
```
Consider two given points (0, 0) and (ğ‘¥ğ‘, ğ‘¦ğ‘), ğ‘¥ğ‘ > 0, ğ‘¦ğ‘ â‰¤ 0, on a vertical plane
```
```
and a path connecting them. We allow a bead to start at rest at (0, 0) and slide
```
down the path driven only by gravity and without friction. For what shape of the
```
path will the bead reach (ğ‘¥ğ‘, ğ‘¦ğ‘) in minimal time (if such a minimal path exists)?
```
```
The optimal path from (0, 0) to (5, âˆ’1.5) is shown in Figure 8.15.
```
Figure 8.15. Brachistochrone trajectory.
We solve this problem using optimal control principles. Motivated by Zer-
```
meloâ€™s boat, we take the angle of the curve ğœƒ = arctan(ğ‘¦â€²/ğ‘¥â€²) as our control vari-
```
able, and we assume âˆ’ ğœ‹2 â‰¤ ğœƒ â‰¤ ğœ‹2 with ğœƒ = 0 being horizontal.
The solution begins by using conservation of energy 12 ğ‘šğ‘ 2 +ğ‘šğ‘”ğ‘¦ â‰¡ 0 to solve
```
for speed ğ‘  as only depending on the (negative) ğ‘¦-coordinate:
```
ğ‘  = âˆšâˆ’2ğ‘”ğ‘¦.
132 Chapter 8. Two Dimensions
The equations of motion are then
ğ‘¥â€² = âˆšâˆ’2ğ‘”ğ‘¦ cos ğœƒ,
ğ‘¦â€² = âˆšâˆ’2ğ‘”ğ‘¦ sin ğœƒ.
```
With ğœƒ as our control we want to minimize ğ‘‡ subject to the restrictions ğ‘¥(ğ‘‡) = ğ‘¥ğ‘,
```
```
ğ‘¦(ğ‘‡) = ğ‘¦ğ‘. Applying Principle IV with ğ‘”(ğ‘¥, ğ‘¦, ğœƒ, ğ‘¡) = 0 and ğº(ğ´, ğµ, ğ‘‡) = ğ‘‡, our
```
Hamiltonian is
ğ» = ğœ†ğ‘¥âˆšâˆ’2ğ‘”ğ‘¦ cos ğœƒ + ğœ†ğ‘¦âˆšâˆ’2ğ‘”ğ‘¦ sin ğœƒ
```
= âˆšâˆ’2ğ‘”ğ‘¦ (ğœ†ğ‘¥ cos ğœƒ + ğœ†ğ‘¦ sin ğœƒ) .
```
```
(8.1)
```
Our costate equations are
âˆ’ğœ†â€²ğ‘¥ = 0,
```
âˆ’ğœ†â€²ğ‘¦ = âˆšâˆ’ ğ‘”2ğ‘¦ (ğœ†ğ‘¥ cos ğœƒ + ğœ†ğ‘¦ sin ğœƒ)
```
making ğœ†ğ‘¥ constant.
The stationarity condition
```
0 = ğœ•ğ»ğœ•ğœƒ = âˆšâˆ’2ğ‘”ğ‘¦ (âˆ’ğœ†ğ‘¥ sin ğœƒ + ğœ†ğ‘¦ cos ğœƒ)
```
determines the control variable ğœƒ in terms of the costate variables:
tan ğœƒ =
ğœ†ğ‘¦
```
ğœ†ğ‘¥. (8.2)
```
We could now try to solve our system of four differential equations with boundary
```
conditions ğ‘¥(0) = 0, ğ‘¦(0) = 0, ğ‘¥(ğ‘‡) = ğ‘¥ğ‘, and ğ‘¦(ğ‘‡) = ğ‘¦ğ‘. This gets really messy.
```
The following clever tricks make for a better approach. Keep in mind this is
a famous problem, worked over by mathematicians for many years. The solution
presented here may feel like it comes out of nowhere, but it is a method that has
evolved over time as a good approach through the complicated details.
Since ğ‘‡ is free, we have by Principle IV that 0 = ğœ•ğºğœ•ğ‘‡ + ğ». With ğœ•ğºğœ•ğ‘‡ = 1 and
ğ» constant on optimal trajectories, we get ğ» â‰¡ âˆ’1 on optimal trajectories. Then
```
equation (8.1) yields
```
```
âˆ’1 = âˆšâˆ’2ğ‘”ğ‘¦ (ğœ†ğ‘¥ cos ğœƒ + ğœ†ğ‘¦ sin ğœƒ) . (8.3)
```
```
Clever Trick #1: From equation (8.2) we have ğœ†ğ‘¦ = ğœ†ğ‘¥ sin ğœƒ/ cos ğœƒ. Substi-
```
```
tute this into equation (8.3) to conclude (âˆ— check this âˆ—)
```
```
âˆ’ cos(ğœƒ) = âˆšâˆ’2ğ‘”ğ‘¦ ğœ†ğ‘¥. (8.4)
```
```
This holds for all ğ‘¡. Taking ğ‘¡ = 0, ğ‘¦(0) = 0, we get ğœƒ(0) = âˆ’ ğœ‹2 , so we al-
```
ways start off sliding straight down. This makes sense: we want to build up some
velocity as quickly as possible.
8.4. The Brachistochrone Problem 133
```
Clever Trick #2: Differentiate equation (8.4) with respect to time
```
```
sin(ğœƒ) ğ‘‘ğœƒğ‘‘ğ‘¡ = âˆšâˆ’ ğ‘”2ğ‘¦ğ‘‘ğ‘¦ğ‘‘ğ‘¡
```
```
and substitute the equation of motion ğ‘¦â€² = âˆšâˆ’2ğ‘”ğ‘¦ sin ğœƒ to conclude (âˆ— check
```
```
this âˆ—)
```
ğ‘‘ğœƒ
ğ‘‘ğ‘¡ = âˆ’ğ‘”ğœ†ğ‘¥.
From this and the fact that ğœ†â€²ğ‘¥ = 0, we have that ğœƒ is linear in time. Rather
interesting. I donâ€™t know why that should be the case, other than the fact that it
is.
```
This pretty much reveals our control function: ğœƒ(ğ‘¡) = âˆ’ğ‘¡ğ‘”ğœ†ğ‘¥ âˆ’ ğœ‹/2. Now itâ€™s
```
```
just a matter of finding the right value for the constant ğœ†ğ‘¥ to connect start (0, 0)
```
```
to end (ğ‘¥ğ‘, ğ‘¦ğ‘).
```
```
Taking ğ‘¡ = ğ‘‡ and writing ğœƒ(ğ‘‡) = ğœƒğ‘‡ in equation (8.4) we can solve for the
```
constant
ğœ†ğ‘¥ = âˆ’ cos ğœƒğ‘‡
âˆšâˆ’2ğ‘”ğ‘¦ğ‘
```
(8.5)
```
making
ğ‘‘ğœƒ
ğ‘‘ğ‘¡ = âˆš
ğ‘”
```
âˆ’2ğ‘¦ğ‘cos ğœƒğ‘‡ . (8.6)
```
```
Now substitute ğœ†ğ‘¥ from equation (8.5) back into equation (8.4) and solve for
```
```
ğ‘¦, yielding the succinct expression (âˆ— verify âˆ—)
```
ğ‘¦ = ğ‘¦ğ‘cos
```
2(ğœƒ)
```
```
cos2(ğœƒğ‘‡ ) .
```
This expresses ğ‘¦ as a function of ğœƒ, suggesting that we try to parameterize
the curve with respect to ğœƒ rather than ğ‘¡. We just need to get ğ‘¥ as a function of ğœƒ,
which we can do by using. . .
Clever Trick #3: Express ğ‘‘ğ‘¥/ğ‘‘ğœƒ as a function of ğœƒ using the equation of
```
motion ğ‘¥â€² = âˆšâˆ’2ğ‘”ğ‘¦ cos ğœƒ and equation (8.6) to get
```
ğ‘‘ğ‘¥
ğ‘‘ğœƒ =
ğ‘‘ğ‘¥
ğ‘‘ğ‘¡
ğ‘‘ğœƒ
ğ‘‘ğ‘¡
= âˆšâˆ’2ğ‘”ğ‘¦ cos ğœƒ
âˆš
ğ‘”
âˆ’2ğ‘¦ğ‘cos ğœƒğ‘‡
= âˆ’2ğ‘¦ğ‘ cos
2 ğœƒ
cos2 ğœƒğ‘‡.
Now integrate this with respect to ğœƒ using the initial value of ğ‘¥ = 0 when ğœƒ =
```
âˆ’ğœ‹/2 to get (âˆ— verify âˆ—)
```
```
ğ‘¥ = âˆ’ğ‘¦ğ‘ğœ‹+2ğœƒ+sin(2ğœƒ)2 cos2 ğœƒğ‘‡,
```
```
ğ‘¦ = ğ‘¦ğ‘cos2(ğœƒ)cos2(ğœƒğ‘‡ ) .
```
```
(8.7)
```
These expressions for ğ‘¥ and ğ‘¦ yield a cycloidâ€”the curve traced by a point on
```
the perimeter of a rolling wheel (see Exercise 8.9).
```
Knowing that the optimal path is a cycloid provides a geometric interpreta-
```
tion of the solution: from the set of cycloids that start at (0, 0), choose the one
```
```
that passes through (ğ‘¥ğ‘, ğ‘¦ğ‘) (see Figure 8.16.
```
134 Chapter 8. Two Dimensions
```
Figure 8.16. Cycloids originating from (0, 0) are brachis-
```
tochrone trajectories.
Key Points
In this chapter we vectorized our methods allowing for a multidimensional state space.
Our main focus was two-dimensional state spaces but the methods stated in Principle
IV generalize to any finite number of dimensions. Principle IV allows for a free end-
time. Free endpoint conditions will be explored in the next chapter.
Two dimensions allow for modeling forces and accelerations, and examples demon-
strate how the theory applies to thrust in physical systems. We examined cases of mul-
tiple local optimums.
We studied Zermelo navigation that involved steering a unit speed vehicle and saw
the linear tangent law, a form that often arises in such systems.
We took a careful look at the brachistochrone problem, a classic mathematical
construct with fascinating properties that has intrigued scholars for well over 300 years.
Exercises
```
Exercise 8.1(hs). Rocket Sled on Ice, Fixed T. Consider Example 8.1 in the case of no
```
```
friction: for ğ‘¥â€³ = ğ‘¢ and soft-landing endpoint conditions
```
```
(ğ‘¥(0), ğ‘¦(0)) = (ğ‘¥0, ğ‘¦0), (ğ‘¥(ğ‘‡), ğ‘¦(ğ‘‡)) = (0, 0).
```
Find the control that minimizes
ğ½ = âˆ«
ğ‘‡
0
ğ‘¢2 ğ‘‘ğ‘¡
for a given fixed ğ‘‡.
```
(a) Plot some representative trajectories in the (ğ‘¥, ğ‘¦)-plane. Perhaps choose a fixed
```
```
starting point (ğ‘¥0, ğ‘¦0) and plot the trajectory for a few different values of ğ‘‡. Or fix ğ‘‡
```
and plot a few different starting points.
```
(b) Starting at ğ‘¥0 = âˆ’1 and ğ‘¦0 = 0, compute performance ğ½(ğ‘‡) as a function of
```
stopping time ğ‘‡. Does ğ½ attain a local minimum value? What happens as ğ‘‡ â†’ âˆ?
Exercises 135
```
Exercise 8.2(hs). Rocket Sled on Ice, Free T. Consider Example 8.2 in the case of no
```
```
friction: minimize
```
ğ½ = âˆ«
ğ‘‡
0
1 + ğ‘¢2 ğ‘‘ğ‘¡ = ğ‘‡ + âˆ«
ğ‘‡
0
ğ‘¢2 ğ‘‘ğ‘¡
with ğ‘¥â€³ = ğ‘¢ and specific endpoint conditions
```
(ğ‘¥(0), ğ‘¦(0)) = (âˆ’1, 0), (ğ‘¥(ğ‘‡), ğ‘¦(ğ‘‡)) = (0, 0).
```
```
(a) Compute performance ğ½(ğ‘‡) as a function of stopping time ğ‘‡. How does this
```
compare to the performance in Exercise 8.1? Does ğ½ attain a local minimum value?
```
(b) Solve the problem assuming ğ‘‡ is free, using ğœ•ğºğœ•ğ‘‡ + ğ» = 0. Do you get the same
```
```
answer as in part (a)?
```
```
Exercise 8.3(hs). Rocket Sled on Ice, Free T, multiple solutions. Consider Example 8.3
```
in the case of no friction and a general starting point: minimize
ğ½ = âˆ«
ğ‘‡
0
1 + ğ‘¢2 ğ‘‘ğ‘¡ = ğ‘‡ + âˆ«
ğ‘‡
0
ğ‘¢2 ğ‘‘ğ‘¡
with ğ‘¥â€³ = ğ‘¢ and specific endpoint conditions
```
(ğ‘¥(0), ğ‘¦(0)) = (ğ‘¥0, ğ‘¦0), (ğ‘¥(ğ‘‡), ğ‘¦(ğ‘‡)) = (0, 0)
```
where ğ‘‡ is free.
```
(a) Derive performance ğ½ as a function of ğ‘‡, ğ‘¥0, and ğ‘¦0 and compute the partial
```
derivative ğœ•ğ½ğœ•ğ‘‡ .
```
(b) Find conditions on (ğ‘¥0, ğ‘¦0) under which ğœ•ğ½ğœ•ğ‘‡ = 0 will have multiple zeros in ğ‘‡.
```
```
(c) Identify the points in the second quadrant (ğ‘¥0 < 0, ğ‘¦0 > 0) that have multiple
```
```
zeros with ğ‘‡ > 0 from part (b). These are the starting points where multiple positive
```
```
time solutions to Principle IV are possible. Plot the region in the (ğ‘¥0, ğ‘¦0)-plane, plot
```
some representative trajectories, and determine the minimizing solution.
```
Exercise 8.4(s). Rocket Sled on Ice, geometry. Consider the frictionless Rocket Sled
```
system in Exercise 8.1, ğ‘¥â€³ = ğ‘¢, fixed ğ‘‡,
ğ½ = âˆ«
ğ‘‡
0
ğ‘¢2 ğ‘‘ğ‘¡.
```
(a) Solve for ğ‘¥(0) = âˆ’1, ğ‘¥â€²(0) = 1 to ğ‘¥(1) = 0, ğ‘¥â€²(1) = 0.
```
```
(b) Solve for ğ‘¥(0) = 0, ğ‘¥â€²(0) = 0 to ğ‘¥(1) = 1, ğ‘¥â€²(1) = 1. (Use what you have from
```
```
(a).)
```
```
(c) What value of ğµ would have the least cost for ğ‘¥(0) = 0, ğ‘¥â€²(0) = 1 to ğ‘¥(1) = ğµ,
```
```
ğ‘¥â€²(1) = 1? (Think of an easy solution.)
```
```
Exercise 8.5(h). Consider Zermelo onna Boat (Example 8.4), ğ‘¥â€² = ğœ…ğ‘¦ + cos(ğœƒ), ğ‘¦â€² =
```
```
sin(ğœƒ) and with starting point ğ‘¥(0) = ğ‘¥0, ğ‘¦(0) = ğ‘¦0 and ending point ğ‘¥(ğ‘‡) = ğ‘¥1,
```
```
ğ‘¦(ğ‘‡) = ğ‘¦1.
```
136 Chapter 8. Two Dimensions
Show that the change of variables
Ìƒğ‘¥ = ğ‘¥ âˆ’ ğ‘¥0 âˆ’ ğ‘¦0ğœ…ğ‘¡,Ìƒ
ğ‘¦ = ğ‘¦ âˆ’ ğ‘¦0
```
satisfies the same differential equationsÌƒ ğ‘¥â€² = ğœ…Ìƒğ‘¦ + cos(ğœƒ),Ìƒ ğ‘¦â€² = sin(ğœƒ) but with starting
```
```
conditionsÌƒ ğ‘¦(0) = 0,Ìƒ ğ‘¥(0) = 0 and ending conditionsÌƒ ğ‘¥(ğ‘‡) = ğ‘¥1 âˆ’ ğ‘¥0 âˆ’ ğ‘¦0ğœ…ğ‘‡,Ìƒ ğ‘¦(ğ‘‡) =
```
ğ‘¦1 âˆ’ ğ‘¦0.
While still challenging, this change makes the solution more tractable by standard-
izing on trajectories that start at the origin.
```
Exercise 8.6(s). Consider Zermelo onna Boat (Example 8.4), with a circular water flow
```
```
ğ‘¥â€² = âˆ’ğ‘¦ + cos(ğœƒ),
```
```
ğ‘¦â€² = ğ‘¥ + sin(ğœƒ)
```
```
where we want to navigate from (ğ‘¥0, ğ‘¦0) to (ğ‘¥1, ğ‘¦1) in minimum time.
```
```
(a) Show that any optimal solution will have ğœƒ = ğ‘¡ + ğ›¼ for some constant ğ›¼.
```
```
(b) Find the optimal path from (0, 0) to (ğ‘¥1, ğ‘¦1), and plot a few representative tra-
```
jectories.
```
(c) What is the general form of a solution starting at (ğ‘¥0, ğ‘¦0)? Pick a starting point
```
and plot some representative trajectories.
```
(d) Would the optimal path from (1, 0) to (âˆ’1, 0) go outside the unit circle or cut
```
inside the unit circle? Find the optimal path and plot.
```
Exercise 8.7(h). How would you modify Principle IV to allow for two controls? Ana-
```
lyze the Zermelo example with vertical and horizontal controls:
ğ‘¥â€² = ğ‘¦ + ğ‘¢,
ğ‘¦â€² = ğ‘£.
```
(a) Assume ğ‘‡ is fixed and performance is ğ½ = âˆ«ğ‘‡0 ğ‘¢2 + ğ‘£2 ğ‘‘ğ‘¡. Compute and plot
```
```
the optimal trajectory from (âˆ’1, âˆ’1) to (2, 1) for ğ‘‡ = 3.
```
```
(b) Assume ğ‘‡ is free and performance is ğ½ = ğ‘‡ + âˆ«ğ‘‡0 ğ‘¢2 + ğ‘£2 ğ‘‘ğ‘¡. Compute and plot
```
```
the optimal trajectory from (âˆ’1, âˆ’1) to (2, 1). What is the optimal time?
```
```
Exercise 8.8. The brachistochrone solution (Example 8.6) falls apart if ğ‘¦ğ‘ = 0. What
```
```
is the solution in this case? How much time would it take to traverse from (0, 0) to
```
```
(ğ‘¥ğ‘, 0)?
```
Exercise 8.9. A cycloid is the curve traced by a point on the rim of a wheel as it rolls
```
down the ğ‘¥-axis. If the points starts at (0, 0) and the wheel has radius ğ‘…, verify that the
```
```
curve is described by the parametric equations ğ‘¥(ğœ) = ğ‘…(ğœ âˆ’ sin ğœ), ğ‘¦(ğ‘¡) = ğ‘…(1 âˆ’ cos ğœ).
```
```
Use this to conclude that the solution to the brachistochrone problem (Example 8.6) is
```
a cycloid.
Exercises 137
```
Exercise 8.10(h). In solving the brachistochrone problem (Example 8.6), we assumed
```
speed was related to height by ğ‘  = âˆšâˆ’2ğ‘”ğ‘¦. If we instead assume speed is proportional
to height, ğ‘  = âˆ’ğœ…ğ‘¦ for some ğœ… > 0, then the minimum time trajectories are arcs of
circles centered on the line ğ‘¦ = 0. These paths are geodesics in the hyperbolic half-
plane, so this is another deep connection between optimal control and geometry.
```
Demonstrate this result as follows: take ğœ… = 1 and assume (ğ‘¥(ğ‘¡), ğ‘¦(ğ‘¡)) is an optimal
```
```
trajectory with ğ‘¥(0) = 0, ğ‘¦(0) = âˆ’1, and ğ‘¦â€²(0) = 0. Use Principle IV to conclude that
```
```
(ğ‘¥(ğ‘¡), ğ‘¦(ğ‘¡)) lies on the unit circle ğ‘¥2 + ğ‘¦2 = 1.
```
```
Exercise 8.11(hs). Inventory Scheduling. Let ğ¼(ğ‘¡) be inventory level, let ğ‘†(ğ‘¡) be sales
```
```
rate, and we control production rate ğ‘ƒ(ğ‘¡), so our inventory satisfies ğ¼â€² = ğ‘ƒ âˆ’ ğ‘†. We
```
```
assume the system has been operated in a stable configuration with ğ‘ƒ(ğ‘¡) = ğ¼(ğ‘¡) =
```
```
ğ‘†(ğ‘¡) = 1. We also assume that sales will increase slightly if we overproduce and sales
```
```
will decrease slightly if we underproduce. We model this by ğ‘†â€² = ğ›¼(ğ‘ƒ âˆ’ 1). Suppose
```
```
ğ›¼ = 12 and that at time ğ‘¡ = 0 we have ğ‘†(0) = 1 and ğ¼(0)=1. You need to double the
```
```
inventory in four years, ğ¼(4) = 2, while minimizing production costs âˆ«40 ğ‘ƒ2 ğ‘‘ğ‘¡ and
```
```
returning sales to normal, ğ‘†(4) = 1. Find and graph the optimal ğ‘ƒ(ğ‘¡), ğ¼(ğ‘¡), and ğ‘†(ğ‘¡).
```
Comment on your result. Is it what you expected?
9
Targets
Similar to the one-dimensional case, we can extend the two-dimensional principle to
include free endpoints. With higher dimensions we have more options to consider. For
example we could prescribe a terminal value of one of the state coordinates and allow
the other to be free. We can even prescribe the end location to lie on a smooth curve.
This chapter extends the principle to allow for these cases.
9.1 Free Ends
```
Free endpoints in the one-dimensional case covered by Principle III (Section 6.1) allows
```
```
for an indeterminate endpoint ğ‘¥(ğ‘‡), possibly with an associated end payoff ğº(ğ‘¥(ğ‘‡)),
```
and solving for the optimal endpoint becomes part of the optimization problem.
```
For a two-dimensional state (ğ‘¥(ğ‘¡), ğ‘¦(ğ‘¡)) we can allow one or both end values to be
```
```
indeterminate. For example, suppose that in the Rocket Sled example (Example 8.1)
```
```
we have ğ‘‡ time units to bring the rocket sled to rest, ğ‘¥â€²(ğ‘‡) = 0, but we donâ€™t care where,
```
```
i.e., ğ‘¥(ğ‘‡) is free. Or suppose that Captain Zermelo (Example 8.4) is racing to a vertical
```
```
finish line, so he seeks to minimize ğ‘‡ where ğ‘¥(ğ‘‡) is specified but ğ‘¦(ğ‘‡) is free.
```
Principle III readily generalizes to handle such cases, where the optimal endpoint
values are determined by each costate being equal to the corresponding partial deriva-
tive of the endpoint payoff function ğº.
139
140 Chapter 9. Targets
OPTIMAL PRINCIPLE V
Local optimum, free duration, free endpoint, time dependent, two dimensions
Consider the controlled system
```
(
```
ğ‘¥â€²1
ğ‘¥â€²2
```
) = (
```
```
ğ‘“1(ğ‘¥1, ğ‘¥2, ğ‘¢, ğ‘¡)
```
```
ğ‘“2(ğ‘¥1, ğ‘¥2, ğ‘¢, ğ‘¡)
```
```
) , ğ‘¥1, ğ‘¥2, ğ‘¡ âˆˆ â„, ğ‘¢ âˆˆ ğ’°,
```
```
starting at ğ‘¥1(0) = ğ´1, ğ‘¥2(0) = ğ´2, and objective function
```
```
ğ½ = ğº(ğµ1, ğµ2, ğ‘‡) + âˆ«
```
ğ‘‡
0
```
ğ‘”(ğ‘¥1, ğ‘¥2, ğ‘¢, ğ‘¡) ğ‘‘ğ‘¡
```
```
where ğ‘¥1(ğ‘‡) = ğµ1 and ğ‘¥2(ğ‘‡) = ğµ2.
```
Define the Hamiltonian
```
ğ»(ğ‘¥1, ğ‘¥2, ğœ†1, ğœ†2, ğ‘¢, ğ‘¡) = ğ‘” + ğœ†1ğ‘“1 + ğœ†2ğ‘“2
```
and costate equations
ğœ†â€²1 = âˆ’ ğœ•ğ»ğœ•ğ‘¥1,
ğœ†â€²2 = âˆ’ ğœ•ğ»ğœ•ğ‘¥2.
Then a locally optimal control must satisfy
ğœ•ğ»
ğœ•ğ‘¢ = 0
and the control ğ‘¢ that optimizes ğ½ will optimize ğ» at all times.
Either or both ending locations may be prescribed. Otherwise, the optimal
```
end location(s) must satisfy
```
```
ğœ†1(ğ‘‡) = ğœ•ğºğœ•ğµ1(ğµ1, ğµ2, ğ‘‡),
```
```
ğœ†2(ğ‘‡) = ğœ•ğºğœ•ğµ2(ğµ1, ğµ2, ğ‘‡).
```
The end time ğ‘‡ may be prescribed. Otherwise, the optimal end time ğ‘‡ will
satisfy
ğœ•ğº
```
ğœ•ğ‘‡ (ğµ1, ğµ2, ğ‘‡) + ğ»(ğ‘‡) = 0.
```
Furthermore, if ğ» does not explicitly depend on ğ‘¡, it is constant on optimal trajec-
tories.
Note that you can mix and match the free/fixed end locations. For example speci-
```
fying ğ‘¥1(ğ‘‡) and allowing ğ‘¥2(ğ‘‡) to be free makes ğœ†1(ğ‘‡) free and ğœ†2(ğ‘‡) = ğœ•ğºğœ•ğµ2(ğµ1, ğµ2, ğ‘‡).
```
9.1. Free Ends 141
Example 9.1: Rocket Sled, Freeing the Endpoints
```
Returning to Rocket Sled (Example 8.1), we want to minimize the cost of accel-
```
eration ğ½ = âˆ«ğ‘‡0 ğ‘¢2 ğ‘‘ğ‘¡ for ğ‘¥â€³ = âˆ’ğ‘¥â€² + ğ‘¢. In Example 8.1 we derived minimizing
control ğ‘¢ = âˆ’ 12 ğœ†ğ‘¦ and state-costate trajectories
ğ‘¥ = âˆ’ 12 ğ‘ğ‘¡ âˆ’ 14 ğ‘ğ‘’ğ‘¡ âˆ’ ğ‘ğ‘’âˆ’ğ‘¡ + ğ‘‘,
ğ‘¦ = âˆ’ 12 ğ‘ âˆ’ 14 ğ‘ğ‘’ğ‘¡ + ğ‘ğ‘’âˆ’ğ‘¡,
ğœ†ğ‘¥ = ğ‘,
ğœ†ğ‘¦ = ğ‘ + ğ‘ğ‘’ğ‘¡
with four integration constants ğ‘, ğ‘, ğ‘, ğ‘‘.
For endpoint conditions, suppose we just want to bring the sled to a full stop
as efficiently as possible and we donâ€™t care where it stops. That is, we are given
```
a fixed amount of time ğ‘‡, running costs ğ‘” = ğ‘¢2, a starting point (ğ‘¥0, ğ‘¦0), and an
```
```
endpoint condition for velocity ğ‘¦(ğ‘‡) = 0, while position ğ‘¥(ğ‘‡) is allowed to be any
```
value. Just stop the thing at the specified time.
```
With ğ‘¥(ğ‘‡) free and no end payoff ğº = 0, Principle V stipulates that ğœ†ğ‘¥(ğ‘‡) = 0
```
for an optimal solution. This imposes four constraints:
```
ğ‘¥(0) = ğ‘¥0 = âˆ’ 14 ğ‘ âˆ’ ğ‘ + ğ‘‘,
```
```
ğ‘¦(0) = ğ‘¦0 = âˆ’ 12 ğ‘ âˆ’ 14 ğ‘ + ğ‘,
```
```
ğ‘¦(ğ‘‡) = 0 = âˆ’ 12 ğ‘ âˆ’ 14 ğ‘ğ‘’ğ‘‡ + ğ‘ğ‘’âˆ’ğ‘‡ ,
```
```
ğœ†ğ‘¥(ğ‘‡) = 0 = ğ‘
```
which are linear in the four unknowns ğ‘, ğ‘, ğ‘, ğ‘‘.
Setting ğ‘‡ = 1, ğ‘¥0 = 0, and solving for a given ğ‘¦0 yields
```
ğ‘¥(ğ‘¡) = ğ‘¦0ğ‘’
```
```
ğ‘¡ âˆ’ 1 + ğ‘’2(ğ‘’âˆ’ğ‘¡ âˆ’ 1)
```
1 âˆ’ ğ‘’2 ,
```
ğ‘¦(ğ‘¡) = ğ‘¦0ğ‘’
```
ğ‘¡ âˆ’ ğ‘’2âˆ’ğ‘¡
1 âˆ’ ğ‘’2 ,
with some representative trajectories plotted in Figure 9.1.
142 Chapter 9. Targets
Figure 9.1. Optimal trajectories to zero velocity for ğ‘‡ = 1 and
various stating velocities.
Figure 9.2. Optimal trajectories to zero position for ğ‘‡ = 1 and
various stating velocities.
Switching things up, suppose that we want to get the rocket sled to a specific
position at a specific time but we donâ€™t care about velocity. That is, we have fixed
```
end time ğ‘‡, a given starting point (ğ‘¥0, ğ‘¦0), and our end condition is that ğ‘¥(ğ‘‡) = 0,
```
```
but velocity ğ‘¦(ğ‘‡) is allowed to be any value. Just hit the target at the specified
```
```
time; we donâ€™t care how fast we are going.
```
9.1. Free Ends 143
```
With ğ‘¦(ğ‘‡) free and no end payoff ğº = 0, Principle V stipulates that ğœ†ğ‘¦(ğ‘‡) = 0
```
for an optimal solution. This imposes four constraints:
```
ğ‘¥(0) = ğ‘¥0 = âˆ’ 14 ğ‘ âˆ’ ğ‘ + ğ‘‘,
```
```
ğ‘¦(0) = ğ‘¦0 = âˆ’ 12 ğ‘ âˆ’ 14 ğ‘ + ğ‘,
```
```
ğ‘¥(ğ‘‡) = 0 = âˆ’ 12 ğ‘ğ‘‡ âˆ’ 14 ğ‘ğ‘’ğ‘‡ âˆ’ ğ‘ğ‘’âˆ’ğ‘‡ + ğ‘‘,
```
```
ğœ†ğ‘¦(ğ‘‡) = 0 = ğ‘ + ğ‘ğ‘’ğ‘‡
```
which are linear in the four unknowns ğ‘, ğ‘, ğ‘, ğ‘‘.
For example, setting ğ‘‡ = 1, ğ‘¥0 = âˆ’1 and solving for a selection ğ‘¦0 yields
representative trajectories as shown in Figure 9.2.
```
Note that ğœ†ğ‘¦ = ğ‘ + ğ‘ğ‘’ğ‘¡ which is monotone in time ğ‘¡ and ğœ†ğ‘¦(ğ‘‡) = 0, so ğœ†ğ‘¦(ğ‘¡)
```
does not change sign on 0 < ğ‘¡ < ğ‘‡. Therefore thrust ğ‘¢ = âˆ’ 12 ğœ†ğ‘¦ is either always
positive or always negative on an optimal solution. Initial conditions with high
positive velocity require negative thrust to hit ğ‘¥ = 0 at ğ‘‡ = 1. Initial conditions
with lower positive or negative initial velocity require positive thrust. One of
these trajectories, starting at ğ‘¥0 = âˆ’1 and ğ‘¦0 = 1.582 . . . , is perfectly positioned
```
to drift to ğ‘¥(1) = 1 without applying any thrust, making ğ½ = 0 and resulting in
```
a straight line segment for the trajectory. We find this trajectory by taking ğ‘ = 0
and ğ‘ = 0 and solving for ğ‘, ğ‘, and ğ‘‡ to match the endpoint conditions.
```
Example 9.2: Zermelo, Free ğ‘¦(ğ‘‡)
```
Recall the Zermelo onna Boat Example 8.4, with equations of motion
```
ğ‘¥â€² = ğ‘¦ + cos(ğœƒ),
```
```
ğ‘¦â€² = sin(ğœƒ)
```
and control variable ğœƒ. The goal is to minimize time of travel, so we take
```
ğº(ğ‘¥(ğ‘‡), ğ‘¦(ğ‘‡), ğ‘‡) = ğ‘‡
```
```
and no running costs ğ‘”(ğ‘¥, ğ‘¦, ğ‘¢) = 0.
```
```
Suppose we have a prescribed initial position ğ‘¥(0) = ğ‘¥0, ğ‘¦(0) = ğ‘¦0 and a
```
```
final value for one variable ğ‘¥(ğ‘‡) = 0, allowing both ğ‘‡ and ğ‘¦(ğ‘‡) to be free.
```
The condition tan ğœƒ = ğœ†ğ‘¦/ğœ†ğ‘¥ = ğ¶ âˆ’ ğ‘¡ remains the same as in Example 8.4, so
we again have the linear tangent law
tan ğœƒ =
ğœ†ğ‘¦
```
ğœ†ğ‘¥= ğ¶ âˆ’ ğ‘¡. (9.1)
```
We intuit that for starting values ğ‘¥0 > 0 we would never want to steer away from
```
the finish line, and we take cos(ğœƒ) â‰¤ 0. Applying right-triangle trigonometry to
```
```
equation (9.1), we conclude
```
```
sin(ğœƒ) = âˆ’ ğ¶ âˆ’ ğ‘¡
```
```
âˆš1 + (ğ¶ âˆ’ ğ‘¡)2
```
```
, cos(ğœƒ) = âˆ’ 1
```
```
âˆš1 + (ğ¶ âˆ’ ğ‘¡)2
```
.
144 Chapter 9. Targets
```
Substituting these into our equation of motion and integrating, we get (âˆ— check
```
```
this âˆ—)
```
```
ğ‘¥(ğ‘¡) = ğ´ + ğµğ‘¡ âˆ’ 12 (ğ¶ âˆ’ ğ‘¡)âˆš1 + (ğ¶ âˆ’ ğ‘¡)2 + 12 arcsinh(ğ¶ âˆ’ ğ‘¡),
```
```
ğ‘¦(ğ‘¡) = ğµ + âˆš1 + (ğ¶ âˆ’ ğ‘¡)2.
```
This example differs from Example 8.4 in the endpoint evaluation. In this
```
case we have four free constants ğ´, ğµ, ğ¶, ğ‘‡ and three endpoint conditions ğ‘¥(0) =
```
```
ğ‘¥ğ‘, ğ‘¦(0) = ğ‘¦ğ‘, and ğ‘¥(ğ‘‡) = 0. The fourth condition is supplied by Principle
```
```
V: since the final ğ‘¦ location has no effect on performance, ğœ•ğºğœ•ğ‘¦ = 0, and hence
```
```
ğœ†ğ‘¦(ğ‘‡) = 0. From equation (9.1) we conclude that ğ¶ = ğ‘‡ (âˆ— why? âˆ—). This makes
```
```
ğœƒ(ğ‘‡) either 0 or ğœ‹, meaning that at the final time Zermelo is steering straight into
```
```
(perpendicular to) the ğ‘¥ = 0 line, which makes intuitive sense (âˆ— why? âˆ—).
```
Substituting ğ¶ = ğ‘‡, we are left with a system of three equations and three
```
unknowns:
```
```
ğ‘¥ğ‘ = ğ‘¥(0) = ğ´ âˆ’ 12 ğ‘‡âˆš1 + ğ‘‡2 + 12 arcsinh(ğ‘‡),
```
```
ğ‘¦ğ‘ = ğ‘¦(0) = ğµ + âˆš1 + ğ‘‡2,
```
```
0 = ğ‘¥(ğ‘‡) = ğ´ + ğµğ‘‡
```
which can be solved numerically with some work. A good approach is to elimi-
nate ğ´ and ğµ and get
```
ğ‘¥ğ‘ + ğ‘‡ğ‘¦ğ‘ = 12 âˆš1 + ğ‘‡2 + 12 arcsinh(ğ‘‡).
```
```
Then for any starting (ğ‘¥ğ‘, ğ‘¦ğ‘) numerically approximate ğ‘‡. The set of solution
```
trajectories is depicted in Figure 9.3.
Figure 9.3. Time minimizing paths to ğ‘¥ = 0 for Zermelo.
Note that trajectories starting with slightly positive ğ‘¥0 and 0 < ğ‘¦0 < 1 will
drive straight into the line ğ‘¥ = 0, while trajectories with ğ‘¦0 > 1 drift downwards
and away from the line first.
9.2. Hitting a Curve 145
9.2 Hitting a Curve
9.2.1 General Curves. We can expand Principle V so that the required endpoint
condition is for the trajectory to terminate on a specified curve.
OPTIMAL PRINCIPLE VI
Local optimum, free duration, constrained endpoint, time dependent, two dimensions
Consider the controlled system
```
(
```
ğ‘¥â€²1
ğ‘¥â€²2
```
) = (
```
```
ğ‘“1(ğ‘¥1, ğ‘¥2, ğ‘¢, ğ‘¡)
```
```
ğ‘“2(ğ‘¥1, ğ‘¥2, ğ‘¢, ğ‘¡)
```
```
) , ğ‘¥1, ğ‘¥2, ğ‘¡ âˆˆ â„, ğ‘¢ âˆˆ ğ’°,
```
```
starting at ğ‘¥1(0) = ğ´1, ğ‘¥2(0) = ğ´2 and objective function
```
ğ½ = âˆ«
ğ‘‡
0
```
ğ‘”(ğ‘¥1, ğ‘¥2, ğ‘¢, ğ‘¡) ğ‘‘ğ‘¡
```
```
where ğ‘¥1(ğ‘‡) = ğµ1 and ğ‘¥2(ğ‘‡) = ğµ2.
```
Define the Hamiltonian
```
ğ»(ğ‘¥1, ğ‘¥2, ğœ†1, ğœ†2, ğ‘¢, ğ‘¡) = ğ‘” + ğœ†1ğ‘“1 + ğœ†2ğ‘“2
```
and costate equations
ğœ†â€²1 = âˆ’ ğœ•ğ»ğœ•ğ‘¥1,
ğœ†â€²2 = âˆ’ ğœ•ğ»ğœ•ğ‘¥2.
Then a locally optimal control must satisfy
ğœ•ğ»
ğœ•ğ‘¢ = 0
and the control that optimizes ğ½ will optimize ğ» at all times.
```
If the end location is required to land on a smooth curve (ğµ1, ğµ2) âˆˆ ğ’, then the
```
```
endpoint criteria is that (ğœ†1(ğ‘‡), ğœ†2(ğ‘‡)) is perpendicular to the curve at the point
```
```
(ğ‘¥1(ğ‘‡), ğ‘¥2(ğ‘‡)).
```
The end time ğ‘‡ may be prescribed. Otherwise, the optimal end time ğ‘‡ will
satisfy
```
ğ»(ğ‘‡) = 0.
```
In addition, if ğ» does not explicitly depend on ğ‘¡, it is constant on optimal trajecto-
ries.
```
As with Principle IV (Section 8.1), we state the principle using numerical sub-
```
scripts to emphasize dimensionality and more easily see how the principle would gen-
eralize to higher dimensions. For example, we could require the end state to be on a hy-
persurface, or even require a subset of the end state variables to be on a surface of the ap-
```
propriate dimension. In three dimensions (ğ‘¥1, ğ‘¥2, ğ‘¥3) we could require (ğ‘¥2(ğ‘‡), ğ‘¥3(ğ‘‡))
```
```
to land on a curve while ğ‘¥1(ğ‘‡) is free. This is handy if we want to move a satellite to a
```
circular orbit of specified radius without specifying where along the orbit it must be at
what time.
146 Chapter 9. Targets
```
In practice, we will stick to two dimensions and revert to (ğ‘¥, ğ‘¦) coordinates and
```
subscripts. For simplicity and clarity, we have dispensed with the final payoff function
ğº. The broader theory can incorporate such a payoff, but the above will suffice for our
purposes.
This principle applies to the case where the end conditions are a closed region.
```
For example, we may require |ğ‘¥(ğ‘‡)| â‰¤ ğ›¿ğ‘¥ and |ğ‘¦(ğ‘‡)| â‰¤ ğ›¿ğ‘¦. Principle IV would apply
```
```
by considering the boundary of the region: |ğ‘¥(ğ‘‡)| = ğ›¿ğ‘¥ and |ğ‘¦(ğ‘‡)| = ğ›¿ğ‘¦. One would
```
also have to consider the possibility that an optimum stopping location is interior to the
target region by examining all local optimal solutions that may exist inside the region.
A key point of this principle is that it reinforces insight into what the costate vari-
ables are:
```
At the terminal point (ğ‘¥(ğ‘‡), ğ‘¦(ğ‘‡)) = (ğµğ‘¦, ğµğ‘¥) of an optimal trajectory the
```
```
costate vector (ğœ†ğ‘¥(ğ‘‡), ğœ†ğ‘¦(ğ‘‡)) is the gradient of performance ğ½ with respect to
```
```
variables (ğµğ‘¥, ğµğ‘¦). That is, the costate vector points in the best direction to
```
move the endpoint in order to improve the outcome.
```
Thus if our end state is free, we stop at a place where the costate vector is zero;
```
```
otherwise we could move our end location in the direction of the costate vector (if
```
```
possible) and improve the result. This reflects the property of Lagrange multipliers as
```
the rate of change of optimum with respect to constraint, as explored in Section 2.9.
If our end state is constrained to a curve, we stop at a point where the costate vector
```
is perpendicular to the curve; otherwise we could improve things by moving along the
```
curve in the direction that the costate variable is pointing.
9.2.2 Target lines. Principle VI applies to targets that are horizontal or vertical
lines, and it produces the same results as Principle V.
Example 9.3: Zermelo, Hit Vertical Line ğ‘¥ = 0
Consider the Zermelo onna Boat Example 8.4,
```
ğ‘¥â€² = ğ‘¦ + cos(ğœƒ),
```
```
ğ‘¦â€² = sin(ğœƒ).
```
```
Starting at (ğ‘¥ğ‘, ğ‘¦ğ‘) the goal is to operate the control ğœƒ to reach the vertical line
```
ğ‘¥ = 0 in minimal time.
We already solved this in Example 9.2. Note that the condition in Principle IV
```
that (ğœ†ğ‘¥(ğ‘‡), ğœ†ğ‘¦(ğ‘‡)) is perpendicular to the vertical line ğ‘¥ = 0 yields the condition
```
```
ğœ†ğ‘¦(ğ‘‡) = 0, which is the same as what Principle V gave in Example 9.2.
```
Principle VI applies to any target line and stipulates that the ending value of the
costate vector is perpendicular to the line.
9.2. Hitting a Curve 147
Example 9.4: Rocket Sled, Hit ğ‘¦ = ğ‘¥
Returning to Rocket Sled, Example 8.1, we want to minimize ğ½ = âˆ«ğ‘‡0 ğ‘¢2 ğ‘‘ğ‘¡ for
ğ‘¥â€³ = âˆ’ğ‘¥â€² + ğ‘¢, with minimal trajectories defined by
ğ‘¥ = âˆ’ 12 ğ‘ğ‘¡ âˆ’ 14 ğ‘ğ‘’ğ‘¡ âˆ’ ğ‘ğ‘’âˆ’ğ‘¡ + ğ‘‘,
ğ‘¦ = âˆ’ 12 ğ‘ âˆ’ 14 ğ‘ğ‘’ğ‘¡ + ğ‘ğ‘’âˆ’ğ‘¡,
ğœ†ğ‘¥ = ğ‘,
ğœ†ğ‘¦ = ğ‘ + ğ‘ğ‘’ğ‘¡.
```
Suppose we have a starting point (ğ‘¥0, ğ‘¦0), a fixed end time ğ‘‡, and suppose
```
that our target is now the slanted line ğ‘¦ = ğ‘¥.
```
Principle VI requires the costate vector (ğœ†ğ‘¥, ğœ†ğ‘¦) to be perpendicular to this
```
line at the point of impact. Using a dot product, this requires
```
0 = (ğœ†ğ‘¥(ğ‘‡), ğœ†ğ‘¦(ğ‘‡)) â‹… (1, 1) = ğœ†ğ‘¥(ğ‘‡) + ğœ†ğ‘¦(ğ‘‡).
```
```
For any starting point ğ‘¥(0) = ğ‘¥0, ğ‘¦(0) = ğ‘¦0, our constraints are then
```
```
ğ‘¥(0) = ğ‘¥0 = âˆ’ 14 ğ‘ âˆ’ ğ‘ + ğ‘‘,
```
```
ğ‘¦(0) = ğ‘¦0 = âˆ’ 12 ğ‘ âˆ’ 14 ğ‘ + ğ‘,
```
```
ğ‘¥(ğ‘‡) = 1 = âˆ’ 12 ğ‘ğ‘‡ âˆ’ 14 ğ‘ğ‘’ğ‘‡ âˆ’ ğ‘ğ‘’âˆ’ğ‘‡ + ğ‘‘,
```
```
ğœ†ğ‘¥(ğ‘‡) + ğœ†ğ‘¦(ğ‘‡) = 0 = 2ğ‘ + ğ‘ğ‘’ğ‘‡
```
which is linear in the parameters ğ‘, ğ‘, ğ‘, ğ‘‘.
Taking ğ‘‡ = 1 and solving for some starting points on a line ğ‘¦ = ğ‘¥ + 3 yields
representative trajectories in Figure 9.4.
Figure 9.4. Optimal paths to the line ğ‘¦ = ğ‘¥ for ğ‘‡ = 1 from
various starting points.
148 Chapter 9. Targets
```
Note that one of these trajectories, starting at (âˆ’1.254 . . . , 2.254 . . . ), is a straight
```
line segment requiring no thrust and has no cost.
9.2.3 Target Circles. An interesting application of Principle VI is when the target
is a circle centered at the origin ğ‘¥2 + ğ‘¦2 = ğœŒ2. This can arise as a relaxation of the
soft-landing cases, where you donâ€™t have to exactly hit ğ‘¥ = 0, ğ‘¦ = 0, you just have to
get within a circle of radius ğœŒ.
```
The costate criteria from Principle VI require the costate vector (ğœ†ğ‘¥, ğœ†ğ‘¦) to be per-
```
pendicular to the target curve at the point of intersection. For a circle centered at the
```
origin, this makes the costate vector (ğœ†ğ‘¥, ğœ†ğ‘¦) parallel to the position vector (ğ‘¥, ğ‘¦). This
```
parallel condition can be expressed as ğ‘¦/ğ‘¥ = ğœ†ğ‘¦/ğœ†ğ‘¥, so if our target curve is ğ‘¥2 +ğ‘¦2 = ğœŒ2,
we get the endpoint conditions
```
ğ‘¥(ğ‘‡)2 + ğ‘¦(ğ‘‡)2 = ğœŒ2,
```
```
ğ‘¦(ğ‘‡)
```
```
ğ‘¥(ğ‘‡) =
```
```
ğœ†ğ‘¦(ğ‘‡)
```
```
ğœ†ğ‘¥(ğ‘‡) .
```
```
(9.2)
```
The following example explores this idea for the brachistochrone problem.
Example 9.5: Brachistochrone, Circle Target
Consider the brachistochrone problem, Example 8.6. Assume we have a vertical
plane with constant gravity and we have a bead at location ğ‘¥ğ‘ on the ğ‘¥-axis. We
want this bead to slide to the unit circle ğ‘¥2 + ğ‘¦2 = 1 in minimal time just using
gravity. Solution curves are shown in Figure 9.5.
Figure 9.5. Minimum time brachistochrone trajectories to the
unit circle.
To derive the set of solution curves, recall that the brachistochrone trajecto-
ries are parameterized by the angle ğœƒ of the tangent to the curve,
ğ‘‘ğ‘¦
ğ‘‘ğ‘¥ =
ğ‘¦â€²
ğ‘¥â€² = tan ğœƒ,
```
and that the optimal control is determined by tan(ğœƒ) = ğœ†ğ‘¦/ğœ†ğ‘¥.
```
```
Combining this with equations (9.2) we have that at time ğ‘‡,
```
```
ğ‘¦â€²(ğœƒğ‘‡ )
```
```
ğ‘¥â€²(ğœƒğ‘‡ ) = tan(ğœƒğ‘‡ ) =
```
```
ğœ†ğ‘¦(ğ‘‡)
```
```
ğœ†ğ‘¥(ğ‘‡) =
```
```
ğ‘¦(ğœƒğ‘‡ )
```
```
ğ‘¥(ğœƒğ‘‡ ) . (9.3)
```
9.2. Hitting a Curve 149
This implies that at the point of reaching the unit circle,
```
ğ‘¦â€²(ğœƒğ‘‡ )
```
```
ğ‘¥â€²(ğœƒğ‘‡ ) =
```
```
ğ‘¦(ğœƒğ‘‡ )
```
```
ğ‘¥(ğœƒğ‘‡ ) ,
```
meaning that the velocity vector of the bead is perpendicular to the circle. This
makes sense: you end up driving straight into the target surface to minimize time.
```
Note that since (ğ‘¥(ğ‘‡), ğ‘¦(ğ‘‡)) is on the unit circle, equation (9.3) implies ğ‘¦(ğœƒğ‘‡ ) =
```
```
sin(ğœƒğ‘‡ ) and ğ‘¥(ğœƒğ‘‡ ) = cos(ğœƒğ‘‡ ).
```
```
Our equations for the brachistochrone solutions (Example 8.6, equation (8.7)),
```
```
shifted to start at (ğ‘¥ğ‘, 0), are
```
```
ğ‘¥ = ğ‘¥ğ‘ âˆ’ ğ‘¦ğ‘ğœ‹ + 2ğœƒ + sin(2ğœƒ)2 cos2 ğœƒ
```
ğ‘‡
,
ğ‘¦ = ğ‘¦ğ‘cos
```
2(ğœƒ)
```
```
cos2(ğœƒğ‘‡ ) .
```
```
Given any starting location (ğ‘¥ğ‘, 0) we look for terminal values by substituting
```
```
ğ‘¦ğ‘ = sin(ğœƒğ‘‡ ) and ğ‘¥ğ‘ = cos(ğœƒğ‘‡ ) into the above equation for ğ‘¥, to get a single
```
equation
```
cos(ğœƒğ‘‡ ) = ğ‘¥ğ‘ âˆ’ sin(ğœƒğ‘‡ ) ğœ‹ + 2ğœƒğ‘‡ + sin(2ğœƒğ‘‡ )2 cos2 ğœƒ
```
ğ‘‡
for which we numerically estimate a solution ğœƒğ‘‡ for any given ğ‘¥ğ‘. This yields our
```
final parameterized optimal path from (ğ‘¥ğ‘, 0) on the ğ‘¥-axis to (ğ‘¥ğ‘, ğ‘¦ğ‘) on the unit
```
```
circle:
```
```
ğ‘¥ = ğ‘¥ğ‘ âˆ’ sin(ğœƒğ‘‡ ) ğœ‹ + 2ğœƒ + sin(2ğœƒ)2 cos2 ğœƒ
```
ğ‘‡
,
```
ğ‘¦ = sin(ğœƒğ‘‡ ) cos
```
```
2(ğœƒ)
```
```
cos2(ğœƒğ‘‡ ) .
```
Depending on starting location, we have either a positive parameterization âˆ’ğœ‹/2 <
ğœƒ < ğœƒğ‘‡ or a negative parameterization âˆ’ğœ‹/2 > ğœƒ > ğœƒğ‘‡ .
9.2.4 Target Curves with Endpoints. Implicit in the above principle is that the
curve ğ’ is differentiable at the terminal point of the optimal trajectory. We can extend
this principle to cases where the curve has an endpoint, as explored in the next example.
Example 9.6: Zermelo, Half-Line
Consider the Zermelo example where the target is the half-line ğ‘¥ = 0 and ğ‘¦ â‰¤ 1/2.
```
Suppose that in the full-line case (Example 9.2) the optimal trajectory (ğ‘¥(ğ‘¡), ğ‘¦(ğ‘¡))
```
```
from the initial point (ğ‘¥ğ‘, ğ‘¦ğ‘) hits the line ğ‘¥ = 0 at a point where ğ‘¦ â‰¤ 12 . Then
```
```
that trajectory is still optimal in the half-line case (âˆ— why? âˆ—). Figure 9.6 shows
```
```
the set of optimal trajectory paths from the full-line case (see Figure 9.3) that are
```
still valid for this half-line case.
150 Chapter 9. Targets
Figure 9.6. Optimal trajectories to the line that hit below ğ‘¦ = 12
remain valid.
```
Figure 9.7. The optimal trajectory from (0, 2) to the half-line
```
extends backwards in time.
We can take any of the trajectories that hit the target from the right and ex-
tend them backwards in time. For example,
```
ğ‘¥(ğ‘¡) = 12 ((3.2692 âˆ’ ğ‘¡)âˆš1 + (3.2692 âˆ’ ğ‘¡)2 âˆ’ arcsinh(3.2692 âˆ’ ğ‘¡))
```
âˆ’4.6381 + ğ‘¡1.4187,
```
ğ‘¦(ğ‘¡) = âˆš1 + (3.2692 âˆ’ ğ‘¡)2 âˆ’ 1.4187
```
```
is the approximation of an optimal trajectory with ğ‘¥(0) = 0 and ğ‘¦(0) = 2. By
```
the Principle of Optimality, for any ğ‘¡0 > 0 this is the optimal path of travel to the
9.2. Hitting a Curve 151
```
target from the intermediate point ğ‘¥(ğ‘¡0), ğ‘¦(ğ‘¡0). This is also true going backwards
```
in time, ğ‘¡0 â‰¤ 0, as the trajectory uniquely solves the Pontryagin conditions. This
trajectory, extended backwards in time, is highlighted in Figure 9.7.
Doing this with all the trajectories that hit from the right produces the set of
optimal trajectories for the half-line problem shown in Figure 9.8.
Figure 9.8. All optimal trajectories that hit the half-line from
the right extend backwards in time.
```
This defines a region in the (ğ‘¥, ğ‘¦)-plane covered by trajectories that hit the
```
half-line along a trajectory from the full-line case.
What are the boundaries of this region? The lower part is bounded by the op-
```
timal trajectory that terminates at (0, 12 ) coming from the left. On the upper part
```
```
it is bounded by the optimal full-line trajectory that terminates at (0, 12 ) coming
```
from the right and its continuation backwards in time to âˆ’âˆ. These trajectories
are highlighted in Figure 9.8.
No optimal trajectory for the half-line case can cross either of these bound-
ary trajectories: if each trajectory is a local minimizer, then we canâ€™t have two
trajectories emerge from the intersection point. Principle of Optimality again.
Now consider the remaining region between the two bounding trajectories.
For any trajectory starting in this region, the only point on the half-line that any
```
optimal trajectory can reach is the point (0, 1/2): the trajectory canâ€™t reach any
```
other point on the half-line without crossing a boundary trajectory. So the solu-
tion for any initial point between these boundary trajectories is a trajectory ter-
```
minating at (0, 1/2). The solution must therefore be the time minimizing from
```
```
the initial point to (0, 1/2), as would be solved in the original Zermelo Example
```
8.4.
The full set of solutions for this problem is shown in Figure 9.9.
152 Chapter 9. Targets
Figure 9.9. Optimal trajectories that donâ€™t hit the line below
```
ğ‘¦ = 1/2 must hit the point (0, 1/2), completing the solution on
```
the plane.
Key Points
Chapter 8 introduced Principle IV which allows for a multidimensional state space and
free end time.
In this chapter we introduced Principles V and VI which allows multidimensional
state space with free end locations, and this requires some consideration for the greater
freedom allowed in the higher dimensions.
Principle V allows for the endpoint of one or both state variables to be free in a
two-dimensional system, and it allows an endpoint payoff function.
Principle VI addresses cases where the endpoint is required to land on a curve or
boundary. The solution illuminates that the costate vector is the gradient of perfor-
mance with respect to restriction.
Exercises
```
Exercise 9.1. The Zermelo examples (Examples 9.2 and 9.3) had Captain Zermelo rac-
```
ing to a vertical line. Explore the case of a horizontal line in the following.
```
(a) What is the optimal solution for a horizontal finish line ğ‘¦ = 0? In Examples
```
9.2 and 9.3, Zermelo crosses the finish line perpendicularly. Does this still hold?
```
(b) What about a horizontal finish line ğ‘¦ = ğ‘˜? What is the optimal solution? Does
```
Zermie cross the finish line perpendicularly? If not, explain why this case differs.
```
Exercise 9.2(hs). Rocket Sled on Ice, Target Line. Consider the frictionless Rocket Sled
```
system in Exercise 8.1, ğ‘¥â€³ = ğ‘¢, for fixed ğ‘‡ = 1,
ğ½ = âˆ«
1
0
ğ‘¢2 ğ‘‘ğ‘¡.
Exercises 153
```
(a) Solve for the optimal trajectory starting at (0, 0) and ending on ğ‘¦ âˆ’ ğ‘¥ = 1.
```
```
(b) Investigate the more general target ğ‘¦ âˆ’ ğ‘šğ‘¥ = 1 > 0. Describe the different
```
behaviors for ğ‘š < 0, ğ‘š = 0, 0 < ğ‘š < 1, and ğ‘š > 1.
```
(c) Consider the unit circle target ğ‘¥2 + ğ‘¦2 = 1. It would be difficult to solve a
```
```
general starting point (ğ‘¥0, ğ‘¦0) (âˆ— try it âˆ—). But here is what you can do: take ğ‘¡ = 0 as the
```
time of hitting the unit circle, compute conditions for an optimal trajectory, and then
plot these trajectories backwards in time. Do this, plot some representative trajectories,
analyze, and comment.
Exercise 9.3. Example 9.6 had Zermelo racing to a half-line target ğ‘¥ = 0, ğ‘¦ â‰¤ 1/2.
```
Sketch the solutions for the following half-lines (no need to computeâ€”just sketch by
```
```
hand).
```
```
(a) ğ‘¥ = 0, ğ‘¦ â‰¤ 2.
```
```
(b) ğ‘¦ = 0, ğ‘¥ â‰¤ 0.
```
```
(c) ğ‘¦ = 1, ğ‘¥ â‰¤ 0.
```
```
Exercise 9.4(h). Consider the Zermelo example with a circular water flow (as in Ex-
```
```
ercise 8.6):
```
```
ğ‘¥â€² = âˆ’ğ‘¦ + cos(ğœƒ),
```
```
ğ‘¦â€² = ğ‘¥ + sin(ğœƒ).
```
```
(a) Analyze the minimal time path from a general starting point (ğ‘¥0, ğ‘¦0) to the
```
horizontal line ğ‘¦ = 0.
```
(b) Analyze the minimal time path from a general starting point (ğ‘¥0, ğ‘¦0) to the
```
circle ğ‘¥2 + ğ‘¦2 = ğ‘˜.
```
(c) Analyze the minimal time path from the origin (0, 0) to the line ğ‘¦ = ğ‘˜. What
```
```
can you say about a more general starting point (ğ‘¥0, ğ‘¦0)?
```
```
(d) Analyze the minimal time path from a general starting point (ğ‘¥0, ğ‘¦0) to the
```
half-line ğ‘¦ = 0, ğ‘¥ â‰¤ 0 ?
Exercise 9.5. Continuing with Exercise 8.7, consider Zermelo with horizontal and ver-
tical controls
ğ‘¥â€² = ğ‘¦ + ğ‘¢,
ğ‘¦â€² = ğ‘£
```
and an initial position of (ğ‘¥0, ğ‘¦0). Solve for the optimal solution assuming the follow-
```
ing, and plot a representative trajectory.
```
(a) Suppose ğ‘‡ is fixed, ğ‘¦(ğ‘‡) is free, end condition ğ‘¥(ğ‘‡) = ğ‘¥1 is given, and perfor-
```
mance is ğ½ = âˆ«ğ‘‡0 ğ‘¢2 + ğ‘£2 ğ‘‘ğ‘¡.
154 Chapter 9. Targets
```
(b) Suppose ğ‘‡ is free, ğ‘¦(ğ‘‡) is free, end condition ğ‘¥(ğ‘‡) = ğ‘¥1 is given, and perfor-
```
mance is ğ½ = ğ‘‡ + âˆ«ğ‘‡0 ğ‘¢2 + ğ‘£2 ğ‘‘ğ‘¡.
```
(c) Suppose ğ‘‡ is fixed, ğ‘¥(ğ‘‡) is free, end condition ğ‘¦(ğ‘‡) = ğ‘¦1 is given, and perfor-
```
mance is ğ½ = âˆ«ğ‘‡0 ğ‘¢2 + ğ‘£2 ğ‘‘ğ‘¡.
```
(d) Suppose ğ‘‡ is free, ğ‘¥(ğ‘‡) is free, end condition ğ‘¦(ğ‘‡) = ğ‘¦1 is given, and perfor-
```
mance is ğ½ = ğ‘‡ + âˆ«ğ‘‡0 ğ‘¢2 + ğ‘£2 ğ‘‘ğ‘¡.
```
Exercise 9.6(hs). Suppose that Captain Zermelo (Example 8.4) starts at location (ğ‘, ğ‘)
```
and wants to pilot his boat to get as far to the East as possible in ğ‘‡ time units. That is,
```
ğ‘‡ is fixed, ğ‘¥(ğ‘‡) and ğ‘¦(ğ‘‡) are both free, and you want to maximize ğ‘¥(ğ‘‡). How does he
```
```
do it? What is ğœƒ(ğ‘‡) and why does the value make intuitive sense?
```
```
Exercise 9.7(s). Fun Slide. Determine the shape of a slide that would move the slider
```
a maximum horizontal distance in one minute. That is, use the brachistochrone model
```
with initial position (0, 0), free end location, and payoff function ğ½ = ğ‘¥(ğ‘‡). Use Earth
```
```
gravity (ğ‘” = âˆ’9.8 m/sec2) and no friction, and use a fixed time ğ‘‡.
```
10
Switching Controls
and Stationarity
Principles Iâ€“VI provide necessary conditions for controls to be locally optimal and thus
represent an ideal balance between costs and benefits. In these principles, we saw that
an optimal control will optimize the Hamiltonian ğ» at all times. These techniques
work fine when the optimal solution is interior to the set of allowed controls. We iden-
tify these controls by the stationarity condition ğœ•ğ»ğœ•áµ† = 0, and we can distinguish maxi-
mizing and minimizing controls by the sign of ğœ•2ğ»/ğœ•ğ‘¢2.
However, many controls are optimized at extremesâ€”cars go the fastest when you
put the pedal to the metal. In calculus max-min problems, we check where the deriv-
ative is zero, but we must also remember to check the endpoints.
This chapter explores cases where there are limitations on ğ‘¢, say, ğ›¼ â‰¤ ğ‘¢ â‰¤ ğ›½, and
the optimum occurs at one of the endpoint values. The key idea is that at every point
```
of an optimal path the optimal control ğ‘¢ produces the maximum (or minimum) value
```
of ğ» over all allowable controls, and this may occur at the limit of allowed values. This
helps us identify when we should operate a control at an extreme of allowed range. In
some interesting cases we switch instantaneously between extreme values, and these
```
are called bang-bang (yes, really) controls.
```
Another possibility is that ğœ•ğ»ğœ•áµ† = 0 with no concavity, ğœ•2ğ»ğœ•áµ†2 = 0, making it unclear
whether the control is a maximum, minimum, or neither. These stationary controls
can form segments of an overall optimal control strategy.
10.1 Extreme Controls
The following motivating example shows where Principle V fails to lead us to an opti-
mal solution. Why, yes, as a matter of fact, this is rocket science.
155
156 Chapter 10. Switching Controls and Stationarity
Example 10.1: Big Silver Rocket Ship
A Big Silver Rocket Ship ascends vertically from a flat Earth with constant gravity
and no air friction. The rocket accelerates by burning fuel at a rate of ğ›½ kg/sec
and ejecting seriously hot gas out its tail end at ludicrous speed ğœ‡ m/sec creating
a force of ğ›½ğœ‡ kg m/sec2. We control the fuel burn rate ğ›½. With a gravitational
constant ğ‘” and spacecraft mass ğ‘š we have the Newtonian equation
ğ‘šğ‘¦â€³ = âˆ’ğ‘”ğ‘š + ğ›½ğœ‡.
Taking into account that our Big Silver Rocket Ship is losing mass at rate ğ›½, we
get the following system:
ğ‘¦â€² = ğ‘£,
ğ‘£â€² = ğ›½ğœ‡ğ‘š âˆ’ ğ‘”,
ğ‘šâ€² = âˆ’ğ›½.
We want to control the fuel burn rate ğ›½ to lift our Big Silver Rocket Ship to a
height ğ‘¦ğ‘‡ using as little fuel as possible. So we want to minimize total fuel burn:
ğ½ = âˆ«
ğ‘‡
0
ğ›½ ğ‘‘ğ‘¡
where ğ‘‡ is free. Our Hamiltonian is
```
ğ» = ğ›½ + ğœ†1ğ‘£ + ğœ†2 ( ğ›½ğœ‡ğ‘š âˆ’ ğ‘”) + ğœ†3(âˆ’ğ›½)
```
with
ğœ•ğ»
ğœ•ğ›½ = 1 + ğœ†2
ğœ‡
ğ‘š âˆ’ ğœ†3.
This is a problem. The control ğ›½ doesnâ€™t show up in this expression, so setting
ğœ•ğ»
ğœ•ğ›½ to zero doesnâ€™t allow us to solve for ğ›½.
In fact, the solution to this problem is an infinitely positive burn rate ğ›½ = âˆ.
```
Fuel is heavy; we donâ€™t want to carry it up with the rocket. The most efficient
```
solution is to burn all the fuel at once right at the beginning, shooting the rocket
like a bullet from a gun with just enough initial velocity to reach the target height.
This is how Jules Verne envisioned launching a rocket in his 1865 novel From
Earth to Moon.
Previously, we looked for locally optimal solutions by satisfying ğœ•ğ»ğœ•áµ† = 0. However,
the key idea is that ğ» is optimized on optimal trajectories. If there are restrictions on
our control, then we want ğ» to be as optimal as possible given those restrictions. At
any point on an optimal trajectory, we choose the control value from the set of allowed
values that produces the most extreme value of ğ». This is made explicit in the following
principle.
10.1. Extreme Controls 157
OPTIMAL PRINCIPLE VII
Global optimum, free duration, free endpoint, time dependent, two dimensions
Consider the controlled system
```
(
```
ğ‘¥â€²1
ğ‘¥â€²2
```
) = (
```
```
ğ‘“1(ğ‘¥1, ğ‘¥2, ğ‘¢, ğ‘¡)
```
```
ğ‘“2(ğ‘¥1, ğ‘¥2, ğ‘¢, ğ‘¡)
```
```
) , ğ‘¥1, ğ‘¥2, ğ‘¡ âˆˆ â„, ğ‘¢ âˆˆ ğ’°,
```
```
starting at ğ‘¥1(0) = ğ´1, ğ‘¥2(0) = ğ´2 and objective function
```
```
ğ½ = ğº(ğµ1, ğµ2, ğ‘‡) + âˆ«
```
ğ‘‡
0
```
ğ‘”(ğ‘¥1, ğ‘¥2, ğ‘¢, ğ‘¡) ğ‘‘ğ‘¡
```
```
where ğ‘¥1(ğ‘‡) = ğµ1 and ğ‘¥2(ğ‘‡) = ğµ2.
```
Define the Hamiltonian
```
ğ»(ğ‘¥1, ğ‘¥2, ğœ†1, ğœ†2, ğ‘¢, ğ‘¡) = ğ‘” + ğœ†1ğ‘“1 + ğœ†2ğ‘“2
```
and costate equations
ğœ†â€²1 = âˆ’ ğœ•ğ»ğœ•ğ‘¥1,
ğœ†â€²2 = âˆ’ ğœ•ğ»ğœ•ğ‘¥2.
IfÌƒ ğ‘¢ maximizes ğ½ and satisfies the endpoint conditions, then at all times
```
ğ»(ğ‘¥1, ğ‘¥2, ğœ†1, ğœ†2,Ìƒ ğ‘¢, ğ‘¡) â‰¥ ğ»(ğ‘¥1, ğ‘¥2, ğœ†1, ğœ†2, ğ‘¢, ğ‘¡)
```
for all admissible controls ğ‘¢, and ifÌƒ ğ‘¢ minimizes ğ½, then at all times
```
ğ»(ğ‘¥1, ğ‘¥2, ğœ†1, ğœ†2,Ìƒ ğ‘¢, ğ‘¡) â‰¤ ğ»(ğ‘¥1, ğ‘¥2, ğœ†1, ğœ†2, ğ‘¢, ğ‘¡)
```
for all admissible controls ğ‘¢.
Either or both ending locations may be prescribed. Otherwise the optimal end
```
location(s) must satisfy
```
```
ğœ†1(ğ‘‡) = ğœ•ğºğœ•ğµ1(ğµ1, ğµ2, ğ‘‡),
```
```
ğœ†2(ğ‘‡) = ğœ•ğºğœ•ğµ2(ğµ1, ğµ2, ğ‘‡).
```
The end time ğ‘‡ may be prescribed. Otherwise, the optimal end time ğ‘‡ will
satisfy
ğœ•ğº
```
ğœ•ğ‘‡ (ğµ1, ğµ2, ğ‘‡) + ğ»(ğ‘‡) = 0.
```
Note that as we consider controls at operational extremes, we may lose the condi-
tion that ğ» is constant on optimal trajectories.
This principle makes it clear that to optimize performance ğ½, the control at any
specific time must produce the most optimal value of ğ» for the given set of allowed
controls. This concept holds regardless of whether the control is a locally optimal con-
trol or an extreme control at the boundary of allowable controls.
158 Chapter 10. Switching Controls and Stationarity
More precisely, for a maximization problem and for any given current values of
state ğ‘¥1, ğ‘¥2 and costate ğœ†1, ğœ†2 we want to choose the controlÌƒ ğ‘¢ from the set of allow-
```
able controls ğ’° that maximizes ğ»(ğ‘¥1, ğ‘¥2, ğœ†1, ğœ†2, ğ‘¢, ğ‘¡), and we can express this concept
```
succinctly with the argmax function as
```
Ìƒğ‘¢ = argmaxáµ†âˆˆğ’° {ğ»(ğ‘¥1, ğ‘¥2, ğœ†1, ğœ†2, ğ‘¢, ğ‘¡)} .
```
That is, for any given state position ğ‘¥1, ğ‘¥2, costate position ğœ†1, ğœ†2, and at any time ğ‘¡, we
```
have thatÌƒ ğ‘¢ is the value in ğ’° that produces a maximum for ğ»(ğ‘¥1, ğ‘¥2, ğœ†1, ğœ†2, ğ‘¢, ğ‘¡).
```
Pontryagin takes the daunting task of optimizing a cumulative measure ğ½ = âˆ« ğ‘” ğ‘‘ğ‘¡
over a trajectory in state space and recasts the problem, using a differential form of
Lagrange multipliers, into optimizing an instantaneous measure ğ» at every point along
a trajectory in state-costate space.
This was demonstrated in the proofs from Section 6.3 where we showed that if the
```
costate equations are satisfied, then (see equations (6.2) and (6.3))
```
```
Î”ğ½âˆ— = ğ½âˆ—(ğ‘¥, ğ‘¢ + ğ›¿2, ğœ†) âˆ’ ğ½âˆ—(ğ‘¥, ğ‘¢, ğœ†)
```
```
= âˆ«ğ‘‡0ğœ•ğ»ğœ•áµ† (ğ‘¥, ğ‘¢) ğ›¿2 ğ‘‘ğ‘¡ + ğ‘œ(ğ›¿2)
```
```
â‰ˆ âˆ«ğ‘‡0 ğ»(ğ‘¥, ğ‘¢ + ğ›¿2) âˆ’ ğ»(ğ‘¥, ğ‘¢) ğ‘‘ğ‘¡ + ğ‘œ(ğ›¿2).
```
Now suppose ğ» is monotone in ğ‘¢, say, ğœ•ğ»ğœ•áµ† > 0. Then by the above, you can always
increase performance ğ½ by increasing ğ‘¢ to ğ‘¢ + ğ›¿2. So one would increase ğ‘¢ up to the
boundary of allowable controls.
10.2 Bang-Bang Controls
Many optimal control problems involve operating the control value as large or as small
as it can possibly go. Some problems require switching between extreme values of the
control, referred to as bang-bang controls.
Example 10.2: Big Silver Rocket Ship II
Returning to the Big Silver Rocket Ship, Example 10.1, suppose we have restric-
tions on our burn rate: 0 â‰¤ ğ›½ â‰¤ ğ›½max.
The Hamiltonian is
```
ğ» = ğ›½ + ğœ†1ğ‘£ + ğœ†2 ( ğ›½ğœ‡ğ‘š âˆ’ ğ‘”) + ğœ†3(âˆ’ğ›½) (10.1)
```
which is linear in our control ğ›½:
ğœ•ğ»
ğœ•ğ›½ = 1 + ğœ†2
ğœ‡
ğ‘š âˆ’ ğœ†3.
We want to minimize ğ», so if ğœ•ğ»ğœ•ğ›½ > 0, we take ğ›½ = 0, and if ğœ•ğ»ğœ•ğ›½ < 0, we take
ğ›½ = ğ›½max.
10.2. Bang-Bang Controls 159
To be clear, at any given time the plot of ğ» as a function of ğ›½ is a line. As time
proceeds the slope of this line may change. If the line is sloped downwards, we
maximize our control, ğ›½ = ğ›½max, and if the line is sloped upwards, we minimize
our control, ğ›½ = 0. If the slope of the line changes from negative to positive, we
immediately switch from ğ›½ = 0 to ğ›½ = ğ›½max. This is a bang-bang solution where
we switch from one extreme to the other when ğœ•ğ»ğœ•ğ›½ changes sign.
We can analyze how the slope ğœ•ğ»ğœ•ğ›½ evolves in time. We have costate equations
ğœ†â€²1 = 0,
ğœ†â€²2 = âˆ’ğœ†1,
ğœ†â€²3 = ğœ†2ğ›½ğœ‡ğ‘š2
```
making ğœ†1 a constant, ğœ†1 = ğœ…. Differentiating equation (10.1) we have
```
ğ‘‘
ğ‘‘ğ‘¡
ğœ•ğ»
ğœ•ğ›½ = ğœ†
â€²2ğœ‡
ğ‘š âˆ’ ğœ†2
ğœ‡
ğ‘š2 ğ‘š
â€² âˆ’ ğœ†â€²3
```
= âˆ’ğœ… ğœ‡ğ‘š âˆ’ ğœ†2ğœ‡ğ‘š2 (âˆ’ğ›½) âˆ’ ğœ†2ğ›½ğœ‡ğ‘š2
```
= âˆ’ğœ… ğœ‡ğ‘š
and thus ğœ•ğ»ğœ•ğ›½ is monotonically decreasing in time. The change in control occurs
when this slope switches from positive to negative. This indicates that an optimal
```
control will have a single switch from on (maximum burn) to off (zero burn).
```
```
Also note that with ğ‘‡ and ğ‘£(ğ‘‡) free, we have from Principle VII that ğ»(ğ‘‡) = 0
```
```
and ğœ†2(ğ‘‡) = 0. Assuming ğœ†1 = ğœ… is nonzero and ğ›½ = 0 at time ğ‘‡, we can plug
```
```
into the Hamiltonian and conclude ğ‘£(ğ‘‡) = 0 (âˆ— check âˆ—), verifying our intuition
```
that the rocket reaches the endpoint height condition at apogee.
This defines the form of an optimal solution. We have two periods, starting
```
with 0 â‰¤ ğ‘¡ â‰¤ ğ‘‡burn where we have ğ›½ = ğ›½max and initial conditions ğ‘¦(0) = 0,
```
```
ğ‘£(0) = 0, and we burn all of our fuel in this phase. Then we have the second
```
period ğ‘‡burn < ğ‘¡ â‰¤ ğ‘‡ where we coast with ğ›½ = 0 which ends with the prescribed
```
height ğ‘¦(ğ‘‡) = ğ‘¦ğ‘‡ , zero velocity ğ‘£(ğ‘‡) = 0, and mass equal to the unfueled rocket
```
```
weight ğ‘š(ğ‘‡) = ğ‘šğ‘‡ . The trick is to solve for the correct ğ‘‡burn given the required
```
height ğ‘¦ğ‘‡ , rocket weight ğ‘šğ‘‡ , and maximum burn rate ğ›½max.
In Exercise 10.1 we outline the solution steps to get a 1,000 kg rocket up to
25 km with exhaust velocity 800 m/sec and maximum burn rate of ğ›½max = 60
kg/sec, and we show that the optimal solution is to burn at the maximum rate for
```
39 seconds and then coast for 59 seconds. Height ğ‘¦(ğ‘¡) and velocity ğ‘£(ğ‘¡) for this
```
solution is plotted in Figure 10.1.
160 Chapter 10. Switching Controls and Stationarity
Figure 10.1. Height and velocity for optimal control plotted
against time.
Note that this problem has a three-dimensional state space, with Principle
VII easily accommodating the additional dimension.
10.3 Rocket Races
Consider the Rocket Sled examples: ğ‘¥â€³ = âˆ’ğ‘¥â€² + ğ‘¢, but now instead of conserving fuel,
we want to minimize time: ğ½ = âˆ«ğ‘‡0 1 ğ‘‘ğ‘¡ or ğ½ = ğ‘‡, with ğ‘‡ free. We will refer to these
minimum time problems as Rocket Race problems. To prevent infinite burn rates we
restrict |ğ‘¢| â‰¤ 1.
Example 10.3: Rocket Race I
Consider the system
ğ‘¥â€² = ğ‘¦,
ğ‘¦â€² = âˆ’ğ‘¦ + ğ‘¢
```
with control bounded by |ğ‘¢| â‰¤ 1 and we want to transition from (ğ‘¥ğ‘, ğ‘¦ğ‘) to
```
```
(ğ‘¥ğ‘, ğ‘¦ğ‘) in minimum time.
```
The first issue is attainability. Examining the phase portrait for the most
```
extreme positive value of ğ‘¢ = 1, we can see that if our starting position has ğ‘¦(0) =
```
```
ğ‘¦0 < 1, we will never be able to attain ğ‘¦(ğ‘¡) > 1 for ğ‘¡ > 0 since the restricted thrust
```
cannot overcome the increasing friction at higher velocities. Similar reasoning
applies to ğ‘¢ = âˆ’1, and phase portraits for ğ‘¢ = Â±1 are depicted in Figure 10.2.
10.3. Rocket Races 161
```
Figure 10.2. Phase portraits in (ğ‘¥, ğ‘¦) for extreme controls ğ‘¢ =
```
âˆ’1 and ğ‘¢ = +1.
```
It follows by symmetry that for any trajectory with |ğ‘¦(0)| < 1 we would have
```
```
|ğ‘¦(ğ‘¡)| < 1 for all ğ‘¡ â‰¥ 0 and all possible controls in our allowed set |ğ‘¢(ğ‘¡)| â‰¤ 1. We
```
```
will thus restrict our attention to the band âˆ’1 < ğ‘¦ < 1 in the (ğ‘¥, ğ‘¦)-plane, and
```
in the following we show that we can reach any point in this set from any other
point.
Suppose we have prescribed starting and ending positions
```
ğ‘¥(0) = ğ‘¥0, ğ‘¥(ğ‘‡) = ğ‘¥ğ‘‡ ,
```
```
ğ‘¦(0) = ğ‘¦0, ğ‘¦(ğ‘‡) = ğ‘¦ğ‘‡
```
with |ğ‘¦ğ‘‡ | < 1 and free ğ‘‡.
We want to minimize
ğ½ = âˆ«
ğ‘‡
0
1 ğ‘‘ğ‘¡.
Our Hamiltonian is
```
ğ» = 1 + ğœ†1ğ‘¦ + ğœ†2(âˆ’ğ‘¦ + ğ‘¢)
```
which is linear in ğ‘¢, so we are likely in a bang-bang situation. The coefficient of
ğ‘¢ is ğœ†2, so if ğœ†2 is positive, we take ğ‘¢ = âˆ’1, and if ğœ†2 is negative, we take ğ‘¢ = 1.
This describes how to work our control in terms of the costates:
```
ğ‘¢ = sgn(ğœ†2).
```
The costate equations are
ğœ†â€²1 = 0,
ğœ†â€²2 = âˆ’ğœ†1
implying ğœ†2 is linear in time, ğœ†2 = ğ¶1ğ‘¡ + ğ¶2, which means that we switch our
control exactly once, from +1 to âˆ’1, or from âˆ’1 to +1. This alone is sufficient
information to solve the problem.
Since we have ğ‘¢ equal to either +1 or âˆ’1 we consider the two systems
ğ‘¥â€² = ğ‘¦,
ğ‘¦â€² = âˆ’ğ‘¦ + 1
and
ğ‘¥â€² = ğ‘¦,
ğ‘¦â€² = âˆ’ğ‘¦ âˆ’ 1
162 Chapter 10. Switching Controls and Stationarity
and we think of our control as jumping between the upward moving ğ‘¢ = +1 and
downward moving ğ‘¢ = âˆ’1 systems. Overlaid phase portraits are shown in Figure
10.3.
```
Figure 10.3. Overlaid (ğ‘¥, ğ‘¦)-phase portraits for extreme con-
```
trols ğ‘¢ = Â±1.
Using these two systems, we can now map out our optimal solutions for any
```
given target point. For example, suppose our terminal point is taken to be (1, 0.4).
```
```
Each of the two systems has a single trajectory leading to (1, 0.4), as shown in
```
Figure 10.4.
```
Figure 10.4. Optimal trajectories to terminal point (ğ‘¥ğ‘, ğ‘¦ğ‘) =
```
```
(0.4, 1.0) using ğ‘¢ = +1 or ğ‘¢ = âˆ’1.
```
Initial points to the right of these trajectories will flow downwards with ğ‘¢ =
âˆ’1 until they hit the lower incoming ğ‘¢ = +1 trajectory, and they switch to that
trajectory. Initial points to the left will flow with ğ‘¢ = +1 until they hit and switch
to the upper incoming ğ‘¢ = âˆ’1 trajectory. These solutions are depicted in Figure
10.5
10.4. Stationarity 163
```
Figure 10.5. Optimal trajectories to terminal point (ğ‘¥ğ‘, ğ‘¦ğ‘) =
```
```
(0.4, 1.0) switching between ğ‘¢ = +1 and ğ‘¢ = âˆ’1.
```
Note that all possible starting points are accounted for in this method. For
```
any endpoint (ğ‘¥ğ‘, ğ‘¦ğ‘), we can take any starting point (ğ‘¥ğ‘, ğ‘¦ğ‘) and connect the two
```
with a time minimizing trajectory.
10.4 Stationarity
Controls inside the range of allowable controls are called stationary if ğœ•ğ»ğœ•áµ† = 0. In some
cases these controls have ğœ•2ğ»ğœ•áµ†2 = 0, and this can happen when ğ» is linear in ğ‘¢. Such
controls can be optimal, or they can be used as segments or â€œholding patternsâ€ in an
overall optimal trajectory as demonstrated in the following example.
Example 10.4: Fisheries
Suppose that the size of a fish population ğ‘¥ is modeled by the logistic equation
```
with growth and carrying capacity normalized to one, ğ‘¥â€² = ğ‘¥(1 âˆ’ ğ‘¥). We can
```
control the rate ğ‘¢ at which we harvest the fish, to get the system
```
ğ‘¥â€² = ğ‘¥(1 âˆ’ ğ‘¥) âˆ’ ğ‘¢
```
with restriction 0 â‰¤ ğ‘¢ â‰¤ 1.
We have a fixed time period 0 â‰¤ ğ‘¡ â‰¤ ğ‘‡ and we want to maximize harvest
âˆ«
ğ‘‡
0
ğ‘¢ ğ‘‘ğ‘¡
while matching the endpoint conditions
```
ğ‘¥(0) = ğ´ < 1 and ğ‘¥(ğ‘‡) = ğµ < 1.
```
164 Chapter 10. Switching Controls and Stationarity
The Hamiltonian is
```
ğ» = ğ‘¢ + ğœ†(ğ‘¥(1 âˆ’ ğ‘¥) âˆ’ ğ‘¢)
```
with
ğœ•ğ»
ğœ•ğ‘¢ = 1 âˆ’ ğœ† and
ğœ•2ğ»
ğœ•ğ‘¢2 = 0.
Here ğ» is linear in ğ‘¢ and thus has no local maxima or minima. If ğœ•ğ»ğœ•áµ† â‰  0, we
would want to take ğ‘¢ as an extreme, yielding a bang-bang type control. If ğœ•ğ»ğœ•áµ† < 0,
we would take the minimum value ğ‘¢ = 0 and not harvest. If ğœ•ğ»ğœ•áµ† > 0, we would
take the maximum harvest rate ğ‘¢ = 1.
However, a control which maintains ğœ•ğ»ğœ•áµ† = 0 could also be a candidate for
optimization. In this case we would need the costate identically equal to one,
```
ğœ† â‰¡ 1, to hold ğœ•ğ»ğœ•áµ† = 0 (âˆ— why? âˆ—). Here every value of ğ‘¢ is maximal/minimal,
```
and the conditions of Principle VII would still be satisfied.
```
The costate equation is ğœ†â€² = âˆ’ ğœ•ğ»ğœ•ğ‘¥ = âˆ’ğœ†(1 âˆ’ 2ğ‘¥), and for a constant value
```
```
ğœ† â‰¡ 1 we would need a constant value ğ‘¥ â‰¡ 12 . With ğ‘¥â€² = ğ‘¥(1 âˆ’ ğ‘¥) âˆ’ ğ‘¢, fixing
```
ğ‘¥ = 12 requires ğ‘¢ â‰¡ 14 .
```
This makes intuitive sense: ğ‘¥ = 12 is where the growth rate ğ‘¥â€² = ğ‘¥(1 âˆ’ ğ‘¥)
```
is maximized: the resource is producing fish at the highest possible rate, and we
are harvesting at a rate ğ‘¢ = 1/4 to maintain this maximum production.
Note the structure: as long as ğœ† = 1, any value of ğ‘¢ will satisfy optimality.
However, only ğ‘¢ â‰¡ 1/4 will keep ğœ† = 1. This is similar to the structure of mixed
Nash equilibria in game theory.
It follows that in order to maximize overall production, our control must
always be in one of three states:
â€“ Maximum harvest: ğ‘¢ = 1,
â€“ Minimum harvest: ğ‘¢ = 0,
â€“ Stationary harvest: ğ‘¢ = 14 with ğ‘¥ = 12 .
These are our only options with which to build an optimal control strategy.
For example, suppose ğ‘‡ = 8 and ğ´ = ğµ = 7/8. The structure of the solution
is intuitively clear: begin with a maximal harvest rate ğ‘¢ = 1 until the system
reaches a state of maximum production at ğ‘¥ = 1/2. Then switch to stationary
```
harvest ğ‘¢ = 1/4 for as long as you can. Since you have to match ğ‘¥(8) = 7/8, you
```
would switch out of stationary harvest to no harvest, ğ‘¢ = 0, just in time to let the
```
system recover to the specified end value. Optimal state ğ‘¥(ğ‘¡) and control ğ‘¢(ğ‘¡) are
```
plotted in Figure 10.6.
10.4. Stationarity 165
Figure 10.6. State and control for optimal solution plotted
against time.
These three time periods are calculated as follows. For the initial maximum
```
harvest time period, solve ğ‘¥â€² = ğ‘¥(1 âˆ’ ğ‘¥) âˆ’ 1 with ğ‘¥(0) = 7/8 to get (a symbolic
```
```
processor is recommended)
```
```
ğ‘¥(ğ‘¡) = 12 âˆ’ âˆš32 tan( âˆš32 ğ‘¡ âˆ’ arctan( âˆš34 ))
```
```
= 0.5 âˆ’ (0.8660 . . . ) tan(âˆ’(0.4086 . . . ) + (0.8660 . . . )ğ‘¡).
```
```
Set this equal to a half, ğ‘¥(ğ‘¡) = 1/2, and solve to approximate ğ‘‡1 = 0.472 . . . as the
```
end of the first time period.
```
Next, consider the ending time period. Solve ğ‘¥â€² = ğ‘¥(1 âˆ’ ğ‘¥) with ğ‘¥(8) = 7/8
```
to get
```
ğ‘¥(ğ‘¡) = 7ğ‘’
```
ğ‘¡
ğ‘’8 + 7ğ‘’ğ‘¡ .
```
Set this equal to a half, ğ‘¥(ğ‘¡) = 1/2, and solve to get ğ‘‡2 = 6.054 . . . as the beginning
```
of the zero harvest time period.
So the solution is to operate at ğ‘¢ = 1 for 0 â‰¤ ğ‘¡ â‰¤ ğ‘‡1, then ğ‘¢ = 1/4 for
ğ‘‡1 â‰¤ ğ‘¡ < ğ‘‡2, and finally ğ‘¢ = 0 for ğ‘‡2 â‰¤ ğ‘¡ â‰¤ 1.
We arrived at this three-phase solution intuitively given our three choices
of control values. However, sometimes intuition misleads. One can puzzle out
each of the possibilities and more conclusively arrive at this three-phase solution
being the only viable choice.
For example, at the beginning of the time period, we donâ€™t have ğ‘¥ = 1/2 so
the stationary ğ‘¢ = 1/4 solution is excluded. The ğ‘¢ = 0 solution would only be
```
optimal if ğœ•ğ»ğœ•áµ† < 0, which would require ğœ† > 1. But then with ğœ†â€² = âˆ’ğœ†(1 âˆ’ 2ğ‘¥),
```
```
ğ‘¥â€² = ğ‘¥(1 âˆ’ ğ‘¥), ğœ† > 1, and ğ‘¥ > 1/2, we would have both ğœ† and ğ‘¥ increasing, main-
```
taining ğœ•ğ»ğœ•áµ† < 0, implying that we would always have ğ‘¢ = 0 and never harvest any
```
fish. This cannot be optimal, and we would overshoot the endpoint ğ‘¥(8) = 7/8.
```
166 Chapter 10. Switching Controls and Stationarity
We conclude that our only option is to start with ğ‘¢ = 1. We cannot keep ğ‘¢ = 1,
otherwise we would completely deplete the fish population and hence be unable
```
to get back up to ğ‘¥(8) = 7/8, and so we must switch away from ğ‘¢ = 1 at some
```
point. Continuing with this type of reasoning we can conclude that our three-
period solution that we constructed intuitively from our three choices of control
value is in fact our only logical choice.
Key Points
In this chapter we explored cases where the range of our control is restricted and the
optimal control occurs at the endpoint of the allowed range. Finding solutions is still
guided by looking for optimal values of the Hamiltonian, but in these cases we find the
optimum at endpoints of allowed control values. In many of these cases the Hamilton-
ian is linear in the control variable.
This method introduces interesting cases where the optimal control will instan-
taneously switch from one end of the range to the other. Such bang-bang solutions
are analyzed geometrically and often involve stitching together phase portraits from
extreme controls and piecing together trajectories to match endpoint conditions.
Stationary solutions are where the Hamiltonian has a degenerate critical point,
neither a local max nor a local min, and can form part of an overall optimal solution.
Exercises
```
Exercise 10.1(hs). Complete the calculations for the Big Silver Rocket Ship problem
```
```
(Example 10.2). You have a rocket that weighs 1,000 kg without any fuel and that has
```
exhaust velocity 800 m/sec and maximum burn rate of 60 kg/sec. You need to get this
rocket up to 25 km with a minimum amount of fuel.
Use the simplified model in Examples 10.1 and 10.2 for ğ‘¦â€², ğ‘£â€², and ğ‘šâ€², and ğœ‡ = 800,
and the gravitational coefficient ğ‘” = 9.8.
A good approach is to consider starting with ğ¾ kg of fuel and implement the â€œmax
burn all the fuel then coastâ€ strategy, and ask how high the rocket will go. This gives
max height as a function of ğ¾, which you can solve for the required 25 km height. Here
are the suggested steps:
```
(a) Show that if you are at height ğ‘¦1, traveling at vertical velocity ğ‘£1, and you coast
```
```
with burn rate ğ›½ = 0, you will reach a maximum height of ğ‘¥1 + ğ‘£21/(2ğ‘”) at the apex.
```
```
(b) Suppose you start with an amount ğ¾ kg of fuel, so your initial mass is ğ‘š0 =
```
1,000 + ğ¾, and suppose you burn this fuel at the maximum rate of ğ›½ = 60 kg/sec. Solve
the differential equations from Example 10.1 to determine your height and vertical
speed when this fuel is depleted at ğ‘‡1 = ğ¾/60 seconds.
```
(c) Combine parts (a) and (b) to express the apex height as a function of the amount
```
of fuel ğ¾. Numerically approximate where this function hits 25,000 m.
Exercises 167
```
Exercise 10.2(s). You control the rate ğ‘¢(ğ‘¡) â‰¥ 0 in gal/min that water pours into a
```
reservoir of unlimited capacity, and you have a maximum pour rate of ğ‘¢ â‰¤ 200 gal/min.
```
The reservoir leaks; the amount of water ğ‘¥(ğ‘¡) in the reservoir satisfies ğ‘¥â€² = âˆ’.1ğ‘¥ + ğ‘¢.
```
The reservoir is empty and must be filled to 1,000 gal in 60 minutes.
```
(a) How do you operate the control so as to use the least amount of water? Guess
```
the answer first and then see if the techniques in this chapter support your guess. Can
```
you show a single switch? Can you show the direction of the switch (on to off, or off to
```
```
on)?
```
```
(b) How do you use the control to maximize that average amount of water in the
```
```
reservoir over the one-hour period (you still are required to have 1,000 gal at the end
```
```
of the hour)?
```
```
Exercise 10.3. Rocket Race on Ice I. Consider the Rocket Race problem (Example 10.3)
```
in the case of no friction ğ‘¥â€³ = ğ‘¢ and where burn rates are limited to |ğ‘¢| â‰¤ 1.
```
(a) Construct some trajectories just using ğ‘¢ = Â±1. In particular, for starting loca-
```
```
tion (âˆ’1, 2) and end location (1, 2), construct trajectories that do the following:
```
```
(i) Have one switch from ğ‘¢ = +1 to ğ‘¢ = âˆ’1.
```
```
(ii) Have one switch from ğ‘¢ = âˆ’1 to ğ‘¢ = +1.
```
```
(iii) Have multiple switches, say, from ğ‘¢ = +1 to âˆ’1 to +1, or something.
```
```
(b) Of the trajectories you constructed in part (a), which one has minimal time?
```
```
(c) Construct the Hamiltonian ğ» assuming you want to minimize total time ğ‘‡ and
```
show it is linear in ğ‘¢, and so this is likely to be a bang-bang situation.
```
(d) Does Principle VII indicate that an optimal trajectory will have a single switch?
```
Does it indicate the direction of the switch?
```
Exercise 10.4. Rocket Race on Ice II. Consider the Rocket Race problem (Example 10.3)
```
in the case of no friction ğ‘¥â€³ = ğ‘¢, where you want to minimize total time ğ‘‡ and where
burn rates are limited to |ğ‘¢| â‰¤ 1.
```
(a) Suppose your starting location is (0, 2) in the position/velocity phase plane.
```
```
What points (ğ‘¥ğ‘, ğ‘¦ğ‘) can you reach with a constant ğ‘¢ = 1? What points can you reach
```
with a single switch from ğ‘¢ = 1 to ğ‘¢ = âˆ’1?
```
(b) What points can you reach from (0, 2) with a constant ğ‘¢ = âˆ’1? What points
```
can you reach with a single switch from ğ‘¢ = âˆ’1 to ğ‘¢ = 1?
```
(c) Generalize your results. What is the optimal trajectory between any two points
```
```
(ğ‘¥ğ‘, ğ‘¦ğ‘) and (ğ‘¥ğ‘, ğ‘¦ğ‘).
```
```
Exercise 10.5(s). Rocket Race on Ice III. Consider the Rocket Race problem (Example
```
```
10.3) in the case of no friction ğ‘¥â€³ = ğ‘¢, where you want to minimize total time ğ‘‡ and
```
where burn rates are limited to |ğ‘¢| â‰¤ 1.
168 Chapter 10. Switching Controls and Stationarity
```
Suppose that from any starting point, you want to get to the origin (0, 0) in minimal
```
time. The set of optimal trajectories is plotted in Figure 10.7.
Figure 10.7. Soft landing for the Rocket Race on Ice.
```
(a) Explain this figure. In what region is ğ‘¢ = +1? In what region is ğ‘¢ = âˆ’1?
```
```
(b) For an initial point (ğ‘¥0, ğ‘¦0) in the region with ğ‘¢ = âˆ’1, show that it takes ğ‘¡ = ğ‘¦0
```
time units to reach the horizontal axis. At what ğ‘¥ value does the trajectory hit the ğ‘¥-
axis?
```
(c) For an initial point (ğ‘¥0, 0) on the positive ğ‘¥-axis, how much time does it take
```
```
to reach the origin (0, 0)?
```
```
(d) Combine results (b) and (c) to derive the minimal time required to reach the
```
```
origin from any initial point (ğ‘¥0, ğ‘¦0).
```
```
(e) Can you compute the travel time between any two points (ğ‘¥0, ğ‘¦0) and (ğ‘¥1, ğ‘¦1)?
```
```
Exercise 10.6(h). Fisheries. In the fisheries model, Example 10.4, we have ğ‘¥â€² =
```
```
ğ‘¥(1 âˆ’ ğ‘¥) âˆ’ ğ‘¢ and ğ½ = âˆ«ğ‘‡0 ğ‘¢ ğ‘‘ğ‘¡. Suppose you have an 8-year time interval, ğ‘‡ = 8,
```
and the following conditions.
```
(a) The allowed harvesting rate was rather low, 0 â‰¤ ğ‘¢ â‰¤ .15. What would your
```
```
optimal control be for endpoint conditions ğ‘¥(0) = .85, ğ‘¥(8) â‰¥ .85? How about ğ‘¥(0) =
```
```
.75, ğ‘¥(8) â‰¥ .75?
```
```
(b) Suppose 0 â‰¤ ğ‘¢ â‰¤ 1 and the endpoint condition is ğ‘¥(8) = .85, but the initial fish
```
```
population is low, ğ‘¥(0) = .25. Would you want to wait until ğ‘¥ = .5 to begin harvesting,
```
or would it make sense to start harvesting before ğ‘¥ reaches .5?
Exercises 169
```
(c) Suppose 0 â‰¤ ğ‘¢ â‰¤ 1 and the initial condition is ğ‘¥(0) = .85, but the end condition
```
```
ğ‘¥(8) â‰¥ 0 is free. Would you want to harvest all the fish? Or would you just keep
```
harvesting at ğ‘¢ = 1/4? Or something in between?
```
Exercise 10.7(h). Rocket Race on Ice IV. Consider the Rocket Sled on Ice problem, ğ‘¥â€³ =
```
ğ‘¢ with |ğ‘¢| â‰¤ 1, where we want to transition from ğ‘¥ = âˆ’2, ğ‘¦ = 0 to ğ‘¥ = 2, ğ‘¦ = 0. We
can conclude from previous exercises that the minimum time path requires ğ‘‡ = 4 time
units with control ğ‘¢ = 1 for 0 â‰¤ ğ‘¡ < 2 and ğ‘¢ = âˆ’1 for 2 â‰¤ ğ‘¡ â‰¤ 4.
Now suppose we have a fixed time ğ‘‡ â‰¤ 4, and we still have limits |ğ‘¢| â‰¤ 1, but now
we want to minimize total fuel burn
ğ½ = âˆ«
ğ‘‡
0
|ğ‘¢| ğ‘‘ğ‘¡.
```
(a) Show that the Hamiltonian, as a function of ğ‘¢, has the form ğ» = |ğ‘¢| + ğ´ + ğµğ‘¢.
```
```
(b) What are the possible minima for a function ğ» in this form over |ğ‘¢| â‰¤ 1? How
```
does the minimum value depend on ğ´ and ğµ? Draw pictures.
```
(c) Show that for our Hamiltonian, the coefficient ğµ is linear in time ğ‘¡. From this,
```
argue that any optimal control will have at most two switches with ğ‘¢ switching âˆ’1 â†’
0 â†’ +1 or +1 â†’ 0 â†’ âˆ’1.
```
(d) With ğ‘¥â€² = ğ‘¦, sketch some representative solutions for various values of ğ‘‡ in
```
```
the (ğ‘¥, ğ‘¦)-plane.
```
Exercise 10.8. This computational exercise emphasizes that the Pontryagin conditions
are necessary, not sufficient, for an optimal solution. It is important to spend some time
exploring a problem to make sure that naive application of Pontryagin does not produce
a suboptimal solution.
In Example 7.3 we have a controlled system ğ‘¥â€² = ğ‘¢ with a specific end time ğ‘‡ and
```
endpoint conditions ğ‘¥(0) = 1 and ğ‘¥(ğ‘‡) = 2 with performance ğ½ = âˆ«ğ‘‡012 (ğ‘¥2 âˆ’ ğ‘¢2) ğ‘‘ğ‘¡
```
to be maximized. Applying necessary conditions for optimality, we get ğ‘¢ = ğœ† and
solutions
```
ğ‘¥ = ğ‘¥0 cos(ğ‘¡) + ğœ†0 sin(ğ‘¡),
```
```
ğœ† = âˆ’ğ‘¥0 sin(ğ‘¡) + ğœ†0 cos(ğ‘¡).
```
```
(a) Show that for ğ‘‡ = 3ğœ‹/2 we can match the endpoint conditions with ğ‘¥0 = 1
```
and ğœ†0 = âˆ’2 with ğ½ = âˆ’2.
```
(b) Now for ğ‘‡ = ğœ‹/2, show that you can match the following endpoints, each case
```
with a payoff of zero:
```
ğ‘¥(0) = 1, ğ‘¥( ğœ‹2 ) = 0, ğ½ = 0,
```
```
ğ‘¥(0) = 0, ğ‘¥( ğœ‹2 ) = 0, ğ½ = 0,
```
```
ğ‘¥(0) = 0, ğ‘¥( ğœ‹2 ) = 2, ğ½ = 0.
```
```
Use this to argue that we can improve upon the solution from part (a) by using a switch-
```
ing control.
170 Chapter 10. Switching Controls and Stationarity
```
(c) Repeat part (b) for the given allowed times and endpoints, and verify the cor-
```
responding payoffs for 0 < ğœ < ğœ‹:
```
ğ‘¥(0) = 1, ğ‘¥( ğœ‹2 ) = 0, ğ½ = 0,
```
```
ğ‘¥(0) = 0, ğ‘¥(ğœ‹ âˆ’ ğœ) = 0, ğ½ = 0,
```
```
ğ‘¥(0) = 0, ğ‘¥(ğœ) = 2 , ğ½ = âˆ’2 cot(ğœ).
```
```
Use this to argue that you can use switching controls to match ğ‘¥(0) = 1 and ğ‘¥(3ğœ‹/2) = 2
```
and get an arbitrarily large payoff.
```
(d) Use these techniques to show that you can attain an arbitrarily large payoff
```
```
and match ğ‘¥(0) = 1 and ğ‘¥(ğ‘‡) = 2 for any allowed time ğ‘‡ > ğœ‹.
```
```
Exercise 10.9(h). Soft-Landing Oscillator. This is a classic exercise in optimal control.
```
You control a frictionless oscillator ğ‘¥â€³ = âˆ’ğ‘¥ by exerting a force ğ‘¢, making ğ‘¥â€³ = âˆ’ğ‘¥ + ğ‘¢
```
with bounds |ğ‘¢| â‰¤ 1. The oscillator starts at position ğ‘¥(0) and velocity ğ‘¦(0) and you
```
```
need to bring it to rest ğ‘¥(ğ‘‡) = ğ‘¦(ğ‘‡) = 0 in minimal time ğ‘‡.
```
The position/velocity phase portrait for the solution is shown in Figure 10.8.
Figure 10.8. Minimum time soft landing for a simple oscillator with
bounded acceleration control.
Justify this solution with the following steps.
```
(a) Show this system is bang-bang. Sketch the phase portraits for ğ‘¢ = 1 and ğ‘¢ =
```
âˆ’1.
```
(b) For ğ‘¢ = Â±1, find orbits that terminate at (0, 0). Which parts of these orbits
```
would be part of an optimal trajectory?
```
(c) Show that optimal trajectories will switch controls every ğœ‹ time units.
```
```
(d) Complete the picture.
```
Exercises 171
```
Exercise 10.10(h). Youâ€™re sitting in a room at 70âˆ˜ğ¹, you have 2 oz of cream stored in
```
a refrigerator at 40âˆ˜ğ¹, and you have a boiling hot 10 oz cup of coffee sitting in front
```
of you with temperature ğ‘¥(ğ‘¡) that satisfies ğ‘¥â€² = ğ‘˜(70 âˆ’ ğ‘¥) for some cooling coefficient
```
ğ‘˜ > 0. How should you add the cream in order to bring the coffee down to a drinkable
temperature, say, 150âˆ˜ğ¹, in a minimum time?
11
Time, Value, and
Hamilton-Jacobi-Bellman
Equation
Pontryaginâ€™s principles allow us to find necessary conditions for an optimal control
given a performance measure, starting position, and end conditions. The principles
impose a dynamic structure on Lagrange multipliers to create a state-costate system of
ordinary differential equations.
The Hamilton-Jacobi-Bellman approach is another way to address the problem
using partial derivatives and has deep historical roots that predate Pontryagin. The
key idea for continuous systems is to consider the optimal performance as a function of
starting location and time and to derive a partial differential equation for this function.
This can be interpreted as an extension of Bellmanâ€™s dynamic programming for discrete
```
systems (look this up). The approach is quite beautiful. The drawback is that partial
```
differential equations are hard to work with. This chapter explores the basic concepts
of the theory and derives the Hamilton-Jacobi-Bellman equation for one-dimensional
systems.
11.1 Time
Minimal time problems consider optimal controls that minimize time, as explored in
```
the Zermelo examples (Examples 8.4, 8.5, 9.2, 9.6), the brachistochrone examples (Ex-
```
```
amples 8.6 9.5), and the Rocket Race example (Example 10.3).
```
```
These problems are unique in that the Principle of Optimality (Section 5.2) applies
```
directly to state space. Trajectories donâ€™t typically intersect, and we can think of travel
time as a function of location, as we explore in the following revisit to the brachis-
tochrone problem.
173
174 Chapter 11. Time, Value, and HJB Equation
Example 11.1: Brachistochrone Time
The brachistochrone problem was explored in Example 8.6 as the minimal time
```
path for a bead to slide from (0, 0) to (ğ‘¥1, ğ‘¦1) in a vertical plane under the influence
```
of gravity. We control the motion of the bead by the angle of descent ğœƒ, and the
optimal paths turn out to be cycloids.
```
Given a terminal point (ğ‘¥1, ğ‘¦1) we can compute the optimal path and de-
```
```
termine the minimum time required to reach that point from the origin (0, 0).
```
```
We could then consider this minimal time as a function Ëœğ‘‡(ğ‘¥1, ğ‘¦1) of the terminal
```
```
point (ğ‘¥1, ğ‘¦1). What would this function look like?
```
Turning this around, if we had exactly ğ‘‡ seconds, where could we get to from
```
(0, 0) using these optimal paths? This would be a level curve, or isochrone, of the
```
```
minimal time function Ëœğ‘‡(ğ‘¥1, ğ‘¦1), several of which are plotted in Figure 11.1.
```
```
Figure 11.1. Equal time curves for brachistochrone paths from (0, 0).
```
```
Dropping straight down to an endpoint of the form (0, ğ‘¦1) would be the far-
```
```
thest one could get away from (0, 0) in a given time ğ‘‡, and these curves have a
```
terminal angle of ğœƒğ‘‡ = âˆ’ğœ‹/2. Other terminal points along the isochrone have
terminal angles âˆ’3ğœ‹/2 < ğœƒğ‘‡ â‰¤ ğœ‹/2. Terminal points on the positive ğ‘¥-axis have
```
ğœƒğ‘‡ = ğœ‹/2 and have the shortest final distance from (0, 0) for a given time along
```
an optimal path.
Keep in mind that these are optimal paths, and so they represent the outer
limit of where we could go. If we had, say, two time units it seems intuitive that
we could reach any point inside the finite region in the lower half-plane bounded
by the ğ‘‡ = 2 isochrone by using a less efficient path. But we certainly could not
reach any point outside this region in two time units.
Computing the function Ëœğ‘‡ and these isochrones is challenging. The point
```
is that every point (ğ‘¥1, ğ‘¦1) of the vertical plane has an associated value Ëœğ‘‡(ğ‘¥1, ğ‘¦1)
```
that represents the minimum amount of time required to slide from the origin to
that point using gravity.
11.1. Time 175
The above example considered how far we could travel in a given amount of time.
The following examples are time-to-target problems, which consider how long it takes
to reach a specific end condition.
```
These problems involve a system ğ‘¥â€² = ğ‘“1(ğ‘¥, ğ‘¦, ğ‘¢), ğ‘¦â€² = ğ‘“2(ğ‘¥, ğ‘¦, ğ‘¢), a set of allowed
```
```
controls ğ’°, a specified target such as a point or a curve ğ’, and a starting point (ğ‘¥0, ğ‘¦0).
```
The objective is to find the allowed control that transitions from the starting point to
the target in the least amount of time. For these systems, we want to compute the
```
minimum time Ëœğ‘‡(ğ‘¥, ğ‘¦) to reach the target from a general starting point (ğ‘¥, ğ‘¦).
```
Time minimizing paths have the unique property that every time unit spent on the
path reduces the remaining time by one time unit. In a perfect world, if you are 1 hour
into a 3-hour trip, you must have 2 hours left. Otherwise something has gone off track.
```
More precisely, as explored in Example 8.5, if a minimal time trajectory (ğ‘¥(ğ‘¡), ğ‘¦(ğ‘¡)) takes
```
ğ‘‡ time units to traverse from point ğ´ to point ğ¶ and if at time ğ‘¡ = 1 the trajectory is at
location ğµ, then it must take ğ‘‡ âˆ’ 1 time units to traverse from ğµ to ğ¶.
On a minimal time path, every hour reduces the remaining time by one hour. Ev-
ery second reduces the remaining time by one second. Every microsecond. . . , and so
on, and the concept applies all the way down to the limiting infinitesimal level.
The remaining time on a minimal path decreases at a rate of one time unit per time
unit,
ğ‘‘
ğ‘‘ğ‘¡
```
Ëœğ‘‡(ğ‘¥(ğ‘¡), ğ‘¦(ğ‘¡)) = âˆ’1.
```
In fact, minimizing paths are uniquely characterized by this property. Think about
```
it: the time minimizing path must have this property, but no other path will. The
```
following example explores this concept.
Example 11.2: Rocket Race Time
```
Consider the Rocket Sled with friction and bounded control (Example 10.3):
```
ğ‘¥â€² = ğ‘¦,
ğ‘¦â€² = âˆ’ğ‘¦ + ğ‘¢
Figure 11.2. Time minimizing trajectories to bring the rocket
sled to zero velocity, ğ‘¦ = 0.
176 Chapter 11. Time, Value, and HJB Equation
with control ğ‘¢ restricted by |ğ‘¢| â‰¤ 1, and we want to minimize travel time between
```
a specified starting location (ğ‘¥0, ğ‘¦0) and the horizontal line ğ‘¦ = 0. The solution
```
is to use ğ‘¢ â‰¡ âˆ’1 if ğ‘¦0 > 0 and ğ‘¢ â‰¡ +1 if ğ‘¦0 < 0, as shown in Figure 11.2.
```
Given a starting point (ğ‘¥0, ğ‘¦0) with ğ‘¦0 > 0 we have ğ‘¢ â‰¡ âˆ’1 and the solution
```
```
trajectory (âˆ— check this âˆ—)
```
```
ğ‘¥(ğ‘¡) = ğ‘¥0 + ğ‘¦0 âˆ’ ğ‘¡ + 1 âˆ’ ğ‘’âˆ’ğ‘¡(ğ‘¦0 + 1),
```
```
ğ‘¦(ğ‘¡) = âˆ’1 + ğ‘’âˆ’ğ‘¡(ğ‘¦0 + 1).
```
```
We solve this for ğ‘¦(ğ‘¡) = 0 to find ğ‘¡ = ln(1 + ğ‘¦0). A similar argument for
```
ğ‘¦0 < 0 allows us to conclude that our time function is
```
Ëœğ‘‡(ğ‘¥, ğ‘¦) = ln(1 + |ğ‘¦|).
```
Note that we dropped the subscripts from the starting locationâ€”we want to start
thinking of this as a general function of ğ‘¥ and ğ‘¦.
```
We note that Ëœğ‘‡(ğ‘¥, ğ‘¦) = ln(1 + |ğ‘¦|) satisfies Ëœğ‘‡(ğ‘¥, 0) = 0. Next we check that
```
ğ‘‘
```
ğ‘‘ğ‘¡ Ëœğ‘‡ = âˆ’1. For an optimal trajectory in the upper half-plane ğ‘¦(ğ‘¡) > 0
```
ğ‘‘
```
ğ‘‘ğ‘¡ Ëœğ‘‡(ğ‘¥(ğ‘¡), ğ‘¦(ğ‘¡)) =
```
ğœ• Ëœğ‘‡
ğœ•ğ‘¥ ğ‘¥â€² +
ğœ• Ëœğ‘‡
ğœ•ğ‘¦ ğ‘¦â€²
```
= (0)(ğ‘¦) + ( 11+ğ‘¦ )(âˆ’ğ‘¦ âˆ’ 1)
```
= âˆ’1
and similarly for the lower half-plane.
Suppose now that our target is the line ğ‘¥ + ğ‘¦ = 0. Optimal solutions are
ğ‘¢ = 1 for ğ‘¥0 + ğ‘¦0 > 0 and ğ‘¢ = âˆ’1 for ğ‘¥0 + ğ‘¦0 < 0, as shown in Figure 11.3.
Figure 11.3. Time minimizing trajectories to bring the rocket
sled to the target condition ğ‘¥ + ğ‘¦ = 0.
We could calculate our travel time starting at ğ‘¥0, ğ‘¦0 as before, calculate the
trajectory, and express time as a function of starting point.
But in this case a much easier solution presents itself. Consider the function
```
Ëœğ‘‡(ğ‘¥, ğ‘¦) = |ğ‘¥ + ğ‘¦|.
```
11.1. Time 177
For ğ‘¥ + ğ‘¦ > 0 we have
ğ‘‘
ğ‘‘ğ‘¡
```
Ëœğ‘‡(ğ‘¥(ğ‘¡), ğ‘¦(ğ‘¡)) = ğ‘¥â€²(ğ‘¡) + ğ‘¦â€²(ğ‘¡) = âˆ’1
```
and on the target
```
Ëœğ‘‡(ğ‘¥, ğ‘¦) = 0 for ğ‘¥ + ğ‘¦ = 0.
```
These properties alone indicate that this must be the optimal travel time function
for ğ‘¥ + ğ‘¦ > 0. We arrive at this conclusion without doing any specific trajectory
calculations. We can do a similar verification for starting points with ğ‘¥ + ğ‘¦ < 0.
With a bit more effort, we can analyze the soft-landing problem where our
```
target is the point (0, 0), as shown in Figure 11.4 (see Exercise 10.5).
```
Figure 11.4. Time minimizing paths for the soft-landing con-
dition, with bang-bang control.
In this case the minimal time function for the ğ‘¢ = âˆ’1 region is
```
Ëœğ‘‡(ğ‘¥, ğ‘¦) = ln |ğ‘¦ + 1| + ln(
```
```
1 + âˆš1 âˆ’ (ğ‘¦ + 1)ğ‘’âˆ’(ğ‘¥+ğ‘¦)
```
```
1 âˆ’ âˆš1 âˆ’ (ğ‘¦ + 1)ğ‘’âˆ’(ğ‘¥+ğ‘¦)
```
```
) ,
```
```
as can be verified (with some effort. . . ) by showing ğœ• Ëœğ‘‡ğœ•ğ‘¡ (ğ‘¥(ğ‘¡), ğ‘¦(ğ‘¡)) = âˆ’1 in this
```
```
region and that Ëœğ‘‡(ğ‘¥, ğ‘¦) has the correct values on the incoming ğ‘¢ = +1 trajectory
```
```
to (0, 0).
```
```
The general rule is that any minimum time function Ëœğ‘‡(ğ‘¥, ğ‘¦) for a given target must
```
```
satisfy two properties: ğ‘‘ğ‘‘ğ‘¡ Ëœğ‘‡(ğ‘¥(ğ‘¡), ğ‘¦(ğ‘¡)) = âˆ’1 on minimal paths and Ëœğ‘‡ = 0 on the tar-
```
get set. The cool thing is that the converse is true: any function that satisfies these
two properties must be a minimal time function. We have now lifted our optimization
analysis from considering individual trajectories to specifying necessary properties of
a spatial function.
In the above Rocket Race example we had closed form solutions for a few carefully
selected target sets. There may not be a nice closed form solution for other target sets,
and the minimal time function would have to be numerically approximated. This type
of reasoning would be under the general topic of nonlinear first-order partial differen-
tial equations.
178 Chapter 11. Time, Value, and HJB Equation
11.2 Performance
The differential structures and concepts of the previous section extend to more general
optimization problems.
```
In a basic optimal control problem we are given a dynamical system ğ±â€² = ğ‘“(ğ±, ğ‘¢, ğ‘¡),
```
a starting location ğ±0, some form of endpoint conditions, and a performance criteria ğ½
that we want to maximize or minimize over all allowable controls ğ‘¢ âˆˆ ğ’°.
We then compute conditions necessary for the control to be optimal and arrive at
our maximum or minimum value for ğ½, which we will denote asÌƒ ğ½. That is,Ìƒ ğ½ is the best
performance we can achieve given the constraints of the problem. This value can be
expressed as a function of whatever subset of the constraints we want to consider. For
example, the minimal cost of bringing a Rocket Sled to a soft landing in a given amount
of time depends of the initial position and velocity of the sled and the amount of time
we are allowed.
Example 11.3: Rocket Sled Performance
Consider the Rocket Sled with friction ğ‘¥â€² = ğ‘¦, ğ‘¦â€² = âˆ’ğ‘¦ + ğ‘¢ and performance
ğ½ = âˆ«ğ‘‡0 ğ‘¢2 ğ‘‘ğ‘¡. Suppose we want the minimum cost to move from a given initial
```
position and velocity (ğ‘¥0, ğ‘¦0) to (0, 0) in exactly ğ‘‡ = 1 time units. This minimum
```
```
cost would be a function,Ìƒ ğ½(ğ‘¥0, ğ‘¦0), of starting position (ğ‘¥0, ğ‘¦0).
```
Figure 11.5. Performance values for optimal trajectories from
some representative starting points.
Some representative trajectories and associated costs are plotted in Figure
11.5. Note that trajectories starting in the second quadrant travel directly towards
```
(0, 0) and have lower cost than those starting in the first quadrant which have to
```
change direction. To get a better sense of how cost relates to starting position, we
11.2. Performance 179
could examine all starting positions that have a minimal cost of, say,Ìƒ ğ½ = 1, as
represented in Figure 11.6. Under this condition of unit cost, trajectories starting
in the second quadrant can start from farther away than trajectories in the first
quadrant.
Figure 11.6. The set of starting points with a performance
value ofÌƒ ğ½ = 1.
What we really want is to compute minimal cost as a function of starting
```
position,Ìƒ ğ½(ğ‘¥0, ğ‘¦0). To compute this function, recall our solutions from Examples
```
8.1 and 8.2:
ğ‘¥ = âˆ’ 12 ğ‘ğ‘¡ âˆ’ 14 ğ‘ğ‘’ğ‘¡ âˆ’ ğ‘ğ‘’âˆ’ğ‘¡ + ğ‘‘,
ğ‘¦ = âˆ’ 12 ğ‘ âˆ’ 14 ğ‘ğ‘’ğ‘¡ + ğ‘ğ‘’âˆ’ğ‘¡,
```
ğ‘¢ = âˆ’ 12 (ğ‘ + ğ‘ğ‘’ğ‘¡)
```
with four integration constants ğ‘, ğ‘, ğ‘, ğ‘‘.
Minimal cost is then
Ìƒğ½ = âˆ«
1
0
ğ‘¢2 ğ‘‘ğ‘¡ = âˆ«
1
0
1
```
4 (ğ‘ + ğ‘ğ‘’ğ‘¡)2 ğ‘‘ğ‘¡ =
```
1
```
8 (4ğ‘ğ‘(ğ‘’ âˆ’ 1) + ğ‘2(ğ‘’2 âˆ’ 1) + 2ğ‘2).
```
```
Conditions for the fixed end time ğ‘‡ = 1, fixed endpoint (0, 0), and variable start-
```
```
ing point (ğ‘¥0, ğ‘¦0) are
```
```
ğ‘¥(0) = ğ‘¥0 = âˆ’ 14 ğ‘ âˆ’ ğ‘ + ğ‘‘,
```
```
ğ‘¦(0) = ğ‘¦0 = âˆ’ 12 ğ‘ âˆ’ 14 ğ‘ + ğ‘,
```
```
ğ‘¥(1) = 0 = âˆ’ 12 ğ‘ âˆ’ 14 ğ‘ğ‘’ âˆ’ ğ‘ğ‘’âˆ’1 + ğ‘‘,
```
```
ğ‘¦(1) = 0 = âˆ’ 12 ğ‘ âˆ’ 14 ğ‘ğ‘’ + ğ‘ğ‘’âˆ’1.
```
180 Chapter 11. Time, Value, and HJB Equation
This is a set of four equations which are linear in the four unknowns ğ‘, ğ‘, ğ‘, ğ‘‘.
Solving yields
```
ğ‘ = âˆ’2 ğ‘¥0(ğ‘’ + 1) + ğ‘¦0(ğ‘’ âˆ’ 1)ğ‘’ âˆ’ 3 ,
```
```
ğ‘ = 4 ğ‘¥0(ğ‘’ âˆ’ 1) + ğ‘¦0(ğ‘’ âˆ’ 2)(ğ‘’ âˆ’ 3)(ğ‘’ âˆ’ 1) ,
```
```
ğ‘ = âˆ’ğ‘’ ğ‘¥0(ğ‘’ âˆ’ 1) + ğ‘¦0(ğ‘’ âˆ’ 3)(ğ‘’ âˆ’ 1) ,
```
```
ğ‘‘ = âˆ’2 ğ‘¥0(ğ‘’ âˆ’ 1) + ğ‘¦0(ğ‘’ âˆ’ 3)(ğ‘’ âˆ’ 1) .
```
Substituting these into our minimal cost function yields a paraboloid:
```
Ìƒğ½(ğ‘¥0, ğ‘¦0) = âˆ’ (ğ‘’
```
```
2 âˆ’ 1)ğ‘¥20 + 2(ğ‘’ âˆ’ 1)2ğ‘¥0ğ‘¦0 + (ğ‘’2 âˆ’ 4ğ‘’ + 5)ğ‘¦20
```
```
(ğ‘’ âˆ’ 3)(ğ‘’ âˆ’ 1) .
```
```
Figure 11.7. Level curves of optimal performanceÌƒ ğ½(ğ‘¥, ğ‘¦) as a
```
```
function of starting points.
```
```
This function tells us the minimum cost of moving from (ğ‘¥0, ğ‘¦0) to (0, 0), and
```
level curves are plotted in Figure 11.7. The function has lower values for starting
positions to the left of ğ‘¥ = 0 with positive velocity, and it has significantly greater
values for starting positions to the right of ğ‘¥ = 0 with positive velocity.
We solved this for a fixed end time ğ‘‡ = 1. We could likewise solve for any
given end time ğ‘‡ to get a function telling us the minimum cost of getting from
```
(ğ‘¥0, ğ‘¦0) to (0, 0) in ğ‘‡ time units:
```
```
Ìƒğ½(ğ‘¥0, ğ‘¦0, ğ‘‡) = (ğ‘’2ğ‘‡ âˆ’1)ğ‘¥20+2(ğ‘’ğ‘‡ âˆ’1)
```
```
2ğ‘¥0ğ‘¦0+(ğ‘’2ğ‘‡ âˆ’4ğ‘’ğ‘‡ +2ğ‘‡+3)ğ‘¦20
```
```
(ğ‘’ğ‘‡ âˆ’1)(ğ‘’ğ‘‡ (ğ‘‡âˆ’2)+ğ‘‡+2) . (11.1)
```
```
Note that this is a function of three variables, location (ğ‘¥0, ğ‘¦0) and allowed time ğ‘‡.
```
11.3. Hamilton-Jacobi-Bellman Equation 181
11.3 Hamilton-Jacobi-Bellman Equation
For a control system with a fixed target, optimal performance can be analyzed as a func-
tion of starting conditions and total time. As we saw for the time minimizing problems,
these performance functions have certain properties that uniquely define them.
Hamilton-Jacobi-Bellman theory considers more general performance functions,
```
such as equation (11.1) in the previous example, and specifies a partial differential
```
equation that such functions must satisfy.
```
In general, partial differential equations (PDEs) are hard to solve. We explore this
```
topic with some basic cases where closed form solutions are workable.
11.3.1 Value. The Hamilton-Jacobi-Bellman analysis relies on the concept of a value
function, which we develop as follows for a free end time ğ‘‡.
```
Suppose we have a controlled system ğ‘¥â€² = ğ‘“(ğ‘¥, ğ‘¢) with initial state ğ‘¥(0) = ğ‘¥0,
```
where we want to minimize cost
```
ğ½ = ğº(ğ‘¥(ğ‘‡)) + âˆ«
```
ğ‘‡
0
```
ğ‘”(ğ‘¥, ğ‘¢, ğ‘¡) ğ‘‘ğ‘¡
```
for a fixed ğ‘‡ and free endpoint.
The Pontryagin approach would be to form the Hamiltonian
```
ğ»(ğ‘¥, ğ‘¢, ğœ†, ğ‘¡) = ğ‘”(ğ‘¥, ğ‘¢, ğ‘¡) + ğœ†ğ‘“(ğ‘¥, ğ‘¢, ğ‘¡),
```
define the costate equations, find the optimal control that optimizes the Hamiltonian,
and incorporate boundary conditions as appropriate. All well and good.
```
Now suppose that instead of a prescribed initial location ğ‘¥(0) = ğ‘¥0, we simply
```
find ourselves at some location ğ‘¥ğ‘¡ at some time ğ‘¡ < ğ‘‡ and must optimize from that
point forward to the end time and location. Our payoff from that point forward with
```
ğ‘¥â€² = ğ‘“(ğ‘¥, ğ‘¢, ğ‘¡) could be defined as
```
```
ğ‘‰(ğ‘¥ğ‘¡, ğ‘¡) = mináµ† {ğº(ğ‘¥(ğ‘‡)) + âˆ«
```
ğ‘‡
ğ‘¡
```
ğ‘”(ğ‘¥, ğ‘¢, ğœ) ğ‘‘ğœ} with ğ‘¥â€² = ğ‘“(ğ‘¥, ğ‘¢, ğœ), ğ‘¥(ğ‘¡) = ğ‘¥ğ‘¡.
```
This is referred to as the value function of the optimization problem. So if we start at
```
ğ‘¡ = 0 with ğ‘¥(0) = ğ‘¥0, our minimum cost would be ğ‘‰(ğ‘¥0, 0). But ğ‘‰(ğ‘¥0, ğ‘¡) is much more
```
general, giving us our minimum cost from any starting point ğ‘¥ğ‘¡ at any time ğ‘¡.
Keep in mind that ğ‘‰ is defined for a fixed given end time ğ‘‡. A better notation may
```
be ğ‘‰ ğ‘‡ (ğ‘¥, ğ‘¡), but weâ€™ll stick with the standard notation.
```
Note how this value differs from our payoff function from Section 11.2. The payoff
```
functionÌƒ ğ½(ğ‘¥, ğ‘‡) is the optimal performance starting at location ğ‘¥ with an end time of ğ‘‡.
```
The value function assumes a fixed end time ğ‘‡ and is a measure of the remaining payoff
```
to be gained (or cost to be paid) if we are at location ğ‘¥ at time ğ‘¡. For the autonomous
```
```
case, where ğ‘“ and ğ‘” do not depend on time ğ‘¡, the two concepts are related as ğ‘‰(ğ‘¥, ğ‘¡) =Ìƒ
```
```
ğ½(ğ‘¥, ğ‘‡ âˆ’ ğ‘¡).
```
There are two things to note about ğ‘‰. First, at the end time ğ‘¡ = ğ‘‡ we must have
```
ğ‘‰(ğ‘¥, ğ‘‡) = ğº(ğ‘¥). This follows from the definition in that âˆ«ğ‘‡ğ‘‡ ğ‘” ğ‘‘ğ‘¡ = 0. If you find
```
yourself at location ğ‘¥ and you are out of time ğ‘¡ = ğ‘‡, then youâ€™re done and there is
```
nothing to be gained or lost beyond the final value ğº(ğ‘¥).
```
```
Second, ğ‘‰(ğ‘¥(ğ‘¡), ğ‘¡) is decreasing in time. Under optimal control, your remaining
```
costs have to decrease as you proceed.
182 Chapter 11. Time, Value, and HJB Equation
Although we are focusing on minimizing cost, the same applies if we were max-
imizing profit: you get a greater payoff if you start earlier in the process than if you
jump into the middle of the process. Your remaining payoff always decreases as you
proceed through a task.
```
To make this clear, suppose that ğ‘‡ = 2 and that ğ‘¢(ğ‘¡) is your optimal control that
```
```
generates a trajectory ğ‘¥(ğ‘¡). For simplicity, we can assume ğº(ğµ) = 0, although the
```
```
argument works without this assumption. Then using the Principle of Optimality (an
```
optimal trajectory from ğ´ to ğ¶ that passes through ğµ is also optimal from ğ´ to ğµ and
```
from ğµ to ğ¶) we have
```
```
âˆ«20 ğ‘”(ğ‘¥(ğœ), ğ‘¢(ğœ), ğ‘¡) ğ‘‘ğœ = âˆ«10 ğ‘”(ğ‘¥(ğœ), ğ‘¢(ğœ), ğœ) ğ‘‘ğœ + âˆ«21 ğ‘”(ğ‘¥(ğœ), ğ‘¢(ğœ), ğœ) ğ‘‘ğœ,
```
```
ğ‘‰(ğ‘¥(0), 0) = âˆ«10 ğ‘”(ğ‘¥(ğœ), ğ‘¢(ğœ), ğœ) ğ‘‘ğœ + ğ‘‰(ğ‘¥(1), 1).
```
```
Assuming a fixed time ğ‘‡, the function ğ‘‰(ğ‘¥(ğ‘¡), ğ‘¡) is the minimum cost starting from
```
```
ğ‘¥(ğ‘¡) at time ğ‘¡. Here ğ‘‰(ğ‘¥(0), 0) is the minimum cost from ğ‘¥(0) at time ğ‘¡ = 0 and
```
```
ğ‘‰(ğ‘¥(1), 1) is the minimum cost from ğ‘¥(1) at ğ‘¡ = 1. The term âˆ«10 ğ‘”(ğ‘¥(ğœ), ğ‘¢(ğœ), ğœ) ğ‘‘ğœ
```
```
is the minimum cost of getting from ğ‘¥(0) at time ğ‘¡ = 0 to ğ‘¥(1) at time ğ‘¡ = 1.
```
We apply these ideas to a small time interval to derive a first-order partial differ-
```
ential equation for the value function ğ‘‰(ğ‘¥, ğ‘¡). Suppose we operate an optimal control
```
ğ‘¢ that takes the system from ğ‘¥, ğ‘¡ to ğ‘¥ + Î”ğ‘¥, ğ‘¡ + Î”ğ‘¡. Then we must have
```
âˆ«ğ‘‡ğ‘¡ ğ‘”(ğ‘¥(ğœ), ğ‘¢(ğœ), ğœ) ğ‘‘ğœ = âˆ«ğ‘¡+âˆ†ğ‘¡ğ‘¡ ğ‘”(ğ‘¥(ğœ), ğ‘¢(ğœ), ğœ) ğ‘‘ğœ + âˆ«ğ‘‡ğ‘¡+âˆ†ğ‘¡ ğ‘”(ğ‘¥(ğœ), ğ‘¢(ğœ), ğœ) ğ‘‘ğœ,
```
```
ğ‘‰(ğ‘¥, ğ‘¡) = âˆ«ğ‘¡+âˆ†ğ‘¡ğ‘¡ ğ‘”(ğ‘¥, ğ‘¢, ğœ) ğ‘‘ğœ + ğ‘‰(ğ‘¥ + Î”ğ‘¥, ğ‘¡ + Î”ğ‘¡).
```
The first-order approximation for the last two components above are
```
âˆ«ğ‘¡+âˆ†ğ‘¡ğ‘¡ ğ‘”(ğ‘¥, ğ‘¢, ğœ) ğ‘‘ğœ = ğ‘”(ğ‘¥, ğ‘¢, ğ‘¡) Î”ğ‘¡ + ğ‘œ(Î”ğ‘¡),
```
```
ğ‘‰(ğ‘¥ + Î”ğ‘¥, ğ‘¡ + Î”ğ‘¡) = ğ‘‰(ğ‘¥, ğ‘¡) + ğœ•ğ‘‰ğœ•ğ‘¡ (ğ‘¥, ğ‘¡)Î”ğ‘¡ + ğœ•ğ‘‰ğœ•ğ‘¥ (ğ‘¥, ğ‘¡)Î”ğ‘¥ + ğ‘œ(Î”ğ‘¡, Î”ğ‘¥)
```
```
= ğ‘‰(ğ‘¥, ğ‘¡) + ğœ•ğ‘‰ğœ•ğ‘¡ (ğ‘¥, ğ‘¡)Î”ğ‘¡ + ğœ•ğ‘‰ğœ•ğ‘¥ (ğ‘¥, ğ‘¡)ğ‘“(ğ‘¥, ğ‘¢, ğ‘¡)Î”ğ‘¡ + ğ‘œ(Î”ğ‘¡).
```
Putting this together as an optimization problem,
```
ğ‘‰(ğ‘¥, ğ‘¡) = mináµ† {âˆ«
```
ğ‘¡+âˆ†ğ‘¡
ğ‘¡
```
ğ‘”(ğ‘¥, ğ‘¢, ğœ) ğ‘‘ğœ + ğ‘‰(ğ‘¥ + Î”ğ‘¥, ğ‘¡ + Î”ğ‘¡)}
```
```
= mináµ† {ğ‘”(ğ‘¥, ğ‘¢, ğ‘¡)Î”ğ‘¡ + ğ‘‰(ğ‘¥, ğ‘¡) + ğœ•ğ‘‰ğœ•ğ‘¡ (ğ‘¥, ğ‘¡)Î”ğ‘¡ + ğœ•ğ‘‰ğœ•ğ‘¥ (ğ‘¥, ğ‘¡)ğ‘“(ğ‘¥, ğ‘¢, ğ‘¡)Î”ğ‘¡ + ğ‘œ(Î”ğ‘¡)}
```
```
= ğ‘‰(ğ‘¥, ğ‘¡) + ğœ•ğ‘‰ğœ•ğ‘¡ (ğ‘¥, ğ‘¡)Î”ğ‘¡ + mináµ† {ğ‘”(ğ‘¥, ğ‘¢, ğ‘¡)Î”ğ‘¡ + ğœ•ğ‘‰ğœ•ğ‘¥ (ğ‘¥, ğ‘¡)ğ‘“(ğ‘¥, ğ‘¢, ğ‘¡)Î”ğ‘¡} + ğ‘œ(Î”ğ‘¡).
```
```
Canceling ğ‘‰(ğ‘¥, ğ‘¡), dividing out Î”ğ‘¡, and rearranging yields
```
```
âˆ’ ğœ•ğ‘‰ğœ•ğ‘¡ (ğ‘¥, ğ‘¡) = mináµ† {ğ‘”(ğ‘¥, ğ‘¢, ğ‘¡) + ğœ•ğ‘‰ğœ•ğ‘¥ (ğ‘¥, ğ‘¡)ğ‘“(ğ‘¥, ğ‘¢, ğ‘¡)} .
```
```
This is the basic Hamilton-Jacobi-Bellman (HJB) equation.
```
11.3. Hamilton-Jacobi-Bellman Equation 183
HAMILTON-JACOBI-BELLMAN EQUATION
The value function
```
ğ‘‰(ğ‘¥ğ‘¡, ğ‘¡) = mináµ† {ğº(ğ‘¥(ğ‘‡)) + âˆ«ğ‘‡ğ‘¡ ğ‘”(ğ‘¥, ğ‘¢, ğœ) ğ‘‘ğœ}
```
with
```
ğ‘¥â€² = ğ‘“(ğ‘¥, ğ‘¢, ğœ), ğ‘¥(ğ‘¡) = ğ‘¥ğ‘¡
```
must satisfy the Hamilton-Jacobi-Bellman partial differential equation:
```
âˆ’ ğœ•ğ‘‰ğœ•ğ‘¡ (ğ‘¥, ğ‘¡) = mináµ† {ğ‘”(ğ‘¥, ğ‘¢, ğ‘¡) + ğœ•ğ‘‰ğœ•ğ‘¥ (ğ‘¥, ğ‘¡)ğ‘“(ğ‘¥, ğ‘¢, ğ‘¡)} .
```
Weâ€™ve skimmed over a number of details, such as existence and various smooth-
ness assumptions on ğ‘” and ğ‘“.
Note that the right-hand side of the HJB equation reflects the Hamiltonian form
ğ» = ğ‘” + ğœ†ğ‘“ with ğœ•ğ‘‰ğœ•ğ‘¥ filling the role of the costate ğœ†. This reinforces our insight that the
costate is the marginal performance with respect to state, or ğœ•ğ‘‰ğœ•ğ‘¥ . In fact, the Hamilton-
Jacobi-Bellman PDE can be expressed in terms of the Hamiltonian:
```
âˆ’ ğœ•ğ‘‰ğœ•ğ‘¡ (ğ‘¥, ğ‘¡) = mináµ† {ğ»(ğ‘¥, ğ‘¢, ğœ•ğ‘‰ğœ•ğ‘¥ , ğ‘¡)} .
```
11.3.2 Applying Hamilton-Jacobi-Bellman. We can solve optimization prob-
```
lems by solving the HJB PDE and using boundary conditions ğ‘‰(ğ‘‡, ğ‘¥(ğ‘‡)) = ğº(ğ‘¥(ğ‘‡)).
```
This approach differs from that of Pontryagin, but it produces the same conclusions
and relies on the same foundational concepts.
Example 11.4: Integrator
As in Example 4.3, consider controlled growth ğ‘¥â€² = ğ‘¢, fixed time ğ‘‡, and free end
```
location ğ‘¥(ğ‘‡), and we want to minimize
```
ğ½ = âˆ«
ğ‘‡
0
ğ‘¥2 + ğ‘¢2 ğ‘‘ğ‘¡.
The HJB equation for this system is
```
âˆ’ ğœ•ğ‘‰ğœ•ğ‘¡ = mináµ† {ğ‘¥2 + ğ‘¢2 + ğœ•ğ‘‰ğœ•ğ‘¥ ğ‘¢} .
```
This is quadratic in ğ‘¢ with a minimum at ğ‘¢ = âˆ’ 12ğœ•ğ‘‰ğœ•ğ‘¥ , yielding
```
âˆ’ ğœ•ğ‘‰ğœ•ğ‘¡ = ğ‘¥2 âˆ’ 14 ( ğœ•ğ‘‰ğœ•ğ‘¥ )
```
2
```
which is our HJB PDE (âˆ— check these steps âˆ—).
```
This is a nonlinear PDE. Through divine inspiration, or, just thinking about
the quadratic term ğ‘¥2, we guess a solution of the form
```
ğ‘‰(ğ‘¥, ğ‘¡) = ğ‘¥2 ğ‘ (ğ‘¡)
```
```
for some unknown function ğ‘ (ğ‘¡).
```
Plugging this in and simplifying yields
âˆ’ğ‘ â€² = 1 âˆ’ ğ‘ 2.
184 Chapter 11. Time, Value, and HJB Equation
This first-order differential equation is quadratic in the dependent variable. Dif-
ferential equations of this form are called Riccati equations and have well-refe-
renced methods for deriving solutions in closed form.
```
In this example the end cost is ğº(ğ‘¥(ğ‘‡)) = 0, yielding a boundary condition
```
```
ğ‘‰(ğ‘¥, ğ‘¡) = 0, making ğ‘ (ğ‘‡) = 0. This leads to the value function (âˆ— verify this âˆ—)
```
```
ğ‘‰(ğ‘¥, ğ‘¡) = ğ‘¥2 ğ‘’
```
2ğ‘‡ âˆ’ ğ‘’2ğ‘¡
ğ‘’2ğ‘‡ + ğ‘’2ğ‘¡ .
```
If we were to have an end cost of ğº(ğ‘¥(ğ‘‡)) = 12 ğ‘¥(ğ‘‡)2, we would have a boundary
```
```
condition ğ‘‰(ğ‘¥, ğ‘‡) = 12 ğ‘¥2 making ğ‘ (ğ‘‡) = 12 and a value function of (âˆ— check âˆ—)
```
```
ğ‘‰(ğ‘¥, ğ‘¡) = ğ‘¥2 3ğ‘’
```
2ğ‘‡ âˆ’ ğ‘’2ğ‘¡
3ğ‘’2ğ‘‡ + ğ‘’2ğ‘¡ .
```
These two payoff functions ğº(ğµ) = 0 and ğº(ğµ) = ğµ2 are readily matched by
```
solutions to the Riccati equation. Other payoff functions would be significantly
more challenging to accommodate.
The following example demonstrates the HJB technique for maximizing a payoff
where we use a trick to satisfy the boundary conditions.
Example 11.5: King Tinyâ€™s Value Function
King Tiny wants to maximize the utility of consumption âˆ«100 âˆšğ‘¢ ğ‘‘ğ‘¡ subject to
exponential growth minus consumption ğ‘¥â€² = 0.2ğ‘¥ âˆ’ ğ‘¢.
```
For ğ‘‡ = 10, the value ğ‘‰(ğ‘¥, ğ‘¡) is the remaining utility to be gained assuming
```
the economy is at ğ‘¥ at time ğ‘¡ < 10.
The HJB equation would then be
```
0 = ğœ•ğ‘‰ğœ•ğ‘¡ + maxáµ† {ğ‘¢1/2 + ğœ•ğ‘‰ğœ•ğ‘¥ (0.2 ğ‘¥ âˆ’ ğ‘¢)} .
```
```
A local max is ğ‘¢ = 1/ (2 ğœ•ğ‘‰ğœ•ğ‘¥ )
```
2
producing
0 = ğœ•ğ‘‰ğœ•ğ‘¡ + 1
4 ğœ•ğ‘‰ğœ•ğ‘¥
- .2 ğ‘¥ ğœ•ğ‘‰ğœ•ğ‘¥
```
which is our HJB PDE (âˆ— check these steps âˆ—).
```
This is a nonlinear PDE, a notoriously difficult animal to tame, and it is not
obvious how to proceed. Thinking about the 1/ ğœ•ğ‘‰ğœ•ğ‘¥ and ğ‘¥ ğœ•ğ‘‰ğœ•ğ‘¥ terms, one may guess
```
that ğ‘‰(ğ‘¥, ğ‘¡) should have a ğ‘¥1/2 term, and after some trial and error a solution form
```
```
ğ‘‰ = (ğ‘¥ğ‘Š(ğ‘¡))1/2
```
works pretty well.
Substituting
ğœ•ğ‘‰
ğœ•ğ‘¥ =
1
2 ğ‘¥
âˆ’ 12 ğ‘Š12 ,
ğœ•ğ‘‰
ğœ•ğ‘¡ =
1
2 ğ‘¥
1
2 ğ‘Š âˆ’
1
2 ğ‘Š â€²
11.3. Hamilton-Jacobi-Bellman Equation 185
yields
0 = ğ‘Š â€² + 1 + 15 ğ‘Š.
```
Solving this linear first-order equation brings us to (âˆ— check this âˆ—)
```
```
ğ‘‰(ğ‘¥, ğ‘¡) = (ğ‘¥ (ğ‘’ğ¾âˆ’ğ‘¡/5 âˆ’ 5))1/2
```
where ğ¾ is the constant of integration.
```
For boundary conditions, we reason that ğ‘‰(0, ğ‘¡) = 0 for all ğ‘¡: if we have
```
no capital, ğ‘¥ = 0, then we have nothing to spend or invest. This condition is
automatically satisfied by the above solution for any ğ¾.
```
We also argue that ğ‘‰(ğ‘¥, 10) = 0, because at ğ‘¡ = 10 we are simply out of time:
```
âˆ«1010 âˆšğ‘¢ ğ‘‘ğ‘¡ = 0. We can satisfy this condition by choosing ğ¾ = 2 + ln 5 to make
```
ğ‘‰(ğ‘¥, ğ‘¡) = (5ğ‘¥(ğ‘’2âˆ’ğ‘¡/5 âˆ’ 1))1/2 .
```
```
And now we have a problem. Nowhere have we used or required ğ‘¥(10) = 2, and
```
there seems to be nowhere in this solution to incorporate this requirement.
Here is how we reason our way out of this conundrum: if we were to have a
```
fixed time ğ‘‡ = 10 but a free endpoint ğ‘¥(ğ‘‡), then the above would be a solution
```
that satisfies the HJB equation. But if we did have a free endpoint, it stands to
```
reason that ğ‘¥(ğ‘‡) = 0, since any leftover money could have been spent to increase
```
performance. So we assume the above value function is for a fixed time ğ‘‡ = 10
```
and free endpoint ğ‘¥(ğ‘‡), which makes ğ‘¥(10) = 0.
```
```
So how do we make ğ‘¥(10) = 2? At time ğ‘¡ = 0 set aside just enough money
```
to accrue to 2 dollars after 10 years at growth rate of 0.2, which would be 2/ğ‘’2 =
0.2707 . . . . Then optimize spending the rest of the funds, 1 âˆ’ 2/ğ‘’2, with the un-
```
derstanding that they will be depleted. So ğ‘‰(ğ‘¥, ğ‘¡) is the solution for ğ‘¥(10) = 0,
```
```
whereas Ë†ğ‘‰ = ğ‘‰(ğ‘¥ âˆ’ 2/ğ‘’2, ğ‘¡) is the solution to our King Tiny problem with ğ‘¥(10) =
```
2.
Indeed, we check that
```
Ë†ğ‘‰(1, 0) = ğ‘‰(1 âˆ’ 2/ğ‘’2, 0) = âˆš5(1 âˆ’ 2/ğ‘’2)(ğ‘’2 âˆ’ 1) = 4.827 . . .
```
corresponds to our calculation in Example 4.4.
We conclude this chapter with an HJB analysis of the Rocket Sled soft-landing
problem.
Example 11.6: Rocket Sled Value
In Example 11.3 we considered the Rocket Sled with friction ğ‘¥â€² = ğ‘¦, ğ‘¦â€² = âˆ’ğ‘¦ + ğ‘¢,
performance ğ½ = âˆ«ğ‘‡0 ğ‘¢2 ğ‘‘ğ‘¡, and we computed the minimum cost to move from a
```
given initial position and velocity (ğ‘¥, ğ‘¦) to (0, 0) in ğ‘‡ time units as
```
```
Ìƒğ½(ğ‘¥0, ğ‘¦0, ğ‘‡) = (ğ‘’2ğ‘‡ âˆ’1)ğ‘¥20+2(ğ‘’ğ‘‡ âˆ’1)
```
```
2ğ‘¥0ğ‘¦0+(ğ‘’2ğ‘‡ âˆ’4ğ‘’ğ‘‡ +2ğ‘‡+3)ğ‘¦20
```
```
(ğ‘’ğ‘‡ âˆ’1)(ğ‘’ğ‘‡ (ğ‘‡âˆ’2)+ğ‘‡+2) .
```
This is an optimal performance function, not a value function. A value func-
tion starts with a specified end time ğ‘‡ and is a function of remaining cost/payoff
```
at time ğ‘¡ < ğ‘‡. This is an autonomous case (ğ‘“ and ğ‘” do not depend on ğ‘¡), so for
```
186 Chapter 11. Time, Value, and HJB Equation
any given ğ‘‡, the value function would be
```
ğ‘‰(ğ‘¥, ğ‘¦, ğ‘¡) =Ìƒ ğ½(ğ‘¥, ğ‘¦, ğ‘‡ âˆ’ ğ‘¡).
```
Making this substitution and simplifying yields
```
ğ‘‰(ğ‘¥, ğ‘¦, ğ‘¡) = ğ‘’
```
```
2ğ‘¡(ğ‘¦2(2ğ‘¡âˆ’2ğ‘‡âˆ’3)+ğ‘¥2âˆ’2ğ‘¥ğ‘¦)+4ğ‘¦ğ‘’ğ‘¡+ğ‘‡ (ğ‘¥+ğ‘¦)âˆ’ğ‘’2ğ‘‡ (ğ‘¥+ğ‘¦)2
```
```
(ğ‘’ğ‘‡ âˆ’ğ‘’ğ‘¡)(ğ‘’ğ‘¡(ğ‘¡âˆ’ğ‘‡âˆ’2)+ğ‘’ğ‘‡ (ğ‘¡âˆ’ğ‘‡+2)) (11.2)
```
```
This function ğ‘‰(ğ‘¥, ğ‘¦, ğ‘¡) would have to satisfy the HJB equation for two state
```
```
variables:
```
```
âˆ’ ğœ•ğ‘‰ğœ•ğ‘¡ (ğ‘¥, ğ‘¡) = mináµ† {ğ‘¢2 + ğœ•ğ‘‰ğœ•ğ‘¥ ğ‘¦ + ğœ•ğ‘‰ğœ•ğ‘¦ (âˆ’ğ‘¦ + ğ‘¢)} .
```
```
We calculate that ğ‘¢ = âˆ’ 12ğœ•ğ‘‰ğœ•ğ‘¦ minimizes ğ‘¢2 + ğœ•ğ‘‰ğœ•ğ‘¥ ğ‘¦ + ğœ•ğ‘‰ğœ•ğ‘¦ (âˆ’ğ‘¦ + ğ‘¢) and our HJB
```
PDE reduces to
âˆ’ ğœ•ğ‘‰ğœ•ğ‘¡ = âˆ’ 14ğœ•ğ‘‰ğœ•ğ‘¦
2
- ğ‘¦ ( ğœ•ğ‘‰ğœ•ğ‘¥ âˆ’ ğœ•ğ‘‰ğœ•ğ‘¦ ) .
```
The gentle reader may wish to whip out a pencil and verify that expression (11.2)
```
satisfies this PDE. Or not.
Key Points
Pontryaginâ€™s method covered in previous chapters involves systems of ordinary differ-
ential equations in state-costate space to find necessary conditions for an optimal con-
trol, and it derives directly from Lagrange multipliers.
In this chapter we explored an alternate approach to optimal control using partial
```
differential equations (PDEs). We directly formulated a PDE for time optimal problems
```
and explored the Hamilton-Jacobi-Bellman PDE for the more general value function
in optimal control problems. The advantage of this approach is that it defines perfor-
mance as a function of starting location and time. The disadvantage is that the PDEs
are typically difficult to solve and challenging to numerically estimate.
The Hamilton-Jacobi-Bellman approach is closely related to Pontryaginâ€™s methods
and arises directly from the Principle of Optimality applied on the scale of differentials.
Exercises
Exercise 11.1. Rocket Race on Ice V. Consider the Rocket Race without friction ğ‘¥â€³ = ğ‘¢
with limits |ğ‘¢| â‰¤ 1.
```
(a) Compute the minimum time Ëœğ‘‡(ğ‘¥0, ğ‘¦0) for transition from (ğ‘¥0, ğ‘¦0) to the hori-
```
```
zontal line ğ‘¦ = 0. Verify that ğ‘‘ğ‘‘ğ‘¡ Ëœğ‘‡(ğ‘¥(ğ‘¡), ğ‘¦(ğ‘¡)) = âˆ’1 along optimal trajectories.
```
```
(b) Compute the minimum time Ëœğ‘‡(ğ‘¥0, ğ‘¦0) for transition from (ğ‘¥0, ğ‘¦0) to the ver-
```
```
tical line ğ‘¥ = 0. Verify that ğ‘‘ğ‘‘ğ‘¡ Ëœğ‘‡(ğ‘¥(ğ‘¡), ğ‘¦(ğ‘¡)) = âˆ’1 along optimal trajectories.
```
Exercise 11.2. Rocket Race on Ice VI. Consider the Rocket Race without friction ğ‘¥â€³ = ğ‘¢
```
with limits |ğ‘¢| â‰¤ 1 and the goal is to reach the origin (0, 0) in minimal time from a
```
```
general starting point. In Exercise 10.5 it was calculated that for an initial point (ğ‘¥0, ğ‘¦0)
```
in the ğ‘¢ = âˆ’1 region the minimal time was
```
Ëœğ‘‡(ğ‘¥0, ğ‘¦0) = ğ‘¦0 + âˆšğ‘¥0 + 12 ğ‘¦20.
```
Exercises 187
Verify this as follows:
```
(a) Show that ğ‘‘ğ‘‘ğ‘¡ Ëœğ‘‡(ğ‘¥(ğ‘¡), ğ‘¦(ğ‘¡)) = âˆ’1 for an optimal trajectory (ğ‘¥(ğ‘¡), ğ‘¦(ğ‘¡)) in the ğ‘¢ =
```
âˆ’1 region.
```
(b) Show that the function has the correct value, Ëœğ‘‡(ğ‘¥, ğ‘¦) = ğ‘¦, for any point (ğ‘¥, ğ‘¦)
```
on the incoming ğ‘¢ = +1 trajectory to the origin.
```
Exercise 11.3(hs). Formulate and solve the HJB PDE for the controlled system ğ‘¥â€² =
```
```
ğ‘¥ + ğ‘¢ with cost ğ½ = ğ‘¥2(ğ‘‡) + âˆ«ğ‘‡0 ğ‘¥2 + ğ‘¢2 ğ‘‘ğ‘¡, where ğ‘‡ is fixed and ğ‘¥(ğ‘‡) is free.
```
Exercise 11.4. Formulate the HJB equation for the general linear-quadratic case of
```
minimizing ğº(ğ‘¥(ğ‘‡)) + âˆ«ğ‘‡0 ğ‘ğ‘¥2 + ğ‘ğ‘¢2 ğ‘‘ğ‘¡ with ğ‘¥â€² = ğ‘šğ‘¥ + ğ‘¢ for a fixed ğ‘‡ and free ğ‘¥(ğ‘‡).
```
```
Show that a guess of the form ğ‘‰ = ğ‘¥2ğ‘ (ğ‘¡) leads to a Riccati differential equation.
```
```
Exercise 11.5(hs). Construct the value function for King Tiny with a discounted future
```
```
(Example 4.5). Check that ğ‘‰(1 âˆ’ 2/ğ‘’2, 0) = 3.540 . . . .
```
```
Exercise 11.6(s). Consider minimizing ğ½ = âˆ«ğ‘‡0 ğ‘¢2 ğ‘‘ğ‘¡ with ğ‘¥â€² = ğ‘¢, fixed time ğ‘‡, and
```
```
ğ‘¥(ğ‘‡) = 0.
```
```
(a) Solve this using Principle I with ğ‘¥(0) = ğ‘¥0, and determineÌƒ ğ½ as a function of
```
ğ‘¥0 and ğ‘‡.
```
(b) Derive the HJB equation for this system.
```
```
(c) Using your result from part (a), show that ğ‘‰(ğ‘¥, ğ‘¡) =Ìƒ ğ½(ğ‘¥, ğ‘‡ âˆ’ ğ‘¡) satisfies the
```
```
PDE from part (b).
```
```
Exercise 11.7(hs). Consider the Rocket Sled without friction ğ‘¥â€³ = ğ‘¢ and with perfor-
```
mance ğ½ = âˆ«ğ‘‡0 ğ‘¢2 ğ‘‘ğ‘¡.
```
(a) Calculate the minimum cost to move from a given initial position and velocity
```
```
(ğ‘¥, ğ‘¦) to (0, 0) in ğ‘‡ time units as a functionÌƒ ğ½(ğ‘¥, ğ‘¦, ğ‘‡).
```
```
(b) Formulate the Hamilton-Jacobi-Bellman equation for the value function
```
```
ğ‘‰(ğ‘¥, ğ‘¦, ğ‘¡).
```
```
(c) Show that ğ‘‰(ğ‘¥, ğ‘¦, ğ‘¡) =Ìƒ ğ½(ğ‘¥, ğ‘¦, ğ‘‡ âˆ’ ğ‘¡) satisfies this equation.
```
```
Exercise 11.8(s). Canoe IV: Value. Recall the Canoe problems, Exercises 6.4â€“6.6, where
```
you have a controlled system ğ‘¥â€² = 1 + ğ‘¢, a fixed time ğ‘‡, a given starting point, a free
```
endpoint ğ‘¥(ğ‘‡), and a payoff function
```
```
ğ½ = ğ‘¥(ğ‘‡) (4 âˆ’ ğ‘¥(ğ‘‡)) âˆ’ âˆ«
```
ğ‘‡
0
ğ‘¢2 ğ‘‘ğ‘¡.
```
(a) Derive the value function directly from the definition
```
```
ğ‘‰(ğ‘¥ğ‘¡, ğ‘¡) = maxáµ† {ğº(ğ‘¥(ğ‘‡)) + âˆ«ğ‘‡ğ‘¡ ğ‘”(ğ‘¥, ğ‘¢, ğœ) ğ‘‘ğœ} .
```
188 Chapter 11. Time, Value, and HJB Equation
I recommend using this approach: we know from previous work that the control will
be constant, ğ‘¢ = ğ‘˜, so for a fixed ğ‘‡ > 0, any ğ‘¡ < ğ‘‡, any ğ‘¥ğ‘¡, solve for the above maximum
```
over ğ‘˜ assuming ğ‘¥â€² = 1 + ğ‘˜ with ğ‘¥(ğ‘¡) = ğ‘¥ğ‘¡.
```
```
(b) Compute the HJB equation and show that your value function ğ‘‰(ğ‘¥, ğ‘¡) satisfies
```
the PDE.
12
Differential Games
In optimal control, an agent optimizes their choices in a contest against nature. In
dynamic games, multiple agents are operating in the same environment, and each will
optimize their choices against nature plus what they anticipate the other agents may
be doing. This is game theory. The difference between an optimization problem and a
game is that a game requires opponents with brains who will anticipate your actions.
Depending on the game, the agentâ€™s goals may benefit each other, interfere with
each other, or be in direct opposition to each other. Pontryaginâ€™s principles readily
generalize to these situations and allow concurrent optimizations for all agents.
12.1 Games
Nash equilibria are places where games are in equilibrium. Suppose we have a game
in which player ğ¼ uses strategy ğ´ and player ğ¼ğ¼ uses strategy ğµ. The strategy pair ğ´, ğµ is
a Nash equilibrium if:
When ğ¼ğ¼ plays ğµ, ğ¼ can do no better than to play ğ´.
When ğ¼ plays ğ´, ğ¼ğ¼ can do no better that to play ğµ.
For example, in the driving game players ğ¼ and ğ¼ğ¼ must decide to drive on the right
or left side of the road when approaching each other. If they both drive on the right,
they pass each other without incident and each gets a payoff of one point for their
coordinated solution. The same is true if they both drive on the left. But if one drives
on the right and the other drives on the left, then they crash into each other and they
each get a payoff of zero.
Both players driving on the right is a Nash equilibrium. If ğ¼ drives on the right, ğ¼ğ¼
can do no better than to also drive on the right and can do quite a bit worse by driving
on the left, and vice versa. Both players driving on the left is also a Nash equilibrium,
and it behooves one to pay attention to which Nash equilibria are in effect in whatever
culture one may find oneself.
189
190 Chapter 12. Differential Games
Nash equilibria are important because if a game is to be played consistently and
rationally, it must be played at a Nash equilibrium.
Curiously, in the driving game, randomizing 50-50 to drive on the right or left is
also a Nash equilibriumâ€”if everyone else is doing it, you can do no better than to do
so yourself. A Nash equilibrium is necessary for consistent rational play, but it is not
sufficient. There are many nuances to the theory of games, but we will stick to a basic
application of Pontryagin to identify Nash equilibria in differential games.
12.2 Differential Games
Differential games are played out over a period of time with continuous actions. We
motivate the topic with the following classic example that we can intuitively reason
without getting mathematically rigorous.
Example 12.1: Guard the Target
Two agents run around anywhere on a flat playing field and are equally matched
in speed and agility. Agent ğ¼ğ¼ is the invader and wants to get as close to a target
region ğ‘… as she possibly can, and agent ğ¼ is the defender and must try to keep
player ğ¼ğ¼ as far away as he possibly can. An example of this game is depicted in
Figure 12.1. The game ends when the two players intercept each other, and the
distance from the point of interception to the target region is the payoff for player
ğ¼ğ¼ and the cost for player ğ¼.
Figure 12.1. Player ğ¼ is guarding the shaded target region from
player ğ¼ğ¼.
How will this game play out if both players play optimally? See Exercise 12.1
for the solution. Check out Paul Nahinâ€™s book [18] for other interesting examples
of pursuit and evasion dynamics.
12.2.1 Pontryagin Games. Two agents, player ğ¼ and player ğ¼ğ¼, can influence the
state ğ‘¥ of some system, which may be a vector of values, according to a specified dy-
namic
```
ğ‘¥â€² = ğ‘“(ğ‘¥, ğ‘¢, ğ‘£)
```
12.2. Differential Games 191
where player ğ¼ operates control ğ‘¢ and player ğ¼ğ¼ operates control ğ‘£, and the game begins
```
with some initial state ğ‘¥(0) = ğ‘¥0.
```
The game terminates at time ğ‘‡ and the payoffs for players ğ¼ and ğ¼ğ¼ are, respectively,
```
ğ½ğ¼ = ğºğ¼ (ğ‘¥(ğ‘‡)) + âˆ«ğ‘‡0 ğ‘”ğ¼ (ğ‘¥, ğ‘¢, ğ‘£) ğ‘‘ğ‘¡,
```
```
ğ½ğ¼ğ¼ = ğºğ¼ğ¼ (ğ‘¥(ğ‘‡)) + âˆ«ğ‘‡0 ğ‘”ğ¼ğ¼ (ğ‘¥, ğ‘¢, ğ‘£) ğ‘‘ğ‘¡,
```
which we assume each player wants to maximize. The idea of a Nash equilibrium is a
pair of control functions ğ‘¢âˆ—1, ğ‘¢âˆ—2 such that:
Given ğ‘¢2 = ğ‘¢âˆ—2, ğ½ğ¼ is maximized at ğ‘¢1 = ğ‘¢âˆ—1.
Given ğ‘¢1 = ğ‘¢âˆ—1, ğ½ğ¼ğ¼ is maximized at ğ‘¢2 = ğ‘¢âˆ—2.
Applying Pontryagin, the agents ğ¼ and ğ¼ğ¼ would have the following Hamiltonians,
```
respectively:
```
```
ğ»ğ¼ = ğ‘”ğ¼ (ğ‘¥, ğ‘¢, ğ‘£) + ğœ†ğ¼ ğ‘“(ğ‘¥, ğ‘¢, ğ‘£),
```
```
ğ»ğ¼ğ¼ = ğ‘”ğ¼ğ¼ (ğ‘¥, ğ‘¢, ğ‘£) + ğœ†ğ¼ğ¼ ğ‘“(ğ‘¥, ğ‘¢, ğ‘£)
```
and the Nash criteria would be
```
ğ‘¢âˆ— = argmaxáµ† {ğ»ğ¼ (ğ‘¥, ğ‘¢, ğ‘£âˆ—))},
```
```
ğ‘£âˆ— = argmaxğ‘£ {ğ»ğ¼ğ¼ (ğ‘¥, ğ‘¢âˆ—, ğ‘£)}
```
for all ğ‘¡. Costates would be driven by dynamics
```
ğœ†â€²ğ¼ = âˆ’ ğœ•ğœ•ğ‘¥ ğ»ğ¼ (ğ‘¥, ğ‘¢, ğ‘£),
```
```
ğœ†â€²ğ¼ğ¼ = âˆ’ ğœ•ğœ•ğ‘¥ ğ»ğ¼ğ¼ (ğ‘¥, ğ‘¢, ğ‘£).
```
Nash equilibria are where repeated games tend to settle, or where perfectly predict-
ing agents will conclude is the only rational way to proceed. This generalizes to control
theory where each player is optimized with respect to the other playerâ€™s control.
A basic class of such games is the linear-quadratic games, with
ğ‘¥â€² = ğ´ğ‘¢ + ğµğ‘£ + ğ¶ğ‘¥
and
ğ‘”ğ¼ = ğ›¼ğ¼ ğ‘¢2 + ğ›½ğ¼ ğ‘£2 + ğ›¾ğ¼ ğ‘¥2 ğ‘‘ğ‘¡,
ğ‘”ğ¼ğ¼ = ğ›¼ğ¼ğ¼ ğ‘¢2 + ğ›½ğ¼ğ¼ ğ‘£2 + ğ›¾ğ¼ğ¼ ğ‘¥2.
The following example is a simple case of a linear-quadratic game.
Example 12.2: Rugby Maul
```
Suppose a rugby ball is at position ğ‘¥(0) = 0 on the field, and with time limit
```
```
ğ‘‡ = 2, team ğ¼ deploys effort ğ‘¢ to maximize ğ‘¥(2) and team ğ¼ğ¼ deploys effort ğ‘£ to
```
```
minimize ğ‘¥(2), resulting in a dynamic
```
ğ‘¥â€² = ğ›¼ğ‘¢ âˆ’ ğ›½ğ‘£
192 Chapter 12. Differential Games
where ğ›¼, ğ›½ > 0 represent the abilities of teams ğ¼ and ğ¼ğ¼, respectively. Making an
effort comes with a cost, so letâ€™s say the payoffs for teams ğ¼ and ğ¼ğ¼ are
```
ğ½ğ¼ = ğ‘¥(2) âˆ’ âˆ«20 ğ‘¢2 ğ‘‘ğ‘¡,
```
```
ğ½ğ¼ğ¼ = âˆ’ğ‘¥(2) âˆ’ âˆ«20 ğ‘£2 ğ‘‘ğ‘¡.
```
Team ğ¼ will apply Pontryaginâ€™s method and construct a Hamiltonian
```
ğ»ğ¼ = âˆ’ğ‘¢2 + ğœ†ğ¼ (ğ›¼ğ‘¢ âˆ’ ğ›½ğ‘£)
```
which they will maximize as
ğœ•áµ†ğ»ğ¼ = âˆ’2ğ‘¢ + ğœ†ğ¼ ğ›¼ = 0 âŸ¹ ğ‘¢ = 12 ğœ†ğ¼ ğ›¼.
Team ğ¼ will consider the costate dynamic ğœ†â€²ğ¼ = âˆ’ ğœ•ğ»ğ¼ğœ•ğ‘¥ = 0 making ğœ†ğ¼ constant.
```
Since ğ‘¥(ğ‘¡) is free and the end payoff is ğºğ¼ (ğµ) = ğµ, team ğ¼ infers ğœ†ğ¼ = ğºâ€²ğ¼ (ğµ) = 1.
```
```
Putting this together, player ğ¼ applies effort ğ‘¢ = 12 ğ›¼ (âˆ— check this âˆ—).
```
```
Team ğ¼ğ¼ applies similar reasoning with ğ»ğ¼ğ¼ = âˆ’ğ‘£2 +ğœ†ğ¼ğ¼ (ğ›¼ğ‘¢âˆ’ğ›½ğ‘£) and ğºğ¼ğ¼ (ğµ) =
```
âˆ’ğµ and concludes that their Nash equilibrium choice is to apply effort ğ‘£ = 12 ğ›½
```
(âˆ— check this âˆ—).
```
This results in a dynamic
ğ‘¥â€² = 12 ğ›¼2 âˆ’ 12 ğ›½2
```
and so the ball ends up at ğ‘¥(2) = ğ›¼2 âˆ’ ğ›½2.
```
The resulting payoffs are
ğ½ğ¼ = 12 ğ›¼2 âˆ’ ğ›½2,
ğ½ğ¼ğ¼ = âˆ’ğ›¼2 + 12 ğ›½2.
If the two teams are evenly matched, say, ğ›¼ = ğ›½ = 1, then the ball ends up
at ğ‘¥ = 0 and each team gets a payoff of ğ½ğ¼ = ğ½ğ¼ğ¼ = âˆ’1/2. If one team is stronger,
say, ğ›¼ = 2 and ğ›½ = 1, the ball will end up at ğ‘¥ = 3. The first team expends effort
ğ‘¢ = 1 at a cost of âˆ«20 1 ğ‘‘ğ‘¡ = 2 and net payoff ğ½ğ¼ = 1. The weaker team expends
less effort ğ‘£ = 1/2 at a cost of âˆ«20 1/4 ğ‘‘ğ‘¡ = 1/2 and a net payoff of ğ½ğ¼ğ¼ = âˆ’7/2.
Note that the amount of effort a team expends depends only on their own
ability and not on the ability or strategy of their opponent.
The following is a game of cooperation, with a nonlinear control.
Example 12.3
Two agents cooperate in a game where progress is proportional to each playerâ€™s
efforts as
```
ğ‘¥â€² = (ğ‘¢ âˆ’ 1)(ğ‘£ âˆ’ 1)
```
where ğ‘¢ is agent ğ¼â€™s control and ğ‘£ is agent ğ¼ğ¼â€™s control, and we stipulate ğ‘¢, ğ‘£ â‰¥ 1.
So either player making minimal effort results in zero progress.
12.3. War 193
```
There is a fixed end time ğ‘‡, initial value ğ‘¥(0) = 0, and each player values
```
```
the end result differently: player ğ¼ has value ğ›¼ğ‘¥(ğ‘‡) and player ğ¼ğ¼ has value ğ›½ğ‘¥(ğ‘‡).
```
Assuming costs are quadratic in effort, we have payoff functions
```
ğ½ğ¼ = ğ›¼ğ‘¥(ğ‘‡) âˆ’ âˆ«ğ‘‡0 ğ‘¢2 ğ‘‘ğ‘¡,
```
```
ğ½ğ¼ğ¼ = ğ›½ğ‘¥(ğ‘‡) âˆ’ âˆ«ğ‘‡0 ğ‘£2 ğ‘‘ğ‘¡.
```
From player ğ¼â€™s perspective we have
```
ğ»ğ¼ = âˆ’ğ‘¢2 + ğœ†ğ¼ (ğ‘¢ âˆ’ 1)(ğ‘£ âˆ’ 1)
```
with
ğœ•
```
ğœ•ğ‘¢ ğ»ğ¼ = âˆ’2ğ‘¢ + ğœ†ğ¼ (ğ‘£ âˆ’ 1)
```
```
and ğ»ğ¼ is maximized at ğ‘¢ = ğœ†ğ¼ (ğ‘£ âˆ’ 1)/2.
```
```
With ğœ•ğ»ğ¼ğœ•ğ‘¥ = 0 we have that ğœ†ğ¼ is constant. Since ğ‘¥(ğ‘‡) is free, we have ğœ†ğ¼ =
```
```
ğºâ€²(ğµ) = ğ›¼, and we conclude ğ‘¢ = ğ›¼(ğ‘£ âˆ’ 1)/2.
```
From player ğ¼ğ¼â€™s perspective we have
```
ğ»ğ¼ğ¼ = âˆ’ğ‘£2 + ğœ†ğ¼ğ¼ (ğ‘¢ âˆ’ 1)(ğ‘£ âˆ’ 1)
```
```
and by similar reasoning we conclude ğ‘£ = ğ›½(ğ‘¢ âˆ’ 1)/2.
```
```
Simultaneously solving ğ‘¢ = ğ›¼(ğ‘£ âˆ’ 1)/2 and ğ‘£ = ğ›½(ğ‘¢ âˆ’ 1)/2 yields
```
```
ğ‘¢ = ğ›¼(1 + ğ›½)ğ›¼ğ›½ âˆ’ 1 , ğ‘£ = ğ›½(1 + ğ›¼)ğ›¼ğ›½ âˆ’ 1 .
```
For example, if the players valued the outcome at rates ğ›¼ = 2 and ğ›½ = 1, then
player ğ¼ would expend effort ğ‘¢ = 4 and get a net payoff of ğ½ğ¼ = 4ğ‘‡, and player ğ¼ğ¼
```
would expend less effort ğ‘£ = 3 and get a net payoff of ğ½ğ¼ğ¼ = 3ğ‘‡/2 (âˆ— verify this âˆ—).
```
Note the game-theoretic structure of this Nash equilibrium. If we just as-
sume that player ğ¼ will use ğ‘¢ = 4 and then player ğ¼ğ¼ maximizes accordingly, she
will optimize by playing ğ‘£ = 3. If we assume player ğ¼ğ¼ uses ğ‘£ = 3, then player ğ¼
will optimize by playing ğ‘¢ = 4. No other pair of controls will have this property:
the Nash equilibrium is a unique solution.
Pontryagin analysis of games generalizes to higher-dimensional state spaces, often
with one variable for each player.
The situation could be a zero-sum game where the payers are exactly in direct
```
opposition: ğ½1 = âˆ’ğ½2. Here we would have a single Hamiltonian that one player wants
```
to maximize and the other wants to minimize, as explored in the next section.
12.3 War
There are several ways to model armed conflict. One example that is popular in the
differential games literature is Attrition and Attack popularized by Rufus Isaacs and
```
Avner Friedman ([7, 8]).
```
194 Chapter 12. Differential Games
Example 12.4: Attrition and Attack
Tweedle-Dee and Tweedle-Dum agreed to have a Battle. Dee and Dum have mil-
itary resources ğ‘¥ and ğ‘¦, respectively, and the Battle will have a fixed time pe-
riod [0, ğ‘‡]. For a short battle, Dee and Dum would simply bring all resources to
the battlefield, and victory would go to the one who brought the most resources.
Bringing resources to the battlefield is the â€œattackâ€ strategy.
For a longer Battle, it may be good to divert some of your military from the
battlefield and direct them to guerrilla attacks on the enemyâ€™s supply line, thereby
reducing the enemyâ€™s military power. This is the â€œattritionâ€ strategy.
Dee and Dum have fixed military industrial production rates ğ‘šğ‘¥ and ğ‘šğ‘¦.
Without any guerrilla attacks, their military resources would grow as
ğ‘¥â€² = ğ‘šğ‘¥,
ğ‘¦â€² = ğ‘šğ‘¦.
Dee can control the proportion, ğ‘¢ğ‘¥, of his forces that he dedicates to guerrilla
attacks, and the effectiveness of these attacks is reflected by the parameter ğ‘ğ‘¥, and
similarly for Dum, resulting in
ğ‘¥â€² = ğ‘šğ‘¥ âˆ’ ğ‘ğ‘¦ğ‘¢ğ‘¦ğ‘¦,
ğ‘¦â€² = ğ‘šğ‘¦ âˆ’ ğ‘ğ‘¥ğ‘¢ğ‘¥ğ‘¥.
Victory goes to the combatant that brings the most net resources to the battle-
field over the given time period. We take the payoff as the difference in battlefield
resources over the time period:
ğ½ = âˆ«
ğ‘‡
0
```
(1 âˆ’ ğ‘¢ğ‘¥) ğ‘¥ âˆ’ (1 âˆ’ ğ‘¢ğ‘¦) ğ‘¦ ğ‘‘ğ‘¡.
```
Dee wants to maximize this, while Dum wants to minimize it.
We will show that optimal play results in the following:
â€¢ Near the end time ğ‘‡, both players dedicate all forces to the battlefield, ğ‘¢ğ‘¥ =
ğ‘¢ğ‘¦ = 0.
â€¢ If ğ‘‡ is sufficiently large, both players will start out dedicating all resources to
```
attrition: ğ‘¢ğ‘¥ = ğ‘¢ğ‘¦ = 1.
```
```
â€¢ Each player makes a single switch from attrition to attack (bang-bang).
```
â€¢ The player with the largest effect parameter, ğ‘ğ‘–, is the last player to switch.
```
Generically, we would have ğ‘ğ‘¥ â‰  ğ‘ğ‘¦, and we can assume ğ‘ğ‘¥ > ğ‘ğ‘¦ (See Exer-
```
```
cise 12.4 for the case ğ‘ğ‘¥ = ğ‘ğ‘¦.) With this assumption, Dum (playing ğ‘¦) switches
```
```
first and Dee (playing ğ‘¥) switches second.
```
12.3. War 195
We will see that this defines three epochs. Working backwards we will have:
Final Epoch: ğ‘‡âˆ’1 < ğ‘¡ < ğ‘‡: All forces dedicated to attacking, ğ‘¢ğ‘¥ = ğ‘¢ğ‘¦ = 0.
Middle Epoch: ğ‘‡âˆ’2 < ğ‘¡ < ğ‘‡âˆ’1: Dee is attrition ğ‘¢ğ‘¥ = 1, and Dum is attacking
ğ‘¢ğ‘¦ = 0.
Beginning Epoch: 0 < ğ‘¡ < ğ‘‡âˆ’2: All forces dedicated to attrition, ğ‘¢ğ‘¥ = ğ‘¢ğ‘¦ = 1.
To battle, then:
This is a zero-sum game with a single Hamiltonian
```
ğ» = (1 âˆ’ ğ‘¢ğ‘¥)ğ‘¥ âˆ’ (1 âˆ’ ğ‘¢ğ‘¦)ğ‘¦ + ğœ†ğ‘¥(ğ‘šğ‘¥ âˆ’ ğ‘ğ‘¦ğ‘¢ğ‘¦ğ‘¦) + ğœ†ğ‘¦(ğ‘šğ‘¦ âˆ’ ğ‘ğ‘¥ğ‘¢ğ‘¥ğ‘¥)
```
```
which is linear in ğ‘¢ğ‘¥ and ğ‘¢ğ‘¦, and therefore the controls are bang-bang (or station-
```
```
ary, but this possibility is eliminated in Exercise 12.5). And so the agents either
```
dedicate all their forces to the battlefield or all to guerrilla attacks.
At each point in time, Dee wants to operate his control to maximize ğ». We
have ğœ•ğ»
```
ğœ•áµ†ğ‘¥= âˆ’ğ‘¥ âˆ’ ğœ†ğ‘¦ğ‘ğ‘¥ğ‘¥ = âˆ’ğ‘¥(1 + ğœ†ğ‘¦ğ‘ğ‘¥)
```
so, assuming ğ‘¥ > 0, Dee has control strategy
ğœ†ğ‘¦ > âˆ’1/ğ‘ğ‘¥ âŸ¹ ğœ•ğ»ğœ•áµ†ğ‘¥< 0 âŸ¹ ğ‘¢ğ‘¥ = 0,
ğœ†ğ‘¦ < âˆ’1/ğ‘ğ‘¥ âŸ¹ ğœ•ğ»ğœ•áµ†ğ‘¥> 0 âŸ¹ ğ‘¢ğ‘¥ = 1.
Contrariwise, Dum wants to minimize ğ». So
ğœ•ğ»
```
ğœ•áµ†ğ‘¦= ğ‘¦ âˆ’ ğœ†ğ‘¥ğ‘ğ‘¦ğ‘¦ = ğ‘¦(1 âˆ’ ğœ†ğ‘¥ğ‘ğ‘¦)
```
yields Dumâ€™s control strategy:
ğœ†ğ‘¥ < 1/ğ‘ğ‘¦ âŸ¹ ğœ•ğ»ğœ•áµ†ğ‘¦> 0 âŸ¹ ğ‘¢ğ‘¦ = 0,
ğœ†ğ‘¥ > 1/ğ‘ğ‘¦ âŸ¹ ğœ•ğ»ğœ•áµ†ğ‘¦< 0 âŸ¹ ğ‘¢ğ‘¦ = 1.
The costate equations will drive the timing for the optimal strategies and are
given by
```
ğœ†â€²ğ‘¥ = âˆ’ ğœ•ğ»ğœ•ğ‘¥ = âˆ’(1 âˆ’ ğ‘¢ğ‘¥) + ğœ†ğ‘¦ğ‘ğ‘¥ğ‘¢ğ‘¥,
```
```
ğœ†â€²ğ‘¦ = âˆ’ ğœ•ğ»ğœ•ğ‘¦ = (1 âˆ’ ğ‘¢ğ‘¦) + ğœ†ğ‘¥ğ‘ğ‘¦ğ‘¢ğ‘¦.
```
```
Since the end locations for ğ‘¥ and ğ‘¦ are free, we have ğœ†ğ‘¥(ğ‘‡) = ğœ†ğ‘¦(ğ‘‡) = 0. This
```
```
shows that the end-game is to have all forces on the battlefield, ğ‘¢ğ‘¥(ğ‘‡) = ğ‘¢ğ‘¦(ğ‘‡) =
```
0. Thus the final epoch has
ğœ†â€²ğ‘¥ = âˆ’1,
ğœ†â€²ğ‘¦ = 1.
Figure 12.2 shows the final epoch in the costate plane, with the terminal
```
point being (ğœ†ğ‘¥, ğœ†ğ‘¦) = (0, 0).
```
196 Chapter 12. Differential Games
Figure 12.2. Optimal solution for final epoch in costate space.
Under the assumption ğ‘ğ‘¥ > ğ‘ğ‘¦, Dee will be the last to switch strategies,
at time ğ‘‡âˆ’1 = ğ‘‡ âˆ’ 1/ğ‘ğ‘¥. The final epoch for time ğ‘‡âˆ’1 â‰¤ ğ‘¡ â‰¤ ğ‘‡ begins with
```
ğœ†ğ‘¥(ğ‘‡âˆ’1) = 1/ğ‘ğ‘¥ and ğœ†ğ‘¦(ğ‘‡âˆ’1) = âˆ’1/ğ‘ğ‘¥.
```
The middle epoch has ğœ†ğ‘¥ < 1/ğ‘ğ‘¦ and ğœ†ğ‘¦ < âˆ’1/ğ‘ğ‘¥ making ğ‘¢ğ‘¥ = 1, ğ‘¢ğ‘¦ = 0,
and
ğœ†â€²ğ‘¥ = ğœ†ğ‘¦ğ‘ğ‘¦,
ğœ†â€²ğ‘¦ = 1.
Note ğœ†â€²ğ‘¦ > 0 and ğœ†â€²ğ‘¥ < 0. The beginning of the middle epoch is characterized by
Dum switching strategies when ğœ†ğ‘¥ = 1/ğ‘ğ‘¦. This epoch is shown in Figure 12.3.
Figure 12.3. Optimal solution for middle and final epoch in
costate space.
12.3. War 197
To determine the beginning of the middle epoch we need to solve the follow-
ing for ğ‘‡âˆ’2:
ğœ†â€²ğ‘¥ = ğœ†ğ‘¦ğ‘ğ‘¥,
ğœ†â€²ğ‘¦ = 1,
```
ğœ†ğ‘¥(ğ‘‡âˆ’1) = 1/ğ‘ğ‘¥,
```
```
ğœ†ğ‘¦(ğ‘‡âˆ’1) = âˆ’1/ğ‘ğ‘¥,
```
```
ğœ†ğ‘¥(ğ‘‡âˆ’2) = 1/ğ‘ğ‘¦
```
where ğ‘‡âˆ’1 = ğ‘‡ âˆ’ 1/ğ‘ğ‘¥. This is not terribly difficult, but it gets messy real fast if
you just jump in and start swinging. The following is a better approach.
```
Let ğ‘ˆ(ğ‘¡) = ğœ†ğ‘¥(ğ‘¡ + ğ‘‡âˆ’2) âˆ’ 1/ğ‘ğ‘¥ and ğ›¿ = 1/ğ‘ğ‘¦ âˆ’ 1/ğ‘ğ‘¥. The above then reduces
```
to solving for ğœ– in the following:
ğ‘ˆâ€³ = ğ‘ğ‘¥,
```
ğ‘ˆ(ğœ–) = 0,
```
```
ğ‘ˆâ€²(ğœ–) = âˆ’1,
```
```
ğ‘ˆ(0) = ğ›¿.
```
```
This system can be solved as (âˆ— check this âˆ—)
```
ğœ– = 1ğ‘
ğ‘¥
```
(âˆ’1 +
```
âˆš
2 ğ‘ğ‘¥ğ‘
ğ‘¦
```
âˆ’ 1) .
```
Then ğœ– is the time length of the middle epoch and
ğ‘‡âˆ’2 = ğ‘‡âˆ’1 âˆ’ ğœ– = ğ‘‡ âˆ’ 1ğ‘
ğ‘¥
```
(
```
âˆš
2 ğ‘ğ‘¥ğ‘
ğ‘¦
```
âˆ’ 1) .
```
The first epoch 0 < ğ‘¡ < ğ‘‡âˆ’2 is then defined by
ğœ†â€²ğ‘¥ = ğœ†ğ‘¦ğ‘ğ‘¦,
ğœ†â€²ğ‘¦ = ğœ†ğ‘¥ğ‘ğ‘¥
with eigenvalues ğ‘’1, ğ‘’2 for associated eigenvectors ğ¯1, ğ¯2,
```
ğ‘’1 = âˆšğ‘ğ‘¥ğ‘ğ‘¥ â‡„ ğ¯ğŸ = (âˆšğ‘ğ‘¥, âˆšğ‘ğ‘¦) ,
```
```
ğ‘’2 = âˆ’âˆšğ‘ğ‘¥ğ‘ğ‘¥ â‡„ ğ¯ğŸ = (âˆšğ‘ğ‘¥, âˆ’âˆšğ‘ğ‘¦) .
```
Since ğœ†â€²ğ‘¥ < 0 and ğœ†â€²ğ‘¦ > 0 in this region, it is clear that neither player will cross a
switching threshold until the end of the first epoch 0 < ğ‘¡ < ğ‘‡âˆ’2. The full costate
trajectory is depicted in Figure 12.4.
198 Chapter 12. Differential Games
Figure 12.4. Complete optimal solution in costate space.
Weâ€™re done with the costates in this case. We only needed them to determine
the switching times
ğ‘‡âˆ’1 = ğ‘‡ âˆ’ 1ğ‘ğ‘¥,
```
ğ‘‡âˆ’2 = ğ‘‡ âˆ’ 1ğ‘ğ‘¥ (âˆš2 ğ‘ğ‘¥ğ‘ğ‘¦âˆ’ 1) .
```
To solve for the trajectories, just turn things around and stitch together the
three epochs. We assume that the states ğ‘¥ and ğ‘¦ remain positive. Things get more
complicated, perhaps not even well defined, if one of the states hits zero.
```
We begin with prescribed values for ğ‘¥(0) and ğ‘¦(0) and flow under the system
```
ğ‘¥â€² = ğ‘šğ‘¥ âˆ’ ğ‘ğ‘¦ğ‘¦,
ğ‘¦â€² = ğ‘šğ‘¦ âˆ’ ğ‘ğ‘¥ğ‘¥
```
for time 0 < ğ‘¡ < ğ‘‡âˆ’2. Use the terminal states ğ‘¥(ğ‘‡âˆ’2) and ğ‘¦(ğ‘‡âˆ’2) as new initial
```
states to flow under
ğ‘¥â€² = ğ‘šğ‘¥ âˆ’ ğ‘ğ‘¦ğ‘¦,
ğ‘¦â€² = ğ‘šğ‘¦
```
for time ğ‘‡âˆ’2 < ğ‘¡ < ğ‘‡âˆ’1. Then again use the terminal states ğ‘¥(ğ‘‡âˆ’1) and ğ‘¦(ğ‘‡âˆ’1) as
```
new initial states to flow under
ğ‘¥â€² = ğ‘šğ‘¥,
ğ‘¦â€² = ğ‘šğ‘¦
for time period ğ‘‡âˆ’1 < ğ‘¡ < ğ‘‡.
Exercises 199
```
The payoff is then (remember that we assume ğ‘ğ‘¥ > ğ‘ğ‘¦)
```
ğ½ = âˆ«
ğ‘‡âˆ’1
ğ‘‡âˆ’2
âˆ’ğ‘¦ ğ‘‘ğ‘¡ + âˆ«
ğ‘‡
ğ‘‡âˆ’1
ğ‘¥ âˆ’ ğ‘¦ ğ‘‘ğ‘¡.
Tweedle-Dee wins if this is positive, Tweedle-Dum if this is negative.
If they can figure this out, maybe they donâ€™t have to battle and can work
together to fix the rattle, before the monstrous crow arrives.
Key Points
Pontryaginâ€™s principles can be applied to game theory, where multiple agents are each
trying to optimize with respect to the situation and their assumption as to what the
other agents will do. The solution is that each player co-optimizes their own Hamil-
tonian.
This extends the concept of a Nash equilibrium to games defined by differential
equations.
Exercises
```
Exercise 12.1(h). The solution to the Guard the Target game is depicted in Figure 12.5.
```
Provide an intuitive explanation for this solution.
Figure 12.5. Solution to the Guard the Target game.
Exercise 12.2. This linear-quadratic exercise is similar to Example 12.2, but it consid-
ers a case of combining efforts towards a common goal.
```
Suppose our initial state is ğ‘¥(0) = 0 and we have a fixed end time ğ‘‡ = 2 and a
```
dynamic ğ‘¥â€² = ğ›¼ğ‘¢ + ğ›½ğ‘£ where ğ‘¢ is player ğ¼â€™s control and ğ‘£ is player ğ¼ğ¼â€™s control, and
```
ğ›¼, ğ›½ > 0 represent the players respective abilities. Each player wants to maximize ğ‘¥(2)
```
and incurs a cost for exercising their control. Their payoffs are
```
ğ½ğ¼ = ğ‘¥(2) âˆ’ âˆ«20 ğ‘¢2 ğ‘‘ğ‘¡,
```
```
ğ½ğ¼ğ¼ = ğ‘¥(2) âˆ’ âˆ«20 ğ‘£2 ğ‘‘ğ‘¡.
```
200 Chapter 12. Differential Games
Analyze optimal play and outcomes for this game under different scenarios:
```
(a) Equal ability ğ›¼ = 1, ğ›½ = 1.
```
```
(b) Unequal ability ğ›¼ = 2, ğ›½ = 1.
```
```
(c) Total slacker ğ›¼ = 4, ğ›½ = 0.
```
Comment on your results. How does player ğ¼â€™s optimal strategy depend on player ğ¼ğ¼â€™s
```
ability (and vice versa)?
```
```
Exercise 12.3(s). Consider the cooperation problem from Example 12.3 where the
```
players have equal valuation of the outcome but incur different costs.
```
We have ğ‘¥â€² = (ğ‘¢ âˆ’ 1)(ğ‘£ âˆ’ 1) with ğ‘¢, ğ‘£ > 1 and payoffs
```
```
ğ½ğ¼ = ğ‘¥(ğ‘‡) âˆ’ âˆ«ğ‘‡0 ğ›¾ğ‘¢2 ğ‘‘ğ‘¡,
```
```
ğ½ğ¼ğ¼ = ğ‘¥(ğ‘‡) âˆ’ âˆ«ğ‘‡0 ğœŒğ‘£2 ğ‘‘ğ‘¡
```
```
with a fixed end time ğ‘‡ and initial value ğ‘¥(0) = 0.
```
```
(a) Find the Nash equilibrium.
```
```
(b) Suppose ğ›¾ = 1/2 and ğœŒ = 1. Who makes a greater effort? Who gets a greater
```
payoff?
```
Exercise 12.4(h). What happens if ğ‘ğ‘¥ = ğ‘ğ‘¦ in Attrition and Attack (Example 12.4)?
```
Can you formulate who will win in this case?
```
Exercise 12.5(h). Show that a stationary solution is not viable in Attrition and Attack
```
```
(Example 12.4).
```
Exercise 12.6. Grow or Aggress. Two competitors have resources ğ‘¥ and ğ‘¦. They can
each decide whether to allocate these towards their own growth or aggression against
the other:
```
ğ‘¥â€² = ğ›¼ğ‘¥(1 âˆ’ ğ‘¢ğ‘¥)ğ‘¥ âˆ’ ğ›½ğ‘¦ğ‘¢ğ‘¦ğ‘¦,
```
```
ğ‘¦â€² = ğ›¼ğ‘¦(1 âˆ’ ğ‘¢ğ‘¦)ğ‘¦ âˆ’ ğ›½ğ‘¥ğ‘¢ğ‘¥ğ‘¥.
```
The payoff is the difference in resources at the end of a fixed time period:
```
ğ½ = ğ‘¥(ğ‘‡) âˆ’ ğ‘¦(ğ‘‡),
```
which ğ¼ wants to maximize and ğ¼ğ¼ wants to minimize.
```
Assume ğ‘‡ is fixed, ğ‘¥(0), ğ‘¦(0) are given, ğ‘¥(ğ‘‡), ğ‘¦(ğ‘‡) are free. Consider three cases:
```
```
(a) Robust: ğ›¼ > ğ›½. Take ğ›¼ğ‘¥ = 2, ğ›¼ğ‘¦ = 3, and ğ›½ğ‘¥ = ğ›½ğ‘¦ = 1.
```
```
(b) Fragile: ğ›¼ < ğ›½. Take ğ›½ğ‘¥ = 2, ğ›½ğ‘¦ = 3, and ğ›¼ğ‘¥ = ğ›¼ğ‘¦ = 1.
```
```
(c) Mixed: Take ğ›¼ğ‘¥ = 3, ğ›½ğ‘¥ = 2, and ğ›¼ğ‘¦ = ğ›½ğ‘¦ = 1.
```
Analyze. Find optimal strategies and switching times. Is it possible to formulate
who will win for given starting positions and parameters? What happens if one player
hits zero?
13
Calculus of Variations
In the applied world of engineering and economics Pontryaginâ€™s theory is a subfield of
control theory. In the abstract mathematical world the theory is usually regarded as
a subfield of calculus of variations, and many treatments of optimal control start with
this topic.
The original problem in calculus of variations is the isoperimetric problem of en-
closing the greatest area with a fixed length for perimeter. This dates back to the 4th
century BC in the story of Dido, the first Queen of Carthage. Itâ€™s an interesting story,
```
and thereâ€™s a pretty cool painting by Gregorio Lazzarini too (look them up).
```
The modern analytic theory of the field started with Euler and Lagrange in the
1750s, then evolved to include more general and increasingly misnamed isoperimetric
problems, and blossomed into our favorite optimal control principles in the mid 1950s
with the work of Lev Pontryagin and his students.
We give a quick rundown of the Euler-Lagrange theory and general isoperimetric
```
problems; they are interesting, powerful, and historically significant. Anyone work-
```
ing in optimal control should know about them, as these ideas are closely related to
Pontryaginâ€™s techniques.
We wonâ€™t go into the beautiful Euler-Lagrange proofs, but the reader is encouraged
to seek them out.
13.1 Euler-Lagrange
```
The basic Euler-Lagrange problem is to find the function ğ‘¦(ğ‘¥) that minimizes
```
ğ½ = âˆ«
ğ‘¥2
ğ‘¥1
```
ğ‘”(ğ‘¦, ğ‘¦â€², ğ‘¥) ğ‘‘ğ‘¥
```
```
for a given function ğ‘”, fixed ğ‘¥1, ğ‘¥2, and fixed endpoints ğ‘¦(ğ‘¥1) = ğ‘¦1 and ğ‘¦(ğ‘¥2) = ğ‘¦2.
```
Solutions to this problem are called extremals.
```
Euler and Lagrange showed that a solution ğ‘¦(ğ‘¥) must necessarily satisfy
```
ğœ•ğ‘”
ğœ•ğ‘¦ âˆ’
ğ‘‘
```
ğ‘‘ğ‘¥ (
```
ğœ•ğ‘”
```
ğœ•ğ‘¦â€² ) = 0.
```
201
202 Chapter 13. Calculus of Variations
Here we have ğ‘¥ as the independent variable and ğ‘¦, ğ‘¦â€² as dependent variables. Note
that the derivative with respect to ğ‘¥ is the full derivative: you have to account for all
dependencies.
Example 13.1
We want to find the extremals for
âˆ«
2
1
```
(ğ‘¦â€²)2ğ‘¥3 ğ‘‘ğ‘¥
```
```
with ğ‘¥(1) = 0 and ğ‘¥(2) = 3.
```
```
This is an Euler-Lagrange problem with ğ‘”(ğ‘¦, ğ‘¦â€², ğ‘¥) = (ğ‘¦â€²)2ğ‘¥3. A solution
```
must satisfy
```
0 = ğœ•ğ‘”ğœ•ğ‘¦ âˆ’ ğ‘‘ğ‘‘ğ‘¥ ( ğœ•ğ‘”ğœ•ğ‘¦â€² ) = âˆ’ ğ‘‘ğ‘‘ğ‘¥ (2ğ‘¦â€²ğ‘¥3)
```
making ğ‘¦â€²ğ‘¥3 a constant, so ğ‘¦â€² = ğ‘ğ‘¥âˆ’3. We conclude ğ‘¦ = ğ‘ğ‘¥âˆ’2 + ğ‘ for some
```
constants ğ‘, ğ‘. We resolve these constants using boundary conditions ğ‘¥(1) = 0
```
```
and ğ‘¥(2) = 3 to conclude
```
ğ‘¦ = 4 âˆ’ 4ğ‘¥2 .
This function minimizes the value of the above integral over all possible functions
with the prescribed boundary conditions.
The following is a classic.
Example 13.2
We want to prove that the shortest distance between two points is a straight line.
```
Let the points be (ğ‘, ğ‘) and (ğ‘, ğ‘‘), and suppose ğ‘ < ğ‘ and the minimizing
```
```
path is a function ğ‘¦(ğ‘¥).
```
So we want to minimize
âˆ«
ğ‘
ğ‘âˆš
```
(1 + (ğ‘¦â€²)2 ğ‘‘ğ‘¥
```
```
subject to ğ‘¦(ğ‘) = ğ‘ and ğ‘¦(ğ‘) = ğ‘‘.
```
```
We have ğ‘”(ğ‘¦, ğ‘¦â€², ğ‘¥) = âˆš(1 + (ğ‘¦â€²)2, which is a function only of ğ‘¦â€², making
```
ğœ•ğ‘”
ğœ•ğ‘¦ = 0 and
ğœ•ğ‘”
ğœ•ğ‘¥ = 0.
Euler-Lagrange then implies
```
0 = ğ‘‘ğ‘‘ğ‘¥ ( ğœ•ğ‘”ğœ•ğ‘¦â€² )
```
and hence ğœ•ğ‘”ğœ•ğ‘¦â€² must be constant. Computing ğ¶ = ğœ•ğ‘”ğœ•ğ‘¦â€² we get
```
ğ¶ = 12 (1 + (ğ‘¦â€²)2)âˆ’1/2 (2ğ‘¦â€²).
```
13.2. Isoperimetric Problems 203
Solving this for ğ‘¦â€² yields
```
ğ‘¦â€² = ğ¶2/(1 âˆ’ ğ¶2),
```
```
which is constant. Thus ğ‘¦(ğ‘¥) is a line. Cool.
```
Exercise 5.11 derives this same result in the form of a control problem.
13.2 Isoperimetric Problems
Isoperimetric problems deal with optimizing
ğ½ = âˆ«
ğ‘¥2
ğ‘¥1
```
ğ‘”(ğ‘¦, ğ‘¦â€², ğ‘¥) ğ‘‘ğ‘¥
```
subject to an integral constraint
ğ¶ = âˆ«
ğ‘¥2
ğ‘¥1
```
ğ‘˜(ğ‘¦, ğ‘¦â€², ğ‘¥) ğ‘‘ğ‘¥
```
```
plus boundary values ğ‘¦(ğ‘¥1) = ğ‘¦1, ğ‘¦(ğ‘¥2) = ğ‘¦2.
```
It can be shown that solutions to this problem must be extreme values of the ex-
pression
âˆ«
ğ‘¥2
ğ‘¥1
```
ğ‘”(ğ‘¦, ğ‘¦â€², ğ‘¥) + ğœ† ğ‘˜(ğ‘¦, ğ‘¦â€², ğ‘¥) ğ‘‘ğ‘¥
```
where we introduce ğœ† as a Lagrange multiplier. Note the Hamiltonian structure of the
integrand. We can now solve this problem using Euler-Lagrange, where the additional
free parameter ğœ† allows for matching the integral constraint.
Example 13.3
We want to minimize
âˆ«
1
0
```
(ğ‘¦â€²)2 ğ‘‘ğ‘¥
```
subject to
âˆ«
1
0
ğ‘¦ ğ‘‘ğ‘¥ = 1
```
with ğ‘¦(0) = 2, ğ‘¦(1) = 4. We formulate
```
âˆ«
1
0
```
(ğ‘¦â€²)2 + ğœ†ğ‘¦ ğ‘‘ğ‘¥
```
```
and apply Euler-Lagrange with ğ‘”(ğ‘¦, ğ‘¦â€², ğ‘¥) = (ğ‘¦â€²)2 + ğœ†ğ‘¦:
```
ğœ•ğ‘”
ğœ•ğ‘¦ âˆ’
ğ‘‘
```
ğ‘‘ğ‘¥ (
```
ğœ•ğ‘”
```
ğœ•ğ‘¦â€² ) = ğœ† âˆ’
```
ğ‘‘
```
ğ‘‘ğ‘¥ (2ğ‘¦
```
```
â€²) = ğœ† âˆ’ 2ğ‘¦â€³ = 0
```
which has solution
ğ‘¦ = 14 ğœ†ğ‘¥2 + ğ‘ğ‘¥ + ğ‘.
204 Chapter 13. Calculus of Variations
```
We use the constraints ğ‘¦(0) = 2, ğ‘¦(1) = 4, and âˆ«10 ğ‘¦ ğ‘‘ğ‘¥ = 1 to get three equations
```
```
to resolve the three unknown values ğ‘, ğ‘, and ğœ†, and we conclude that (âˆ— check
```
```
this âˆ—)
```
ğ‘¦ = 12ğ‘¥2 âˆ’ 10ğ‘¥ + 2.
13.3 Conversions
Euler-Lagrange deals with optimizing an integral with some boundary values.
Isoperimetric problems deal with optimizing an integral subject to an integral con-
straint and some boundary values.
Control theory deals with optimizing an integral subject to a differential equation
constraint and boundary values.
Euler-Lagrange can quickly be cast as control problems by swapping the spatial
coordinates ğ‘¥ and ğ‘¦ for time and state coordinates ğ‘¡ and ğ‘  and introducing a control
ğ‘ â€² = ğ‘¢:
ğ‘¥ âŸ¶ ğ‘¡ âŸ¶ ğ‘¡,
```
ğ‘¦(ğ‘¥) âŸ¶ ğ‘ (ğ‘¡) âŸ¶ ğ‘ ,
```
```
ğ‘¦â€²(ğ‘¥) âŸ¶ ğ‘ â€²(ğ‘¡) âŸ¶ ğ‘¢.
```
Thus we seek to optimize
ğ½ = âˆ«
ğ‘¡2
ğ‘¡1
```
ğ‘”(ğ‘ , ğ‘¢, ğ‘¡) ğ‘‘ğ‘¡
```
subject to
ğ‘ â€² = ğ‘¢.
Indeed, every one-dimensional optimal control problem with ğ‘ â€² = ğ‘¢ is secretly an
Euler-Lagrange problem in disguise.
Isoperimetric problems can also be cast as optimal control problems. Start as above
by phrasing the problem in state ğ‘  and time ğ‘¡ language of dynamics. We handle the
integral constraint
ğ¶ = âˆ«
ğ‘‡
0
```
ğ‘˜(ğ‘ , ğ‘¢, ğ‘¡) ğ‘‘ğ‘¡
```
```
by introducing an additional state variable ğ‘Ÿ with ğ‘Ÿâ€² = ğ‘˜(ğ‘ , ğ‘¢, ğ‘¡), and boundary condi-
```
```
tions ğ‘Ÿ(0) = 0 and ğ‘Ÿ(ğ‘‡) = ğ¶. We thus seek to optimize
```
ğ½ = âˆ«
ğ‘‡
0
```
ğ‘”(ğ‘ , ğ‘¢, ğ‘¡) ğ‘‘ğ‘¡
```
with
ğ‘ â€² = ğ‘¢,
```
ğ‘Ÿâ€² = ğ‘˜(ğ‘ , ğ‘¢, ğ‘¡)
```
13.3. Conversions 205
and constraints
```
ğ‘ (0) = ğ‘¦1, ğ‘ (ğ‘‡) = ğ‘¦2,
```
```
ğ‘Ÿ(0) = 0, ğ‘Ÿ(ğ‘‡) = ğ¶.
```
The following example is the classic isoperimetric problem reformulated as an
optimal control problem: showing that a circle encloses the greatest area for a given
perimeter.
Example 13.4
We want to maximize the area under a function over the interval [âˆ’1, 1] where
the function is zero at the endpoints and has an arclength of ğœ‹.
Using the spatial coordinates, we start with an isoperimetric formulation of
maximizing
ğ½ = âˆ«
1
âˆ’1
ğ‘¦ ğ‘‘ğ‘¥
subject to
âˆ«
1
âˆ’1âˆš
```
1 + (ğ‘¦â€²)2 ğ‘‘ğ‘¥ = ğœ‹
```
with
```
ğ‘¦(âˆ’1) = 0 and ğ‘¦(1) = 0.
```
```
This assumes the minimizing curve is a differentiable function. (Can this as-
```
```
sumption be justified?)
```
We now convert to a control problem: maximize
ğ½ = âˆ«
1
âˆ’1
```
ğ‘ (ğ‘¡) ğ‘‘ğ‘¡
```
subject to
ğ‘ â€² = ğ‘¢,
```
ğ‘Ÿâ€² = (1 + ğ‘¢2)1/2
```
and boundary conditions
```
ğ‘ (âˆ’1) = 0, ğ‘ (1) = 0,
```
```
ğ‘Ÿ(âˆ’1) = 0, ğ‘Ÿ(1) = ğœ‹.
```
Applying Principle IV, the Hamiltonian is
```
ğ» = ğ‘  + ğœ†ğ‘ ğ‘¢ + ğœ†ğ‘Ÿ(1 + ğ‘¢2)1/2
```
with stationarity condition
```
0 = ğœ•ğ»ğœ•ğ‘¢ = ğœ†ğ‘  + ğœ†ğ‘Ÿğ‘¢(1 + ğ‘¢2)1/2 .
```
The costate equations ğœ†â€²ğ‘  = âˆ’1 and ğœ†â€²ğ‘Ÿ = 0 imply ğœ†ğ‘  = ğ‘ âˆ’ ğ‘¡ and ğœ†ğ‘Ÿ = ğ‘ for some
constants ğ‘, ğ‘.
206 Chapter 13. Calculus of Variations
```
Normally we would solve for ğ‘¢ in terms the costates (might be messy), but
```
here we take a different approach and utilize some of the geometry of the prob-
lem.
First we substitute costate forms into 0 = ğœ•ğ»ğœ•áµ† to get
```
0 = (ğ‘ âˆ’ ğ‘¡) + ğ‘ ğ‘¢(1 + ğ‘¢2)1/2
```
or
```
0 = ğ‘(1 + ğ‘¢2)1/2 âˆ’ ğ‘¡(1 + ğ‘¢2)1/2 + ğ‘ğ‘¢. (13.1)
```
Second, we reason that an optimal solution should be symmetric about ğ‘¡ = 0
```
(how so?). And therefore (âˆ— why? âˆ—)
```
âˆ«
1
âˆ’1
```
ğ‘¡ (1 + ğ‘¢2)1/2 ğ‘‘ğ‘¡ = 0.
```
Combine this with integral constraints
```
ğœ‹ = âˆ«1âˆ’1(1 + ğ‘¢2)1/2 ğ‘‘ğ‘¡,
```
0 = âˆ«1âˆ’1 ğ‘¢ ğ‘‘ğ‘¡
```
and integrate equation (13.1) to conclude ğ‘ = 0.
```
```
Returning to equation (13.1) and solving
```
```
0 = âˆ’ğ‘¡(1 + ğ‘¢2)1/2 + ğ‘ğ‘¢
```
for ğ‘¢ gives us
```
ğ‘¢ = |ğ‘¡|(ğ‘2 âˆ’ ğ‘¡2)âˆ’1/2.
```
Integrating ğ‘ â€² = ğ‘¢ yields
```
ğ‘  = (ğ‘2 âˆ’ ğ‘¡2)1/2 + ğ‘.
```
```
This function ğ‘ (ğ‘¡) is the arc of a circle in the (ğ‘ , ğ‘¡)-plane of radius ğ‘ centered at
```
ğ‘  = ğ‘, ğ‘¡ = 0 and containing the points ğ‘  = 0 at ğ‘¡ = Â±1. Representative circular
arcs are shown in Figure 13.1. Requiring an arclength of ğœ‹ forces ğ‘ = 0 and ğ‘ = 1,
making the curve a half-circle.
```
Figure 13.1. Solutions are arcs of circles through (âˆ’1, 0) and (1, 0).
```
Note that specifying an arclength less than ğœ‹ would still produce a circular
arc.
Exercises 207
Key Points
Calculus of variations is the mathematical field of optimization in function space, with
Euler-Lagrange theory as its fundamental concept. The elements of this theory are
closely related to the mathematical foundation of optimal control, and in fact Pontrya-
ginâ€™s principles can be used to solve Euler-Lagrange and isoperimetric problems.
Exercises
Exercise 13.1. Use Euler-Lagrange to find the extremals for:
```
(a) âˆ«21 (ğ‘¦â€²)2/ğ‘¥3 ğ‘‘ğ‘¥ with ğ‘¦(1) = 2, ğ‘¦(2) = 17.
```
```
(b) âˆ«ğœ‹/20 ğ‘¦2 âˆ’ (ğ‘¦â€²)2 âˆ’ 2ğ‘¦ sin ğ‘¥ ğ‘‘ğ‘¥ with ğ‘¦(0) = 1, ğ‘¦(ğœ‹/2) = 1.
```
```
(c) âˆ«ğœ‹0 (ğ‘¦â€²)2 + 2ğ‘¦ sin ğ‘¥ ğ‘‘ğ‘¥ with ğ‘¦(0) = 0, ğ‘¦(ğœ‹) = 0.
```
```
Exercise 13.2(h). Derive Euler-Lagrange from Pontryagin. (This is rather backwards,
```
```
like using the Pythagorean triangle to prove 9 + 16 = 25.)
```
```
Exercise 13.3(s). Convert each of the following isoperimetric problems to control prob-
```
lems and solve.
```
(a) Optimize âˆ«20 (ğ‘¦â€²)2 ğ‘‘ğ‘¥ subject to âˆ«20 ğ‘¦ ğ‘‘ğ‘¥ = 2 with ğ‘¦(0) = 0, ğ‘¦(2) = 1.
```
```
(b) Optimize âˆ«ğœ‹0 (ğ‘¦â€²)2 ğ‘‘ğ‘¥ subject to âˆ«ğœ‹0 ğ‘¦2 ğ‘‘ğ‘¥ = ğœ‹/2 with ğ‘¦(0) = 0, ğ‘¦(2) = 1. Find
```
```
the value(s) for ğ½. What are the maximum and minimum values?
```
Appendix A
Table of Principles
```
â€¢ Principle 0 (Section 3.2): one dimension, discrete time, fixed initial position, free
```
final position, fixed final time, performance is independent of time, control is un-
constrained.
```
â€¢ Principle I (Section 4.1): one dimension, continuous time, fixed initial and final
```
position, fixed final time, performance is independent of time, control is uncon-
strained.
```
â€¢ Principle II (Section 4.2): one dimension, continuous time, fixed initial and fi-
```
nal position, fixed final time, performance may depend on time, control is uncon-
strained.
```
â€¢ Principle III (Section 6.1): one dimension, continuous time, fixed initial position,
```
final position may be fixed or free, final time may be fixed or free, performance may
depend on time, control is unconstrained.
```
â€¢ Principle IV (Section 8.1): two dimensions, continuous time, fixed initial position,
```
final position may be fixed or free, fixed final time, performance may depend on
time, control is unconstrained.
```
â€¢ Principle V (Section 9.1): two dimensions, continuous time, fixed initial position,
```
final position may be fixed or free, final time may be fixed or free, performance may
depend on time, control is unconstrained.
```
â€¢ Principle VI (Section 9.2.1): two dimensions, continuous time, fixed initial posi-
```
tion, final position restricted to a curve, final time may be fixed or free, performance
may depend on time, control is unconstrained.
```
â€¢ Principle VII (Section 10.1): two dimensions, continuous time, initial and final
```
positions may be fixed or free, final time may be fixed or free, performance depends
on time, control is bounded.
209
Appendix B
Two-Dimensional
Linear Systems
This is a very succinct treatment of solving a two-dimensional system of linear differ-
ential equations to support the examples and exercises in the text. This is not a com-
plete review, and it employs computational short cuts. Analyzing systems of first-order
equations is a standard topic in differential equations, and readers are encouraged to
review the material as needed.
This topic also involves some very basic tools from matrix algebra, specifically
eigenvalues and eigenvectors for a 2 Ã— 2 matrix.
The Setup.
To find a general solution to a system of two first-order linear autonomous equa-
tions
ğ‘¥â€² = ğ‘ğ‘¥ + ğ‘ğœ†,
ğœ†â€² = ğ‘ğ‘¥ + ğ‘‘ğœ†
we form the coefficient matrix
ğŒ = [
ğ‘ ğ‘
ğ‘ ğ‘‘
]
and cast the problem as a linear differential equation in two-dimensional space:
```
(
```
ğ‘¥â€²
ğœ†â€²
```
) = [
```
ğ‘ ğ‘
ğ‘ ğ‘‘
```
] (
```
ğ‘¥
ğœ†
```
) .
```
We look for eigenvalues for the coefficient matrix by analyzing the quadratic equation
involving the trace and determinant of the matrix:
```
ğœ…2 âˆ’ Tr(ğŒ)ğœ… + Det(ğŒ) = ğœ…2 âˆ’ (ğ‘ + ğ‘‘)ğœ… + (ğ‘ğ‘‘ âˆ’ ğ‘ğ‘).
```
This quadratic is the characteristic polynomial for the matrix ğŒ.
211
212 Appendix B. Two-Dimensional Linear Systems
Two Real Eigenvalues.
If this quadratic has two distinct real roots ğœ…1 and ğœ…2, we look for eigenvectors in
the form of
â¡
â¢
â¢
â¢
â¢
â¢
â¢
â¢
â¢
â£
```
ğğ‘– = (
```
ğœ…ğ‘– âˆ’ ğ‘‘
ğ‘
```
) if ğ‘ â‰  0
```
```
ğğ‘– = (
```
ğ‘
ğœ…ğ‘– âˆ’ ğ‘
```
) if ğ‘ â‰  0
```
â¤
â¥
â¥
â¥
â¥
â¥
â¥
â¥
â¥
â¦
.
If both ğ‘ = 0 and ğ‘ = 0, then your original system was actually two independent differ-
ential equations. Note that any scalar multiple of an eigenvector is also an eigenvector,
so we may scale our vectors to make them look nicer.
Once we have our two eigenvectors, ğ1, ğ2, we have the general solution to the
system as
```
(
```
ğ‘¥â€²
ğœ†â€²
```
) = ğ¶1 ğ‘’ğœ…1ğ‘¡ ğ1 + ğ¶2 ğ‘’ğœ…2ğ‘¡ ğ2.
```
That is, this form will match any solution to this system for appropriate choice of con-
stants ğ¶1 and ğ¶2.
For example, suppose we have
ğ‘¥â€² = 4ğ‘¥ + 2ğœ†,
ğœ†â€² = âˆ’ğ‘¥ + ğœ†.
Then
ğŒ = [
4 2
âˆ’1 1
]
```
with Tr(ğŒ) = 5 and Det(ğŒ) = 6 and eigenvalues ğœ…1 = 3 and ğœ…2 = 2 (yes, this ex-
```
```
ample was chosen so the numbers would work out nicely). We have corresponding
```
eigenvectors
```
ğ1 = (
```
2
âˆ’1
```
) , ğ2 = (
```
1
âˆ’1
```
) .
```
Our general solution to this system is then
```
(
```
ğ‘¥
ğœ†
```
) = ğ¶1ğ‘’3ğ‘¡ (
```
2
âˆ’1
```
) + ğ¶2ğ‘’2ğ‘¡ (
```
1
âˆ’1
```
)
```
or
ğ‘¥ = 2ğ¶1ğ‘’3ğ‘¡ âˆ’ ğ¶2ğ‘’2ğ‘¡,
ğœ† = âˆ’ğ¶1ğ‘’3ğ‘¡ âˆ’ ğ¶2ğ‘’2ğ‘¡.
Appendix B. Two-Dimensional Linear Systems 213
We can match boundary values with the constants ğ¶1, ğ¶2. If we have the two-point
```
boundary problem ğ‘¥(0) = âˆ’1 and ğ‘¥(1) = 0, we would solve
```
âˆ’1 = 2ğ¶1 âˆ’ ğ¶2,
0 = 2ğ¶1ğ‘’3 âˆ’ ğ¶2ğ‘’2
```
to get ğ¶1 = 1/(2ğ‘’ âˆ’ 2) and ğ¶2 = ğ‘’/(ğ‘’ âˆ’ 1).
```
Complex Eigenvalues.
The quadratic characteristic polynomial
```
ğœ…2 âˆ’ Tr(ğŒ)ğœ… + Det(ğŒ) = ğœ…2 âˆ’ (ğ‘ + ğ‘‘)ğœ… + (ğ‘ğ‘‘ âˆ’ ğ‘ğ‘)
```
```
has complex roots if Tr(ğŒ)2 âˆ’ 4 Det(ğŒ) < 0. These will be in the form of a complex
```
conjugate pair
```
ğ›¼ Â± ğ‘–ğ›½ = âˆ’ 12 Tr(ğŒ) Â± 12 âˆšTr(ğŒ)2 âˆ’ 4 Det(ğŒ).
```
These are in fact eigenvalues for the matrix and have complex-valued eigenvectors, and
everything in the previous section still works, but we really want to stick with a real-
valued state space.
Fortunately, things work out pretty nicely. For complex conjugate eigenvalues ğ›¼ Â±
ğ‘–ğ›½ the general solution for ğ‘¥ is
```
ğ‘¥ = ğ¶1ğ‘’ğ›¼ğ‘¡ cos(ğ›½ğ‘¡) + ğ¶2ğ‘’ğ›¼ğ‘¡ sin(ğ›½ğ‘¡).
```
```
Note that ğ‘¥(0) = ğ¶1, which is convenient.
```
The most straightforward way to get the associated equation for ğœ† is to go back to
the original system equation ğ‘¥â€² = ğ‘ğ‘¥ + ğ‘ğœ†, plug in the above form for ğ‘¥, and solve for
ğœ†.
For example, suppose we have
ğ‘¥â€² = ğ‘¥ âˆ’ 2ğœ†,
ğœ†â€² = 2ğ‘¥ + ğœ†.
Then
ğŒ = [
1 âˆ’2
2 1
]
```
with Tr(ğŒ) = 2 and Det(ğŒ) = 5 and eigenvalues ğ›¼ Â± ğ›½ğ‘– = 1 Â± 2ğ‘–. Our general solution
```
for ğ‘¥ is
```
ğ‘¥ = ğ¶1ğ‘’ğ‘¡ cos(2ğ‘¡) + ğ¶2ğ‘’ğ‘¡ sin(2ğ‘¡).
```
```
For two-point boundary condition ğ‘¥(0) = âˆ’1 and ğ‘¥(1) = 0 we would solve for ğ¶1 = âˆ’1
```
```
and ğ¶2 = cot(1) making
```
```
ğ‘¥ = âˆ’ğ‘’ğ‘¡ cos(ğ‘¡) + ğ‘’ğ‘¡ cot(1) sin(ğ‘¡).
```
To get the associated ğœ†, we would substitute into ğ‘¥â€² = ğ‘¥ âˆ’ 2ğœ† and solve for
```
ğœ† = 12 (ğ‘¥ âˆ’ ğ‘¥â€²) = âˆ’ 12 ğ‘’ğ‘¡(cos(ğ‘¡) cot(1) + sin(ğ‘¡)).
```
Appendix C
Hints
A little help. . .
```
Hint 1.4 . (a) Use a numerical approximation to find zeros of the partial derivatives.
```
```
(b) Youâ€™ll want to use a computer algebra or numerical approximation package to ac-
```
tually approximate a solution.
Hint 1.5 . Take ğ‘¢0 = âˆ’2 and consider the payoff as a function of ğ‘¢1.
```
Hint 1.7 . (e) Assume ğ‘‰ ğ‘ (ğ‘) = ğ¾ğ‘ ğ´2. Can you justify this assumption?
```
Hint 2.7 . Solving five equations that are linear in five unknowns. Lots of calculations,
best to use computer algebra.
```
Hint 2.9 . You should get a maximum of one in part (a), so âˆ‘ ğ‘¥ğ‘–ğ‘¦ğ‘– â‰¤ 1.
```
Hint 2.10 . Proof by contradiction.
Hint 3.3 . Negative values for ğ‘¥ and ğ‘¢ are allowed.
```
Hint 4.11 . (d) Show ğ½ â†’ 100 ğ‘”â€²(0) as ğ‘‡ â†’ âˆ.
```
```
Hint 4.13 . (b) Use symmetry to simplify the calculations.
```
```
Hint 4.15 . (b) Show ğ‘¢/ğ‘¤ < 1 for ğ‘¡ = 0 and ğ‘¢/ğ‘¤ > 1 for ğ‘¡ = ğ‘‡.
```
```
Hint 5.2 . Use Exercise 5.1. Is it a saddle or a (linear) center?
```
```
Hint 5.3 . (b) ğ´2 âˆ’ ğµ2 = (ğ´ âˆ’ ğµ)(ğ´ + ğµ).
```
```
Hint 5.5 . (f) Use a multi-angle formula for sin(1 + ğœ‹/4).
```
215
216 Appendix C. Hints
Hint 6.6 . First show that ğ‘¢ = 0 or ğ‘¢ = âˆ’2 are the only possible values for ğ‘¢ to be
```
optimal. For part (c) show that if ğ‘‡ > 0, then either of these choices is worse than
```
taking ğ‘‡ = 0.
Hint 6.9 . Show that ğ» must be identically zero on an optimal trajectory. Use this to
get ğ‘¢ in terms of ğ‘¥.
```
Hint 6.11 . (a) Show ğ‘¢ = ğœ†ğ‘¥. Show that ğ» as a function of ğ‘¥ and ğœ† reduces to a
```
quadratic function of the product ğ‘¥ğœ†, which must then be constant on level curves.
```
Alternatively, one can show that (ğœ†ğ‘¥)â€² = ğœ†ğ‘¥â€² + ğœ†â€²ğ‘¥ = 0. (b) ğ» must be identically equal
```
```
to one on optimal trajectories. (c) You know ğ‘¢ is constant, so set ğ‘¢ = ğ‘˜ and derive ğ½ as
```
a function of ğ‘˜ and ğ‘‡.
```
Hint 7.6 . The Hamiltonian is constant ğ» = 12 csc2(ğ‘‡/2).
```
Hint 7.8 . ğ‘¢ is constant in all cases.
Hint 8.1 . Retrace the steps in Example 8.1.
Hint 8.2 . Retrace the steps in Example 8.2.
Hint 8.3 . Retrace the steps in Example 8.3. ğœ•ğ½ğœ•ğ‘‡ can be factored.
```
Hint 8.7 . (b) Use ğ» is constant, always equal to ğ»(0).
```
Hint 8.10 . This is a challenging problem, and there may be several different ap-
```
proaches. Hereâ€™s one possible way: (1) get costate equations ğœ†â€²ğ‘¥ = 0, ğœ†â€²ğ‘¦ = âˆšğœ†2ğ‘¥ + ğœ†2ğ‘¦,
```
```
(2) use ğ» â‰¡ âˆ’1 to get ğ‘¦ = âˆ’1/âˆšğœ†2ğ‘¥ + ğœ†2ğ‘¦, (3) derive ğ‘¦ = âˆ’1/ cosh(ğ‘¡), ğ‘¥ = tanh(ğ‘¡).
```
```
Hint 8.11 . ğ‘ƒ is the control, ğ» = ğ‘ƒ2 + ğœ†ğ¼ (ğ‘ƒ âˆ’ ğ‘†) + ğœ†ğ‘† (ğ‘ƒ âˆ’ 1)/2.
```
```
Hint 9.2 . (c) The conditions would be ğ‘¥(0)2 +ğ‘¦(0)2 = 1 and ğœ†ğ‘¥(0)ğ‘¦(0)âˆ’ğœ†ğ‘¦(0)ğ‘¥(0) = 0.
```
```
Hint 9.4 . (a) ğœƒ(ğ‘‡) = Â±ğœ‹/2. (b) There is a nice geometric interpretation.
```
Hint 9.6 . Rework Example 8.4 using Principle V and the new conditions.
```
Hint 10.1 . Calculations are messy; use a computer algebra.
```
Hint 10.6 . At any time in the process, your only choices are ğ‘¢ = 0, ğ‘¢ = ğ‘¢max, or, if
ğ‘¥ = 1/2, ğ‘¢ = 1/4. Your only opportunity to change strategies is when ğœ•â„ğœ•áµ† changes sign
```
or becomes zero. (c) Find the optimal time to switch from stasis (ğ‘¥ = 1/2, ğ‘¢ = 1/4) to
```
maximum harvest.
```
Hint 10.7 . (b) Consider ğµ < âˆ’1, âˆ’1 < ğµ < 1, and 1 < ğµ. The value ğ´ wonâ€™t affect the
```
location of the minimum. Take ğ´ = 0 and sketch some examples.
Appendix C. Hints 217
```
Hint 10.9 . (b) For ğ‘¢ = âˆ’1, the circle (ğ‘¥âˆ’1)2 +ğ‘¦2 = 1 contains the trajectory that termi-
```
```
nates at (0, 0). Only the lower half of the circle would be part of an optimal trajectory,
```
```
as ğ‘¢ = 1 would be a faster option to get from (ğ‘¥, ğ‘¦), ğ‘¥ > 0, ğ‘¦ > 0, to (ğ‘¥, âˆ’ğ‘¦).
```
Hint 10.10 . Use qualitative properties of the phase portrait.
```
Hint 11.3 . Follow the structure of Example 11.4; guess ğ‘‰(ğ‘¥, ğ‘¡) = ğ‘¥2ğ‘ (ğ‘¡).
```
```
Hint 11.5 . ğ‘‰ = (ğ‘¥ğ‘Š(ğ‘¡))1/2 would be a good guess, with ğ‘Š(10) = 0.
```
Hint 11.7 . See Example 11.6. Symbolic processor recommended.
Hint 12.1 . Consider the perpendicular bisector of the line segment between ğ¼â€™s and
ğ¼ğ¼â€™s initial positions. Then ğ¼ can mirror ğ¼ğ¼â€™s movements with respect to this line.
Hint 12.4 . Use pictures.
Hint 12.5 . Stationarity would require ğœ†ğ‘¦ â‰¡ âˆ’1/ğ‘ğ‘¥ and ğœ†ğ‘¥ â‰¡ 1/ğ‘ğ‘¦. How could you
operate the controls to attain this?
Hint 13.2 . Use the conversions from Section 13.3.
Appendix D
Solutions
There are no problems here.
```
Solution 1.4 . (a) ğ½(ğ‘¥1, ğ‘¦1) = ğ‘¥21 + ğ‘¦21 + ((10 âˆ’ ğ‘¥1)2 + ğ‘¦21)/(|ğ‘¦1| + 1) has a minimum
```
```
at approximately ğ‘¥1 = 2.337 . . . , ğ‘¦1 = Â±2.278 . . . . (b) (0, 0) â†’ (1.373 . . . , 1.535 . . . ) â†’
```
```
(4.854 . . . , 2.748 . . . ) â†’ (10, 0).
```
```
Solution 1.7 . (a) ğ‘‰3(ğ‘) = 64ğ‘2/21, ğ‘‰2(ğ‘) = 16ğ‘2/5.
```
```
Solution 2.1 . Maximum is âˆš3/2; minimum is âˆ’âˆš3/2.
```
```
Solution 2.2 . Minimum of 121/32 at (5/4, 5/4, âˆ’1/2).
```
Solution 2.5 . Minimum is 1 + 5ğœ… âˆ’ 10âˆšğœ….
```
Solution 2.7 . ğ½(ğ‘, ğ‘) = âˆ’ (5ğ‘2 âˆ’ 10ğ‘ğ‘ + ğ‘2) /20.
```
```
Solution 3.1 . (a) ğ‘¢ğ‘– = ğœ†ğ‘–+1. (b) ğœ†ğ‘– = ğœ†ğ‘–+1 âˆ’1. (c) ğœ†ğ‘ = 4. (e) ğ½ = âˆ’(4)(90)+(100âˆ’ 12 )+
```
```
(99 âˆ’ 2) + (97 âˆ’ 92 ) + (94 âˆ’ 8), fifteen bucks.
```
```
Solution 3.2 . (a) ğ‘¢ğ‘– = âˆ’ğœ†ğ‘–+1/2. (b) ğœ†ğ‘– = âˆ’ğœ†2ğ‘–+1/4. (c) ğœ†ğ‘ = 4. (e) ğ½ = 4 for any value
```
of ğ‘.
Solution 3.4 . ğ‘¢ğ‘– = âˆ’ğ‘¥ âˆ’ ğœ†ğ‘–/2, ğœ†ğ‘ = âˆ’100, and ğœ†ğ‘– = 0 for ğ‘– = 1, . . . , ğ‘.
```
Solution 4.1 . ğ‘¥(ğ‘¡) = ğ‘’2(ğ‘’âˆ’ğ‘¡ âˆ’ ğ‘’âˆ’2ğ‘¡)/(ğ‘’ âˆ’ 1).
```
```
Solution 4.2 . ğ‘¥ = ğ‘’ğ‘¡âˆ’1((3 âˆ’ ğ‘’)ğ‘¡ + ğ‘’).
```
```
Solution 4.3 . ğ‘¥(ğ‘¡) = (ğ‘¡ + 1) ln(ğ‘¡ + 1) âˆ’ ğ‘¡ + 1, ğ‘¢ = âˆ’ğœ† = 1 + ğ‘¡, ğ» = 2.
```
219
220 Appendix D. Solutions
```
Solution 4.4 . ğ‘¥(ğ‘¡) = ğ‘¡3 + 1, ğœ† constant, ğ‘¢ = âˆš3 ğ‘¡, ğ» = âˆš3 ğ‘¡2/2.
```
```
Solution 4.5 . ğ‘¥ = ğµ(ğ‘’âˆ’2ğ‘¡ âˆ’ 1)/(ğ‘’âˆ’2 âˆ’ 1), ğ» = âˆ’4ğ‘’âˆ’2ğ‘¡ğµ2/(ğ‘’âˆ’2 âˆ’ 1)2.
```
```
Solution 4.6 . (a) ğµ â‰¥ ğ‘’. (b) 1 â‰¤ ğµ â‰¤ 2ğ‘’ âˆ’ 1.
```
```
Solution 4.7 . (a) ğµ > ğ´ğ‘’. (b) ğ‘¥ = ğ‘’ğ‘¡âˆ’1((ğµ âˆ’ ğ´ğ‘’)ğ‘¡ + ğ´ğ‘’).
```
```
Solution 4.12 . (a) ğ‘¢party = .2(2 âˆ’ ğ‘’2)/(1 âˆ’ ğ‘’.2(10âˆ’ğ‘)). (d) Spend all excess money all at
```
once at the very last instant in a big screaming blowout par-tay.
```
Solution 4.13 . (a) ğ½ = 2 âˆ«1/ğ¾0 (ğ¾ğ‘¡ âˆ’ 1)2 + ğ¾2 ğ‘‘ğ‘¡ = 2(ğ¾ + 1/(3ğ¾)).
```
```
Solution 4.15 . (a) ğ‘¢ = ğ‘¤ğ‘’ğ‘Ÿ(ğ‘¡âˆ’ğ‘‡)(ğ‘’ğ‘Ÿğ‘‡ âˆ’ 1)/(ğ‘Ÿğ‘‡).
```
```
Solution 5.3 . (a) ğ‘¥(ğ‘¡) = (ğ‘’ğ‘¡ âˆ’ ğ‘’âˆ’ğ‘¡)/(ğ‘’2 âˆ’ ğ‘’âˆ’2). (c), (d) ğ‘¥(ğ‘¡) = (ğ‘’1+ğ‘¡ âˆ’ ğ‘’âˆ’1âˆ’ğ‘¡)/(ğ‘’2 âˆ’ ğ‘’âˆ’2).
```
```
Solution 5.5 . (d), (e) ğ‘¥ = âˆ’ cos ğ‘¡ + sin ğ‘¡. (f) ğ‘¥ = âˆš2 sin ğ‘¡ matches with ğœ = ğ‘¡ âˆ’ ğœ‹/4.
```
```
Solution 5.6 . ğ‘¢ = 2ğœ† making ğ» = (ğ‘¥ + ğœ†)2.
```
```
Solution 5.7 . (a) ğ‘¢ = âˆ’2ğœ†, assuming ğ‘¥ â‰  0. (b) ğ» = âˆ’ğ‘¥ğœ†2. (d) ğ‘¥ = âˆ’ğ¶2(ğ¶1 âˆ’ ğ‘¡)2.
```
```
Solution 5.8 . (a) ğ½(ğµ) = âˆ’ğµ2/2.
```
```
Solution 5.9 . ğ½ = ln(ğµ âˆ’ ğ´ğ‘’) âˆ’ 12 . Note ğµ âˆ’ ğ´ğ‘’ > 0 by Exercise 4.7.
```
```
Solution 5.10 . ğ‘¥ = ğ´ cos( ğœ‹2 ğ‘¡) + ğµ sin( ğœ‹2 ğ‘¡), and ğ½(ğ´, ğµ) = âˆ’ğœ‹ğ´ğµ.
```
```
Solution 6.4 . ğµ = 3ğ‘‡/(1 + ğ‘‡), ğ½ = ğ‘‡(8 âˆ’ ğ‘‡)/(1 + ğ‘‡).
```
```
Solution 6.5 . ğ» = ğœ†(1 + ğœ†/4), and so ğ‘¢ = 0 or ğ‘¢ = âˆ’2.
```
```
Solution 6.8 . (a) ğ½ = âˆšğ‘‡. (b) ğ½ = 1/ğ‘‡. So ğ½ â†’ 0 as ğ‘‡ â†’ âˆ. (c) ğ½ = ğ‘‡1âˆ’ğ›¼. If ğ›¼ = 1,
```
```
then ğ½ = ğ‘¥(ğ‘‡) = 1.
```
```
Solution 6.9 . ğ‘¥ = 9/(9 + ğ‘’ğ‘¡)
```
```
Solution 6.10 . ğ‘¢(ğ‘¡) = ğ‘¤ğ‘’ğ‘Ÿğ‘¡(ğ‘’ğ‘Ÿğ‘‡ âˆ’ 1)/(ğ‘Ÿ(ğ‘ğ‘’ğ‘Ÿğ‘¡ + ğ‘‡ğ‘’ğ‘Ÿğ‘‡ )).
```
```
Solution 6.11 . (b) ğ‘‡ = 23 ln(ğµ).
```
```
Solution 6.12 . (a) ğ½(ğ‘‡) = ğ›¼2ğ‘ƒ24ğ‘Ÿ (1 âˆ’ ğ‘’âˆ’ğ‘Ÿğ‘‡ )ğ‘’âˆ’ğ‘Ÿğ‘‡ . (b) ğ» = ğ‘2ğ‘ƒ2ğ‘’(ğ‘Ÿ(ğ‘¡ âˆ’ 2ğ‘‡))/4.
```
```
Solution 7.1 . (a) ğ» = âˆ’ğ‘¥ğœ†âˆ’ğœ†2/2. (b) ğ‘¥(ğ‘¡) = (ğ‘’ğ‘‡âˆ’ğ‘¡âˆ’ğ‘’ğ‘¡)/(ğ‘’ğ‘‡ âˆ’1). (c) ğ‘¥(ğ‘¡) = (ğ‘’ğ‘¡(1âˆ’2ğ‘’âˆ’ğ‘‡ )
```
- ğ‘’ğ‘‡âˆ’ğ‘¡(2 âˆ’ ğ‘’âˆ’ğ‘‡ ))/(ğ‘’ğ‘‡ âˆ’ ğ‘’âˆ’ğ‘‡ ).
Appendix D. Solutions 221
```
Solution 7.2 . (a) ğ‘¥(ğ‘¡) = (2ğ‘’ğ‘‡âˆ’ğ‘¡ âˆ’ 4ğ‘’ğ‘¡âˆ’ğ‘‡ )/(ğ‘’ğ‘‡ âˆ’ 2ğ‘’âˆ’ğ‘‡ ).
```
```
Solution 7.5 . (a) ğ½(ğ‘‡) = âˆ’ cot(ğ‘‡/2).
```
Solution 7.8 . ğ‘¢ = ğµ/ğ‘‡ is constant in all cases. If ğµ is free, ğµ it must satisfy ğœ•ğºğœ•ğµ =
âˆ’2ğµ/ğ‘‡. If ğ‘‡ is free, ğ‘‡ must satisfy ğœ•ğºğœ•ğ‘‡ = âˆ’ğµ2/ğ‘‡2.
```
Solution 8.1 . (b) ğ½(ğ‘‡) = 12/ğ‘‡3.
```
```
Solution 8.2 . (a) ğ½(ğ‘‡) = 12/ğ‘‡3 + ğ‘‡. (b) ğ‘‡ = Â±âˆš6.
```
```
Solution 8.3 . (a) ğ½ = (ğ‘‡4 + 12ğ‘¥20 + 12ğ‘‡ğ‘¥0ğ‘¦0 + 4ğ‘‡2ğ‘¦20)/ğ‘‡3 with ğ½â€²(ğ‘‡) =
```
```
(ğ‘‡2 âˆ’ 2ğ‘‡ğ‘¦0 âˆ’ 6ğ‘¥0) (ğ‘‡2 + 2ğ‘‡ğ‘¦0 + 6ğ‘¥0) /ğ‘‡4. (b) ğ½â€² = 0 for ğ‘‡ = âˆ’ğ‘¦0 Â± âˆšğ‘¦20 âˆ’ 6ğ‘¥0
```
```
and ğ‘‡ = ğ‘¦0 Â± âˆšğ‘¦20 + 6ğ‘¥0. (c) Get multiple positive solutions for ğ‘‡ when 6|ğ‘¥0| < ğ‘¦20.
```
```
Solution 8.4 . (b) Reverse time and direction. (c) ğµ = 1 has a cost of zero.
```
```
Solution 8.6 . (b) ğ‘¥(ğ‘¡) = ğ‘¡ cos(ğ‘¡+ğ‘˜), ğ‘¦(ğ‘¡) = ğ‘¡ sin(ğ‘¡+ğ‘˜), ğ‘‡2 = ğ‘¥21 +ğ‘¦21, tan(ğ‘‡+ğ‘˜) = ğ‘¦1/ğ‘¥1.
```
```
(c) ğ‘¥ = ğ‘¡ cos(ğ‘¡ + ğ‘˜) + ğ‘¥0 cos(ğ‘¡) âˆ’ ğ‘¦0 sin(ğ‘¡), ğ‘¦ = ğ‘¡ sin(ğ‘¡ + ğ‘˜) + ğ‘¥0 sin(ğ‘¡) + ğ‘¦0 cos(ğ‘¡).
```
```
(d) ğ‘‡ = 1.478 . . . , ğœ… = 2.402 . . . . Cuts inside: rotational speed of water is constant, so
```
there would be no savings to going outside.
```
Solution 8.11 . ğ‘ƒ(ğ‘¡) = (3/8)ğ‘¡ + (1/4), ğ¼(ğ‘¡) = âˆ’(1/32)ğ‘¡3 + (3/8)ğ‘¡2 âˆ’ (3/4)ğ‘¡ + 1, ğ‘†(ğ‘¡) =
```
```
(3/32)ğ‘¡2 âˆ’ (3/8)ğ‘¡ + 1.
```
```
Solution 9.2 . (a), (b) ğœ†ğ‘¥ = ğ‘, ğœ†ğ‘¦ = âˆ’ğ‘ğ‘¡ + ğ‘, ğ‘¢ = âˆ’ 12 ğœ†ğ‘¦. Use ğ‘¥(0) = 0 and ğ‘¦(0) = 0 to
```
```
get ğ‘¥(ğ‘¡) = ğ‘ğ‘¡3/12 and ğ‘¦ = ğ‘ğ‘¡2/4. End costate is ğœ†ğ‘¥ + ğ‘šğœ†ğ‘¦ = 0 for lines ğ‘¦ âˆ’ ğ‘šğ‘¥ = 1.
```
```
Solution 9.6 . ğœ†ğ‘¥(ğ‘‡) = 1, ğœ†ğ‘¦(ğ‘‡) = 0, tan(ğœƒ) = ğœ… (ğ‘‡ âˆ’ ğ‘¡).
```
Solution 9.7 . In one minute, you can get over 7 kilometers away by dropping 3 kilo-
meters. Your final velocity would be nearly 900 km/hr. Yeeehaaa.
```
Solution 10.1 . (b) ğ‘£ = âˆ’ğ‘”ğ‘¡ + 800 ln(1000 + ğ¾) âˆ’ 800 ln(1000 + ğ¾ âˆ’ 60ğ‘¡),
```
```
ğ‘¦ = (1/6)(âˆ’3ğ‘¡(âˆ’1600 + ğ‘”ğ‘¡) âˆ’ 80(1000 + ğ¾ âˆ’ 60ğ‘¡) ln(1000 + ğ¾) +
```
```
80(1000 + ğ¾ âˆ’ 60ğ‘¡) ln(1000 + ğ¾ âˆ’ 60ğ‘¡)).
```
```
Solution 10.2 . (a) One switch, off to on. (b) One switch, on to off.
```
```
Solution 10.5 . (c) âˆšğ‘¥0.
```
```
Solution 11.3 . ğ‘‰ = 2ğ‘¥2ğ‘’2ğ‘¡/(3ğ‘’2ğ‘‡ âˆ’ ğ‘’2ğ‘¡).
```
```
Solution 11.5 . ğ‘‰ = ğ‘’.1ğ‘¡âˆš10ğ‘¥(ğ‘’.1ğ‘¡ âˆ’ ğ‘’).
```
222 Appendix D. Solutions
```
Solution 11.6 . (a) ğ‘¢ = âˆ’ğ‘¥0/ğ‘‡, ğ‘¥ = ğ‘¥0(1 âˆ’ ğ‘¡/ğ‘‡), ğ½ = ğ‘¥20/ğ‘‡.
```
```
Solution 11.7 . (a)Ìƒ ğ½(ğ‘¥, ğ‘¦, ğ‘‡) = 4(3ğ‘¥2 + 3ğ‘‡ğ‘¥ğ‘¦ + ğ‘‡2ğ‘¦2)/ğ‘‡3.
```
```
Solution 11.8 . (a) ğ‘‰(ğ‘¥, ğ‘¡) = ((2ğ‘‡ âˆ’ 2ğ‘¡ + ğ‘¥)(4 âˆ’ ğ‘¥) âˆ’ (ğ‘‡ âˆ’ ğ‘¡)2)/(1 + ğ‘‡ âˆ’ ğ‘¡).
```
```
Solution 12.3 . (a) ğ‘¢ = (1 + ğœŒ)/(1 âˆ’ ğ›¾ğœŒ), ğ‘£ = (1 + ğ›¾)/(1 âˆ’ ğ›¾ğœŒ).
```
```
Solution 13.3 . (a) ğ‘¦ = 2ğ‘¥ âˆ’ 3ğ‘¥2/4. (b) ğ‘¦ = sin ğ‘›ğ‘¥, ğ½ = ğ‘›2ğœ‹/2.
```
Bibliography
[1] Martino Bardi and Italo Capuzzo-Dolcetta, Optimal control and viscosity solutions of Hamilton-Jacobi-
Bellman equations, with appendices by Maurizio Falcone and Pierpaolo Soravia, Systems & Control:
Foundations & Applications, BirkhÃ¤user Boston, Inc., Boston, MA, 1997, DOI 10.1007/978-0-8176-
4755-1. MR1484411
[2] Stephen Barnett, Introduction to mathematical control theory, Oxford Applied Mathematics and Com-
puting Science Series, Clarendon Press, Oxford, 1975. MR441413
[3] Richard Bellman, Introduction to the mathematical theory of control processes. Vol. II: Nonlinear pro-
cesses, Mathematics in Science and Engineering, Vol. 40-II, Academic Press, New York-London, 1971.
MR278767
[4] Richard Bellman, Adaptive control processes: a guided tour, Princeton University Press, Princeton, NJ,
1961. MR134403
[5] Arthur E. Bryson Jr. and Yu Chi Ho, Applied optimal control: optimization, estimation, and control,
```
revised printing, Hemisphere Publishing Corp., Washington, DC; distributed by Halsted Press [John
```
Wiley & Sons, Inc.], New York-London-Sydney, 1975. MR446628
[6] Avinash K. Dixit, Optimization in economic theory, 2nd ed., Oxford University Press, Oxford, 1990.
[7] Avner Friedman, Differential games, Pure and Applied Mathematics, Vol. XXV, Wiley-Interscience [A
division of John Wiley & Sons, Inc.], New York-London, 1971. MR421700
[8] Rufus Isaacs, Differential games: a mathematical theory with applications to warfare and pursuit, control
and optimization, John Wiley & Sons, Inc., New York-London-Sydney, 1965. MR210469
[9] Morton I. Kamien and Nancy L. Schwartz, Dynamic optimization: the calculus of variations and opti-
mal control in economics and management, 2nd ed., Advanced Textbooks in Economics, vol. 31, North-
Holland Publishing Co., Amsterdam, 1991. MR1159711
[10] Donald E. Kirk, Optimal control theory: an introduction, Dover Publications, 2004.
[11] Suzanne Lenhart and John T. Workman, Optimal control applied to biological models, Chapman &
Hall/CRC Mathematical and Computational Biology Series, Chapman & Hall/CRC, Boca Raton, FL,
2007. MR2316829
[12] Mark Levi, Classical mechanics with calculus of variations and optimal control: an intuitive introduction,
Mathematics Advanced Study Semesters, University Park, PA, Student Mathematical Library, vol. 69,
American Mathematical Society, Providence, RI, 2014, DOI 10.1090/stml/069. MR3156230
[13] Frank L. Lewis, Draguna L. Vrabie, and Vassilis L. Syrmos, Optimal control, 3rd ed., John Wiley & Sons,
Inc., Hoboken, NJ, 2012, DOI 10.1002/9781118122631. MR2953185
[14] Daniel Liberzon, Calculus of variations and optimal control theory: a concise introduction, Princeton
University Press, Princeton, NJ, 2012. MR2895149
[15] George Leitmann, The calculus of variations and optimal control: an introduction, Mathematical Con-
cepts and Methods in Science and Engineering, 24, Posebna Izdanja. [Special Editions], Plenum Press,
New York-London, 1981. MR641031
[16] Jack W. Macki and Aaron Strauss, Introduction to optimal control theory, Undergraduate Texts in Math-
ematics, Springer-Verlag, New York-Berlin, 1982. MR638591
[17] Mike Mesterton-Gibbons, A primer on the calculus of variations and optimal control theory, Stu-
dent Mathematical Library, vol. 50, American Mathematical Society, Providence, RI, 2009, DOI
10.1090/stml/050. MR2522971
[18] Paul J. Nahin, Chases and escapes: the mathematics of pursuit and evasion, Princeton University Press,
Princeton, NJ, 2007. MR2319182
[19] V. G. Boltyanskiy, R. V. Gamkrelidze, and L. Pontryagin, Mathematical theory of optimal processes, Wiley,
New York, 1962.
223
224 Bibliography
[20] Enid R. Pinch, Optimal control and the calculus of variations, Oxford Science Publications, The Claren-
don Press, Oxford University Press, New York, 1993. MR1221086
[21] I. Michael Ross, A primer on Pontryaginâ€™s principle in optimal control, Collegiate Publishers, Carmel,
CA, 2009.
[22] E. O. Roxin, Control theory and its applications, Stability and Control: Theory, Methods and Applica-
tions, vol. 4, Gordon and Breach Science Publishers, Amsterdam, 1997. MR1635393
[23] Atle Seierstad and Knut SydsÃ¦ter, Optimal control theory with economic applications, Advanced Text-
books in Economics, vol. 24, North-Holland Publishing Co., Amsterdam, 1987. MR887536
[24] Aaron Strauss, An introduction to optimal control theory, Lecture Notes in Operations Research and
Mathematical Economics, Vol. 3, Springer-Verlag, Berlin-New York, 1968. MR233052
[25] Fredi TrÃ¶ltzsch, Optimal control of partial differential equations: theory, methods and applications, trans-
lated from the 2005 German original by JÃ¼rgen Sprekels, Graduate Studies in Mathematics, vol. 112,
American Mathematical Society, Providence, RI, 2010, DOI 10.1090/gsm/112. MR2583281
[26] Bruce van Brunt, The calculus of variations, Universitext, Springer-Verlag, New York, 2004, DOI
10.1007/b97436. MR2004181
[27] Thomas A. Weber, Optimal control theory with applications in economics, with a foreword by A.
V. Kryazhimskiy, MIT Press, Cambridge, MA, 2011, DOI 10.7551/mitpress/9780262015738.001.0001.
MR2857389
Index
allowable controls, 52, 60, 91
argmax, 158
attainable, 59
attrition and attack, 193
bang-bang, 155, 159
big-O, 15
brachistochrone, 131, 148, 154, 174
calculus of variations, 201
canoe exercises, 95, 96, 187
control dynamic, 35
control function, 6
costate, 8, 38, 40, 49, 73, 106, 183
cycloids, 131
derivative, concept of, 13
differential games, 190
Eulerâ€™s technique, 90
Euler-Lagrange, 201
extremals, 201
fisheries, 163, 168
game theory, 189
Hamilton-Jacobi-Bellman, 181, 183
Hamiltonian, 32, 92
Hamiltonian systems, 67
HJB equation, 181, 183
integrator examples, 50, 53, 60, 71, 72, 74, 87,
102, 107, 183
inventory scheduling, 137
isochrone, 174
isoparametric, 201, 203
King Tiny, 51, 55, 57, 59, 65, 70, 184, 187
Lagrange multipliers, 8, 20, 23, 29, 37, 39, 41, 43,
92
lambda, 23, 27, 29, 183
life cycle, 66, 97
linear approximation, 14
linear tangent law, 127
linear-quadratic, 99
little-o, 16
max/min, existence of, 17
multiple constraints, 24
Nash equilibria, 189
normals, 26
performance, 50
Pontryagin, 6
Principle 0, 38, 209
Principle I, 52, 209
Principle II, 57, 209
Principle III, 82, 209
Principle IV, 116, 209
Principle V, 140, 209
Principle VI, 145, 209
Principle VII, 157, 209
Principle of Optimality, 71, 74, 150, 151, 182
Principles, list of, 209
rocket races, 160
rocket ship, 158
rocket sled, 117, 118, 120, 123, 141, 147, 175,
178, 185
soft landing, 117, 123, 134, 170, 177, 178
state space, 6
stationary, 20, 21, 29, 132, 155, 163
thrust programnming, 116
trajectory, 6
value function, 2, 4, 181
Zermelo, 125, 143, 146, 149
zero sum, 193, 195
225
AMS / MAA TEXTBOOKS
Optimal control theory concerns the study of dynamical systems
where one operates a control parameter with the goal of optimizing
a given payoff function. This textbook provides an accessible,
examples-led approach to the subject. The text focuses on systems
modeled by differential equations, with applications drawn from a
wide range of topics, including engineering, economics, fi nance,
and game theory. Each topic is complemented by carefully prepared
exercises to enhance understanding.
The book begins with introductory chapters giving an overview of the subject and covering
the necessary optimization techniques from calculus. After this, Pontryaginâ€™s method is
developed for control problems on one-dimensional state spaces, culminating in the study
of linear-quadratic systems. The core material is rounded out by the consideration of high-
er-dimensional systems. The text concludes with more advanced topics such as bang-bang
controls and differential game theory. A fi nal chapter examines the calculus of variations,
giving a brief overview of the Euler-Lagrange theory and general isoperimetric problems.
Designed for undergraduates in mathematics, physics, or economics, Optimal Control
Theory can be used in a structured course or for self-study. The treatment is highly acces-
sible and only requires a familiarity with multivariable calculus, differential equations, and
basic matrix algebra.
TEXT/75
For additional information
and updates on this book, visit
www.ams.org/bookpages/text-75