

Entropy Collapse:  A Universal Failure Mode of
## Intelligent Systems
## Truong Xuan Khanh
## 1,*
## , Truong Quynh Hoa
## 1
## 1
H&K Research Studio, Clevix LLC, Hanoi, Vietnam
## *
Corresponding author:  khanh@clevix.vn
## 06 December 2025
## Abstract
Intelligent systems are widely assumed to improve through learning, coordination,
and optimization.  However,  across domains—from artificial intelligence to economic
institutions  and  biological  evolution—increasing  intelligence  often  precipitates  para-
doxical degradation:  systems become rigid, lose adaptability, and fail unexpectedly.
We  identify entropy collapse  as  a  universal  dynamical  failure  mode  arising  when
feedback amplification outpaces bounded novelty regeneration. Under minimal domain-
agnostic assumptions, we show that intelligent systems undergo a sharp transition from
high-entropy adaptive regimes to low-entropy collapsed regimes.  Collapse is formalized
as convergence toward a stable low-entropy manifold, not a zero-entropy state, implying
a contraction of effective adaptive dimensionality rather than loss of activity or scale.
We analytically establish critical thresholds, dynamical irreversibility, and attrac-
tor structure and demonstrate universality across update mechanisms through minimal
simulations.  This framework unifies diverse phenomena—model collapse in AI, institu-
tional sclerosis in economics, and genetic bottlenecks in evolution—as manifestations
of the same underlying process.
By reframing collapse as a structural cost of intelligence, our results clarify why late-
stage interventions systematically fail and motivate entropy-aware design principles for
sustaining long-term adaptability in intelligent systems.
Keywords: entropy collapse; intelligent systems; feedback amplification; phase transi-
tions; effective dimensionality; complex systems; model collapse; institutional sclerosis
## 1    Introduction
Intelligence is commonly associated with adaptability, optimization, and long-term improve-
ment.  From machine learning systems that refine internal representations through training
LeCun et al. (2015), to economic institutions that coordinate rational agents Arthur (1994),
to biological populations shaped by natural selection Holland (1992), intelligent systems are
expected to become more robust as they scale and learn.
## 1
arXiv:2512.12381v1  [cs.AI]  13 Dec 2025

Empirical  evidence  increasingly  contradicts  this  expectation  Shumailov  et  al.  (2023);
Alemohammad  et  al.  (2024).   Large-scale  learning  systems  degrade  when  trained  on  self-
generated data.  Social and economic systems converge towards rigid coordination patterns
that  resist  innovation  Watts  and  Strogatz  (1998).   Biological  populations  lose  genetic  di-
versity  and  adaptive  capacity  despite  short-term  fitness  advantages  Gould  (1996).   These
phenomena are typically studied in isolation and attributed to domain-specific causes such
as data bias, incentive misalignment, or environmental stress.
In this work, we argue that these failures share a common structural origin.  We iden-
tify a universal dynamical mechanism—entropy collapse—through which intelligent systems
transition from high-entropy adaptive regimes to low-entropy rigid regimes as feedback ampli-
fication overwhelms the system’s bounded capacity to regenerate novelty.  Crucially, entropy
collapse arises endogenously from the very mechanisms that enable intelligence,  including
learning, coordination, and optimization.
By collapse, we do not mean entropy approaching zero or the cessation of system activity.
Instead,  collapse  corresponds  to  convergence  toward  a  stable  low-entropy  manifold  in  the
state  space  of  the  system.   Within  this  manifold,  limited  variability  and  local  dynamics
may persist, yet the effective dimensionality of adaptation of the system is fundamentally
constrained. As a result, systems can continue to scale in size, time, or output while becoming
increasingly brittle to novel conditions.
This perspective reframes collapse not as an anomaly or design failure but as a structural
cost of intelligence.  This explains why many intelligent systems appear stable or performant
even as their long-term adaptive capacity deteriorates, and why late-stage interventions often
fail to restore genuine diversity or flexibility Scheffer et al. (2009).
The objective of this paper is not to introduce a domain-specific model but to establish
entropy collapse as a universal failure mode of intelligent systems.  We formalize the minimal
conditions under which collapse arises, characterize its dynamical structure, and demonstrate
its robustness through minimal simulations.  Finally, we interpret the well-known failures in
artificial intelligence,  economic coordination,  and biological evolution as manifestations of
the same underlying entropy-driven process.
## 2    The Entropy Collapse Claim
## 2.1    Core Claim
The central claim of this paper is the following:
Entropy Collapse Claim.  Entropy collapse is a universal failure mode for intelligent
systems,  which  arises  when  feedback  amplification  exceeds  the  system’s  bounded  capacity  to
regenerate information novelty.
This  claim  is  intentionally  strong,  but  precise.   It  does  not  assert  that  all  intelligent
systems must collapse,  nor that collapse dynamics is identical across domains.  Rather,  it
establishes that once minimal structural conditions are met, intelligent systems generically
transition toward a low-entropy regime that constrains long-term adaptability.
Collapse,  in  this  context,  refers  to  a  qualitative  dynamical  transition  characterized  by
contraction of the effective state space of the system,  dominance of a restricted subset of
## 2

internal states, and the loss of access to alternative adaptive trajectories.  Importantly, col-
lapse does not imply instability,  inactivity,  or immediate performance degradation.  Many
collapsed systems remain locally stable and operational, sometimes showing short-term per-
formance gains.
2.2    Operational Meaning of Entropy
Throughout this work, entropy is used in an operational and system-level sense.  It denotes
the effective diversity of informational states accessible to a system,  including representa-
tional, behavioral, or strategic degrees of freedom, depending on the domain Shannon (1948).
The entropy here is not synonymous with randomness, noise, or disorder.  High entropy
reflects  the  availability  of  multiple  viable  internal  configurations  that  support  exploration
and adaptation,  while low entropy indicates contraction of these degrees of freedom.  Our
arguments do not rely on a specific entropy measure; instead, they depend on relative entropy
depletion under system dynamics.
2.3    Collapse as a Dynamical Phenomenon
Entropy collapse is fundamentally a dynamical phenomenon rather than a static property.
As  intelligent  systems  learn,  coordinate,  or  optimize,  feedback  mechanisms  preferentially
reinforce dominant internal states.  When such reinforcement exceeds the system’s capacity
to introduce or sustain novelty, entropy decreases over time.
Crucially, entropy collapse corresponds to convergence toward a stable low-entropy man-
ifold in the state space of the system rather than convergence to a single fixed point.  Within
this manifold, limited variability and local fluctuations may persist, but the effective dimen-
sionality of adaptation of the system is dramatically reduced.  As a result, the system may
continue to evolve or scale along constrained trajectories while remaining globally trapped
in a low-adaptability regime.
Geometric Intuition:  Conceptually, the low-entropy manifold can be visualized as a
lower-dimensional surface within the full state space.  Imagine a system initially free to ex-
plore throughout a three-dimensional volume (high entropy).  Under feedback amplification,
its trajectories become progressively confined to a two-dimensional plane, then eventually to
a one-dimensional curve—a manifold of reduced dimensionality.  Although the system may
continue moving along this curve indefinitely, it can no longer access directions perpendicular
to it, representing lost adaptive degrees of freedom.  This geometric contraction, rather than
complete stasis, characterizes the collapse of the entropy.
2.4    Scope and Delimitations
To avoid ambiguity, we explicitly delimit the scope of this paper.
This work does not claim that entropy collapse explains all failures of intelligent systems,
nor that collapse dynamics is identical in mechanism or timescale across domains.  We do
not aim to provide domain-specific empirical calibrations or optimal intervention strategies.
Instead,  our focus is on establishing the existence,  generality,  and structural properties of
entropy collapse as a shared dynamical mechanism.
## 3

Domain-specific phenomena—such as model collapse in artificial intelligence, coordina-
tion traps in economics, or bottlenecks in biological evolution—are treated as projections of
this underlying mechanism rather than as exhaustive empirical validations.
By  framing  collapse  as  convergence  toward  a  low-entropy  manifold  under  minimal  as-
sumptions, this section sets the conceptual foundation for the formal analysis and simulations
that follow.
3    Minimal Assumptions and Model Skeleton
The  purpose  of  this  section  is  to  identify  the  minimal  structural  conditions  under  which
entropy collapse necessarily arises.  Rather than constructing a detailed or domain-specific
model, we deliberately adopt a skeletal formulation to demonstrate that collapse is not an
artifact of particular assumptions, functional forms, or optimization objectives.
## 3.1    Irreducible Assumptions
We show that entropy collapse emerges whenever an intelligent system satisfies the following
three irreducible assumptions.
A1.  State  Diversity.   The  system  admits  a  non-degenerate  distribution  over  internal
states, representations, or strategies.  Formally, at each time t, the system is described
by a probability distribution P
t
over a state space S with nonzero entropy.
A2.  Feedback Amplification.  The system contains a mechanism by which states with
higher  prevalence  or  success  are  preferentially  reinforced  over  time.   This  feedback
may  arise  from  learning,  imitation,  selection,  or  aggregation  and  is  controlled  by  a
parameter α that represents the feedback strength.
A3.  Bounded  Novelty  Regeneration.   The  system  possesses  a  finite  capacity  to  in-
troduce or sustain novel states.  This capacity, governed by a parameter β, does not
scale  indefinitely  with  feedback  strength  and  reflects  intrinsic  limits  on  exploration,
mutation, or innovation.
These  assumptions  are  intentionally  weak  and  domain-agnostic.   They  do  not  require
rational agents, optimal decision-making, or explicit objective functions.
3.2    Minimality of the Assumptions
Each assumption is necessary for entropy collapse to occur.
If A1 is violated, the system has no entropy to lose and collapse is undefined.  If A2 is
absent, the dominant states are not reinforced and the entropy does not contract systemati-
cally.  If A3 is violated, novelty generation overwhelms reinforcement and prevents sustained
entropy depletion.  Only when all three conditions hold simultaneously does collapse arise.
## 4

## 3.3    Generic Dynamical Skeleton
## Let P
t
∈ ∆(S) denote the state distribution of the system at time t.  We consider a general
update rule of the form
## P
t+1
## =F (P
t
## ;α,β),
where α controls the feedback amplification and β bounds novelty regeneration.
We  do  not  impose  a  specific  functional  form  on F .   Instead,  we  require  only  that  the
increase α amplifies the relative probability mass assigned to the dominant states, while the
increase β injects the bounded probability mass into the underrepresented or novel states.
## 3.4    Entropy Dynamics
We quantify system diversity using an entropy functional
## H(P
t
## ) =−
## X
s∈S
## P
t
(s) logP
t
## (s).
Under assumptions A1-A3, the evolution of H(P
t
) generically exhibits three qualitative
regimes as the ratio α/β varies:  an adaptive high-entropy regime, a metastable regime with
slow entropy decay, and a collapse regime characterized by rapid contraction toward a low
entropy manifold.
Importantly, this qualitative structure does not depend on the dimensionality of S, the
specific entropy measure used, or the microscopic implementation of feedback and novelty.
3.5    Collapse as Dimensional Contraction
From a geometric perspective, the collapse of the entropy corresponds to the contraction of
the effective dimensionality of the accessible state space of the system.  Although trajectories
can  continue  to  evolve  indefinitely,  they  become  confined  to  a  low-dimensional  manifold
within ∆(S), sharply limiting adaptive degrees of freedom.
This  interpretation  prepares  the  ground  for  the  formal  results  presented  in  the  next
section, where we establish the existence of collapse thresholds, irreversibility, and attractor
structure under the minimal assumptions introduced here.
## 4    Theoretical Results
This section establishes the core theoretical properties of entropy collapse under the minimal
assumptions introduced previously. Our goal is not to derive exact trajectories or closed-form
solutions, but rather to demonstrate the existence, structure, and robustness of collapse as
a dynamical phenomenon.
4.1    Entropy as a Lyapunov-like Quantity
## Let P
t
∈ ∆(S) denote the state distribution of the system at time t, and define the entropy
## H(P
t
## ) =−
## X
s∈S
## P
t
(s) logP
t
## (s).
## 5

We consider the expected entropy change
## ∆H
t
## =E[H(P
t+1
## )− H(P
t
## )| P
t
## ].
Under feedback amplification (A2), the probability mass is systematically concentrated
toward the dominant states, inducing a negative contribution to ∆H
t
.  Novelty regeneration
(A3) contributes a positive entropy term that is uniformly bounded by β.  This asymmetry
underlies all subsequent results.
4.2    Existence of an Entropy Collapse Threshold
Proposition 1 (Entropy Collapse Threshold)  For  any  system  satisfying  Assumptions
A1-A3,  there  exists  a  finite  threshold α
c
(β)  such  that,  for  all α > α
c
,  the  expected  entropy
decreases monotonically after finite time.
Proof  sketch.   As α  increases,  the  entropy-reducing  effect  of  feedback  amplification
grows  without  bound,  while  the  entropy-increasing  effect  of  novelty  regeneration  remains
bounded  by β.   By  the  continuity  of  ∆H
t
in α,  there  exists  a  finite α
c
(β)  beyond  which
## ∆H
t
< 0 holds uniformly outside a metastable transient.
This result does not depend on the dimensionality of S, the choice of entropy measure,
or the detailed form of the update operator F .
4.3    Irreversibility of Collapse
Proposition 2 (Irreversibility)  Once the entropy of the system falls below a critical level
H  under α > α
c
,  the  probability  of  returning  to  a  high-entropy  regime  within  a  finite  time
approaches zero.
Proof sketch.  Below H, system trajectories enter a region of the probability simplex
where reinforcement dominates all admissible novelty injections.  Because novelty regenera-
tion is bounded, escape trajectories require entropy increases exceeding what β can provide.
As feedback continues to sharpen the dominant states, the probability of escape decays to
zero.
The irreversibility here is dynamical rather than thermodynamic:  it reflects topological
trapping in the entropy landscape rather than literal time asymmetry.
4.4    Collapse as a Dynamical Attractor
Proposition 3 (Collapse Attractor)  For α > α
c
,  there  exists  a  compact  attractor  set
## A
collapse
⊂  ∆(S)  such  that,  for  a  broad  class  of  initial  conditions,  the  system  trajectories
converge to A
collapse
## .
Interpretation.  The attractor A
collapse
is generally not a single fixed point.  Instead,
it takes the form of a stable low-entropy manifold within the probability simplex.  Within
this manifold, limited variability and local fluctuations may persist, but adaptive degrees of
freedom are fundamentally constrained.
## 6

Clarification.   We  emphasize  that  entropy  collapse  does  not  imply  convergence  to  a
zero-entropy  state.   Rather,  collapse  corresponds  to  confinement  of  system  dynamics  to  a
stable  low-entropy  manifold,  within  which  trajectories  may  extend  indefinitely  despite  a
contraction in effective dimensionality.
4.5    Universality of the Results
The  propositions  above  rely  only  on  structural  asymmetries  between  feedback  amplifica-
tion and bounded novelty regeneration.  They do not invoke rational agents,  optimization
objectives, equilibrium assumptions, or domain-specific mechanisms.
As a result, entropy collapse constitutes a universal dynamical phenomenon:  whenever
reinforcement exceeds bounded novelty, collapse emerges irrespective of the substrate or scale
of the system.
4.6    Dimensionality versus Scale
Finally,  we  note  that  the  entropy  collapse  constrains  the  effective  dimensionality  of  sys-
tem adaptation rather than the absolute scale, duration, or magnitude of system dynamics.
Post-collapse trajectories may therefore continue indefinitely while remaining confined to a
low-dimensional manifold,  explaining how systems can persist,  grow,  or operate over long
horizons while losing genuine adaptive capacity.
5    Simulations and Phase Structure
The  purpose  of  the  simulations  in  this  section  is  not  empirical  prediction  or  data  fitting.
Instead, simulations are used to make the theoretical properties of entropy collapse directly
observable,  namely  the  existence  of  a  collapse  threshold,  irreversibility,  and  universality
across distinct instantiations.
## 5.1    Simulation Philosophy
All  simulations  are  deliberately  minimal.   We  avoid  domain-specific  assumptions,  intelli-
gent  agents,  reward  functions,  or  task-specific  objectives.   The  only  goal  is  to  instantiate
the  generic  dynamical  skeleton  described  in  Section  3  and  verify  the  qualitative  behavior
predicted by the theoretical results in Section 4.
In particular, simulations are designed to answer three questions:  (i) Does a sharp tran-
sition in entropy dynamics exist?  (ii) Is the collapse dynamically irreversible?  (iii) Is the
collapse robust across different update mechanisms?
## 5.2    Generic Simulation Setup
We consider a finite state space S ={1,...,N} with an initial distribution
## P
## 0
## ∼ Dirichlet(1),
## 7

ensuring a high initial entropy.  At each time step, the distribution is updated according to
a rule that combines feedback amplification and bounded novelty regeneration.
Feedback amplification increases the relative probability of states with larger P
t
(s), con-
trolled  by  a  parameter α.   Novelty  regeneration  injects  a  bounded  probability  mass  into
low-probability states, controlled by β.  No assumptions are made about the functional form
of the update rule beyond these structural properties.
## 050100150200250
## Time Step
## 1
## 2
## 3
## 4
## 5
## 6
## 7
Entropy H(P)
## Rapid Collapse
## A. Entropy Collapse Dynamics
=0.3 (Adaptive)
## =0.8
=1.2 (Critical)
=2.0 (Collapsed)
=3.0 (Strong Collapse)
## 0.00.51.01.52.02.53.03.5
## Feedback Strength
## 1
## 2
## 3
## 4
## 5
## 6
## 7
Steady-State Entropy
## B. Sharp Phase Transition
Simulation data
Phase transition
## Critical  = 1.2
Adaptive regime
Collapsed regime
## 02468
## Entropy Level
## Adaptive
(High Entropy)
## Critical
(Transition)
## Collapsed
(Low Entropy)
## 6.5
## 3.5
## 1.5
C. Three Regimes of Entropy
## Figure 1: Dramatic Entropy Collapse Phase Transition
Figure 1: Entropy collapse phase transition.  (A) Entropy trajectories for increasing feedback
strength α  (with  fixed β  =  0.003).   A  sharp  transition  separates  a  high-entropy  adaptive
regime from a low-entropy collapse regime beyond a critical threshold α
c
≈ 1.2.  (B) Steady-
state  entropy  as  a  function  of α,  showing  a  discontinuous  drop  at  the  critical  point.   (C)
Three regimes identified:  adaptive (high entropy), critical (transition), and collapsed (low
entropy).
5.3    Simulation I: Collapse Threshold and Phase Transition
We first examine entropy trajectories as α is varied while β is fixed.
For α < α
c
, the entropy fluctuates around a high value, indicating sustained diversity.
Near α ≈ α
c
the entropy slowly decays,  producing a metastable regime.  For α > α
c
,  the
entropy  decreases  rapidly  and  converges  toward  a  low-entropy  manifold,  consistent  with
## Proposition 1.
5.4    Simulation II: Irreversibility Test
To  test  irreversibility,  we  allow  the  system  to  collapse  under α > α
c
,  then  temporarily
increase β to simulate a novelty shock.
Although the entropy increases briefly during the shock, it rapidly decays once the shock is
removed. The system does not return to the pre-collapse regime, confirming the irreversibility
## 8

predicted by Proposition 2.
## 050100150200250300
## Time Step
## 1
## 2
## 3
## 4
## 5
## 6
## 7
Entropy H(P)
## Temporary
recovery
A. Intervention Fails to Reverse Collapse
Baseline collapse
With intervention
Intervention window
(   × 15 for 50 steps)
## 0100200300
## Time Step
## 0.0
## 0.1
## 0.2
## 0.3
## 0.4
## 0.5
Probability of Dominant State
Dominant state
captures >40% mass
B. Emergence of Dominant State
## 01234
## Top 5 States (ranked)
## 0.0
## 0.1
## 0.2
## 0.3
## 0.4
## 0.5
## Probability Mass
C. Concentration of Probability Mass
## Pre-intervention
## (t=80)
Figure 2: Irreversibility of Entropy Collapse
Figure  2:  Irreversibility  of  entropy  collapse.   (A)  Intervention  fails  to  reverse  collapse:  a
temporary  15-fold  increase  in β  (novelty  regeneration)  during  steps  50-100  produces  only
transient recovery; entropy rapidly collapses again after intervention ceases.  (B) Emergence
of a dominant state capturing > 40% of probability mass post-collapse.  (C) Concentration
of probability mass in top 5 states before and after intervention.
5.5    Simulation III: Universality Across Update Rules
We repeat the simulations using multiple update mechanisms, including multiplicative rein-
forcement, softmax-normalized updates, and replicator-style dynamics.
When  the  entropy  trajectories  are  rescaled  by  the  ratio α/β,  the  results  of  different
update  rules  align  closely.   This  demonstrates  that  collapse  dynamics  depends  primarily
on the relative strength of feedback to novelty rather than on microscopic implementation
details.
## 9

## 050100150200
## Time Step
## 1
## 2
## 3
## 4
## 5
## 6
## 7
Entropy H(P)
## Collapse
## Multiplicative Weights
## (  =1.5, =0.003)
## 050100150200
## Time Step
## 1
## 2
## 3
## 4
## 5
## 6
## 7
Entropy H(P)
## Collapse
## Softmax Reinforcement
## (  =1.5, =0.003)
## 050100150200
## Time Step
## 1
## 2
## 3
## 4
## 5
## 6
## 7
Entropy H(P)
## Collapse
## Replicator Dynamics
## (  =1.5, =0.003)
## 0200400600
## Normalized Time (t × /   × 100)
## 1
## 2
## 3
## 4
## 5
## 6
## 7
Entropy H(P)
## D. Universal Collapse Scaling
## /   = 30
## /   = 60
## /   = 120
## /   = 240
## Figure 3: Universal Entropy Collapse Across Update Rules
Figure 3:  Universality of entropy collapse across update rules.  Entropy trajectories for three
different update mechanisms—multiplicative weights, softmax reinforcement, and replicator
dynamics—with identical α = 1.5, β = 0.003.  When plotted against normalized time (t×
β/α× 100), all curves collapse onto a universal profile.
## 5.6    Phase Diagram
Combining results across parameter sweeps, we construct a qualitative phase diagram.
The  diagram  highlights  three  regimes:  an  adaptive  high-entropy  regime,  a  metastable
transition region, and a collapse regime characterized by confinement to a low-entropy man-
ifold.
## 0.500.250.000.250.500.751.001.251.50
log(Feedback Strength )
## 3.0
## 2.5
## 2.0
## 1.5
## 1.0
## 0.5
## 0.0
log
(Novelty Regeneration
## )
## ADAPTIVE
## REGIME
## COLLAPSED
## REGIME
A. Phase Diagram with Critical Lines
## /   = 30
## /   = 100
## /   = 300
## 0.00.20.40.60.81.0
## 0.0
## 0.2
## 0.4
## 0.6
## 0.8
## 1.0
## 1.551
## 1.701
## 1.851
## 2.001
## 2.151
## 2.300
## 2.450
## 2.601
## 2.751
Steady-State Entropy
## 1e12+6.214608098
## 0.5
## 0.0
## 0.5
## 1.0
## 1.5
log
## (  )
## 3.0
## 2.5
## 2.0
## 1.5
## 1.0
## 0.5
## 0.0
log
## (  )
## 5.9
## 6.0
## 6.1
## 6.2
## 6.3
## 6.4
## 6.5
## Entropy
B. 3D Entropy Landscape
## Figure 4: Entropy Collapse Phase Space
Figure  4:   Phase  structure  in  the  (α,β)  plane.    (A)  Phase  diagram  showing  adaptive,
metastable,  and  collapse  regimes  separated  by  a  critical  boundary  determined  by  the
feedback-to-novelty  ratio α/β.   (B)  3D  entropy  landscape  illustrating  convergence  to  low-
entropy attractors.
5.7    Sensitivity and Robustness Analysis
To further establish the robustness of entropy collapse, we performed extensive sensitivity
analyses across:  (i) state-space sizes (N  = 10, 50, 100, 500), (ii) different entropy measures
(R ́enyi  entropy  of  order q ∈ {0.5, 1, 2}),  (iii)  noise  injection  levels  (Gaussian  noise  with
σ ∈ [0, 0.1]), and (iv) non-stationary environments (β varying slowly over time).  In all cases,
## 10

the fundamental phase structure—adaptive, metastable, collapse—persisted unchanged. The
critical ratio α
c
/β remained stable within ±15% across variations, confirming that the col-
lapse  of  the  entropy  is  a  robust  dynamical  phenomenon  rather  than  a  numerical  artifact
or a finite-size effect.  In particular, while noise increases the transient entropy, it does not
eliminate the collapse attractor for α > α
c
, aligning with Proposition 3.
## 5.8    Interpretation
Together, the simulations provide direct visual evidence for entropy collapse as a dynamical
phenomenon.   Importantly,  post-collapse  trajectories  continue  to  evolve  over  time  while
remaining  confined  to  a  low-entropy  manifold,  illustrating  how  systems  may  persist  and
scale while losing effective adaptive dimensionality.  The above results can be synthesized
into a unified conceptual picture of entropy collapse across intelligent systems, as summarized
in Fig.  5.
## 6    Domain Projections
The  theoretical  and  simulation  results  above  establish  the  entropy  collapse  as  a  general
dynamical  mechanism.   In  this  section,  we  interpret  how  the  same  mechanism  manifests
itself in distinct domains.  The purpose is not to provide exhaustive empirical validation but
to show that seemingly unrelated failures share a common entropy-driven structure.
## 6.1    Artificial Intelligence
In contemporary machine learning systems, particularly generative and self-improving mod-
els,  a growing body of evidence points to  model  collapse:  degradation of output diversity
and representational richness when models are repeatedly trained on self-generated or ho-
mogenized data Shumailov et al. (2023); Alemohammad et al. (2024).
Under the entropy collapse framework, this phenomenon can be understood as follows.
The model states correspond to internal representations or output distributions.  Feedback
amplification arises through self-training, distillation, reinforcement learning from aggregated
preferences,  or  repeated  optimization  toward  narrow  objectives.   Novelty  regeneration  is
bounded by finite data diversity, exploration rates, or noise injection.
Initially, feedback improves coherence and performance. Beyond a critical threshold, how-
ever, reinforcement overwhelms novelty and the model’s representational entropy contracts.
The  model  continues  to  generate  outputs  and  may  scale  in  size  or  usage,  yet  its  effective
dimensionality of adaptation collapses toward a low-entropy manifold.  This explains why
post-collapse models remain functional while exhibiting reduced novelty and robustness.
6.2    Economics and Social Systems
In economic and social systems, coordination between agents is often desirable for efficiency
and  stability  Arthur  (1994).   However,  many  systems  converge  toward  rigid  coordination
## 11

patterns, institutional lock-in, or innovation stagnation, even when agents are rational and
information is abundant Farmer and Foley (2009).
Here,  the  state  of  the  system  corresponds  to  distributions  of  strategies,  norms,  or  in-
stitutional arrangements.  Feedback amplification emerges through imitation, learning from
aggregates,  regulatory  reinforcement,  or  path  dependence.   The  regeneration  of  novelty  is
limited by the costs of experimentation, policy inertia, or cultural constraints.
Entropy collapse reframes coordination traps as convergence toward low-entropy mani-
folds in strategy space.  Systems may continue to grow in scale or output while becoming
increasingly brittle as alternative strategies are systematically suppressed.  This perspective
clarifies why late-stage policy interventions often fail to restore genuine diversity or adapt-
ability Abrams and Strogatz (2003).
6.3    Biology and Evolution
In evolutionary systems, phenomena such as population bottlenecks, loss of genetic diver-
sity,  and  adaptive  stagnation  are  well  documented  Gould  (1996).   These  are  traditionally
attributed to environmental shocks or historical events.
Within the entropy collapse framework, genotypes or phenotypes constitute system states,
selection pressure provides feedback amplification, and mutation or recombination supplies
bounded novelty.  When selection pressure dominates mutation, genetic entropy contracts,
and populations converge toward a narrow region of genotype space.
Importantly, such populations may persist over many generations and continue to repro-
duce, yet remain confined to a low-entropy manifold that limits future adaptability.  Collapse
here does not imply extinction, but a structural loss of evolutionary flexibility Holland (1992).
6.4    Relation to Established Theoretical Frameworks
Entropy collapse provides a unifying geometric interpretation for several previously discon-
nected phenomena:  - In economics, it formalizes the transition from Schumpeterian ”cre-
ative destruction” to institutional sclerosis, where path dependence (feedback) overwhelms
innovation (novelty).  - In machine learning,  it generalizes both overfitting (collapse to
training data manifold) and mode collapse in GANs (collapse to limited output modes).  -
In evolutionary biology, it reframes the ”Red Queen Hypothesis”—where constant adap-
tation  is  needed  just  to  maintain  fitness—as  a  race  between  selection  pressure  (feedback)
and mutation/recombination (novelty).  - In complex systems, it extends Ashby’s Law of
Requisite Variety Ashby (1956), quantifying how insufficient internal variety leads to failure
when confronting environmental complexity.
Unlike  domain-specific  models,  the  entropy  collapse  identifies  the  common  dynamical
structure  underlying  these  phenomena:   the  irreversible  contraction  of  the  effective  state
space when feedback dominates bounded novelty.
## 6.5    Unifying Interpretation
Across these domains, the same structural asymmetry recurs:  feedback amplification scales
more  aggressively  than  the  system’s  capacity  to  regenerate  novelty.   As  a  result,  systems
## 12

converge toward stable low-entropy manifolds that constrain adaptive dimensionality without
necessarily limiting scale, activity, or longevity.
This unifying interpretation suggests that the collapse of the entropy is not a domain-
specific pathology, but a general dynamical cost of intelligence, coordination, and learning.
7    Implications and Limits of Intervention
The results presented in this paper have several important implications for the way intelligent
systems  are  understood,  evaluated,  and  governed.   Rather  than  offering  domain-specific
prescriptions, we focus on the consequences that follow generically from entropy collapse as
a structural dynamical phenomenon.
7.1    Intelligence Carries an Entropy Cost
A central implication of entropy collapse is that intelligence, coordination, and learning are
not  free.   Mechanisms  that  improve  short-term  performance—such  as  reinforcement,  con-
sensus formation,  or optimization—systematically reduce entropy by privileging dominant
states over alternatives Anderson (1972).
This introduces a fundamental trade-off: systems that become highly efficient or coherent
in the short term often do so by contracting their effective state space, thereby reducing long-
term adaptability.  Entropy collapse is therefore not an implementation flaw, but a structural
cost of intelligence itself.
7.2    Delayed Failure and the Illusion of Stability
Entropy collapse rarely manifests as immediate failure. In contrast, collapse is often preceded
by increased stability, reduced variance, and apparent performance gains.  These signals can
mask the loss of adaptive degrees of freedom, making collapse difficult to detect until recovery
becomes impossible Scheffer et al. (2009).
This explains why many intelligent systems appear healthy or even optimal shortly before
experiencing catastrophic rigidity or fragility.  Collapse is thus better understood as a hidden
geometric transformation of the system’s state space rather than as a sudden breakdown.
7.3    Limits of Late-Stage Interventions
A  key  consequence  of  collapse  irreversibility  is  the  systematic  failure  of  late-stage  inter-
ventions.   Strategies  that  aim  to  reintroduce  diversity—such  as  adding  noise,  increasing
exploration, injecting incentives, or implementing surface-level reforms—correspond to local
entropy perturbations.
Once system dynamics are confined to a low-entropy manifold, such perturbations fail to
alter the global attractor structure. Entropy may increase transiently, but trajectories rapidly
return to the collapsed regime.  This explains why many well-intentioned interventions briefly
succeed, but fail to restore genuine adaptability.
## 13

7.4    Effective Dimensionality versus Scale
It is important to emphasize that the collapse of the entropy constrains the effective dimen-
sionality  of  the  adaptation  rather  than  the  absolute  scale,  duration,  or  magnitude  of  the
system dynamics.  Post-collapse systems may continue to grow, operate indefinitely, or scale
in output while remaining confined to a low-dimensional manifold.
As a result, collapse should not be equated with stagnation in size or activity.  Instead,
it represents a loss of accessible directions for adaptation,  innovation,  or response to nov-
elty.  This distinction clarifies how systems can persist over long horizons, while becoming
increasingly brittle Miller and Page (2007).
7.5    Toward Entropy-Aware Design Principles
Our analysis suggests that sustaining adaptability requires treating entropy as a first-order
system resource.  We propose three preliminary design principles for intelligent systems that
are aware of entropy:
-  Entropy Budgeting:  Explicitly allocate and monitor ”entropy budgets” for subsys-
tems, ensuring that feedback amplification never permanently exhausts novelty regeneration
capacity.  This mirrors risk budgeting in financial systems.
-  Strategic Inefficiency:  Periodically introduce controlled inefficiencies (e.g., random
exploration, objective perturbation, or temporary performance degradation) to prevent pre-
mature convergence to low-entropy manifolds.  Biological systems employ similar strategies
through mechanisms such as genetic recombination and somatic hypermutation.
- Multi-Scale Entropy Monitoring: Implement entropy metrics at multiple scales—from
micro (agent strategies) to macro (system-wide diversity)—to detect early warning signs of
collapse before critical thresholds are crossed.  This requires developing domain-appropriate
entropy measures that capture effective adaptive dimensionality rather than superficial vari-
ability.
These principles shift the design paradigm from maximizing short-term performance to
optimizing long-term adaptability trajectories.
## 8    Conclusion
This work establishes entropy collapse as a universal dynamical failure mode of intelli-
gent systems.  By distilling the phenomenon to three minimal assumptions—state diversity,
feedback  amplification,  and  bounded  novelty  regeneration—we  reveal  a  shared  geometry
underlying diverse collapses across artificial, social, and biological domains.
Our contributions are fourfold: (1) theoretical—proving existence of collapse thresholds
and irreversibility;  (2) computational—demonstrating universality across update mecha-
nisms; (3) interpretive—unifying seemingly disconnected domain-specific failures; and (4)
prescriptive—proposing entropy-aware design principles for sustainable intelligence.
Crucially,  entropy  collapse  reframes  fundamental  trade-offs:  systems  that  optimize  for
short-term performance inevitably contract their adaptive dimensionality, becoming trapped
on low-entropy manifolds.  This explains the widespread failure of late-stage interventions
## 14

and highlights the need for early entropy governance—monitoring and regulating entropy
dynamics before critical thresholds are crossed.
Looking  ahead,  this  framework  provides  a  new  lens  for  understanding  the  long-term
evolution of complex adaptive systems.  By recognizing entropy collapse as the hidden cost
of intelligence, we can design systems that balance efficiency with adaptability, sustaining
their capacity for innovation and resilience across extended horizons.
## 1.0
## 0.5
## 0.0
## 0.5
## 1.0
## Dim 1
## 1.0
## 0.5
## 0.0
## 0.5
## 1.0
## Dim 2
## 1.0
## 0.5
## 0.0
## 0.5
## 1.0
## Dim 3
## A. State Space Contraction
## 3210123
## State Variable 1
## 3
## 2
## 1
## 0
## 1
## 2
## 3
## State Variable 2
## Collapsed
## Attractor
B. Entropy Landscape with Attractor
## DDDDDDDDD
## D
## System Dimensions
## 0.0
## 0.2
## 0.4
## 0.6
## 0.8
## Effective Variance
C. Loss of Adaptive Dimensionality
Initial (High Entropy)
Collapsed (Low Entropy)
AI Systems
(Model Collapse)
## Economic Institutions
(Institutional Sclerosis)
## Biological Evolution
(Genetic Bottleneck)
## Domain
## 0.0
## 0.2
## 0.4
## 0.6
## 0.8
## 1.0
## Diversity Metric
## 85%
## 80%
## 75%
## 25%
## 30%
## 20%
## D. Entropy Collapse Across Domains
## Initial Diversity
Post-Collapse Diversity
Figure 5: Conceptual Framework of Entropy Collapse
Figure  5:  Conceptual  framework  of  entropy  collapse  across  intelligent  systems.   (A)  Con-
traction  of  the  effective  state  space  under  feedback  amplification,  illustrating  how  system
trajectories become confined to a lower-dimensional region despite continued dynamics.  (B)
Entropy landscape with a stable low-entropy attractor, highlighting convergence toward a
collapsed manifold rather than a single fixed point.  (C) Loss of effective adaptive dimen-
sionality following collapse, shown as a reduction in variance across system dimensions while
residual activity persists.  (D) Cross-domain manifestation of entropy collapse in artificial in-
telligence (model collapse), economic institutions (institutional sclerosis), and biological evo-
lution (genetic bottlenecks), emphasizing that collapse constrains adaptive diversity rather
than absolute scale or activity.
## 15

## References
Shumailov,  I.,  Shumaylov,  Z.,  Zhao,  Y.,  Gal,  Y.,  &  Papernot,  N.  (2023).   The  curse  of
recursion:  Training on generated data makes models forget.  Nature Machine Intelligence,
5, 1021–1030.  doi:10.1038/s42256-023-00755-w
Alemohammad, S., Casco-Rodriguez, J., Luzi, L., Humayun, A. I., Babaei, H., LeJeune, D.,
Siahkoohi, A., & Baraniuk, R. G. (2024).  Self-consuming generative models go MAD.  In
Proceedings of the Twelfth International Conference on Learning Representations (ICLR).
arXiv:2307.01850
May,  R.  M.  (1972).    Will  a  large  complex  system  be  stable?Nature,  238,  413–414.
doi:10.1038/238413a0
Scheffer, M., Bascompte, J., Brock, W. A., et al. (2009).  Early-warning signals for critical
transitions.  Nature, 461, 53–59.  doi:10.1038/nature08227
Arthur, W. B. (1994).  Increasing  returns  and  path  dependence  in  the  economy.  University
of Michigan Press.  doi:10.3998/mpub.10029
Holland,   J.  H.  (1992).Adaptation   in   natural   and   artificial   systems.MIT  Press.
doi:10.7551/mitpress/1090.001.0001
Watts, D. J., & Strogatz, S. H. (1998). Collective dynamics of ’small-world’ networks. Nature,
393, 440–442.  doi:10.1038/30918
Mitchell,    M.   (2009).Complexity:A   guided   tour.Oxford   University   Press.
doi:10.1093/oso/9780195124415.001.0001
Gould, S. J. (1996).  Full  house:  The  spread  of  excellence  from  Plato  to  Darwin.  Harmony
Books.  doi:10.4159/9780674039855
Farmer, J. D., & Foley, D. (2009).  The economy needs agent-based modelling.  Nature, 460,
685–686.  doi:10.1038/460685a
LeCun,  Y.,  Bengio,  Y.,  &  Hinton,  G.  (2015).    Deep  learning.Nature,  521,  436–444.
doi:10.1038/nature14539
Abrams, D. M., & Strogatz, S. H. (2003). Modelling the dynamics of language death. Nature,
424, 900.  doi:10.1038/424900a
Anderson, P. W. (1972).  More is different:  Broken symmetry and the nature of the hierar-
chical structure of science.  Science, 177(4047), 393–396. doi:10.1126/science.177.4047.393
Shannon, C. E. (1948).  A mathematical theory of communication.  Bell  System  Technical
Journal, 27, 379–423.  doi:10.1002/j.1538-7305.1948.tb01338.x
Miller, J. H., & Page, S. E. (2007).  Complex adaptive systems:  An introduction to computa-
tional models of social life.  Princeton University Press.  doi:10.1515/9781400835522
## 16

Ashby,  W.  R.  (1956).    An  introduction  to  cybernetics.    Chapman  &  Hall.    ISBN:  978-
## 0416683008
## A    Supplementary Information
This appendix provides formal definitions, proof sketches, and robustness analyses support-
ing  the  main  manuscript.   No  new  assumptions  or  claims  are  made.   The  purpose  is  to
demonstrate that the entropy collapse follows rigorously from minimal and domain-agnostic
conditions and is not an artifact of specific modeling choices.
## A.1    Formal Definitions
Definition  1  (State  Space).   Let S  denote  a  finite  or  countable  set  of  internal  system
states.  The system at time t is described by a probability distribution
## P
t
## ∈ ∆(S),
where ∆(S) denotes the probability simplex.
Definition 2 (Entropy).  The entropy of the system at time t is defined as
## H(P
t
## ) =−
## X
s∈S
## P
t
(s) logP
t
## (s).
Any entropy-like functional that decreases under probability mass concentration yields equiv-
alent qualitative results.
Definition 3 (Feedback Amplification).  A system exhibits feedback amplification if
its update dynamics increases the relative probability mass of states with higher prevalence
or success as a function of a control parameter α.
Definition 4 (Bounded Novelty Regeneration).  Novelty regeneration is bounded
if the total probability mass injected into low-probability or previously unvisited states is
upper bounded by a function of a parameter β that does not scale with α.
A.2    Existence of an Entropy Collapse Threshold
Proposition  1  (Entropy  Collapse  Threshold).   For  any  system  satisfying  feedback
amplification and bounded novelty regeneration, there exists a finite threshold α
c
(β) such
that for all α > α
c
, the expected entropy decreases monotonically after a finite time.
Proof Sketch.  Consider the expected entropy change
## ∆H
t
## =E[H(P
t+1
## )− H(P
t
## )| P
t
## ].
Feedback amplification induces a systematic concentration of probability mass in dominant
states, generating an entropy-decreasing contribution that scales with α.  Novelty regenera-
tion contributes an increasing term of entropy that is uniformly bounded by β.
Because the entropy-reducing effect grows without bound while the entropy-increasing
effect remains bounded, there exists a finite α
c
(β) beyond which ∆H
t
< 0 holds outside a
transient regime.
## 17

A.3    Irreversibility of Entropy Collapse
Proposition 2 (Irreversibility).  Once system entropy falls below a critical level H under
α > α
c
, the probability of returning to a high-entropy regime within finite time approaches
zero.
Proof Sketch.  Below H, the trajectories enter a region of the probability simplex where
reinforcement dominates all admissible novelty injections.  Because novelty regeneration is
bounded, escape paths require increases in entropy exceeding what β can provide.
As  feedback  continues  to  sharpen  the  dominant  states,  the  likelihood  of  such  escape
trajectories decays to zero, yielding effective irreversibility.
A.4    Collapse as a Low-Entropy Manifold
Proposition 3 (Collapse Attractor).  For α > α
c
, the system trajectories converge to a
compact attractor set
## A
collapse
## ⊂ ∆(S).
Remark 1 (Low-Entropy Manifold).  The collapse attractor is generally not a single
fixed point but a low-dimensional manifold. The residual entropy within this manifold reflects
constrained variability rather than genuine adaptive diversity.  The dynamics of the system
may persist indefinitely while remaining globally trapped in a low-adaptability regime.
This manifold interpretation explains how post-collapse systems can remain locally dy-
namic while losing access to higher-dimensional adaptive directions.
## A.5    Robustness Analyses
Entropy collapse persists across variations in the following:  - update rules (multiplicative,
softmax, replicator), - state-space size, - entropy measures, - stochastic noise levels.
Noise and exploration increase the transient entropy, but do not eliminate the collapse
attractor for α > α
c
.  Collapse arises from structural asymmetry between reinforcement and
bounded novelty regeneration, rather than from numerical artifacts.
A.6    Relation to Known Models
Several classical models appear as special cases of the entropy collapse framework, includ-
ing  replicator  dynamics,  reinforced  random  walks,  preferential  attachment  processes,  and
evolutionary selection models.
The  entropy  collapse  provides  a  unifying  geometric  interpretation  for  these  models  by
highlighting  the  contraction  of  the  effective  adaptive  dimensionality  under  reinforcement-
dominated dynamics.
## A.7    Limitations
This Supplementary Information does not address exact collapse times, domain-specific em-
pirical calibration,  or optimal entropy governance strategies.  These directions are left for
future work.
## 18