
##### Volume 78

## Information Systems Engineering and

## Management

**Series Editor**
Álvaro Rocha
_ISEG, University of Lisbon, Lisbon, Portugal_

**Editorial Board**
Abdelkader Hameurlain
_Université Toulouse III Paul Sabatier, Toulouse, France_

Ali Idri
_ENSIAS, Mohammed V University, Rabat, Morocco_

Ashok Vaseashta
_International Clean Water Institute, Manassas, VA, USA_

Ashwani Kumar Dubey
_Amity University, Noida, India_

Carlos Montenegro
_Francisco José de Caldas District University, Bogota, Colombia_

Claude Laporte
_University of Quebec, Québec, QC, Canada_

Fernando Moreira
_Portucalense University, Porto, Portugal_


Francisco Peñalvo
_University of Salamanca, Salamanca, Spain_

Gintautas Dzemyda
_Vilnius University, Vilnius, Lithuania_

Jezreel Mejia-Miranda
_CIMAT—Center for Mathematical Research, Zacatecas, Mexico_

Jon Hall
_The Open University, Milton Keynes, UK_

Mário Piattini
_University of Castilla-La Mancha, Albacete, Spain_

Maristela Holanda
_University of Brasilia, Brasilia, Brazil_

Mincong Tang
_Beijing Jiaotong University, Beijing, China_

Mirjana Ivanovíc
_Department of Mathematics and Informatics, University of Novi Sad, Novi
Sad, Serbia_

Mirna Muñoz
_CIMAT—Center for Mathematical Research, Progreso, Mexico_

Rajeev Kanth
_University of Turku, Turku, Finland_

Sajid Anwar
_Institute of Management Sciences, Peshawar, Pakistan_

Tutut Herawan
_Faculty of Computer Science and Information Technology, University of
Malaya, Kuala Lumpur, Malaysia_


Valentina Colla
_TeCIP Institute, Scuola Superiore Sant’Anna, Pisa, Italy_

Vladan Devedzic
_University of Belgrade, Belgrade, Serbia_

The book series “Information Systems Engineering and Management”
(ISEM) publishes innovative and original works in the various areas of
planning, development, implementation, and management of information
systems and technologies by enterprises, citizens, and society for the
improvement of the socio-economic environment.
The series is multidisciplinary, focusing on technological,
organizational, and social domains of information systems engineering and
management. Manuscripts published in this book series focus on relevant
problems and research in the planning, analysis, design, implementation,
exploration, and management of all types of information systems and
technologies. The series contains monographs, lecture notes, edited
volumes, pedagogical and technical books as well as proceedings volumes.
Some topics/keywords to be considered in the ISEM book series are,
but not limited to: Information Systems Planning; Information Systems
Development; Exploration of Information Systems; Management of
Information Systems; Blockchain Technology; Cloud Computing; Artificial
Intelligence (AI) and Machine Learning; Big Data Analytics; Multimedia
Systems; Computer Networks, Mobility and Pervasive Systems; IT
Security, Ethics and Privacy; Cybersecurity; Digital Platforms and Services;
Requirements Engineering; Software Engineering; Process and Knowledge
Engineering; Security and Privacy Engineering, Autonomous Robotics;
Human-Computer Interaction; Marketing and Information; Tourism and
Information; Finance and Value; Decisions and Risk; Innovation and
Projects; Strategy and People.
Indexed by Google Scholar. All books published in the series are
submitted for consideration in the Web of Science.


For book or proceedings proposals please contact Alvaro Rocha
(amrrocha@gmail.com).

```
OceanofPDF.com
```

_Editors_
Khadija Slimani, Vinay Aseri and Samira Khoulji

## Emotion and Facial Recognition in

## Artificial Intelligence: Sustainable

## Multidisciplinary Perspectives and

## Applications

```
OceanofPDF.com
```

_Editors_
Khadija Slimani
ESIEA (Higher School of Automatic Electronic Computing), Paris, France

Vinay Aseri
School of Cybersecurity and Digital Forensics, Narnarayan Shastri Institute
of Technology (NSIT), affiliated with the National Forensic Sciences
University (NFSU), Ahmedabad, Gujarat, India

Samira Khoulji
Tetuan, Morocco

ISSN 3004-958X e-ISSN 3004-
Information Systems Engineering and Management
ISBN 978-3-032-14777-6 e-ISBN 978-3-032-14778-
https://doi.org/10.1007/978-3-032-14778-

© The Editor(s) (if applicable) and The Author(s), under exclusive license
to Springer Nature Switzerland AG 2026

This work is subject to copyright. All rights are solely and exclusively
licensed by the Publisher, whether the whole or part of the material is
concerned, specifically the rights of translation, reprinting, reuse of
illustrations, recitation, broadcasting, reproduction on microfilms or in any
other physical way, and transmission or information storage and retrieval,
electronic adaptation, computer software, or by similar or dissimilar
methodology now known or hereafter developed.

The use of general descriptive names, registered names, trademarks, service
marks, etc. in this publication does not imply, even in the absence of a
specific statement, that such names are exempt from the relevant protective
laws and regulations and therefore free for general use.


The publisher, the authors and the editors are safe to assume that the advice
and information in this book are believed to be true and accurate at the date
of publication. Neither the publisher nor the authors or the editors give a
warranty, expressed or implied, with respect to the material contained
herein or for any errors or omissions that may have been made. The
publisher remains neutral with regard to jurisdictional claims in published
maps and institutional affiliations.

This Springer imprint is published by the registered company Springer
Nature Switzerland AG
The registered company address is: Gewerbestrasse 11, 6330 Cham,
Switzerland

```
OceanofPDF.com
```

##### Preface

The edited volume _Emotion and Facial Recognition in Artificial
Intelligence: Sustainable Multidisciplinary Perspectives and Applications_
presents an extensive exploration of the convergence between artificial
intelligence (AI), psychology, neuroscience, human–computer interaction,
and sustainable technological innovation. As AI systems evolve from
analytical computation to empathetic and perceptive engagement, the ability
to understand, interpret, and simulate human emotions has become a
frontier in modern science and innovation. This book seeks to capture this
transformative journey, providing a comprehensive and balanced
perspective that encompasses the theoretical, empirical, technical, ethical,
and policy-oriented dimensions of Emotion and Facial Recognition in AI.
Emotion recognition and facial analysis are now central to AI-driven
human interaction, influencing applications ranging from education,
healthcare, and business intelligence to law enforcement, mental health
diagnostics, and governance. The ability of machines to perceive and
respond to subtle human cues such as facial expressions, tone of voice, and
microexpressions reflects an important step toward achieving human-like
intelligence and contextual understanding. However, with this technological
capability comes a spectrum of challenges, including bias in datasets,
privacy concerns, ethical dilemmas, psychological impacts, and
sustainability of AI systems. The book addresses these complexities through
a multidisciplinary framework, integrating insights from data science,
psychology, deep learning, legal studies, and educational technologies.
Chapters are summarized as follows:
The first two chapters introduce the conceptual and psychological
underpinnings of emotion and its representation in computational systems.
Chapter “Introduction to Emotion Expression and AI” provides the
foundation for understanding how emotional data is represented, processed,
and synthesized by intelligent systems. Chapter “Theories of Emotions in
Psychology” delves into classical and modern emotion theories such as
James–Lange, Cannon–Bard, Schachter–Singer, and appraisal models
establishing the scientific framework that supports emotion modeling in AI.
Moving into the technological core, Chapters “Classical Face
Recognition: Geometric Models, Subspace Techniques and Local


Descriptors”–“Detection of Microexpressions and Subtle Emotions” discuss
algorithmic frameworks, feature extraction, and deep learning methods for
emotion recognition. Chapter “Classical Face Recognition: Geometric
Models, Subspace Techniques and Local Descriptors” presents classical
facial recognition techniques including geometric and subspace models,
forming the historical basis for modern recognition systems.
Chapter “Integrating Acoustic Feature Extraction and LSTM Models for
Emotion Classification in Speech” introduces acoustic emotion
classification using LSTM networks, highlighting the fusion of audio and
emotional analytics. Chapter “Emotion Detection in Human-Machine
Interaction Using ML Techniques” bridges machine learning with affective
computing, showing how machines perceive emotional patterns during real-
time human–machine interaction. Chapter “Real Time: 3D Facial
Expression Recognition Using Improved AlexNet Convolutional Network
via Deep-Emotion” introduces advanced 3D modeling using an improved
AlexNet architecture for deep emotion analysis, while Chapter “Feature
Aggregation for Efficient Continual Learning of Complex Facial
Expressions” emphasizes continual learning frameworks for sustained
emotion recognition. Chapter “Advanced Techniques in Facial Landmark
Detection and Feature Extraction for Emotion-Aware AI Systems” provides
an in-depth analysis of facial landmark detection and feature engineering,
and Chapter “Detection of Microexpressions and Subtle Emotions”
explores the intricate science behind microexpression and subtle emotion
detection, which is crucial for forensic and behavioral applications.
The next set of chapters, Chapters “Navigating the Future of Emotion
AI: Technical Barriers, Ethical Concerns, and Sustainable
Advancements”–“Integrating Positive Emotions to Support Self-Directed
Learning”, navigate interdisciplinary domains connecting Emotion AI to
sustainability, ethics, mental health, and education. Chapter “Navigating the
Future of Emotion AI: Technical Barriers, Ethical Concerns, and
Sustainable Advancements” provides a critical discussion on the technical
barriers, ethical challenges, and sustainability frameworks in Emotion AI
development. Chapter “Emotion AI in Mental Health” connects affective
computing with mental health diagnostics, illustrating its potential in early
emotion-based disorder detection and therapy. Chapter “A Conceptual
Framework for Adaptive Student Assessment Using AI-Driven
Recommendations and Facial Expression Recognition” and


Chapter “Integrating AI-Driven Facial Emotion Recognition into E-
Learning Systems: Sustainable Educational Markets Through
Interdisciplinar y Innovations” expand the educational dimension,
introducing frameworks for adaptive learning and AI-driven student
assessment through emotion recognition in e-learning platforms.
Chapter “Integrating Positive Emotions to Support Self-Directed Learning”
continues this theme by exploring positive psychology and emotion
integration to support self-directed learning and educational resilience.
The final segment, Chapters “Emotion AI in Business and Customer
Services”–“Emotion AI: Challenges and Future Directions” broaden the
discussion to industrial, legal, and futuristic perspectives. Chapter “Emotion
AI in Business and Customer Services” showcases the use of Emotion AI in
business intelligence, marketing, and customer interaction systems,
highlighting data-driven personalization and emotion analytics in service
industries. Chapter “Deep Learning Approaches and Technical Challenges
in Facial Emotion Recognition (FER)” consolidates deep learning
methodologies for facial emotion recognition (FER) and presents critical
challenges associated with data imbalance, real-time performance, and
interpretability. Chapter “Emotion Recognition and the Law: Bridging
Technology and Human Rights” introduces a powerful discussion on
emotion recognition and the law, exploring the interface between human
rights, privacy, and AI regulation. Finally, Chapter “Emotion AI: Challenges
and Future Directions” concludes the book by synthesizing emerging
research trends, future applications, and ethical imperatives for sustainable
emotion AI development.
This book is the result of collaborative scholarship from a diverse
community of researchers, technologists, psychologists, and policymakers
from around the world. Each contributor brings unique expertise, ensuring
that the reader gains a holistic understanding of how emotional intelligence
and AI can coexist in a manner that enhances human machine collaboration
without compromising ethical and societal values.
In an era where machines are learning not only to think but also to
“feel”, the objective of this volume is to inspire further inquiry into
emotionally aware, ethically aligned, and sustainable AI ecosystems. It
serves as a valuable reference for academicians, practitioners, students,
policymakers, and innovators seeking to understand how emotion and


intelligence can be harmonized to shape the next generation of human-
centric AI technologies.

```
Dr. Khadija Slimani
Mr. Vinay Aseri
Dr. Samira Khoulji
Paris, France
Ahmedabad, India
Tetuan, Morocco
OceanofPDF.com
```

##### Contents

**Introduction to Emotion Expression and AI**
Rajwinder Singh Mankoo, Shivam Patel, Nikunj Tahilramani,
Anantkumar Patil, Aditya Bhadouria and Shadab Mazhar Roushan

**Theories of Emotions in Psychology**
Bhavika Bhagyesh Lad and Dipti Srivastava

**Classical Face Recognition: Geometric Models, Subspace Techniques
and Local Descriptors**
Mossaab Idrissi Alami, Abderrahmane Ez-zahout and Fouzia Omary

**Integrating Acoustic Feature Extraction and LSTM Models for
Emotion Classification in Speech**
Charan Kumar Nunna and Divya Meena Sundaram

**Emotion Detection in Human-Machine Interaction Using ML
Techniques**
R. Karthick Manoj and S. Aasha Nandhini

**Real Time: 3D Facial Expression Recognition Using Improved AlexNet
Convolutional Network via Deep-Emotion**
Narimane Saad

**Feature Aggregation for Efficient Continual Learning of Complex
Facial Expressions**
Thibault Geoffroy, Myriam Maumy and Lionel Prevost

**Advanced Techniques in Facial Landmark Detection and Feature
Extraction for Emotion-Aware AI Systems**
Shaik Khaja Mohiddin, Shaik Sharmila and Khadija Slimani

**Detection of Microexpressions and Subtle Emotions**
Roopali Pahwa and Vinayak Gupta

**Navigating the Future of Emotion AI: Technical Barriers, Ethical
Concerns, and Sustainable Advancements**


```
Shaik Khaja Mohiddin, Shaik Sharmila and Khadija Slimani
```
**Emotion AI in Mental Health**
Ayushi Shelke, Ashok Kumar, Mukul Yadav and Vinay Aseri

**A Conceptual Framework for Adaptive Student Assessment Using AI-
Driven Recommendations and Facial Expression Recognition**
Amimi Rajae, Radgui Amina and Ibn el haj el Hassane

**Integrating AI-Driven Facial Emotion Recognition into E-Learning
Systems: Sustainable Educational Markets Through Interdisciplinar y
Innovations**
Anirban Ghatak and Miss Setavi Purushottam Thoke

**Integrating Positive Emotions to Support Self-Directed Learning**
Hommane Boudine, Meriem Bentaleb, Driss El Karfa, Khadija Slimani
and Abderrahim Tayebi

**Emotion AI in Business and Customer Services**
Esra Sipahi Döngül

**Deep Learning Approaches and Technical Challenges in Facial
Emotion Recognition (FER)**
Anjanadevi Bondalapati and Slimani Khadija

**Emotion Recognition and the Law: Bridging Technology and Human
Rights**
Sarthak Prasad Sahoo and Shraddha Suman Paikray

**Emotion AI: Challenges and Future Directions**
Anjali Thakur and Gaurav Gupta
_OceanofPDF.com_


##### About the Editors

**Dr. Khadija Slimani** a member of IEEE, has
built a notable career in both academia and
research. She earned her Ph.D. through a
collaborative program between the University
of Ibn Tofail in Morocco and the University of
Technology of Belfort Montbéliard (UTBM) in
France. This joint supervision highlights her
commitment to academic excellence and her
skill in integrating cross-cultural research
environments. Dr. Slimani’s doctoral research
covered a broad range of topics, including
machine learning, deep learning, pattern
recognition, and computer vision, with a
specific focus on academic emotion recognition.
Her interdisciplinary approach, enriched by experiences in both Moroccan
and French academic settings, has contributed significantly to the global
perspective of her work. After completing her Ph.D., Dr. Slimani advanced
her expertise through a postdoctoral position at the University of Poitiers,
France, where she concentrated on Objects DRI (Detection, Recognition,
and Identification). Her innovative use of AI methodologies aimed to
enhance video content filtering for improved security, demonstrating her
ability to address complex problems on an international scale. In addition to
her research, Dr. Slimani has made substantial contributions to the field of
telecommunications. Her interests extend to Next-Generation Networks
(NGN), 5G, and 6G technologies, where she investigates their implications
and potential advancements. This focus on cutting-edge
telecommunications technology underscores her role in shaping the future
of network infrastructure and communication systems. Dr. Slimani has also
been actively involved in teaching at several prestigious engineering
schools in Paris, covering modules such as data science, deep learning,
machine learning, databases, computer vision, and telecommunications. She
currently holds the position of Associate Professor at the Graduate School
of Automatic Electronic Computing in Paris. Her scholarly achievements
include over 21 published articles in esteemed journals and conferences,
with an impressive h-index of 10 in Scopus. Dr. Slimani has also served as


an editor and co-editor for various books, proceedings, and special issues
with leading publishers such as Springer Nature, AAP/CRC, Apress, IGI
Global, and Wiley

**Mr. Vinay Aseri** is an assistant professor of
Cybersecurity and Digital Forensics at the
Narnarayan Shastri Institute of Technology—
Institute of Forensic Sciences and Cyber
Security (NSIT-IFSCS), affiliated with the
National Forensic Sciences University (NFSU),
Ministry of Home Affairs, Government of India.
He holds a Postgraduate degree in Forensic
Science, a PG Diploma in Information
Technology and Security, and a Diploma in
Photography in Forensic Science. He completed
his higher education in Forensic Science from
India’s pioneering National Security and Police
University. Mr. Aseri’s research interests focus
on digital forensics, cybersecurity, and artificial intelligence for national
security. He has an impressive record of numerous published in Scopus-
indexed and SCI, Q1 journals, and numerous book chapters with national
and international publishers. He has also edited eight Scopus-indexed books
with reputed publishers, including Elsevier and CRC Press, and is involved
in multiple ongoing research projects and international collaborations in the
domains of digital forensics and cybersecurity. He served as an assistant
researcher with the National Investigation Agency (NIA) and was selected
as a member of the United Nations (MAG) First Internet Governance
Forum in Riyadh, Saudi Arabia. He has presented his work at 22+ national
and international conferences and workshops and continues to actively
contribute to academic and policy dialogues in technology and national
security. Mr. Aseri’s research innovations include a German patent for
Banana Peel Nano Bio-Char and an Indian copyright for Sugarcane Bagasse
Nano Bio-Char applications in Forensic Science. His achievements have
been recognized with prestigious fellowships, including: InSIG Fellowship
2022 (Hyderabad Chapter) Law Enforcement Policing Fellowship 2024–


2025 (Takshashila Institution—Indian Police Foundation) also serving as
scientific and technical committee member in international conferences as
world class level. Beyond his research and teaching, he contributes as a
reviewer for reputed journals such as MDPI, Springer, Elsevier,
IntechOpen, River Publishers, and IJBPS. He is also an active member of
professional bodies including the Internet Society (ISOC) Delhi and
Mumbai Chapters, ACM, and IEEE.

**Dr. Samira Khoulji** is a professor in
electronics, telecommunications, and computer
science at the National School of Applied
Sciences in Tetuan (ENSATE) of Morocco,
where she also serves as the coordinator for the
Telecommunications and Networks Engineering
program for engineering students. She holds the
position of director of the “Innovative Systems
Engineering” (ISI) research laboratory.
Previously, she was the head of the Department
of Statistics and Computer Science at the
multidisciplinary faculty in Tétouan, where she
was responsible for the professional programs
in “Tourism and ICT” and “Management of
Information Systems (MSI)” as well as the coordinator for the Specialized
Master’s program in “Management of Information Systems and Multimedia
(MSIM). Dr. Samira Khoulji has actively participated in various committees
and councils and has coordinated teaching modules in physics, computer
science, and telecommunications at the multidisciplinary faculty and
ENSAT in Tétouan. As a researcher, she is the author or co-author of
numerous research articles and has contributed as a member to multiple
national and international conference and meeting committees. She has also
been involved in several European research projects, particularly in the
fields of professional integration, tourism, and renewable energy. Dr.
Samira Khoulji has organized several national and international events and
conducted numerous training workshops for doctoral candidates. Her
commitment to research spans many years, with her work primarily


focusing on areas such as intelligent systems, machine learning, big data,
sensor systems, deep learning, mobile communications, Internet of Things,
cybersecurity, and computer vision, including facial detection, matching,
and recognition.Please provide high-resolution photo for Dr. Samira
Khouljiwe dont have any another photo. you can adjust according to your
level.

```
OceanofPDF.com
```

(1)

(2)

(3)

© The Author(s), under exclusive license to Springer Nature Switzerland AG 2026
K. Slimani et al. (eds.), _Emotion and Facial Recognition in Artificial Intelligence: Sustainable
Multidisciplinary Perspectives and Applications_ , Information Systems Engineering and Management
78
https://doi.org/10.1007/978-3-032-14778-3_

## Introduction to Emotion Expression and

## AI

Rajwinder Singh Mankoo^1 , Shivam Patel^2 , Nikunj Tahilramani^2 ,

Anantkumar Patil^3 , Aditya Bhadouria^2 and Shadab Mazhar Roushan^2
College of Science, Technology, Engineering, and Mathematics,
Southern Utah University, Utah University, Cedar City, USA
School of Cyber Security and Digital Forensics, Narnarayan Shastri
Institute of Technology Ahmedabad, Ahmedabad, Gujarat, India
School of Forensic Sciences, Narnarayan Shastri Institute of
Technology Ahmedabad, Ahmedabad, Gujarat, India

```
Rajwinder Singh Mankoo
Email: rajwindersinghmankoo@suumail.net
```
**Abstract**
This chapter discusses the interaction between human emotional
experiences and machine learning frameworks, relating computational
methods of emotion generation and recognition to psychological concepts
like Ekman’s basic emotions and dimensional theories. We discuss how
such models as convolutional neural networks (CNNs), recurrent neural
networks (RNNs), and transformers perceive the facial, vocal, and textual
features to identify emotions, and also synthesize artificial expressions.
Notably, the ethical considerations of privacy, algorithmic bias,
manipulation, and accountability are the key concerns that the chapter
considers as the core of responsible AI development. The examples of
emotion AI in practice (e.g., mental health screening, customer service
chatbots, retail personalization) demonstrate how it can transform the


world, whereas the questions about the emotional understanding and the
effects on society are open-ended, which preconditions the future research.
Overall, the chapter suggests that the success of emotion AI lies in the
ability to balance the innovativeness with the ethical rigor, so that the
technology should improve, but not corrupt, human connection.

**Keywords** Emotion expression – AI – CNN – Facial emotion recognition

##### 1 Foundations of Emotion Expression

**1.1 Defining Emotions: Psychological Perspectives**

Emotions are subjective, momentary, triggered by internal or external
events, characterized by three interrelated factors: physiological (e.g., a
racing heart), outward (e.g., smiling) and conscious (e.g., happiness)
aspects. There are two basic frameworks of emotions that psychologists
usually divide them into:

**Categorical Models** : Propose discrete “basic emotions” (e.g., joy, anger,
fear) supported by Paul Ekman’s cross-cultural research, which identifies
universal facial expressions across diverse societies.
**Dimensional Models** : Frame emotions as points on a continuum of
valence (positive/negative), arousal (calm/excited), and dominance
(controlled/powerful), as described in James Russell’s circumplex model.
Both perspectives are valuable: categorical models simplify recognition
(e.g., labeling a smile as “happy”), while dimensional models capture
nuance (e.g., distinguishing “excited joy” from “content joy”) [ 28 ]. We
adopt a hybrid approach because real-world emotions rarely fit neatly into
boxes—they exist on spectra, shaped by context.

**1.2 Human Emotion Expression: Channels and Universality**

Humans express emotions through four primary channels, each with
universal and culturally variable elements. **Facial expressions** (e.g., raised
eyebrows for surprise, furrowed brows for anger) are the most studied:
Ekman’s work shows six basic expressions are recognizable across 50+
cultures, though intensity and context modify them (e.g., a “polite smile” in
Japan vs. a “genuine smile” in the U.S.) [ 3 , 34 ]. **Vocal prosody** —the tone,
pitch, and rhythm of speech—conveys emotion even without words (e.g., a


trembling voice for fear). **Bodily movements** (e.g., crossed arms for
defensiveness, open posture for openness) and **verbal content** (e.g., “I’m
fine” said sarcastically) complete the picture. Universality arises from
shared biology (e.g., amygdala activation for fear), but culture shapes
_display rules_ (e.g., suppressing anger in formal settings) [ 13 , 21 ]. Because
AI systems have to recognize simple emotional signals as well as
accommodate a range of possible cultural situations, this is an important
balance between universality and diversity.

**1.3 Computational Models of Emotion: From Theory to**

**Algorithms**

In order to create AI that communicates with human emotions, we put
psychological theories into computational models. Categorical models are
the ones that motivate classification algorithms (e.g., convolutional neural
networks [CNNs] that identify facial expressions as either anger or joy).
Dimensional models are used to power regression or clustering (e.g.
predicting valence/arousal scores with text using transformer models).
Hybrid models combine categorical and dimensional models, including the
classification of an expression as being happy (categorical) and the
measurement of its high arousal (dimensional) [ 7 , 41 , 42 ]. We choose such
models to trade off between simplicity (which is crucial when dealing with
real-time applications such as customer service bots) and granularity (which
is imperative when a research is conducted such as mental health
screening). One of the most difficult tasks is to bridge the so-called
empathy gap: AI should not be able to read emotions but understand their
contextual interpretation (e.g., a face is sad, but it might mean grief or
exhaustion). This will require the combination of various channels, such as
facial, vocal, and textual, to reduce ambiguity, which is the way humans
perceive emotional stimuli.

##### 2 AI Systems for Emotion Recognition

Here we discuss the way AI identifies human emotions through sensory
data of four main modalities, which include facial, vocal, physiological, and
textual, and the machine learning algorithms that make this possible. We
also address key challenges that limit current systems’ accuracy and
fairness.


**2.1 Sensory Input Modalities**

We design AI systems to extract emotional cues from human behavior using
four distinct channels, each with unique advantages and constraints:

**Facial Expressions** : Computer vision algorithms analyze video or static
images to identify micro-expressions (e.g., lip tightening for anger, cheek
raising for joy) and macro-expressions (full smiles, frowns).
Convolutional neural networks (CNNs) excel here, as they automatically
learn spatial patterns in pixel data. However, facial recognition [ 23 , 30 ]
struggles with poor lighting, occlusions (e.g., masks), or extreme angles
—common in real-world scenarios.
**Vocal Prosody** : Audio signals provide rich emotional data through tone,
pitch, rhythm, and pauses. For example, a trembling voice indicates fear,
while a rapid, high-pitched tone suggests excitement. Recurrent neural
networks (RNNs) and long short-term memory (LSTM) networks
process sequential audio features, capturing temporal dynamics. Yet
background noise (e.g., crowds, traffic) often corrupts vocal cues,
reducing reliability [ 44 ].
**Physiological Signals** : Wearable sensors (e.g., smartwatches, EEG
headsets) measure biometric changes linked to emotion, such as
increased heart rate (arousal), skin conductance (stress), or brainwave
activity (engagement). While these signals are objective and hard to fake,
they require invasive hardware and struggle with individual variability
(e.g., a athlete’s baseline heart rate differs from a sedentary person’s)
[ 41 – 43 ].
**Textual Content** : Natural language processing (NLP) analyzes written or
spoken language to infer sentiment (e.g., “I’m thrilled” = positive) and
intent (e.g., “Can you help?” = neutral/help-seeking). Transformer models
(e.g., BERT, GPT) dominate here, as they handle context and long-range
dependencies [ 24 , 30 ]. However, text alone misses nonverbal cues—
sarcasm (“Great, another meeting”) or irony (“Nice weather—pouring
rain!”) often confuse AI.
We can combine these modalities (multimodal fusion) to improve
robustness. For instance, a system might use facial data to confirm a
“smile” detected in text, reducing false positives (Fig. 1 ).


**_Fig. 1_** Facial expressions corresponding to basic emotions (Ekman’s 6 + 2 extended)

**2.2 Machine Learning Approaches**

To convert raw sensory data into emotional labels, we leverage three core
machine learning paradigms, each tailored to the structure of the input:

```
Convolutional Neural Networks (CNNs) : Ideal for spatial data (facial
images, spectrograms of speech). CNNs apply filters to detect edges,
textures, and shapes—critical for recognizing a furrowed brow or a rising
pitch. For example, a CNN trained on the FER2013 dataset (48,000 facial
images) achieves ~70% accuracy in classifying basic emotions.
Recurrent Neural Networks (RNNs) and Long Short-Term Memory
(LSTMs) : Suited for sequential data (vocal audio, text). RNNs process
inputs step-by-step, retaining memory of previous steps—essential for
capturing the progression of a sentence (“I was happy... until I saw the
bill”) or a speaker’s tone shift. LSTMs improve upon RNNs by
addressing the “vanishing gradient” problem, allowing them to remember
longer contexts.
Transformer Models : Revolutionized NLP and multimodal tasks.
Transformers use self-attention mechanisms to weigh the importance of
each word (or pixel) in relation to others, enabling them to understand
nuanced language (e.g., “I’m fine” said with a sigh) or fuse facial and
textual data. For instance, a transformer might combine a frowning face
with the text “This is amazing” to flag sarcasm [ 4 , 9 , 22 ].
```

We choose these models because they balance accuracy and efficiency.
CNNs handle visual data quickly, LSTMs manage temporal sequences, and
transformers excel at context—key for real-time applications like customer
service bots or mental health screening tools (Table 1 ).

**_Table 1_** Comparison of deep learning models for emotion recognition and analysis

```
Model type Core
mechanism
```
```
Typical use
cases
```
```
Key
advantages
```
```
Key limitations Real-world
impact
examples
CNNs
(convolutional
neural
networks)
```
```
Extracts
spatial
features (e.g.,
edges,
textures)
through
convolutional
And pooling
layers
```
```
Facial
expression
classification
(e.g., Ekman’s
six emotions);
audio
spectrogram
analysis
```
```
Automatic
feature
learning
Highly
efficient for
image and
audio data
```
```
Sensitive to
lighting
variations and
occlusion
Limited
handling of
temporal
dependencies
```
```
Snapchat filters
(real-time facial
effects);
Amazon Alexa
(voice wake-
word detection)
```
```
RNNs/LSTMs Processes
sequential
data (text,
audio) using
hidden states;
LSTMs
overcome
vanishing
gradient
issues
```
```
Speech emotion
recognition
(e.g., customer
service calls);
text sequence
prediction (e.g.,
empathetic chat
responses)
```
```
Captures
temporal
dependencies
Lower
computational
cost than
transformers
```
```
Slow inference
due to
sequential
nature
Prone to
overfitting on
small datasets
```
```
Google
Assistant
(speech
recognition);
Twitter
sentiment
analysis
```
```
Transformers Employs
self-attention
to determine
input
importance;
enables
parallel
processing
for efficiency
```
```
Multimodal
fusion (facial +
text for
chatbots);
generative tasks
(e.g., emotion-
aware emails)
```
```
Captures
long-range
dependencies
State-of-the-
art
performance
for NLP and
multimodal
tasks
```
```
Requires large
datasets
Computationally
expensive
Risk of
“hallucination”
```
```
ChatGPT (text
generation);
Microsoft
Teams (caption
translation)
```
```
GANs
(generative
adversarial
networks)
```
```
Adversarial
setup:
generator
produces
synthetic
data;
discriminator
```
```
Synthetic
emotion
generation (e.g.,
deepfakes,
virtual
influencers);
virtual avatar
expressions
```
```
Generates
realistic and
high-quality
outputs
Flexible with
unstructured
```
```
Training
instability
(mode collapse)
Ethical and
misuse concerns
(deepfakes)
```
```
DeepFaceLab
(deepfake
creation);
Unreal Engine
(virtual
character
animations)
```

```
Model type Core
mechanism
```
```
Typical use
cases
```
```
Key
advantages
```
```
Key limitations Real-world
impact
examples
evaluates its
realism
```
```
data (images,
audio)
GNNs (graph
neural
networks)
```
```
Utilizes
message
passing
between
nodes to
model
relationships
in graph-
structured
data
```
```
Social emotion
analysis (e.g.,
viral sentiment
in online
communities);
emotion-driven
recommendation
systems (e.g.,
TikTok feeds)
```
```
Captures
relational and
collective
emotion
dynamics
Effective for
sparse or
structured
data
```
```
High
computational
Cost for large
graphs
Performance
depends on
graph structure
quality
```
```
Twitter trend
analysis; TikTok
recommendation
systems
```
**2.3 Challenges in Emotion Recognition**

Despite progress, AI systems face significant hurdles in reliably detecting
human emotions:

```
Ambiguity : The same physical cue can signal multiple emotions. A
frown might indicate anger, concentration, or sadness—context is critical,
but AI often lacks the world knowledge to disambiguate. For example, a
“neutral” face in a job interview could be interpreted as bored or serious,
depending on cultural norms [ 18 , 27 ].
Context Dependency : Emotions are situational. A laugh at a comedy
show is joyful, but the same laugh at a funeral is inappropriate (and likely
forced). Current systems rarely incorporate contextual metadata (e.g.,
location, event type), limiting their ability to adapt.
Algorithmic Bias : Training data often reflects demographic imbalances.
For instance, facial recognition models trained primarily on light-skinned
faces perform poorly on darker skin tones, misclassifying emotions like
“anger” or “disgust” at higher rates. In the same manner, the vocal
prosody models can fail to identify emotions among non-native speakers
of English because the accent variation can manipulate the prosodic
characteristics.
Privacy Concerns : There are significant ethical concerns with the
gathering of biometric information, such as heart rate and facial scans.
Individuals often have no idea that their emotional states are being
monitored; for example, a fitness tracker may be measuring stress levels
```

if an individual is performing work activities without obtaining his or her
explicit consent.
Such issues highlight the gap between artificial intelligence’s technical
precision and the complex, highly nuanced nature of human feeling. These
need to be solved jointly by experts: ethicists need to establish clear
guidelines for ethical and responsible use, engineers need to enhance model
dependability, and psychologists need to enhance emotional models.

##### 3 AI-Driven Emotional Generation

We then shift our focus to the complimentary process—the generation of
emotional responses—based on our previous discussion of how artificial
intelligence understands human emotions (Sect. 2 ). It is crucial in
developing AI systems that can communicate naturally and in a human-like
manner, ranging from empathetic chatbots to virtual assistants that respond
appropriately to users’ emotions. This chapter investigates the goals of
synthetic emotion, describes the approaches to producing affective outputs
across different media, and evaluates how effectively these systems convey
contextual appropriateness and genuineness.

**3.1 Synthetic Emotion in Machines: Goals and Applications**

Synthetic emotion includes the AI-created cues that recreate or instigate the
human emotional conditions. Its aims are dual:

1. Improve User Experience: Increase the level of interest and connection.
    As an illustration, a customer care robot that replies to frustration with
    a cool and reassuring voice calms down the user.
2. Enabling Adaptive Behaviour: Give the AI the opportunity to adapt
    itself to the emotion of a user. Educational software, for instance, might
    simplify a math problem if it detects confusion (via facial or textual
    cues) [ 2 , 38 , 45 ].

Applications span industries:

```
Healthcare : Virtual therapists that deliver supportive messages to
patients with depression.
```

**Gaming** : Non-player characters (NPCs) that react dynamically to player
emotions (e.g., cheering for a win or consoling after a loss).
**Retail** : Chatbots that use humor or urgency to nudge purchases (e.g.,
“Don’t miss out—this sale ends soon!”).
A key distinction exists between **simulated emotion** (mimicking
outward signs, e.g., a smile) and **felt emotion** (true internal states, which AI
cannot experience). Most systems focus on simulation, as felt emotion
remains beyond current technological reach [ 20 , 29 , 39 ].

**3.2 Techniques for Emotional Output: Text, Speech, and**

**Visuals**

Generating emotional outputs requires modality-specific techniques, often
combined for richer interactions:

**Text-Based Emotion Generation**

Natural language processing (NLP) models are the backbone of text-based
emotional generation. **Transformers** (e.g., GPT-4, BERT) are fine-tuned on
datasets like _EmpatheticDialogues_ (pairs of user statements with empathetic
responses) to produce emotionally appropriate replies. For example:

```
Input: “I failed my exam.”
Output: “That sounds really tough—would you like to talk about what
went wrong?” Techniques include:
Sentiment Transfer : Adjusting the tone of a message (e.g., turning a
neutral statement into a encouraging one).
Contextual Adaptation : Using dialogue history to tailor responses (e.g.,
remembering a user’s stress about work and referencing it later).
```
**Speech-Based Emotion Generation**

Neural text-to-speech (TTS) systems with **prosody control** generate
emotional vocal outputs. Models like _Tacotron 2_ or _WaveNet_ manipulate
pitch, volume, and rhythm to convey emotions:

```
Anger: Sharp, high-pitched tones with rapid speech.
Sadness: Low, slow, monotone delivery.
```

Advanced systems integrate **emotion embeddings** (numerical
representations of emotions) into TTS pipelines, allowing developers to
specify desired emotions (e.g., “output a cheerful tone”) [ 6 , 19 , 36 ] (Fig. 2 ).

**_Fig. 2_** AI voice emotion analysis workflow

**Visual-Based Emotion Generation**

Computer graphics and animation drive visual emotional outputs.
**Generative Adversarial Networks (GANs)** synthesize realistic facial
expressions (e.g., a surprised face) by training on datasets like _Facial Action
Coding System (FACS)_. Motion capture technology records human body
language (e.g., slumping shoulders for sadness) and translates it to virtual
avatars.
Multimodal fusion—combining text, speech, and visuals—creates more
immersive experiences. For example, a virtual assistant might say “I’m
sorry to hear that” (speech) while displaying a sympathetic facial
expression (visual) and offering a solution (text) [ 12 , 32 , 33 ].

**3.3 Evaluating Generated Emotion: Validity, Appropriateness,**

**and User Perception**


Evaluating synthetic emotion is challenging because “good” emotion
depends on context and user expectations. We use a mix of **objective** and
**subjective** metrics:

**Objective Metrics**

```
Consistency : Does the output match the intended emotion? For example,
a “happy” text response should have positive sentiment scores (measured
by NLP tools like VADER).
Diversity : Can the system generate a range of emotions (e.g., not just
happiness but also anger or fear)?
Latency : Is the response timely? Delays can disrupt emotional flow (e.g.,
a bot taking 10 s to reply to a user’s frustration).
```
**Subjective Metrics**

```
Realism : Do users perceive the emotion as genuine? Surveys ask
participants to rate how “natural” a response feels (e.g., “On a scale of 1–
5, how believable was the bot’s sympathy?”).
Appropriateness : Does the emotion fit the context? A bot that laughs at
a user’s complaint about a broken product is inappropriate—even if the
laughter is realistic.
Engagement : Do users prefer emotional AI over neutral systems?
Studies track metrics like conversation length or repeat usage [ 17 , 26 ].
```
**Challenges in Emotional Generation**

Despite advances, AI-generated emotion faces significant hurdles:

```
Authenticity Gap : Simulated emotions often lack the subtlety of human
expression (e.g., a bot’s “sadness” may not convey true empathy).
Context Blindness : Systems struggle to adapt to dynamic situations
(e.g., a bot that continues to cheer a user who is clearly upset).
Scalability : Generating diverse emotions for global audiences requires
training data that represents cultural differences (e.g., a “thumbs-up”
gesture is positive in the U.S. but offensive in Greece).
```
**Ethical Risks**


```
Excessively manipulative emotional outputs—for instance, a bot
employing flattery to drive purchases—can exploit user vulnerability. To
maintain trust, it is critical to make people aware of the emotional
constraints of AI (Fig. 3 ).
```
**_Fig. 3_** Computational workflow for facial expression recognition
The challenge of translating human emotions into forms that machines
can interpret is brought out in this part. Although AI can imitate emotional
responses, there is still need to research on how there is a distinction
between imitation and genuine understanding. The ethics of emotion AI are
investigated in the subsequent section, which examines how and when these
technologies impact social interaction, privacy concerns, and core human
values.

##### 4 Ethical and Societal Implications

There are some grave ethical and social concerns raised by the increasing
ability of AI systems to recognize and generate human emotions. The
sensitivity of emotional data, the possibility of misuse, and the influence on
human autonomy are the reasons of these issues. Here, we discuss four


main issues including privacy, bias, manipulation, and accountability and
the ways to harm mitigation.

**4.1 Privacy Concerns in Emotion Data Collection**

The use of AI systems is associated with deeply ethical and social concerns
as they grow able to perceive and produce human emotions. The sensitivity
of emotional data, the possibility of misuse, and the influence on human
autonomy are the reasons of these issues. Here, we discuss four main issues
including privacy, bias, manipulation, and accountability and the ways to
harm mitigation. For example:

The fact that a fitness tracker has recorded a high heart rate during a
meeting might suggest that the person is experiencing anxiety at work.
A home camera that is capable of analyzing facial expressions can record
when a user is sad after phone conversation.
The threat of unauthorized access is urgent: compromised databases
may reveal vulnerable groups (e.g., domestic abuse victims) or be sold off
to third parties (e.g., refused to cover by insurance companies due to the
presence of such emotions). Even with consent, users often lack awareness
of how their data is used—studies show 60% of people do not read privacy
policies detailing emotion data collection [ 5 , 16 , 37 ].
To protect privacy, we advocate for **explicit, granular consent** (e.g.,
“Allow this app to access your facial expressions for emotion
recognition?”) and **data minimization** (collecting only necessary
information). Regulations like the EU’s GDPR and California’s CCPA
already restrict biometric data, but enforcement remains inconsistent
globally.

**4.2 Algorithmic Bias and Fairness in Emotion AI**

Emotion AI inherits biases from training data, leading to discriminatory
outcomes. For example:

```
Facial Recognition : Models trained predominantly on light-skinned
faces misclassify emotions (e.g., “anger”) in dark-skinned individuals at
34% higher rates.
Vocal Analysis : Accents (e.g., Southern U.S. dialects) or speech
impairments (e.g., stuttering) are often misinterpreted as “frustration” or
“confusion,” penalizing non-standard speakers.
```

**Text-Based Sentiment Analysis** : NLP models associate African
American Vernacular English (AAVE) with negative sentiment, even
when the content is neutral.
These biases perpetuate systemic inequities: a hiring bot that rejects
candidates with “anxious” facial expressions may discriminate against
neurodiverse applicants, while a loan approval system that flags “stress” in
low-income users could deny financial aid [ 8 , 14 , 40 ].
Mitigation requires **diverse, representative datasets** (e.g., including
faces of all skin tones, accents, and linguistic styles) and **bias auditing
tools** (e.g., IBM’s AI Fairness 360) to detect and correct disparities.
However, achieving full equity is challenging—cultural differences in
emotional expression (e.g., a “neutral” face in Japan vs. a “serious” face in
the U.S.) complicate universal standards.

**4.3 Manipulation and Exploitation Risks**

AI’s ability to generate emotional responses creates opportunities for
manipulation. For example:

**Persuasive Technology** : Ads that use “urgency” (e.g., “Only 2 left!”) or
“social proof” (e.g., “90% of users loved this!”) trigger impulsive buying
by exploiting fear of missing out (FOMO).
**Deepfakes and Synthetic Media** : AI-generated videos or voices
mimicking celebrities or politicians can spread misinformation (e.g., a
fake clip of a CEO “admitting” fraud).
**Emotional Labor Automation** : Customer service bots that feign
empathy (e.g., “I’m so sorry you’re frustrated!”) may placate users but
mask unresolved issues, eroding trust in institutions.
The line between helpful and harmful manipulation is thin. A therapy
bot that uses gentle encouragement is beneficial, but one that pressures a
user to disclose personal trauma crosses an ethical boundary [ 11 ].
Regulatory frameworks like the EU’s Digital Services Act (DSA)
prohibit “dark patterns” (deceptive design) and require transparency in AI-
generated content. However, global coordination is lacking, and bad actors
often exploit legal loopholes [ 1 , 25 ] (Table 2 ).

**_Table 2_** Ethical and governance issues in emotion AI systems

```
Issue category Specific concerns Potential risks Mitigation strategies
```

```
Issue category Specific concerns Potential risks Mitigation strategies
Privacy Biometric surveillance in
public spaces, data
collection from wearables
(e.g., smartwatches), and
digital footprints on social
media platforms
```
```
Identity theft, emotional
profiling, and stalking
```
```
Implement anonymization
and data minimization;
obtain explicit user
consent; adopt differential
privacy and federated
learning techniques
Bias and
fairness
```
```
Demographic imbalance in
training data, intersectional
bias affecting marginalized
communities, and lack of
representation for low-
resource languages
```
```
Discriminatory
outcomes in hiring or
lending, medical
misdiagnosis, and
exclusion in
voice/emotion analysis
systems
```
```
Use diverse and
representative datasets;
conduct fairness audits;
apply contextual
calibration; adopt
participatory design
approaches
Manipulation Deepfakes and synthetic
media, emotion-triggered
advertising, and emotional
labor automation in
chatbots
```
```
Erosion of public trust,
psychological harm, and
exploitation of
vulnerable groups (e.g.,
children, minors)
```
```
Strengthen content
moderation; enforce
transparency mandates;
promote user awareness
and empowerment;
develop ethical design
guidelines
Accountability Black-box algorithmic
decisions and lack of
model explainability
```
```
Unfair or opaque
decisions (e.g., in
judicial or corporate
contexts), accountability
gaps, and public distrust
```
```
Incorporate explainable AI
(XAI); ensure human-in-
the-loop oversight;
conduct independent
audits and algorithmic
impact assessments
```
**4.4 Accountability and Human Oversight**

When emotion AI makes flawed decisions (e.g., a security system flagging
a “suspicious” person based on a misread facial expression), who is
responsible? The developer? The company deploying the system? The user?
Current laws (e.g., product liability statutes) were not designed for AI,
leaving gaps in accountability. For example:

A hospital using an emotion-recognition tool to assess pain may face
lawsuits if the tool fails to detect suffering in a non-verbal patient.
A social media platform using sentiment analysis to censor “negative”
posts could suppress legitimate criticism.
To address this, we recommend **explainable AI (XAI)** —systems that
justify their emotional classifications (e.g., “We flagged this post as ‘angry’
because it contains aggressive language and capital letters”). Human


oversight is also critical: no AI should make high-stakes decisions (e.g.,
parole recommendations) without human review.
Ultimately, fostering accountability necessitates a multi-stakeholder
framework: developers must explicitly record their systems’ constraints,
organizations should regularly assess for biases, and governing bodies need
to implement regulatory measures. Without these protections, emotion AI
will be able to undermine increasingly public trust [ 15 , 35 ].
This discussion highlights how emotion AI is a sociotechnical complex
system in nature and not merely a technical device. Ensuring a balance
between innovation and responsibility is crucial to its moral use, ensuring
that advances in AI support instead of degrading human dignity.
Consequently, the subsequent section discusses potential future directions
that these ethical principles will dictate as emotion AI continues to evolve.

##### 5 Real-World Applications and Limitations

The following section will examine the ways that emotion AI is being
applied in practice and demonstrate how it has the capability to transform
current procedure and systems. Equally important, however, is recognizing
that certain limitations remain. This section aims to bridge the theoretical
frameworks discussed earlier (Sects. 2 , 3 and 4 ) with the tangible realities
of practical application, thereby highlighting both the significant value and
inherent constraints of current emotion AI technology.

**5.1 Healthcare: Mental Health Screening and Therapy**

An AI neural network utilized within the healthcare sector to enhance
mental health outcomes is classified as emotion AI, with a particular focus
on conditions such as depression, anxiety, and PTSD. For example:

```
Screening: The data collected by wearable cameras (e.g., Fitbit) is
analyzed in terms of physiological indicators (heart rate variability, sleep
patterns) and vocal indicators (tone, pauses) to indicate the first signs of
distress. A 2023 study found that such systems detected 78% of
depression cases in clinical trials, enabling proactive intervention.
Therapy : Chatbots like Woebot use NLP to engage users in cognitive
behavioral therapy (CBT). By recognizing text-based cues of sadness (“I
```

```
can’t get out of bed”) or anger (“My boss is unfair”), Woebot tailors
responses to validate emotions and teach coping strategies [ 10 , 31 ].
```
**Why It Works** : Emotion AI processes data faster than humans, scaling
support to underserved populations (e.g., rural areas with limited
therapists).

**Limitations** : Cultural differences in emotional expression (e.g., stoicism in
East Asian communities) lead to false negatives. Additionally, over-reliance
on AI may reduce human empathy—a core component of therapy.

**5.2 Retail and Marketing: Personalized Experiences**

Retailers use emotion AI to optimize customer interactions and drive sales.
Examples include:

```
Customer Service : Chatbots (e.g., Sephora’s virtual assistant) analyze
text and vocal cues to gauge frustration. If a user says, “I’ve been waiting
an hour!” the bot escalates to a human agent, improving satisfaction.
Marketing : Brands like Netflix and Spotify use sentiment analysis to
recommend content. Netflix tracks viewing habits (e.g., binge-watching
sad dramas) and adjusts thumbnails to match mood (e.g., a tearful scene
for a “sad” user).
```
**Why It Works** : Emotion AI turns passive browsing into personalized
engagement, increasing conversion rates by 20–30%.

**Limitations** : Over-personalization can feel intrusive (e.g., a bot asking,
“Are you stressed about work?” mid-shopping trip). Moreover, bias in
training data (e.g., associating “aggressive” tones with male customers)
may alienate segments of the audience.

**5.3 Education: Adaptive Learning and Student Engagement**

Educators leverage emotion AI to create responsive learning environments:

```
Adaptive Tutoring : Platforms like Duolingo use NLP to detect
confusion (e.g., repeated incorrect answers) and adjust lesson difficulty.
If a student struggles with Spanish grammar, the system slows pace and
adds explanatory videos.
```

```
Classroom Monitoring : Some schools trial emotion-recognition cameras
to monitor student attention. A pilot program in Texas found that alerting
teachers to distracted students reduced off-task behavior by 40%.
```
**Why It Works** : Emotion AI addresses individual learning gaps, making
education more inclusive for neurodiverse students (e.g., autism spectrum
disorder).

**Limitations** : Privacy concerns dominate—parents and educators worry
about constant surveillance. Additionally, overemphasis on “engagement”
(e.g., rewarding compliance over curiosity) may stifle creativity.

**5.4 Entertainment: Gaming and Virtual Agents**

Emotion AI powers immersive experiences in gaming and virtual reality:

```
Non-Player Characters (NPCs) : Games like The Last of Us Part II use
advanced AI to make NPCs react realistically to player emotions. If a
character sees the player crying, they may offer comfort (“It’ll be okay—
we’ll figure this out together”).
Virtual Influencers : Avatars like Lil Miquela use synthesized speech
and facial expressions to engage millions of followers. Their “emotional”
responses (e.g., laughing at jokes, expressing gratitude) blur the line
between fiction and reality.
```
**Why It Works** : Emotion AI makes virtual worlds feel alive, increasing
player immersion and loyalty.

**Limitations** : The “uncanny valley” phenomenon—where near-human AI
appears unsettling—can disrupt immersion. For instance, a non-player
character (NPC) with stiff movements or a tone mismatch may pull users
out of the experience entirely.

**5.5 Key Limitations and Trade-Offs**

In our analysis, we argue that while emotion AI offers significant potential
benefits, its broader adoption is constrained by four critical challenges.
First, we examined data privacy concerns. The reason this is a challenge
is because the collection of sensitive biometric data and behavioral
information raises substantial ethical questions. For instance, we looked at a


2023 study which found that 68% of consumers are hesitant to share
emotion-related data, primarily due to privacy apprehensions.
Second, we considered bias and fairness problems, explaining that
emotion AI systems inadvertently reinforce injustice when training data
reflects societal prejudice. For instance, we discussed a hiring algorithm
that prioritizes aggressive speech tones, which might unintentionally
disadvantage capable female job applicants.
Third, we examined the issue of context ambiguity, by which we meant
that emotion AI is often at a loss to interpret situational nuances correctly. It
is possible, for example, that an AI robot will misinterpret mourners’
expressions of sorrow as evidence of deep depression, leading to
unnecessary or untoward responses.
Fourth, because artificially generated emotional responses often do not
hold the same depth and genuineness as real human feelings, we discovered
that they are not authentic. For instance, a virtual counselor’s sympathy
might sound unnatural or mechanical, which would erode confidence in
users.
A thoughtful synthesis between creativity and ethical responsibility is
needed to address these tensions. Emotion AI can promote more user
involvement and expand the reach of mental health assistance, but it
requires being created and applied attentively because, while it can simulate
empathy, it will never be able to replace the interpersonal connection which
is the core of true emotional empathy.

##### 6 Future Directions and Open Questions

Apart from technical advancements, the future direction of emotion AI will
be shaped by open practical problems and ongoing ethical dilemmas. Apart
from signaling the open questions that will continue to shape the
development of the field, this section highlights three large areas of
research: multimodal integration, the intersection between neuroscience and
AI, and ethical regulation.

**6.1 Advancements in Multimodal Emotion Understanding**

```
Future affective AI systems aim to combine numerous sources of data to
have a better and more precise knowledge of human emotions, as
```

```
opposed to present systems that primarily draw from individual, discrete
modalities (e.g., facial recognition only). For example:
Sensor Fusion: This approach minimizes ambiguity by integrating
physiological data (e.g., heart rate), speech prosody, and facial affect.
Fear may be detected by an AI system that distinguishes a furrowed
eyebrow (facial), a trembling voice (vocal), and a racing heart
(physiological) much more precisely than systems reliant on a single
modality.
Contextual AI: This can be made more accurate by combining emotion
recognition with the context of the situation, for example, between a
social gathering and a business meeting. Rather than understanding the
subtleties of suppressed emotion, present systems might mistake a neutral
face during a hard conversation for evidence of suppressed emotion.
Difficulties: Multimodal data is hard to process, and it needs novel
architectures (e.g., transformer-based fusion models) and enormous and
labeled dataset. Additionally, synchronizing asynchronous inputs (e.g., a
delayed vocal response) remains technically difficult.
```
**Why It Matters** : Multimodal AI could revolutionize fields like mental
health (detecting comorbid emotions) and human–computer interaction
(creating more natural virtual assistants).

**6.2 Neuro-AI Integration: Modeling Emotional States in**

**Neural Networks**

We are beginning to bridge the gap between human cognition and artificial
intelligence by integrating neuroscience with AI. Key initiatives include:

```
Brain-Computer Interfaces (BCIs) : Devices like EEG headsets or
fMRI scanners can decode emotional states directly from neural activity.
For example, a 2023 study used fMRI to predict “frustration” in gamers
with 85% accuracy, paving the way for adaptive game difficulty.
Neuro-Inspired Architectures : Mimicking the brain’s limbic system
(responsible for emotion) in AI models. For instance, spiking neural
networks—designed to replicate neuron firing—could enable AI to “feel”
simulated emotions more authentically.
```
**Challenges** : Brain data is noisy and highly individualized (e.g., one
person’s “happiness” neural signature differs from another’s). Scaling BCIs


for everyday use also requires miniaturization and cost reduction.

**Why It Matters** : Neuro-AI could eliminate guesswork in emotion
detection, making systems more reliable for high-stakes applications (e.g.,
diagnosing depression from brain waves). It may also advance our
understanding of human emotion itself.

**6.3 Ethical Frameworks for Responsible Emotion AI**

**Development**

The ethical pitfalls of current emotion AI (bias, privacy, manipulation)
demand proactive solutions. Future frameworks will need to address:

```
Global Standards : Harmonizing regulations across borders (e.g., the
EU’s AI Act vs. the U.S.’s FTC guidelines) to prevent regulatory
arbitrage. For example, banning emotion-based discrimination in hiring
globally.
Bias Mitigation : Mandating “bias audits” for all emotion AI systems,
with penalties for non-compliance. Tools like IBM’s AI Fairness 360
could become standard, ensuring models are tested for demographic
fairness before deployment.
Transparency : Requiring “emotion explainability”—systems must
justify their emotional classifications (e.g., “We flagged this text as
‘angry’ because it contained aggressive language and capitalization”).
```
**Challenges** : Balancing innovation with regulation. Overly strict rules could
stifle research, while weak frameworks allow harm to persist.

**Why It Matters:** Ethical governance, not a decision, is necessary to
maintain public trust. Without it, emotion artificial intelligence is likely to
repeat the sins of past technologies, such as the notorious racial biases in
facial recognition systems. To ensure that emotion AI serves society in an
equitable way and does not reinforce existing disparities, strong governance
structures—such as global law, third-party audits, and citizen-led design—
are necessary.

**Open Questions Shaping the Future**


The development of emotion Three controversial issues will influence AI,
forcing society to balance the conflict between fundamental human values
and technical potential:

1. **Can AI Ever “Understand” Emotion?** Perhaps the most intimate
    aspect of the human condition is feeling; what constitutes sorrow to one
    may hold a plethora of different meanings for another. Will artificial
    intelligence ever continue to break emotions down into quantifiable
    factors, or might it transcend mere statistical modelling to truly
    comprehend this subjective unpredictability?
2. **Who Owns Emotional Data?** Competing rights issues arise due to the
    fact that emotion AI creates very intimate data, like wearables to
    monitor everyday mood trends. Is the platform that captures and
    processes emotional data its owner, or do individuals retain ownership?
3. **What Is the Line Between Help and Harm?** Emotion AI can do good
    for mental health services, but there are also very real dangers,
    including the danger of dystopian surveillance and misleading
    marketing. When does society need to intervene to prevent potential
    harm, and when should it limit its good applications?

##### 7 Conclusion: Bridging Psychology and

##### Technology

This chapter has explored the intricate connection between human emotion
and artificial intelligence, from the psychological processes that support
emotional expression and perception to the computational approaches that
allow machines to detect and replicate these states. These discoveries
highlight an underlying paradox: while emotion AI can potentially
transform human well-being, proper use requires a systematic, morally
grounded approach marked by humility and rigor (Table 3 ).

**_Table 3_** Evaluation metrics for emotion AI systems

```
Metric type Description Application domain Example
tools/methods
```

```
Metric type Description Application domain Example
tools/methods
Objective
metrics
```
```
Quantifiable and data-driven
performance indicators that
measure the technical accuracy
or efficiency of a model
```
```
Text and speech
synthesis systems
```
```
Perplexity (for NLP
tasks); mean opinion
score (MOS) for audio
evaluation
Subjective
metrics
```
```
Human-rated assessments
focusing on perceived quality,
emotional appropriateness, or
naturalness of outputs
```
```
Applicable across all
output modalities (text,
speech, facial
expressions, etc.)
```
```
Likert-scale surveys;
pairwise comparison
tests
```
```
Contextual
metrics
```
```
Evaluate how well system
responses align with contextual
or situational norms and
expectations
```
```
Chatbot conversations;
virtual agent
interactions
```
```
Task success rate;
empathy quotient (EQ)
scores
```
```
Longitudinal
metrics
```
```
Assess consistency,
adaptability, and emotional
engagement across multiple
sessions or prolonged
interactions
```
```
Embodied
conversational agents;
social and companion
robots
```
```
User retention statistics;
willingness for repeated
interaction
```
**Synthesis of Key Themes**

This chapter begins by situating emotion AI in psychological context,
emphasizing the necessity of understanding fully human emotions, their
biological foundation and their cultural differences, so that we are not
misreading the signals AI is reading. Then, in Sect. 2 , we examined how AI
reads these signals and generates its own emotional responses. In Sect. 3 ,
we concluded that while robots may mimic expressions of emotion, they
cannot replicate the embodied human experience of emotion.
Ethical concerns have always taken center stage. Issues such as privacy,
bias, and manipulation are genuine hurdles to equitable use of emotion AI;
they are not hypothetical (Sect. 4 ). This leads us to a crucial realization:
emotion AI is a socio-technical system that demands balance between
responsibility and creativity. To make it responsible, solving the above
ethical dilemmas requires surmounting challenging real-world hurdles.
Emotion AI’s revolutionary potential and accompanying issues were
illustrated through real-world uses like individualized marketing and mental
health screening (Sect. 5 ). To further develop the field in the future,
concerted efforts will be required in three areas (Sect. 6 ): applying
knowledge from neuroscience, establishing robust ethical governance and


legislation, and combining different types of data through multimodal
approaches.

**The Interdisciplinary Imperative**

Emotion AI is not exclusively a technical endeavor; it is fundamentally a
sociotechnical system. Psychology elucidates the nature of emotions, while
artificial intelligence supplies the mechanisms to interact with them. But to
avoid replicating human biases or creating new harms, we must embed
psychological and ethical expertise into every stage of development. For
example:

A mental health bot designed without input from therapists may prioritize
speed over empathy.
A hiring algorithm trained on biased historical data will perpetuate
inequality.
These examples underscore a simple truth: emotion AI is only as
responsible as the humans who build it.

**A Call to Action**

The future of emotion AI is not predetermined. It will be shaped by the
choices we make today:

**For Developers** : Prioritize transparency (e.g., explaining why an AI
flagged “anger” in a text) and bias mitigation (e.g., diverse training data).
**For Policymakers** : Create agile, global regulations that balance
innovation with protection (e.g., banning emotion-based discrimination).
**For Society** : Engage in public discourse about the role of AI in emotional
life—what we value, what we fear, and what we will not tolerate.
Emotion is the essence of human connection. As we hand over parts of
this connection to machines, we must ensure that AI augments, rather than
replaces, our capacity for empathy. The path forward is not easy, but it is
necessary. By bridging psychology and technology with intentionality, we
can build a future where emotion AI serves humanity—not the other way
around.


##### References

1. Ahuja, K.: Emotion AI in healthcare: application, challenges, and future directions. In:
    Emotional AI and Human-AI Interactions in Social Networking (2023). https:// doi. org/ 10. 1016/
    B978-0-443-19096-4. 00011-0
2.
    al Maruf, A., Khanam, F., Haque, M.M., Jiyad, Z.M., Mridha, M.F., Aung, Z.: Challenges and
    opportunities of text-based emotion detection: a survey. IEEE Access 12 (2024). https:// doi. org/
    10. 1109/ ACCESS. 2024. 3356357
3.
    Baygin, M., Tuncer, I., Dogan, S., Barua, P.D., Tuncer, T., Cheong, K.H., Acharya, U.R.:
    Automated facial expression recognition using exemplar hybrid deep feature generation
    technique. Soft Comput. (2023) **27** , 13, 8721–8737 (2024). https:// doi. org/ 10. 1007/ s00500-023-
    08230-9
4.
    Ciraolo, D., Fazio, M., Calabrò, R.S., Villari, M., Celesti, A.: Facial expression recognition based
    on emotional artificial intelligence for tele-rehabilitation. Biomed. Signal Process. Control **92**
    (2024). https:// doi. org/ 10. 1016/ j. bspc. 2024. 106096
5.
    Costello, F.J., Lee, K.C.: Aristotle’s phronesis as a philosophical foundation in designing the
    algorithmic motivator-driven insulating model (AMOI). In: Lecture Notes in Computer Science
    (Including Subseries Lecture Notes in Artificial Intelligence and Lecture Notes in
    Bioinformatics), vol. 14059, LNCS. https:// doi. org/ 10. 1007/ 978-3-031-48057-7_ 21
6.
    Davila-Gonzalez, S., Martin, S.: Human digital twin in industry 5.0: a holistic approach to
    worker safety and well-being through advanced AI and emotional analytics. Sensors **24** (2)
    (2024). https:// doi. org/ 10. 3390/ s24020655
7.
    Dominguez-Catena, I., Paternain, D., Galar, M.: Metrics for dataset demographic bias: a case
    study on facial expression recognition. IEEE Trans. Pattern Anal. Mach. Intell. **46** (8) (2024).
    https:// doi. org/ 10. 1109/ TPAMI. 2024. 3361979
8.
    Dudeja, U., Dubey, S.K.: Decoding emotions: emotion classification from EEG brain signals
    using AI. In: 2023 10th IEEE Uttar Pradesh Section International Conference on Electrical,
    Electronics and Computer Engineering, UPCON 2023 (2023). https:// doi. org/ 10. 1109/
    UPCON59197. 2023. 10434423
9.
    Exploration of facial emotion detection systems utilizing convolutional neural networks: a
    comprehensive review. Comput. Sci., Eng. Technol. **2** (1) (2024). https:// doi. org/ 10. 46632/ cset/ 2/
    1/ 3
10.
    Ganesh, A., Ramachandiran, R.: An enhanced affective computing-based framework using
    machine learning and medical IoT for the efficient pre-emptive decision-making of mental health
    problems. J. Intell. Fuzzy Syst. (2023). https:// doi. org/ 10. 3233/ jifs-235503
    [Crossref]
11.
    Gould, D.J., Dowsey, M.M., Glanville-Hearst, M., Spelman, T., Bailey, J.A., Choong, P.F.M.,
    Bunzli, S.: Patients’ views on AI for risk prediction in shared decision-making for knee
    replacement surgery: qualitative interview study. J. Med. Internet Res. **25** (1) (2023). https:// doi.
    org/ 10. 2196/ 43632


12.
Gragnaniello, M., Borghese, A., Marrazzo, V.R., Breglio, G., Irace, A., Riccio, M.: A
microcontroller-based system for human-emotion recognition with edge-AI and infrared
thermography. In: Lecture Notes in Electrical Engineering, vol. 1110 LNEE (2024). https:// doi.
org/ 10. 1007/ 978-3-031-48121-5_ 46
13.
Gupta, A.: Facial emotion recognition using machine learning algorithms: methods and
techniques. In: Lecture Notes in Networks and Systems, vol. 796 (2024). https:// doi. org/ 10. 1007/
978-981-99-6906-7_ 7
14.
Gupta, D., Singhal, A., Sharma, S., Hasan, A., Raghuwanshi, S.: Humans’ emotional and mental
well-being under the influence of artificial intelligence. J. Reatt. Ther. Dev. Divers. **6** (6) (2023)
15.
Jiang, H., Zhang, X., Cao, X., Kabbara, J.: PersonaLLM: investigating the ability of GPT-3.5 to
express personality traits and gender differences. ArXiv (2023)
16.
Karadoğan, A.: A bridge between technology and creativity: story writing with artificial
intelligence. İnsan ve Sosyal Bilimler Dergisi **6** (2) (2023). https:// doi. org/ 10. 53048/ johass.
1368950
17.
Kerdvibulvech, C.: A digital human emotion modeling application using metaverse technology
in the post-COVID-19 era. In: Lecture Notes in Computer Science (Including Subseries Lecture
Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), vol. 14029, LNCS (2023).
https:// doi. org/ 10. 1007/ 978-3-031-35748-0_ 33
18.
Kumar, G., Das, T., Singh, K.: Early detection of depression through facial expression
recognition and electroencephalogram-based artificial intelligence-assisted graphical user
interface. Neural Comput. Appl. **36** (12) (2024). https:// doi. org/ 10. 1007/ s00521-024-09437-z
19.
Lozoya, D.C., D’Alfonso, S., Conway, M.: Identifying gender bias in generative models for
mental health synthetic data. In: Proceedings—2023 IEEE 11th International Conference on
Healthcare Informatics, ICHI 2023 (2023). https:// doi. org/ 10. 1109/ ICHI57859. 2023. 00109
20.
Marín-Morales, J., Llanes-Jurado, J., Minissi, M.E., Gómez-Zaragozá, L., Altozano, A., Alcaniz,
M.: Gaze and head movement patterns of depressive symptoms during conversations with
emotional virtual humans. In: 2023 11th International Conference on Affective Computing and
Intelligent Interaction, ACII 2023 (2023). https:// doi. org/ 10. 1109/ ACII59096. 2023. 10388134
21.
McGuffey, C.J., Singh, I., Yilmaz, A., Gulati, D.: Automated detection of facial droop: Ml/AI
based tool for early detection of stroke. Stroke **55** (Suppl_1) (2024). https:// doi. org/ 10. 1161/ str.

55. suppl_ 1. 65
22.
McInerney, K., Keyes, O.: The infopolitics of feeling: How race and disability are configured in
emotion recognition technology. New Media Soc **27** (7) (2025). https:// doi. org/ 10. 1177/
1461444824123591 4
23.
Nadeem, M.: Generative artificial intelligence [GAI]: enhancing future marketing strategies with
emotional intelligence [EI], and social skills? Br. J. Mark. Stud. **12** (1) (2024). https:// doi. org/ 10.
37745/ bjms. 2013/ vol12n1 115
24.
Obaigbena, A., Lottu, O.A., Ugwuanyi, E.D., Jacks, B.S., Sodiya, E.O., Daraojimba, O.D., Lottu,
O.A.: AI and human-robot interaction: a review of recent advances and challenges. GSC Adv.


Res. Rev. **18** (2) (2024). https:// doi. org/ 10. 30574/ gscarr. 2024. 18. 2. 0070
25.
de Oliveira, T.R., Rodrigues, B.B., da Silva, M.M., Spinassé, R.A.N., Ludke, G.G., Gaudio,
M.R.S., Gomes, G.I.R., Cotini, L.G., da Silva Vargens, D., Schimidt, M.Q., Andreão, R.V.,
Mestria, M.: Virtual reality solutions employing artificial intelligence methods: a systematic
literature review. ACM Comput. Surv. **55** (10) (2023). https:// doi. org/ 10. 1145/ 3565020
26.
Rajabhau, K.M., Yathin, G.S., Vardhan, T.H., Maheswari, R., Jagannathan, S.K.: AI-powered
virtual therapist: for enhanced human-machine interaction. In: Explainable Artificial Intelligence
(XAI): Concepts, Enabling Tools, Technologies and Applications (2023). https:// doi. org/ 10. 1049/
pbpc062e_ ch22
27.
Rao, T.P., Patnala, S., Raghavendran, C.V., Lydia, E.L., Lee, Y., Acharya, S., Hwang, J.Y.:
Oppositional brain storm optimization with deep learning based facial emotion recognition for
autonomous intelligent systems. IEEE Access **12** (2024). https:// doi. org/ 10. 1109/ ACCESS. 2024.
3374893
28.
Saisanthiya, D., Supraja, P.: Neuro-facial fusion for emotion AI: improved federated learning
GAN for collaborative multimodal emotion recognition. In: IEIE Trans. Smart Process. Comput.
**13** (1) (2024). https:// doi. org/ 10. 5573/ IEIESPC. 2024. 13. 1. 61
29.
Saranya, N., Priyanka, V., Harini, T., Akhila, B., Hemalatha, M.A., Kaneshka Sre, R.S.: Driver
state monitoring system using AI. In: 2023 International Conference on Computer
Communication and Informatics, ICCCI 2023 (2023). https:// doi. org/ 10. 1109/ ICCCI56745. 2023.
10128174
30.
Shen, Y.: Interaction mode enables user perception recognition and perception optimization: an
AI human-computer interaction study. Appl. Comput. Eng. **31** (1) (2024). https:// doi. org/ 10.
54254/ 2755-2721/ 31/ 20230122
31.
Singh, S.: Emotion recognition for mental health prediction using AI techniques: an overview.
Int. J. Adv. Res. Comput. Sci. **14** (03) (2023). https:// doi. org/ 10. 26483/ ijarcs. v14i3. 6975
32.
Singh, S., Srivastava, N.: Face emotion recognition system for depression detection using AI
techniques. Int. J. Res. Appl. Sci. Eng. Technol. **12** (1) (2024). https:// doi. org/ 10. 22214/ ijraset.

2024. 57890
33.
Sowmya, P., Devi, S.: Emotion AI. In: Lecture Notes in Networks and Systems, vol. 735 LNNS
(2023). https:// doi. org/ 10. 1007/ 978-3-031-37164-6_ 24
34.
Tian, X., Nunes, B.P., Liu, Y., Manrique, R.: Predicting student engagement using sequential
ensemble model. IEEE Trans. Learn. Technol. **17** (2024). https:// doi. org/ 10. 1109/ TLT. 2023.
3342860
35.
Turcian, D., Stoicu-Tivadar, V.l: Real-time detection of emotions based on facial expression for
mental health. Stud. Health Technol. Inform. **309** (2023). https:// doi. org/ 10. 3233/ SHTI230795
36.
Ülgen Sönmez, Y., VAROL, A.: In-depth investigation of speech emotion recognition studies
from past to present—the importance of emotion recognition from speech signal for AI. Intell.
Syst. Appl. **22** (2024). https:// doi. org/ 10. 1016/ j. iswa. 2024. 200351


37.
Vinay, N.V., Kumar, C.S., Asipalli, J., Mouli, M., Sindhu Madhuri, G.: Detection of AI empathy
using deep learning. In: 2023 International Conference on Computer Science and Emerging
Technologies, CSET 2023 (2023). https:// doi. org/ 10. 1109/ CSET58993. 2023. 10346939
38.
Xie, Z., Wang, Z.: Longitudinal examination of the relationship between virtual companionship
and social anxiety: emotional expression as a mediator and mindfulness as a moderator. Psychol.
Res. Behav. Manag. **17** (2024). https:// doi. org/ 10. 2147/ PRBM. S447487
39.
Yaremchenko, O., Pukach, P.: Research of structural and mechanical properties of meat as an
object of processing in meat comminutor. Herald of Khmelnytskyi National University. Tech.
Sci. **319** (2) (2023). https:// doi. org/ 10. 31891/ 2307-5732-2023-319-1-329-337
40.
Yoon, J.H., Yoon, D.H.: Development of IoT-based mobile carrier design and location tracking
device for companion animals: design for cleanliness, hygiene, and health care of humans and
pets. In: Forum of Public Safety and Culture, vol. 22 (2023). https:// doi. org/ 10. 52902/ kjsc. 2023.

22. 109
41.
Yu, D., Bao, L., Yin, B.: Emotional contagion in rodents: a comprehensive exploration of
mechanisms and multimodal perspectives. Behav. Process. **216** (2024). https:// doi. org/ 10. 1016/ j.
beproc. 2024. 105008
42.
Yu, Z., Wang, H., Un, K.: Automatic cinematography for body movement involved virtual
communication. IET Commun. **18** (5) (2024). https:// doi. org/ 10. 1049/ cmu2. 12748
43.
Yue, J.-M., Wang, Q., Liu, B., Zhou, L.: Postoperative accurate pain assessment of children and
artificial intelligence: a medical hypothesis and planned study. World J. Clin. Cases **12** (4) (2024).
https:// doi. org/ 10. 12998/ wjcc. v12. i4. 681
44.
Zhang, Z., Guo, X.: Design and optimization of Lin Chaoxian’s directorial movie
recommendation system based on plot analysis and emotion recognition. Int. J. Intell. Syst. Appl.
Eng. **12** (6s) (2024)
45.
Zhao, W., Sun, Y.: The exploration of emotional aspects of artificial intelligence (AI) in artistic
design. Int. J Interdiscip. Stud. Soc. Sci. **1** (1) (2024). https:// doi. org/ 10. 62309/ bk757m16
_OceanofPDF.com_


(1)

© The Author(s), under exclusive license to Springer Nature Switzerland AG 2026
K. Slimani et al. (eds.), _Emotion and Facial Recognition in Artificial Intelligence: Sustainable
Multidisciplinary Perspectives and Applications_ , Information Systems Engineering and Management
78
https://doi.org/10.1007/978-3-032-14778-3_2

## Theories of Emotions in Psychology

Bhavika Bhagyesh Lad^1 and Dipti Srivastava^1
Unitedworld School of Liberal Arts and Mass Communication
(USLM), Karnavati University, Gandhinagar, India

```
Bhavika Bhagyesh Lad
Email: bhavikahappy2help@gmail.com
```
**Abstract**
This chapter explores various theories within psychology to understand
emotions, focusing on both Western and Indian perspectives. Western
theories based on contributions by Darwin, James-Lange, Cannon-Bard,
Schachter-Singer, Ekman, etc. are discussed. These theories are classified as
Evolutionary, physiological, cognitive, constructivists, and socio-cultural in
their approach. Indian psychological frameworks for Emotions explore
Bharatamuni’s Rasa theory in Natyashastra, emotional management as
mentioned in the Yoga Sutras of Patanjali, Buddhist emotional
classifications, and Panch kosha models. A comparative analysis highlights
epistemic and ontological differences, contrasting the Western emphasis on
biology and cognition with Indian models’ spiritual and aesthetic
orientations. The chapter advocates for an integrative, culturally sensitive
and holistic approach to emotional theory that enriches global psychological
understanding and practice. This chapter also discusses application of
various emotion’s theory in therapeutic and forensic setting.

**Keywords** Emotion – Indian psychology – Western psychology – AI and
emotions


##### 1 Introduction

One time or another, we all may have experienced some level of emotional
arousal or emotional outburst followed after either a positive or negative
life event. Emotions act like fundamental part or building blocks of the
human experience. Emotions shape nature of our relationships, influence
decisions, and sense of self. Emotions give colour, meaning to our lives and
experiences. Yet, for generations psychologists have been intrigued about
science of emotion or from where do these emotions come or unfold and
hence study them. Emotions as a term is very widely used but formally
defining term in universally accepted manner seems to be elusive task.
Word emotion is derived from Latin word “emovere” which means “to
move, move out, or move through or to stir”. Many psychologists have
worked towards understanding or defining the term. One of the common
understandings of ‘Emotion’ refers it to “feelings that generally have both
physiological and cognitive elements and that influence behavior.” [ 35 ].
Emotions can be viewed as a complex reaction to some internal or external
stimuli and it involves physiological and behavioral reactions along with
facial expressions, thoughts, and affective responses. Emotion includes
feeling, thinking, activation of the nervous system, bodily changes, and
behavioral changes such as facial expressions, Thus, emotions can be
viewed as combination or mix of three things, (1) physiological arousal, (2)
expressive overt behavior, and (3) conscious thoughts or cognitions and
feelings accompanying cognitions. There are two different contradicting
views concerning interplay of these three components of emotions viz.,
does physiological arousal precede our emotional experience or cognitions
precede emotions. Theorists are debating over the view of whether
emotional response predominate cognitive processes or response or it is
vice versa. Even though continuing debate about sequence, there is general
agreement on emotions being made up of three parts: subjective
experiences, physiological responses and behavioral responses.

**1.1 Significance of Emotions in Human Experience and**

**Psychology**

Psychology is scientific study of Human Behavior. Studying emotions aids
in understanding human behavior, cognitions and social interaction. It helps
in get an insight into mental health, decision-making processes,


interpersonal communications, and the biological underpinnings of affective
states. Understanding human emotions or being emotionally intelligent is
important as it helps in attainment of psychological wellbeing, social
success, and helps in management of emotions as well [ 105 ]. Additionally,
a deeper understanding of emotions facilitates developing therapeutic
interventions and improving emotional regulation across populations.
The perception of the surroundings and how individuals interact with
the world around them is shaped by emotions. In the case of nature of social
relationships, motivation and even decision making, emotions play an
important role [ 35 ]. Emotion provides essential information about events
occurring in our environment and thus helps in preparing for adaptive
responses, and makes social responses in a way that is constructive for
social bonding [ 27 ]. In the view of [ 77 ], depth and meaning of human
experience greatly depends on emotions.

**1.2 Definition and Components of Emotions**

Emotions have numerous aspects related to them, they are many things at
once, hence there seem to exist many definitions of Emotions. Kleinginna
and Kleinginna [ 54 ] listed around 92 definitions of emotions and
summarised that, to define Emotions more appropriately below 5 pointers
need to be addressed:

(1) something about feeling when we are emotional;

(2) physiological or bodily aspects of emotional feelings;

(3) effects of emotions on perception, behavior, thoughts;

(4) properties of emotions to drive or motivate certain behavior;

(5) expression of emotions by the means of language, facial expression

# and/or gestures [ 77 ].^

American Psychological Association (APA) defines Emotion as “a
complex reaction pattern, involving experiential, behavioral, and
physiological elements, by which an individual attempts to deal with a
personally significant matter or event. The specific quality of the emotion


(e.g., fear, shame) is determined by the specific significance of the event.
Emotion typically involves feeling but differs from feeling in having an
overt or implicit engagement with the world” (“APA dictionary of
psychology,” 2019). Oxford English Dictionary (1996), defined Emotions
as “a strong mental or instinctive feeling such as love or fear involving
many bodily processes, and mental states”. Thus, Emotions can be
understood as psychological state involving below 4 essential components

(1) Physiological Arousal—bodily reactions like palpitation, increase in
heartbeat, flight or fight responses, hormonal changes facial
expressions etc. [ 54 ].

(2) Subjective feelings—subjective awareness, or conscious experience or

# ‘feeling’ that may be positive or negative [ 35 ].^

(3) Cognitive Processes—activation of specific mental processes or
information; a person’s perceptions, expectations and interpretations
[ 60 , 61 ].

(4) Behavioural reactions—overt behavioral reactions or actions (flight,
fight or freeze responses) or expressions, such as gestures, vocal tone,
or facial movements, that communicate emotions to others [ 27 ].

These components interact dynamically, contributing to the richness and
variability of emotional experiences [ 79 ].

**1.3 Overview of Universal Emotions and Cultural Influences**

Emotional reactions occur instinctively, suggesting we do not actively try to
experience them; rather, they emerge with ease. Of all the emotions that
humans undergo, there are seven universal emotions—happiness, sadness,
anger, fear, astonishment, and disgust—that are shared by all people and go
beyond the boundaries of language, culture, and ethnicity [ 29 ]. However,
cultural norms influence the expression and regulation of emotions through
display rules, which stipulate social and cultural contexts [ 69 , 121 ]. These
influences show how we differ in the expression of emotion and emphasize
the need to understand the sociocultural dimension of emotion [ 79 ].

**1.4 Aim and Scope of the Review**


The goal of this chapter is to critically analyze the development of
psychological theories of emotions from classical to modern integrative
frameworks, offering a thorough and up-to-date account. It examines some
of the major Western theories of emotions in conjunction with Indian
philosophical perspectives in order to construct a more holistic account of
emotions that synthesizes scientific understanding, culture, and lived
experience. The chapter also covers applications in forensic psychology and
newer AI trends in emotion research. It is hoped that this interdisciplinary
framework will enrich the understanding of emotions and help stimulate
more culturally informed and scientifically rigorous research.

##### 2 Historical Overview and Foundations of

##### Emotion Research

**2.1 Early Perspectives and Key Milestones in Emotion**

**Research**

Diverse theoretical and empirical researches in over more than a century
have led to evolution of the scientific study of emotions. Earlier researches
were focused on identifying biological and psychological foundations of
emotions and that laid the foundation for modern psychological inquiry.
The study of emotions initially combined observations from physiology,
evolutionary biology, and introspection, which marked a multidisciplinary
approach that has persisted even in current times [ 37 ]. Gendron and
Feldman Barrett [ 37 ] in their research paper titled “Reconstructing the Past:
A Century of Ideas About Emotion in Psychology” provided a
comprehensive historical account of the scientific study of emotion in
psychology. Paper discusses three important approaches towards
researching or theorising emotion in psychology namely, Basic Emotion,
Appraisal, and psychological constructionist approach [ 37 ].

```
Basic Emotion approach stated that Emotions are biologically hardwired,
universal in nature and produce stereotypical expressions and
physiological responses.
Appraisal Approach proposed that Emotions arise from an a person’s
interpretation or appraisal of events, emphasizing the role of meaning-
making.
```

Psychological Constructionist Approach states that Emotions are
constructed from more fundamental psychological components (like
affect and cognition) that are not specific to emotion itself. This approach
explains the variability in emotional experiences and expressions, which
the other two approaches address only superficially (Fig. 1 ).

**_Fig. 1_** Summarising key researchers from three distinct timelines in three approaches to study of
emotion. [ 37 ]
_Source_ Gendron and Feldman Barrett


**2.2 Darwin’s Evolutionary Theory of Emotion (Golden Years)**

Charles Darwin’s pivotal work, The Expression of the Emotions in Man and
Animals (1872), was a pioneering contribution that proposed emotions have
an evolutionary basis. Darwin argued that emotions evolved to serve
adaptive functions crucial for survival and reproduction, such as fear
triggering fight-or-flight responses [ 21 ]. He emphasized that emotional
expressions are universal and conserved across species, providing a
biological continuity between humans and animals [ 27 ]. Darwin’s theory
laid the groundwork for later research focusing on emotions as innate and
biologically hardwired phenomena.

**2.3 William James and Carl Lange’s Contributions**

At the advent of the twentieth century, William [ 48 ] and Carl Lange put
forth James-Lange theory of emotion. They proposed that emotions arise
from the perception of physiological changes in the body and not the other
way around [ 48 , 59 ]. It states that an external stimulus in environment
triggers a bodily reaction, and the brain interprets these physiological
responses leading to experience of a specific emotion. This theory shifted
the focus toward the body’s role or rather brain’s interpretation of
physiological reactions in emotional experience and it also influenced
consequent physiological models of emotion [ 56 ].

**2.4 Behaviorism and Its Impact on Emotion Research (the**

**“Dark Ages”)**

The subsequent “Dark Ages” (circa 1900–1960) saw emotion research
sidelined by behaviorism, though important neurobiological work persisted.
Owing to dominance of behaviorism, focus was more on observable
behaviors and not on internal mental states as objects of research. Study of
emotions was often dismisses as being subjective experience or
unmeasurable phenomenon [ 37 ].

**2.5 Renaissance of Emotion Research in the 1960s and**

**Onward**

The 1960s marked a renaissance in research of emotions which was driven
by cognitive psychology and neurobiology. Magda Arnold’s appraisal
model emphasized the role of cognitive interpretation of events being


central to emotional experience [ 1 ]. Svlvan Tomkins revived the basic
emotion view [ 120 ] However, this basic emotion–appraisal dichotomy had
overshadowed a third, psychological constructionist tradition, which viewed
emotions as constructed from more fundamental psychological components.
Recent scholarship highlights the need to recognize this neglected tradition
for a fuller, more accurate account of emotion research in psychology.
During this era of renaissance also contributed to emergence of
prominent theories of emotions like Schachter and Singer’s two-factor
theory (1962), and Lazarus’ cognitive appraisal theory [ 60 ], which
integrated both physiological and cognitive components. This resurgence
recognized emotions as a legitimate focus of scientific investigation. It also
encouraged multidisciplinary approaches to understand emotions using
psychology, neuroscience, and anthropology [ 37 ].

**2.6 Summarizing Major Research Eras**

Emotion research can be broadly categorized into three historical eras:

```
Golden Years (1855–1899): Characterized by foundational work from
Darwin and James, focusing on biological and physiological bases of
emotion [ 21 , 48 ].
Dark Ages (1900–1959): A period of decline in psychological emotion
research due to the rise of behaviorism, which marginalized internal
emotional states [ 125 ].
Renaissance (1960–Present): Renewed focus on cognitive and
neurobiological mechanisms of emotion, with integration of appraisal
theories and recognition of emotions as complex psychological
phenomena [ 37 ].
```
##### 3 Physiology and Neuroscience of Emotions

**3.1 Role of the Autonomic Nervous System (ANS)**

The Autonomic Nervous System (ANS) plays a pivotal role in the
physiological underpinnings of emotions by regulating involuntary bodily
functions in response to emotional stimuli. The ANS mediates bodily
changes such as heart rate, respiration, and hormonal secretion that prepare
the organism for adaptive reactions like fight, flight, or freeze [ 14 ]. These


autonomic responses are critical components of emotional arousal and form
the physiological basis upon which emotional experiences are built.
The ANS is divided into two branches—the sympathetic nervous system
(SNS) and the parasympathetic nervous system. The sympathetic branch
typically initiates arousal during emotionally charged situations, while the
parasympathetic branch facilitates calming and recovery processes [ 57 ].
During stressful or emotional arousing event, the sympathetic nervous
system activates the “fight-or-flight” response. This leads to increasing
heart rate, dilating pupils, and redirecting blood flow to muscles to prepare
the body for action. While, the parasympathetic nervous system supports
the “rest-and-digest” state, which promotes relaxation and recovery after
emotional arousal [ 113 ]. These physiological changes form the bodily
sensations are often associated with different emotions, for e.g., a racing
heart in fear or a calming breath in relief. The ANS also communicates with
the brain, mainly emotion-related areas like the amygdala and prefrontal
cortex. This allows for feedback which shapes emotional awareness and
regulation [ 113 ]. Dysregulation of the ANS has been linked to emotional
disorders like anxiety and depression. Thus, the ANS plays a crucial role,
and is not just reactive but integrally involved in shaping, experiencing, and
regulating emotions.

**3.2 Key Brain Areas and Neural Networks Involved in**

**Emotion**

To understand neurobiological underpinnings of emotions better, one need
to understand neural mechanisms too. This includes interconnected brain
structures and neurotransmitter system [ 113 ]. Emotional processing
contains a complex network of brain regions, and the amygdala is regarded
as central to detecting and assigning emotional significance to sensory
inputs. The amygdala, often known as _emotional brain_ , is central to
emotion theory. It assesses sensory information and gives emotional
qualities or meaning to those sensory inputs. Amygdala adjusts autonomic
and endocrine functions, along with decision-making, and regulating
instinctive behaviors. The amygdala integrates sensory information and
orchestrates responses to threats or rewards, influencing both behavioral
and physiological reactions [ 62 , 92 ]. The amygdala becomes more active
with negative emotions like anger or fear, and less active with positive


emotions like love. Any dysfunction or damage to amygdala can lead to
problems in controlling emotions, specially fear and aggression [ 113 ].
The thalamus functions as a relay centre, transmitting sensory
information to cortical and subcortical areas and coordinating rapid
emotional responses [ 14 ]. The hypothalamus regulates physiological
expressions of emotion such as hormonal release, autonomic activation, and
homeostatic balance [ 106 ]. Higher cortical areas i.e. regions like the
ventromedial and anterior cingulate cortices in the prefrontal cortex are
involved in conscious emotional experience, decision making and cognitive
appraisal of stimuli [ 83 , 92 ]. According to current research emotions
originate from dynamic, large-scale networks rather than being restricted to
a particular area of the brain. These networks integrate perception,
cognition, and bodily states. It thus reflects that both innate and learned
components of emotional life [ 92 , 113 ]. Integrative systems that connect
perception, thought, and physical states are formed by large-scale networks
such the limbic system and prefrontal cortical areas [ 92 ]. The amygdala and
anterior insula are part of the salience network, which recognises
emotionally charged stimuli and promotes quick reactions [ 112 ]. According
to Buckner et al. [ 12 ], the executive control and default mode networks
support the cognitive evaluation and control of emotions. According to
Barrett, these overlapping networks allow emotions to be adaptable,
context-sensitive, and impacted by past experiences and education.

**3.3 Interaction Between Physiological and Cognitive**

**Components**

Emotional experiences arise from both—the interplay between bottom-up
physiological signals and top-down cognitive processes. Subjective
emotional feelings get shaped by the brain’s cognitive interpretation of raw
affective data that is provided by physiological arousal that ANS and
subcortical brain generates [ 60 , 61 , 108 ]. This interaction between various
bodily functions allows emotions to be both reactive and reflective. This is
achieved by integrating sensory input along with memory, expectations, and
cultural context. For example, two individuals may show similar
physiological arousal but at the same time experience different emotions.
This individual difference comes from how one appraises and labels the
situation cognitively [ 108 ]. This Understanding of bidirectional influence
explains the variability and complexity of human emotional life.


##### 4 Major Psychological Theories of Emotions

**4.1 Evolutionary Theory**

The evolutionary approach to emotions was pioneered by Charles Darwin
and later William McDougall expanded same. Emotions are believed to be
adaptive mechanisms that are shaped by natural selection and enhance
survival and reproductive chances. Darwin asserted that emotions serve
functional purposes i.e. signalling danger, or facilitating social bonding, and
motivating appropriate behavior to meet environmental challenges.
McDougall [ 72 ] further emphasized the instinctual basis of emotions.
Evolutionary approach states emotions are universal and are biological
hardwired. This evolutionary continuity is observed across species [ 21 , 72 ].

**4.2 Physiological Theories**

(a) **James-Lange Theory**

William James and Carl Georg Lange, proposed similar ideas
independently, and now are recognised as James-Lange theory of Emotions.
This theory proposes that emotions are caused by our interpretation of
bodily reactions to events. In other words, physiological arousal or action
precedes emotions we feel. How one interprets the bodily reactions to
stimuli will determine the type of emotion we would feel [ 48 , 59 ]. This
theory highlights the centrality of bodily feedback in emotional experience,
suggesting a causal sequence from physiological arousal to emotional
feeling [ 56 ] (Fig. 2 ).

**_Fig. 2_** James-Lange theory of emotion.
_Source_ [ 56 , p. 262]

Critics for this view point suggest that theory was based on
introspection and correlational research, rather than controlled experiments.
Hence, cannot be verified or generalised. Also, it put too much emphasis on


bodily signals, when evidence suggest emotions can exist before
physiological arousal too [ 56 ].

(b) **Cannon-Bard Theory**

In criticism to James-Lange theory, Walter Cannon and Philip Bard,
proposed second major theory named Canon-Bard theory. This theory views
that emotions and physiological reactions occur simultaneously and
independently. It also suggested that Emotions can be experienced even in
absence of bodily reactions or physiological arousal. Theory indicates that
when a stimuli or event is presented to us, our nervous system coordinates
prompt and simultaneous response of body’s instinctive reaction and mind’s
emotional perception or interpretation. It states our thalamus plays a crucial
role in the same, hence many a times this theory is viewed as thalamic
theory of emotions [ 2 , 14 ]. Cannon-Bard theory contests the idea that
bodily changes alone determine emotion. It emphasizes parallel processing
in the brain [ 79 ] (Fig. 3 ).

**_Fig. 3_** Canon-Bard theory of emotion.
_Source_ Summarised by author (2025)

**4.3 Cognitive Theories**

(a) **Schachter-Singer Two-Factor Theory**

Theory of Emotion proposed by Schachter-Singer is a two-factor theory
which proposes a specific emotional state is produced by the combination
of physiological arousal along with cognitive interpretation of the same or


‘labelling’ of the stimulus or event. Two factors on which emotion
elicitation depends on are—(1) a state of general autonomic arousal, and (2)
second cognitive interpretation or ‘label’ that describes the experience. This
theory accounts for individual difference in emotional experience as it is
based on contextual interpretation of environmental stimuli [ 81 , 108 ]
(Fig. 4 ).

**_Fig. 4_** Schachter-Singer two-factor theory.
_Source_ [ 81 ]

(b) **Lazarus’ Cognitive Appraisal Theory**

The cognitive appraisal theory by Lazarus [ 60 ] suggest that our brain first
appraises a situation, stimulus, or event and the resulting response from that
appraisal is an emotion. There are two levels of appraisals—primary and
secondary. Primary appraisal refers to interpretation of the stimulus or event
which can be positive or negative and secondary appraisal refers to analysis
of resources and our response or reaction. Every emotion has a ‘core
relational theme’ of individual relating to the situation. This core relational
theme refers to underlying meaning person assigns to stimulus which
triggers a specific emotion. The relationship between the person and
environment act as important contributing factors in emotions and this core
relational theme [ 37 , 60 , 61 ] (Fig. 5 ).

**_Fig. 5_** Lazarus cognitive appraisal theory.
_Source_ Summarised by author (2025)

**4.4 Constructivist Theory**


Lisa Feldman Barrett’s theory of constructed emotion (2017) challenges
traditional views of emotions as innate, discrete entities. Alternatively,
Barrett argues that emotions are dynamically constructed by the brain
through a process that integrates three components viz., internal bodily
sensations-signals, prior experiences, and cultural concepts. Emotions are
dependent on context. They are shaped by learning and individual
differences, rather than being fixed biologically hardwired. This theory
highlights predictive and interpretative role the brain plays in forming
emotional experiences. It also accounts for the variability observed across
cultures and individuals.
Theory proposes that emotions are constructed by brain using
predictions based on past experiences. Brain keeps continuously creating
internal models to predict and interpret sensory inputs and bodily states, and
not react to stimulus or world around, which helps adjusting behaviour to
maintain _allostasis._ Allostasis refers to the proactive regulation of bodily
systems to ensure survival and balance. Thus, emotions like fear or anger
are not tied to specific brain area but are mental categories individual
creates from cultural, social, and personal experiences. Emotional episodes
occur when the brain interprets internal and external stimuli as fitting a
specific emotional concept. For e.g., people with amygdala damage can also
experience or recognize fear under certain conditions. This shows that
emotions are not dependent on single brain structures [ 113 ].
Emotions, thus, originate from the brain’s _predictive coding process_
which matches predictions to sensory input and adjusting as needed.
Emotions are shaped over time and vary across cultures. In infants, bodily
states exist before emotions; only when the brain matures enough to make
detailed predictions do emotional experiences arise. This theory challenges
traditional views by emphasizing the brain’s role in constructing emotions
from top-down processes [ 113 ]. Let us understand same taking an example.
Say there is a psychology research scholar aged 26 attending a international
conference and presenting a research paper for the first time. She notices
her heart racing, palms sweaty, and butterflies in stomach. A classical view
of emotion would suggest that these physiological responses would be
interpreted as feeling emotion i.e. fear which is an automatic, hardwired
response to stressful situation of public speaking. From Barrett’s theory
perspective, it would be understood as her brain receiving signals from her
body and comparing same with her past experiences. Based on context,


social norms, her cultural understanding, her brain might construct the
emotion of anxious anticipation and not fear.

**4.5 Social and Cultural Theories**

(a) **Paul Ekman’s Theory of Universal Emotions and Cultural Display**

# Rules^

Paul Ekman and Wallace Friesen, on the basis of their studies concluded
that six types of basic emotions exist universally namely, surprise,
happiness, anger, fear, disgust, and sadness [ 56 , 79 ]. Each of these universal
emotions has unique signals, physiological responses, and timelines.
Although the onset, duration, and decline may differ. There exists some
level of similarities as well as difference in how emotions are experienced
and expressed across various cultures and across gender. ‘Display rule’ that
are part of each culture determines how emotions are expressed
appropriately and these in turn set gender expectations of emotional
experience and expression. Display rules refer to customs or social norms
which are used to regulate emotional expression in a given culture [ 79 ].

(b) **Cultural-Historical Activity Theory (CHAT) by Vygotsky and**

# Leontiev^

L. S. Vygotsky and Aleksei N. Leontiev founded Cultural-historical activity
theory (CHAT). It is a theoretical framework to conceptualise and analyze
relationship between cognition and action. It views human activity as a
dynamic, socially and culturally influenced process. CHAT views emotions
as a phenomenon embedded in human activities and mediated by social and
cultural factors. Since emotions develop via culturally meaningful activities
and internalization of social norms, one cannot separate emotions from
cultural practises and social interactions [ 63 , 124 ]. CHAT highlights how
emotions are dynamic and developing, and how language and culture shape
emotional experiences and regulation [ 33 ].

## 5 Integrative Comparative Summary of Theories

**5.1 Comparative Overview of Major Emotion Theories**


See Table 1.

**_Table 1_** Comparison and summarization of key aspects of each psychological theory

```
Theory Origin of
emotion
```
```
Brain
mechanisms
```
```
Role of culture Expression of
emotion
Evolutionary
theory
```
```
Innate,
biologically
hardwired
```
```
Limbic system
(amygdala,
hypothalamus)
```
```
Minimal; emotions
are universal
```
```
Universal facial
expressions [ 21 ,
27 ]
James-Lange
theory
```
```
Bodily
physiological
changes
```
```
Peripheral nervous
system signals
```
```
Minimal Body-driven,
physiological cues
```
```
Cannon-Bard
theory
```
```
Simultaneous
brain and body
responses
```
```
Thalamus
coordinating
parallel responses
```
```
Minimal Simultaneous
physiological and
subjective
Schachter-Singer
two-factor
```
```
Interaction of
arousal and
cognition
```
```
ANS arousal plus
cortical appraisal
```
```
Moderate; labeling
influenced by
context
```
```
Expression
depends on
cognitive labeling
[ 108 ]
Cognitive
appraisal theory
```
```
Cognitive
evaluation of
stimuli
```
```
Prefrontal cortex
and limbic system
```
```
High; appraisal
shaped by
environment and
culture
```
```
Expression shaped
by appraisal
outcomes [ 60 , 61 ]
```
```
Constructivist
theory
```
```
Constructed
through
experience and
culture
```
```
Distributed brain
networks;
predictive
processing
```
```
Central; culture
and experience
shape emotion
```
```
Highly variable,
context-dependent
```
```
Social-cultural
theories (Ekman,
CHAT)
```
```
Socially
mediated and
culturally shaped
```
```
Neural circuits
modulated by
social context
```
```
Fundamental;
culture shapes
emotional norms
and display rules
```
```
Expression
governed by social
norms and learned
rules [ 27 , 124 ]
```
_Source_ Summarised and compiled by authors (2025)

**5.2 Strengths and Limitations of Each Theory**

```
Evolutionary Theory
```
Strengths: Explains universality and adaptive functions of emotions;
supported by cross-species evidence.


Limitations: May underplay cultural variability and cognitive complexity
[ 27 , 37 ].

```
James-Lange Theory
```
Strengths: Emphasizes bodily feedback and physiological correlates.

Limitations: Difficult to explain emotions without distinct physiological
patterns; lacks evidence for causal order [ 56 ].

```
Cannon-Bard Theory
```
Strengths: Accounts for simultaneous emotional and physiological
responses; supported by neuroanatomical data.

Limitations: Does not explain cognitive appraisal or variability of emotions
[ 79 ].

```
Schachter-Singer Two-Factor Theory
```
Strengths: Integrates cognition and physiology; explains context-dependent
emotions.

Limitations: Ambiguity in defining cognitive labeling processes; some
replication issues in experiments [ 108 ].

```
Cognitive Appraisal Theory
```
Strengths: Highlights individual differences and role of interpretation;
useful for stress and coping research.

Limitations: Complexity in measuring appraisal processes; sometimes
criticized for overemphasizing cognition [ 60 , 61 ].

```
Constructivist Theory
```
Strengths: Accounts for cultural, contextual, and individual variability;
integrates neuroscience and psychology.

Limitations: Abstract nature can challenge operationalization; ongoing
debate on universality versus construction.


```
Social and Cultural Theories
```
Strengths: Emphasizes the social context and cultural norms; explains
variation in emotional expression.

Limitations: May underestimate biological bases; complex to empirically
validate social influences [ 27 , 124 ].

**5.3 Contemporary Syntheses and Debates in Emotion Theory**

In recent time, emotion researchers attempt to merge and integrate diverse
theoretical frameworks. They agree and recognise that emotions are
multifaceted and complex phenomenon incorporating all three biological,
cognitive and sociocultural components. Barrett proposed that emotions
emerge from predictive brain processes that are shaped by bodily states and
cultural learning. This approach to studying emotions incorporates both
physiological and constructivist approach to understanding of emotions.
Likewise contemporary models of emotions highlight dynamic and
distributed processing of emotion, and advocate that a single isolated region
is not involved in emotions, but a large-scale brain network is involved
[ 92 ].
Disagreements still exist on whether emotions are universal or culture
specific. Research is ongoing to find a balance between naturally occurring
emotional reactions (innate tendencies) and culturally shaped emotions
[ 37 ]. The role of cognition—whether emotions come before or are created
by cognitive appraisal—is another point of debate. This debate repeats
earlier disagreements between the James-Lange and Cannon-Bard
viewpoints, but this debate is now enhanced by contributions from
contemporary neuroimaging and cross-cultural data [ 60 , 61 ].
Developments in affective neuroscience and computer modelling offer
new tools to understand complexity of emotions. This has pushed
interdisciplinary approach which integrate both psychological theories,
sociocultural insights and empirical brain data [ 92 ].

##### 6 Indian Perspectives on Emotions

As far as ‘Emotions’ are concerned, in Indian perspective, there is more of a
philosophical approach to it as compared to western theories.


**6.1 Rasa Theory from Natyashastra: Aesthetic and**

**Experiential Emotions**

Emotions are viewed in the aesthetic sense as discussed in Bharat Muni’s
ancient Sanskrit treatise (circa 200 BCE–200 CE) ‘Natyashastra’ and
viewed it as emotional states or ‘Rasas’. Rasa, here is understood in terms
of ‘essence’ or ‘flavour’ i.e. emotional response resulting from artistic
performances like dance, drama or music. Western models of emotions
often categorise emotions as discrete, biologically driven affective states,
whereas Rasa theory views emotions as something dynamic and result of
interplay between the performer, the artistic medium, and the spectator. The
theory identifies nine primary rasas: “love (śṛṅgāra), laughter (hāsya),
compassion (karuṇā), anger (raudra), valour (vīra), fear (bhayānaka),
disgust (bībhatsa), wonder (adbhuta), and tranquillity (śānta)” [ 16 ]. Theory
posits that each Rasa originates from a ‘sthayi bhav’ i.e. a stable emotion.
This rasa experience is heightened by ‘vibhavas’-stimuli or determinant,
‘anubhavas’-expressions or consequents, and ‘vyabhicharibhavas’-
temporary emotions or transitory states [ 9 , 16 ]. Recent neuroscientific
research conducted by Pandey et al. [ 88 ] has found that distinct neural
signatures exist for each rasa. This finding demonstrates that particular
brain oscillations—especially in the delta, beta, and gamma frequency
bands—discriminate between the emotional states elicited by different rasas
[ 88 ]. Rasa theory integrates psychological and social mechanisms through
which personalised emotions are universalised and focus on empathy,
aesthetic experiences and specific role of audiences. Rasa theory offers rich,
diverse, cross-cultural insights into the nature of emotions and it also
emphasises the transformative power which art provides to harmonize and
elevate emotional experiences. Rasas give spiritual and psychological
understanding and are experienced together. Rasa theory promotes empathy
and a sense of interconnection by emphasizing the transforming and shared
influence of emotions in art. In contrast to Western paradigms, Rasa theory
put forth a unique viewpoint on psychology by combining the cognitive,
expressive, and cultural dimensions of emotion. Western theories
concentrate on distinct emotions, whereas Rasa theory advocates universal
experience of emotions which promotes emotional transcendence and
aesthetic satisfaction.


**6.2 Pancha Kosha Theory: Layers of Human Existence and**

**Emotions**

The origins of Pancha Kosha theory, can be found in the _Taittiriya
Upanishad_ within Indian Knowledge systems of Vedas. By the means of
five interwoven sheaths or ‘Koshas’, Panch kosha theory provides a
comprehensive framework to comprehend human personality and emotions.
Koshas are covering of the atman or layers of human existence. From the
material physical body to the delicate spiritual core, each of these kosha’s
functions as an inner layer of existence. It highlights how the body, mind,
and soul are all interconnected. “Annamaya Kosha, Pranayama Kosha,
Manomaya Kosha, Vigyanamaya Kosha, and Anandamaya Kosha” are the
five sheaths, or Koshas. The physical or tangible body is referred to as
Annamaya Kosha; the vital life force or breath is called Pranayama Kosha;
the mental, psychological, or emotional state is called Manomaya Kosha;
the intellectual or wisdom component is called Vigyanamaya Kosha; and
the state of endless liberation or pure happiness is called Anandamaya
Kosha. Contrary to Western trait-based models, in this theory, personality is
viewed as dynamic, and capable of transformation through practices like
yoga and meditation, aiming for self-realization and eternal bliss [ 87 , 107 ].
Emotions are seen to be related to Manomaya Kosha or the mental sheath.
Manomaya Kosha includes feelings, cognitions, desires. Feeling which are
Ego-driven like pride, jealousy, etc. can upset the equilibrium in this sheath
and can result in physical disease i.e. _Vyadhi_ and tension i.e. _Adhi_.
Emotional control, balance or harmony can the achieved by cleaning
Manomaya kosha with yogic techniques like meditation, breathing
exercises and ‘ _Satvavajaya Chikitsa_ ’ which is Ayurvedic psychotherapy.
Satvavajaya Chikitsa is one of the _Trividha Chikitsa_ or threefold treatment
frameworks given by Charaka in Ayurveda. Satvavajaya Chikitsa mainly
focuses on treating mental illness or psychiatric disorders, especially
psychosomatic disorders where mind is affected. Satvavajaya Chikitsa is
correlated to the modern medical treatments like psychotherapy and
Cognitive behavioural therapy (CBT). Aim here is to re-establish normal
mental activities by creating balance in mind using religious or regular
practise of Yama, Niyama, and Pranayama. Satvavajaya Chikitsa is non-
pharmacological approach to manage mental disorders and balance rajas,
and tamas doshas of mind [ 100 ].


While the Anandamaya Kosha symbolises a state of perfect happiness
that transcends transient emotions, the Vigyanamaya Kosha, which governs
intellect and discernment, further refines emotional responses by fostering
wisdom [ 87 ]. This theory suggests that emotions are not just confined to the
brain. They span physical sensations (as in Annamaya kosha), energetic
shifts i.e. pranayama kosha, mental interpretation which is manomaya
kosha, wise evaluation or intellectual description as in Vigyanamaya kosha
and emotional well-being at deeper level which is Anandamaya kosha.
Theory postulates that effective emotional regulation needs working
through all the five layers or koshas from controlling breathing to cognitive
restructuring or reframing to cultivating state of bliss. Emotional
imbalances or dysregulation is viewed as disruptions across these koshas,
suggesting that emotional regulation can be achieved by integrating
approaches that involve body, mind, and spirit (Fig. 6 and Table 2 ).


**_Fig. 6_** Representation of Panch koshas-five elements of human existence.
_Source_ [ 51 ]

**_Table 2_** Summarising relationship between kosha theory and western theory of emotions

```
Kosha Layer
description
```
```
Role in
emotions
```
```
Connection
to western
emotional
theory
```
```
Example Regulation
strategies
```
```
Annamaya kosha
(physical body)
```
```
Gross
physical
body made
of food
```
```
Physiological
expression of
emotions
(heart rate,
muscle
tension,
sweating)
```
```
James-Lange
theory—
emotions
begin with
bodily
changes
```
```
Sweaty palms
and fast heartbeat
before an exam
```
```
Physical
exercise,
progressive
muscle
relaxation,
grounding
techniques
```

```
Kosha Layer
description
```
```
Role in
emotions
```
```
Connection
to western
emotional
theory
```
```
Example Regulation
strategies
```
```
Pranamaya kosha
(energy/breath
body)
```
```
Breath and
life-force
energy
regulating
movement
```
```
Breath rate
and energy
flow shift
with
emotions
```
```
Autonomic
nervous
system
response
affects
emotions
```
```
Short, shallow
breaths during
fear
```
```
Deep
breathing,
pranayama,
paced
breathing
```
```
Manomaya kosha
(mental/emotional
body)
```
```
Mind,
sensory
processing,
and basic
emotional
patterns
```
```
Cognitive
interpretation
shapes
emotional
experience
```
```
Schachter-
Singer theory
—arousal +
interpretation
= emotion
```
```
Hearing loud
noise, thinking
it’s danger → fear
```
```
Cognitive
reframing,
mindfulness,
journaling
```
```
Vijnanamaya
kosha (wisdom
body)
```
```
Higher
intellect,
discernment,
ego
regulation
```
```
Evaluates and
modulates
emotional
responses
```
```
Constructed
emotion
theory—prior
concepts
shape
emotions
```
```
Realizing anger
is due to
misunderstanding
```
```
Self-
reflection,
meditation,
therapy
```
```
Anandamaya
kosha (bliss body)
```
```
Innermost
layer of joy
and peace
```
```
Stable joy
beyond
reactive
emotions
```
```
Broaden-and-
Build Theory
—positive
emotions
expand
resilience
```
```
Feeling deep
contentment
during meditation
```
```
Gratitude
practice,
compassion
meditation,
service
```
_Source_ Summarised and compiled by authors (2025)

**6.3 Indian Yogic Perspective on Emotions**

In Indian school of thought, relationship between Yoga and Emotions can
be seen in ancient philosophical texts like Yoga sutras of Patanjali and the
Bhagvad Gita. Emotions are not viewed as independent psychological
phenomena, but rather viewed as modifications of desire and attachment.
Emotions arise from connection of ahamkara or ego with external world.
The main cause of suffering or duhkha is considered owing to ignorance of
the true self-atman. Controlling one’s emotions is a vital first step on the
path to self-realization [ 101 ]. The trigunas-sattva, rajas, and tamas which
underlie emotional experiences determine its nature and intensity.
Undesirable emotion can be transformed using breathwork and meditative


techniques leading to mental well-being and spiritual growth. Yoga sutras
emphasize mindfulness, breathing exercises, meditation, and self-regulation
techniques for emotional well-being, combining ancient wisdom with
modern psychological insights to understand and master emotions [ 101 ].

**6.4 Emotions as Viewed by Nyaya-Vaisesika, Vedanta,**

**Samkhya, and Ayurveda**

Classical Indian philosophical perspective does not have direct equivalent
of western concept of “emotion”. They analyse phenomenon like rāga
(attraction), dveṣa (aversion), śoka (sorrow), and bhaya (fear) through the
lens of epistemology, metaphysics, and soteriology. They view emotion’s
role in spiritual liberation and its interplay with cognition. They offer
diverse yet complementary insights into emotions:

```
Nyaya-Vaisesika school views emotions as mental states linked to desires
and aversions, analyzed through logic and epistemology [ 99 ]. From the
classical viewpoint, emotions like attraction and aversion are viewed as
mental defects or dosas that arise from ignorance or mithyagyan. This
leads to attachment, misperception, obstructing path to right knowledge
and thus hindering liberation. Pleasure and pain both are seen as
obstacles to attaining nirvana or moksha. Contemporary academicians
reaffirm this classical Nyaya-Vaisesika viewpoint and emphasize that
pramana or right knowledge transcends emotional disturbances [ 89 ].
Vedanta views mind as the seat of Emotions. Emotions are qualities of
manas or mind and not of true self or atman. Detachment and knowledge
can end emotional suffering and help individual to be in true state of self
or ultimate reality i.e. Brahman [ 15 ]. Attributes of mental states like
memory, imagination, fear, love, aversion etc. are seen as emotions
belonging to manas or mind but not of atman i.e. eternal self. To achieve
liberation, one must discriminate the self or atman from these transient
mental functions or emotions, paving way towards Brahman [ 89 ].
Samkhya philosophy views reality or world composed of two
independent principles viz., Purusa and Prakriti. Purusa is consciousness
or spirit, and prakriti is nature or matter. Emotions and mental states arise
from this prakriti [ 89 ]. Samkhya philosophy categorizes emotions as
resulting from interplay of the trigunas i.e. sattva (purity, harmony,
clarity), rajas (activity, passion, restlessness), and tamas (inertia,
```

ignorance, dullness). These gunas affect emotional quality and mental
states [ 102 ]. It is believed that higher sattva dominance will foster
calmness, while dominance of rajas and tamas may stir agitation, or
anxious overthinking or lethargy. Cultivating sattva via meditation will
help bring emotional balance or serenity.
Ayurveda integrates emotions into the framework of _doshas_ (Vata, Pitta,
Kapha) linking emotional disturbances to physiological imbalances and
advocating lifestyle and herbal interventions to restore harmony [ 58 ].
Imbalances in _doshas_ when interact with _trigunas_ (Sattva, rajas, tamas)
affect emotional and physical health. Steer [ 117 ] in study found that vata
imbalance leads to more anxiety, rumination, less mindfulness, similarly,
pitta imbalance can cause poor mood regulation, and stress; kapha
imbalance also causes more stress along with rumination and less
reflection [ 117 ].
These schools of thought together highlight a holistic and integrative
understanding of emotions incorporating body, mind, and spirit. They offer
therapeutic and philosophical pathways for emotional health (Table 3 ).

**_Table 3_** Summarising understanding of emotions in Indian Philosophical School

```
School Classical view Modern insight Example
Nyaya-
Vaisesika
```
```
Emotions are defects that can
impede cognition
```
```
Emotional states cloud
perceptions; liberation can be
achieved by eliminating them
```
```
Attachment to pleasure
distorts truth
```
```
Vedanta Emotions belong to mind
( Manas ), not the self ( Atman )
```
```
True self transcends
emotions
```
```
“I am not my anger”-
leading to detachment
Samkhya Emotions arise from interplay
of trigunas-Sattva, Rajas,
Tamas
```
```
Sattva fosters balance,
equanimity, rajas and tamas
disturb peace
```
```
Overactive mind
(rajas) can be soothed
via Sattva practices
Ayurveda Emotional disturbances arise
owing to dosha imbalances
and dosha-guna interaction
```
```
Dosha-guna profiles correlate
with anxiety, depression, and
mood
```
```
Vata-rajas leads to
anxiety, kapha-tamas
lead to depression
```
_Source_ Summarised and compiled by authors (2025)

**6.5 Contrasts and Complementarities with Western Models**

Emotions are viewed as discrete biological or cognitive events through
Western psychological lenses, but Indian perspectives views emotion in a
deeper integrative and experiential framework. Indian models combine


spiritual, cultural factors, and mind–body unity. Here emotions are viewed
as fluid, dynamic, in the frame of context, and interconnected to broader
existential concerns. Both traditions do recognize the importance of
cognitive appraisal and bodily states in emotional experience.
The Indian viewpoint on emotional regulation by meditation and self-
awareness is similar to Western approaches in clinical psychology which
focus on mindfulness and cognitive-behavioral therapies [ 49 ]. However,
Rasa Theory’s emphasis on aesthetic aspect of emotions enriches the
perception of emotional complexity and transcendent experiences. These
are often underexplored in Western frameworks.
By integrating both Indian and Western perspectives, authors advocate
for a culturally inclusive and multidisciplinary approach to research of
emotion which respects both empirical rigor and philosophical depth.

##### 7 Applications and Implications

**7.1 Emotions in Forensic Psychology**

Within the domain of forensic psychology, emotion play a crucial role.
Theoretical frameworks of emotions are applied to assess, understand,
evaluate and predict human behaviours related to legal investigations.
Emotions or emotional arousal is utilised as scientific tool to assess
truthfulness, or deception or emotional states of people involved in legal
justice system. Tools like polygraph measure physiological stated like
heartbeat, blood pressure and galvanic skin response (GSR) which help
detect deception associated emotional arousal. The Facial Action Coding
System (FACS) developed by Paul Ekman and Wallace V Friesen breaks
down facial expression associated with various emotions via specific
muscle movements. This can help detect deception along with concealed
emotions [ 28 ]. Rationale behind Polygraph tests is rooted in the notion that
deceptive behavior will trigger distinct emotional and physiological
responses. Techniques like voice stress analysis (VSA), and layered voice
analysis (LVA) analyses speech patterns linked to changes in vocal
frequencies which can detect stress levels, cognitive states, emotional states
and hence deception [ 42 ]. Recognising emotions is essential for social
interaction, yet individuals with sexual offenses often show socio-affective
deficits. In a study, Tiberi et al. [ 119 ] found that forensic inpatients (sexual
and non-sexual offenders) displayed poorer emotion recognition and slower


responses than community members, with sexual offenders being especially
cautious in labelling “surprise” [ 119 ]. These tools contribute to forensic
investigations by providing objective and scientific measures of emotional
responses. Their reliability and ethical implications remain subjects of
ongoing debate. They are on case-to-case basis admissible in court of law as
corroborative evidences based on expert testimony.

**7.2 Role of Emotional Intelligence in Mental Health and Social**

**Behavior**

Emotional intelligence (EI) refers to the ability to perceive, appraise,
understand, regulate, and utilize emotions. EI has emerged as a significant
factor influencing mental health, interpersonal relationships, and social
functioning [ 105 ]. High EI is linked with better stress management,
resilience building, and adaptive coping strategies. These skills protect one
against anxiety, depression, and emotional dysregulation [ 111 ]. Emotional
intelligence enhances empathy and social skills, this results in healthier
social interactions and efficient conflict resolution [ 10 ]. Emotion’s role in
occupational setting and educational domain emphasises the importance of
integrating emotional intelligence development in social skills training as
well as mental health therapeutic interventions [ 71 ].

**7.3 Clinical and Therapeutic Implications**

Numerous clinical and therapeutic techniques aimed at improving mental
health and regulating emotions are based on understanding that emotions
are multifaceted and complex. By addressing maladaptive emotional
evaluations and behaviours, cognitive-behavioral therapy (CBT) encourages
more constructive emotional processing [ 7 ]. With their roots in Buddhist
and yogic traditions, mindfulness-based therapies have demonstrated
effectiveness in treating mood and anxiety disorders by emphasising
nonjudgmental awareness of emotional states [ 49 ]. Pharmacology based
treatments that alter emotional circuits linked to conditions like PTSD and
depression have been made easier by affective neuroscience [ 22 , 23 ]. These
interventions show how important emotional intelligence is to creating
successful mental health care plans.

**7.4 Emerging Technologies: AI and Machine Learning in**

**Emotion Recognition and Research**


Emotion research has transformed with advent and progress of technology
like artificial intelligence (AI) and machine learning (ML). AI & ML has
enabled automated detection and analysis of emotional expressions using
diverse data sources like facial images, speech patterns, texts, and other
physiological signals [ 127 ]. Systems for emotion recognition use algorithms
which is trained on large datasets and they do same with accuracy, which
facilitates application in forensic setting, monitoring mental health,
customer service and also human–computer interaction [ 13 ]. AI-driven
tools and systems also support real-time emotion tracking in both virtual
environments, and on social media. This opens up new avenues of
application in behavioural research and therapeutic intervention [ 94 , 97 ]. At
the same time, ethical concerns regarding privacy, biases etc. in emotional
data processing necessitates need for responsible development and
regulation [ 19 ].

##### 8 Future Directions and Emerging Trends

**8.1 Advances in Neuroscience and Psychology Integration**

The integration of neuroscience and psychology continues to deepen our
understanding of emotions by revealing the neural substrates and
mechanisms underlying emotional experiences. Cutting-edge neuroimaging
techniques, such as functional magnetic resonance imaging (fMRI) and
magnetoencephalography (MEG), enable researchers to map brain activity
during emotional processing with increasing precision [ 92 ]. These advances
support the development of more nuanced models that capture the dynamic
interplay between cognition, physiology, and affect. Future research aims to
unravel how brain plasticity influences emotional development and
regulation, potentially informing novel interventions for emotional
disorders [ 23 ].

**8.2 Cross-Cultural and Interdisciplinary Research Prospects**

Growing recognition of cultural variability in emotional experience and
expression underscores the importance of cross-cultural research in emotion
science. Interdisciplinary collaborations between psychology, anthropology,
neuroscience, and philosophy are essential to developing culturally
sensitive theories and assessment tools [ 75 ]. Such approaches facilitate
understanding of how social norms, language, and cultural practices shape


emotions, expanding beyond Western-centric models. Interdisciplinary
work also enriches applications in global mental health, education, and
technology design, fostering inclusive frameworks that respect diverse
emotional realities [ 122 ].

**8.3 Potential for AI and Machine Learning to Decode**

**Emotions**

Artificial intelligence and machine learning hold transformative potential in
decoding and interpreting complex emotional signals. By leveraging large
multimodal datasets—combining facial expressions, vocal tone,
physiological measures, and text—AI systems can detect subtle emotional
cues with growing accuracy [ 127 ]. Advances in deep learning algorithms
promise to enhance emotion recognition in real-world, dynamic
environments, enabling personalized feedback and adaptive interventions
[ 97 ]. However, ethical considerations, including data privacy, bias
mitigation, and transparency, remain critical to responsible AI deployment
in emotion research [ 19 ]. Future developments may integrate AI-driven
emotion decoding with wearable technologies for continuous mental health
monitoring and support [ 94 ].

**8.4 Ethical Considerations in AI and Emotion Recognition**

The use of artificial intelligence (AI) in emotion recognition also raises
important ethical concerns, particularly regarding privacy and bias.
Emotion recognition systems often rely on facial expressions, voice, or
physiological signals to infer affective states. This creates risks of intrusive
surveillance, especially when individuals are unaware that their emotions
are being monitored or analyzed [ 74 ]. Such practices can undermine
autonomy and violate rights to privacy, particularly in sensitive contexts
such as workplaces, schools, or legal settings.
Another critical issue is algorithmic bias. AI models are often trained on
limited or culturally skewed datasets, leading to systematic inaccuracies in
recognizing emotions across different demographic groups. For instance,
emotion recognition systems have been shown to misclassify expressions in
women and people of color more frequently, which can perpetuate social
inequalities [ 4 ]. Misinterpretations in forensic or clinical settings could
have especially harmful consequences, such as reinforcing stereotypes or
leading to unjust decisions.


Thus, while AI-based emotion recognition holds promise, its ethical use
requires transparent data practices, diverse training datasets, and strict
regulation to ensure fairness, accountability, and respect for individual
rights.

**8.5 Personalized Emotion Research: Implications for Mental**

**Health**

Personalized approaches to emotion research emphasize individual
differences in emotional processing shaped by genetics, environment,
culture, and life experiences [ 40 ]. Advances in genomics and neuroimaging
facilitate identification of biomarkers linked to emotional disorders,
enabling tailored therapeutic strategies [ 34 ]. Integration of personalized
data with AI-assisted emotion recognition can support precision psychiatry,
offering customized interventions based on an individual’s unique
emotional profile [ 44 ]. This paradigm shift promises to enhance prevention,
diagnosis, and treatment of mental health conditions, promoting emotional
resilience and well-being on a personalized scale.

##### 9 Conclusion

This review has explored a wide spectrum of psychological theories on
emotions, ranging from evolutionary and physiological models to cognitive,
constructivist, and socio-cultural frameworks. Each theory contributes
unique insights into the complex nature of emotions—highlighting their
biological roots, cognitive appraisals, cultural shaping, and dynamic
construction. The integration of these perspectives reveals that emotions are
multifaceted phenomena involving intertwined physiological,
psychological, and social processes.
Importantly, incorporating multidisciplinary and culturally inclusive
approaches enriches our understanding by bridging Western scientific
models with profound Indian philosophical traditions and emerging
technological innovations. Such a comprehensive view acknowledges the
universality of some emotional processes while respecting cultural
variability and individual differences.
As emotion science continues to evolve, advances in neuroscience,
artificial intelligence, and cross-cultural research promise to deepen our
comprehension and application of emotional knowledge. Future research


must embrace this complexity, balancing empirical rigor with cultural
sensitivity to better address mental health, social functioning, and human
well-being globally.
Ultimately, emotions remain a fundamental yet evolving frontier in
psychology—one that continues to challenge and expand our understanding
of what it means to be human.

##### References

1. Arnold, M.B.: Emotion and Personality, vols. 1–2. Columbia University Press (1960)
2.
    Bard, P.: On emotional expression after decortication. Am. J. Physiol. **107** , 517–524 (1934)
3.
    Barrett, L.F.: Are emotions natural kinds? Perspect. Psychol. Sci. **1** (1), 28–58 (2006)
4.
    Barrett, L.F., Adolphs, R., Marsella, S., Martinez, A.M., Pollak, S.D.: Emotional expressions
    reconsidered: challenges to inferring emotion from human facial movements. Psychol. Sci.
    Public Interes **20** (1), 1–68 (2019). https:// doi. org/ 10. 1177/ 1529100619832930
    [Crossref]
5.
    Barrett, L.F., Mesquita, B., Ochsner, K.N., Gross, J.J.: The experience of emotion. Annu. Rev.
    Psychol. **58** , 373–403 (2007)
6.
    Barrett, L.F., Wager, T.D.: The structure of emotion: evidence from neuroimaging studies. Curr.
    Dir. Psychol. Sci. **15** (2), 79–83 (2006)
7.
    Beck, J.S.: Cognitive Behavior Therapy: Basics and Beyond, 2nd edn. Guilford Press (2011)
8.
    Ben-Shakhar, G., Elaad, E.: The validity of the guilty knowledge test. Psychol. Bull. **129** (5),
    490–504 (2003)
9.
    Bharata Muni: The Natyashastra (M. Ghosh, Trans., 2 vols.). Manisha Granthalaya (1967)
10.
    Brackett, M.A., Rivers, S.E., Salovey, P.: Emotional intelligence: implications for personal,
    social, academic, and workplace success. Soc. Pers. Psychol. Compass **5** (1), 88–103 (2011)
11.
    Bradley, M.M., Lang, P.J.: Affective reactions to pictures: the IAPS in the study of emotion and
    attention. J. Clin. Neurophysiol. **17** (4), 536–546 (2000)
12.
    Buckner, R.L., Andrews-Hanna, J.R., Schacter, D.L.: The brain’s default network. Ann. N. Y.
    Acad. Sci. **1124** , 1–38 (2008)
13.
    Calvo, R.A., D’Mello, S.: Affect detection in intelligent tutoring systems. IEEE Trans. Affect.
    Comput. **1** (1), 3–17 (2010)
14.
    Cannon, W.B.: The James-Lange theory of emotions: a critical examination and an alternative
    theory. Am. J. Psychol. **39** (1/4), 106–124 (1927)


15.
Chakrabarti, A. (ed.): The Bloomsbury Research Handbook of Indian Aesthetics and the
Philosophy of Art. Bloomsbury (2016)
16.
Chakravarty, K., Thenmozhi, M.: An interpretation and understanding of human emotion fear
through Bhayanaka rasa in Dina Mehta’s brides are not for burning. World J. Engl. Lang. **15** (2),
194 (2024). https:// doi. org/ 10. 5430/ wjel. v15n2p194
17.
Clore, G.L., Ortony, A.: Psychological construction in the OCC model. Emot. Rev. **5** (4), 335–
343 (2013)
18.
Cowen, A.S., Keltner, D.: Self-report captures 27 distinct categories of emotion. PNAS **114** (38),
E7900–E7909 (2017)
19.
Cowie, R., Douglas-Cowie, E., Tsapatsoulis, N., Votsis, G., Kollias, S., Taylor, J.G.: Emotion
recognition in human–computer interaction. IEEE Signal Process. Mag. **18** (1), 32–80 (2001)
20.
Damasio, A.R.: Descartes’ Error: Emotion, Reason, and the Human Brain. Putnam (1994)
21.
Darwin, C.: The Expression of the Emotions in Man and Animals. John Murray (1872)
22.
Davidson, R.J., Irwin, W.: The functional neuroanatomy of emotion and affective style. Trends
Cogn. Sci. **3** (1), 11–21 (1999)
23.
Davidson, R.J., McEwen, B.S.: Social influences on neuroplasticity. Nat. Neurosci. **15** (5), 689–
695 (2012)
24.
Decety, J., Jackson, P.L.: The functional architecture of human empathy. Behav. Cogn.
Neurosci. Rev. **3** (2), 71–100 (2004)
25.
Dolan, R.J.: Emotion, cognition, and behavior. Science **298** (5596), 1191–1194 (2002)
26.
Dutton, D.G., Aron, A.P.: Some evidence for heightened sexual attraction under conditions of
high anxiety. J. Pers. Soc. Psychol. **30** (4), 510–517 (1974)
27.
Ekman, P.: An argument for basic emotions. Cogn. Emot. **6** (3/4), 169–200 (1992)
28.
Ekman, P.: Telling Lies, 4th edn. NW. W. Norton (2009)
29.
Ekman, P., Friesen, W.V.: Constants across cultures in the face and emotion. J. Pers. Soc.
Psychol. **17** (2), 124–129 (1971)
30.
Ekman, P. (2004). Emotions Revealed. Times Books.
31.
Ekman, P., Friesen, W.V.: Facial Action Coding System: A Technique for the Measurement of
Facial Movement. Consulting Psychologists Press (1978)
32.
Ekman, P., Friesen, W.V., Hager, J.C.: FACS Investigator’s Guide. Research Nexus (2002)
33.
Engeström, Y.: Learning by Expanding. Orienta-Konsultit (1987)
34.
Etkin, A., Wager, T.D.: Functional neuroimaging of anxiety. Am. J. Psychiatry **164** (10), 1476–
1488 (2007)


35.
Feldman, R.S.: Understanding Psychology, 10th ed. Mc Grow Hill (2018)
36.
Frijda, N.H.: The Emotions. Cambridge University Press (1986)
37.
Gendron, M., Barrett, L.F.: Reconstructing the past: a century of ideas about emotion in
psychology. Emot. Rev. **1** (4), 316–339 (2009)
38.
Gross, J.J.: The emerging field of emotion regulation. Rev. Gen. Psychol. **2** (3), 271–299 (1998)
39.
Gross, J.J., John, O.P.: Individual differences in emotion regulation. J. Pers. Soc. Psychol.
**85** (2), 348–362 (2003)
40.
Gross, J.J., Thompson, R.A.: Emotion regulation: conceptual foundations. In: Gross, J. (ed.)
Handbook of Emotion Regulation, pp. 3–24. Guilford (2007)
41.
Hamann, S.: Mapping discrete and dimensional emotions onto the brain. Neurosci. Lett. **528** (2),
80–84 (2012)
42.
Harnsberger, J.D., Hollien, H., Martin, C.A., Hollien, K.A.: Stress and deception in speech. J.
Forensic Sci. **54** (3), 642–659 (2009)
43.
Hiriyanna, M.: The Indian Conception of Values. Kavyalaya (1954)
44.
Insel, T.R.: The NIMH research domain criteria (RDoC) project: precision medicine for
psychiatry. World Psychiatry **13** (1), 28–35 (2014)
45.
Izard, C.E.: Basic emotions, relations among emotions, and emotion–cognition relations.
Psychol. Rev. **99** (3), 561–565 (1992)
46.
Izard, C.E.: Basic emotions, natural kinds, emotion schemas. Perspect. Psychol. Sci. **2** (3), 260–
287 (2007)
47.
Jack, R.E., Garrod, O.G., Yu, H., Caldara, R., Schyns, P.G.: Cultural confusions show that facial
expressions are not universal. PNAS **109** (19), 7241–7244 (2012)
48.
James, W.: What is an emotion? Mind **9** (34), 188–205 (1884)
49.
Kabat-Zinn, J.: Mindfulness-based interventions in context. Clin. Psychol. Sci. Pract. **10** (2),
144–156 (2003)
50.
Kahneman, D.: Thinking, Fast and Slow. Farrar, Straus and Giroux (2011)
51.
Kavuri, V., Raghuram, N., Malamud, A., Selvan, S.R.: Irritable bowel syndrome: yoga as
remedial therapy. Evid.-Based Complement. Altern. Med. **2015** , 1–10 (2015). https:// doi. org/ 10.
1155/ 2015/ 398156
[Crossref]
52.
Keltner, D., Sauter, D., Tracy, J.L., Cowen, A.: Emotional expression: advances in basic
emotion theory. Emot. Rev. **11** (2), 106–115 (2019)
53.
Keltner, D., Lerner, J.S.: Emotion. In: Fiske, S.T. et al. (ed.) Handbook of Social Psychology,
5th edn., pp. 317–352. Wiley (2010)


54.
Kleinginna, P.R., Kleinginna, A.M.: A categorized list of emotion definitions. Motiv. Emot.
**5** (4), 345–379 (1981)
55.
Kober, H., Barrett, L.F., Joseph, J., Bliss-Moreau, E., Lindquist, K., Wager, T.D.: Functional
grouping and cortical–subcortical interactions in emotion. Neuroimage **42** (2), 998–1031 (2008)
56.
Kosslyn, S.M., Rosenberg, R.S.: Psychology: The Brain, the Person, the World, 3rd edn.
Pearson (2013)
57.
Kreibig, S.D.: Autonomic nervous system activity in emotion: a review. Biol. Psychol. **84** (3),
394–421 (2010)
58.
Lad, V.: Textbook of Ayurveda: Fundamental Principles, vol. 1. The Ayurvedic Press (2002)
59.
Lange, C.G.: The emotions. In: Lange, C.G., James, W. (eds.), Loewenberg, J.C.M. (trans.) The
Emotions. Williams & Wilkins (Original work published 1885) (1922)
60.
Lazarus, R.S.: Psychological Stress and the Coping Process. McGraw-Hill (1966)
61.
Lazarus, R.S., Zajonc, R.B.: The primacy of affect. Am. Psychol. **39** (2), 124–129 (1984)
62.
LeDoux, J.E.: Emotion circuits in the brain. Annu. Rev. Neurosci. **23** , 155–184 (2000)
63.
Leontiev, A. N. (1981). Problems of the Development of the Mind. Progress.
64.
Lerner, J.S., Li, Y., Valdesolo, P., Kassam, K.S.: Emotion and decision making. Annu. Rev.
Psychol. **66** , 799–823 (2015)
65.
Levenson, R.W.: Autonomic specificity and emotion. Emot. Rev. **6** (3), 255–262 (2014)
66.
Lindquist, K.A., Barrett, L.F.: Constructing emotion. Psychol. Sci. **19** (9), 898–903 (2008)
67.
Lindquist, K.A., Wager, T.D., Kober, H., Bliss-Moreau, E., Barrett, L.F.: The brain basis of
emotion: a meta-analytic review. Behav. Brain Sci. **35** (3), 121–143 (2012)
68.
Lykken, D.T.: A Tremor in the Blood: Uses and Abuses of the Polygraph, 2nd edn. Plenum
(1998)
69.
Matsumoto, D., Hwang, H.S.: Culture and emotion: the integration of biological and cultural
contributions. J. Cross Cult. Psychol. **43** (1), 91–118 (2012)
70.
Mauss, I.B., Levenson, R.W., McCarter, L., Wilhelm, F.H., Gross, J.J.: The tie that binds?
Cogn. Emot. **19** (2), 211–232 (2005)
71.
Mayer, J.D., Caruso, D., Salovey, P.: The ability model of emotional intelligence. Emot. Rev.
**8** (4), 290–300 (2016)
72.
McDougall, W.: An Introduction to Social Psychology. Methuen (1908)
73.
McRae, K., Hughes, B., Chopra, S., Gabrieli, J., Gross, J.J., Ochsner, K.N.: The neural bases of
distraction and reappraisal. J. Cogn. Neurosci. **22** (2), 248–262 (2010)


74.
McStay, A.: Emotional AI, soft biometrics and the surveillance of emotional life: an unusual
consensus on privacy. Big Data Soc. **7** (1), 205395172090438 (2020). https:// doi. org/ 10. 1177/
2053951720904386
[Crossref]
75.
Mesquita, B., Frijda, N.H.: Cultural variations in emotions. Psychol. Bull. **112** (2), 179–204
(1992)
76.
Mittelstadt, B.D., Allo, P., Taddeo, M., Wachter, S., Floridi, L.: The ethics of algorithms. Big
Data Soc. **3** (2), 1–21 (2016)
77.
Morgan, C.T., King, R.A., Weisz, J.R., Schopler, J.: Introduction to Psychology, 7th ed.
McGraw-Hill Companies. (1986)
78.
National Research Council: The Polygraph and Lie Detection. National Academies Press
(2003)
79.
Nevid, J.S.: Essentials of Psychology: Concepts and Applications, 4th edn. Cengage (2016)
80.
Niedenthal, P.M.: Embodying emotion. Science **316** (5827), 1002–1005 (2007)
81.
Niedenthal, P.M., Ric, F.: Psychology of Emotion, 2nd edn. Psychology Press (2017)
82.
Oatley, K., Johnson-Laird, P.N.: Towards a cognitive theory of emotions. Cogn. Emot. **1** (1), 29–
50 (1987)
83.
Ochsner, K.N., Gross, J.J.: The cognitive control of emotion. Trends Cogn. Sci. **9** (5), 242–249
(2005)
84.
Ochsner, K.N., Silvers, J.A., Buhle, J.T.: Functional imaging studies of emotion regulation.
Cogn. Affect. Behav. Neurosci. **12** (1), 25–52 (2012)
85.
Ortony, A., Clore, G.L., Collins, A.: The Cognitive Structure of Emotions. Cambridge
University Press (1988)
86.
Ortony, A., Turner, T.J.: What’s basic about basic emotions? Psychol. Rev. **97** (3), 315–331
(1990)
87.
Paijwar, P., Awasthi, H.H., Mishra, D.: Concept of Panchakosha in Vedic Literature.
Vedavijñānabhāsvatī. **6** – **7** , 22–7 (2024). https:// bhu. ac. in/ Site/ Page/ 1_ 3239_ 4515_ 6886_ 3495_
Vedic-Vigyan-Kendra-Research-Journal#forth%20 edition
88.
Pandey, P., Tripathi, R., Miyapuram, K.P.: Classifying oscillatory brain activity associated with
Indian Rasas using network metrics. Brain Inform. **9** (1) (2022). https:// doi. org/ 10. 1186/ s40708-
022-00163-7
89.
Pandit, P., Krieger, W.: A comparative study of emotion in Indian and western philosophy.
Comp. Philos.: Int. J. Constr. Engag. Distinct Approaches World Philos. **15** (1) (2024). https://
doi. org/ 10. 31979/ 2151-6014(2024). 150110
90.
Panksepp, J.: Affective Neuroscience. Oxford University Press (1998)


91.
Patanjali: In: Iyengar, B.K.S. (trans.) Light on the Yoga Sutras of Patanjali. Thorsons (1993)
92.
Pessoa, L.: The Cognitive-Emotional Brain: From Interactions to Integration. MIT Press (2017)
93.
Phan, K.L., Wager, T.D., Taylor, S.F., Liberzon, I.: Functional neuroanatomy of emotion.
Neuroimage **16** (2), 331–348 (2002)
94.
Picard, R.W.: Affective Computing. MIT Press (1997)
95.
Plutchik, R.: The nature of emotions. Am. Sci. **89** (4), 344–350 (2001)
96.
Pollock, S. (ed.): A Rasa Reader: Classical Indian Aesthetics. Columbia University Press (2016)
97.
Poria, S., Cambria, E., Bajpai, R., Hussain, A.: A review of affective computing. Inf. Fusion **37** ,
98–125 (2017)
98.
Poria, S., Hazarika, D., Majumder, N., Mihalcea, R.: Beneath the tip of the iceberg: multimodal
emotion recognition. IEEE Trans. Affect. Comput. **11** (3), 1–20 (2020)
99.
Radhakrishnan, S., Moore, C.A. (eds.): A Sourcebook in Indian Philosophy. Princeton
University Press (1957)
100.
Raghuram, Y.S., Manasa, S.: Sattvavajaya Chikitsa: Introduction, Meaning and Definition
(2024, September 30). https:// [http://www.](http://www.) easyayurveda. com/ 2024/ 09/ 30/ sattvavajaya-chikitsa-
introduction-meaning-and-definition/
101.
Ramaprasad, D.: Emotions: an Indian perspective. Indian J. Psychiatry **55** (Suppl. 2), S153–
S156 (2013). https:// doi. org/ 10. 4103/ 0019-5545. 105514
[Crossref]
102.
Rao, K.R.: Psychology in the Indian Tradition. Springer (2011)
103.
Russell, J.A.: A circumplex model of affect. J. Pers. Soc. Psychol. **39** (6), 1161–1178 (1980)
104.
Russell, J.A., Barrett, L.F.: Core affect, prototypical emotional episodes, and other things. J.
Pers. Soc. Psychol. **76** (5), 805–819 (1999)
105.
Salovey, P., Mayer, J.D.: Emotional intelligence. Imagin. Cogn. Pers. **9** (3), 185–211 (1990)
106.
Saper, C.B.: The central autonomic nervous system. In: Low, P., Benarroch, A. (eds.) Clinical
Autonomic Disorders, pp. 3–19. Lippincott Williams & Wilkins (2002)
107.
Satpathy, B.: Pancha Kosha theory of personality. Intern. J. Indian. Psychol. **6** (2) (2018). https://
doi. org/ 10. 25215/ 0602. 105
108.
Schachter, S., Singer, J.: Cognitive, social, and physiological determinants of emotional state.
Psychol. Rev. **69** (5), 379–399 (1962)
109.
Scherer, K.R.: What are emotions? Soc. Sci. Inf. **44** (4), 695–729 (2005)
110.
Scherer, K.R.: The dynamic architecture of emotion. Cogn. Emot. **23** (7), 1307–1351 (2009)


111.
Schutte, N.S., Malouff, J.M., Thorsteinsson, E.B., Bhullar, N., Rooke, S.: A meta-analytic
investigation of the relationship between emotional intelligence and health. Personality Individ.
Differ. **42** (6), 921–933 (2007)
112.
Seeley, W.W., Menon, V., Schatzberg, A.F., et al.: Dissociable intrinsic connectivity networks
for salience processing. J. Neurosci. **27** (9), 2349–2356 (2007)
113.
Šimić, G., Tkalčić, M., Vukić, V., Mulc, D., Španić, E., Šagud, M., Olucha-Bordonau, F.E.,
Vukšić, M., Hof, P.R.: Understanding emotions: origins and roles of the amygdala.
Biomolecules **11** (6), 823 (2021). https:// doi. org/ 10. 3390/ biom1 1060823
114.
Singer, T., Seymour, B., O’Doherty, J., Kaube, H., Dolan, R., Frith, C.: Empathy for pain.
Science **303** (5661), 1157–1162 (2004)
115.
Sivananda, S.: The Science of Pranayama. Divine Life Society (2002)
116.
Slovic, P., Finucane, M., Peters, E., MacGregor, D.: Risk as analysis and risk as feelings. Risk
Anal. **24** (2), 311–322 (2004)
117.
Steer, E.: A cross comparison between ayurvedic etiology of major depressive disorder and
bidirectional effect of gut dysregulation. J. Ayurveda Integr. Med. **10** (1), 59–66 (2019). https://
doi. org/ 10. 1016/ j. jaim. 2017. 08. 002
[Crossref]
118.
Susskind, J.M., Lee, D.H., Cusi, A., et al.: Fear expressions enhance sensory acquisition. PNAS
**105** (47), 17947–17951 (2008)
119.
Tiberi, L.A., Gillespie, S.M., Saloppé, X., Vicenzutto, A., Pham, T.H.: Recognition of dynamic
facial expressions of emotions in forensic inpatients who have committed sexual offenses: a
signal detection analysis. Front. Psychiatry **15**. https:// doi. org/ 10. 3389/ fpsyt. 2024. 1384789
120.
Tomkins, S.S.: Affect, Imagery, Consciousness, vol. 1. Springer (1962)
121.
Tracy, J.L., Matsumoto, D.: The spontaneous expression of pride and shame. PNAS **105** (33),
11655–11660 (2008)
122.
Tsai, J.L.: Ideal affect. Perspect. Psychol. Sci. **2** (3), 242–259 (2007)
123.
Vuilleumier, P.: How brains beware: neural mechanisms of emotional attention. Trends Cogn.
Sci. **9** (12), 585–594 (2005)
124.
Vygotsky, L.S.: Mind in Society. Harvard University Press (1978)
125.
Watson, J.B.: Psychology as the behaviorist views it. Psychol. Rev. **20** (2), 158–177 (1913)
126.
Zajonc, R.B.: Feeling and thinking: preferences need no inferences. Am. Psychol. **35** (2), 151–
175 (1980)
127.
Zeng, Z., Pantic, M., Roisman, G.I., Huang, T.S.: A survey of affect recognition methods.
Pattern Recogn. **42** (8), 2006–2026 (2009)


_OceanofPDF.com_


(1)

© The Author(s), under exclusive license to Springer Nature Switzerland AG 2026
K. Slimani et al. (eds.), _Emotion and Facial Recognition in Artificial Intelligence: Sustainable
Multidisciplinary Perspectives and Applications_ , Information Systems Engineering and Management 78
https://doi.org/10.1007/978-3-032-14778-3_3

### Classical Face Recognition: Geometric

### Models, Subspace Techniques and Local

### Descriptors

Mossaab Idrissi Alami^1 , Abderrahmane Ez-zahout^1 and Fouzia Omary^1
Faculty of Sciences, Mohammed V University, Rabat, Morocco

```
Mossaab Idrissi Alami
Email: mossaab_idrissialami@um5.ac.ma
```
**Abstract**
Facial recognition research originated from the fundamental task of
distinguishing individuals based on their facial characteristics. Early researchers
did exactly that by measuring distances between eyes, noses and mouths, but
these geometric tricks fell apart as soon as a smile, tilt or shadow got in the way.
So they began looking at the whole face, compressing images into “eigenfaces”
that captured the most common patterns. These principal components were fast
and easy but not very discerning, prompting the search for discriminative features
with Fisherfaces and even higher‑order patterns with Independent Component
Analysis. Others zoomed in on tiny patches, counting local binary patterns or
filtering textures with Gabor wavelets to withstand changes in lighting and
expression. This chapter traces that journey from hand‑measured landmarks to
clever mathematical subspaces and texture codes. It also touches on iconic
datasets like Labeled Faces in the Wild, explains what it means for a system to
falsely accept or reject someone, and reflects on why these “classical” techniques
still matter. Even though deep neural networks now steal the spotlight, the
simplicity and transparency of these older methods keep them useful for
teaching, benchmarking and environments with little data or limited hardware.

**Keywords** Face recognition – Classical approaches – Geometric methods –
Principal component analysis (PCA)/eigenfaces – Linear discriminant analysis


(LDA)/fisherfaces – Independent component analysis (ICA)

###### 1 Introduction and Historical Perspective

The human brain performs face recognition effortlessly. We effortlessly pick out
familiar faces in a crowd, notice subtle expressions and differentiate between
twins, even under poor lighting. Reproducing this remarkable ability in a
machine has been a major focus of computer vision research for over half a
century. Long before the advent of deep learning, researchers built systems that
sought to model how people recognise faces using carefully designed features
and statistical models. These classical face recognition approaches emerged in a
period stretching from the early 1970s to the early 2010s, and they laid the
foundation for the powerful deep neural networks that dominate the field today.
Early face recognition research was heavily influenced by the capabilities and
limitations of the computing hardware of the time. In 1964, Woodrow Bledsoe
and collaborators at Panoramic Research Laboratories built one of the first
semi‑automatic face recognition systems. Their system required an operator to
manually extract coordinates of key facial landmarks (such as the pupils, corners
of the mouth and the width of the head). The program then used these
measurements to compare the input image to a database of mugshots. Although
far from automated, the approach demonstrated that faces could be described by
geometric distances, and it set a precedent for geometric methods. In the 1970s
and 1980s, researchers continued to refine landmark‑based approaches, building
statistical models of facial proportions and using techniques like the Karhunen–
Loève transform to reduce dimensionality. These early efforts highlighted three
enduring challenges: variations in lighting, facial expression and pose.
The 1980s saw a shift toward statistical pattern recognition. Sirovich and
Kirby showed that a set of face images could be represented by a small number
of orthogonal basis vectors derived from the covariance matrix of face images.
These basis vectors came to be known as eigenfaces after Turk and Pentland
applied them for classification [ 1 ]. Instead of measuring distances between
specific landmarks, their system projected images into a lower‑dimensional
subspace spanned by the top eigenvectors of the covariance matrix. This work
demonstrated that faces, though high‑dimensional objects, reside on a
low‑dimensional manifold and can be discriminated using a compact
representation.
Throughout the 1990s and early 2000s, researchers developed a variety of
subspace and local feature methods to improve performance under variations in
illumination, pose and expression. Linear Discriminant Analysis (LDA), also
known as the Fisherface method, sought to maximise separability between


different people while minimising differences within the same person [ 2 ].
Independent Component Analysis (ICA) extended the idea of eigenfaces by
capturing higher‑order statistics, resulting in basis images that correspond to
localized facial parts and outperforming PCA in some experiments [ 3 ]. Local
Binary Patterns (LBP) and Gabor filters extracted texture information from small
patches of the face, enabling more robust recognition under varying lighting and
expression [ 4 , 5 ]. The widespread availability of cameras and the growth of the
internet led to the creation of large face datasets, such as FERET, Yale, ORL and
Labeled Faces in the Wild (LFW) [ 6 ], which allowed researchers to
systematically evaluate algorithms (Fig. 1 ).

**_Fig. 1_** Timeline of classical face recognition approaches
This chapter provides a comprehensive overview of classical face recognition
approaches. Section 2 explores geometric and holistic subspace methods,
detailing algorithms for Principal Component Analysis (PCA), Linear
Discriminant Analysis, Independent Component Analysis and related techniques.
Section 3 focuses on local feature‑based methods, including Local Binary
Patterns, Gabor features, Histogram of Oriented Gradients (HOG) and
Scale‑Invariant Feature Transform (SIFT). Section 4 discusses public datasets
and evaluation metrics such as False Acceptance Rate (FAR), False Rejection
Rate (FRR) and Equal Error Rate (EER) [ 7 ]. Section 5 analyses the strengths and
limitations of classical methods, and Sect. 6 summarises the state of the field and
reflects on the relevance of these techniques in the era of deep learning.

###### 2 Geometric and Holistic Subspace Methods


Classical face recognition methods fall into two main paradigms: geometric
approaches that represent faces through measurements of facial landmarks, and
holistic approaches that treat the entire face image as a vector and project it into a
lower‑dimensional subspace. Holistic methods often require less manual
intervention than geometric ones and have generally performed better in
unconstrained environments.

**2.1 Geometric Face Recognition**
The earliest endeavors in automated facial recognition drew inspiration from how
humans intuitively recognize familiar individuals—often by noting the spatial
arrangement and proportions of facial features. In the 1960s, researchers like
Woody Bledsoe and colleagues attempted to formalize this process using
geometric measurements derived from frontal facial images. Distances between
key anatomical landmarks—such as the eyes, nose, mouth, and jawline—were
manually recorded and structured into feature vectors.
One of the pioneering systems, described in Bledsoe’s 1964 work, utilized
Euclidean and Mahalanobis distance metrics to compare these vectors against
stored templates. Despite being a breakthrough for its time, the system’s
performance was modest. Several core limitations hindered its scalability:

```
Limitations of Geometric Models: Despite being a breakthrough for its time,
early geometric face recognition systems suffered from several critical
limitations that ultimately hindered their scalability and accuracy, especially in
uncontrolled environments.
Pose Sensitivity: Even slight head tilts or rotations (e.g., turning ±15° from a
frontal view) significantly distort the perceived distances and angles between
facial landmarks (eyes, nose, mouth). Since geometric models relied on 2D
measurements, they lacked mechanisms to compensate for 3D head
movement. This made them unreliable in surveillance scenarios where
individuals rarely face the camera directly.
Occlusion Vulnerability: Facial features can be partially or fully obscured by
external elements such as glasses, hats, beards, or scarves. For example, the
presence of thick-rimmed glasses could hide the eye corners, or facial hair
could obstruct the mouth and chin regions. Because geometric models used a
small set of predefined landmarks, any occlusion often led to incorrect or
missing features, reducing recognition accuracy.
Illumination Dependence: Geometric extraction methods—especially those
based on edge detection or gradient information—are highly sensitive to
lighting variations. Shadows cast across the face, backlighting, or uneven
illumination distort the visibility and accuracy of feature detection. These
```

limitations were especially evident when testing systems across different
lighting setups (e.g., indoor vs. outdoor).
Manual Annotation Burden: Early systems (such as Bledsoe’s) required human
operators to manually annotate the coordinates of each facial feature. This
process was time-consuming, prone to human error, and non-scalable for large
databases. Although semi-automated approaches emerged in the 1980s,
reliable automatic landmark detection was not available until much later.
Intra-Class Variability and Feature Ambiguity: Geometric models struggled to
distinguish between individuals with similar facial proportions. For instance,
two people might have nearly identical interocular distances and jaw widths,
especially among family members or twins. Moreover, intra-class variability
due to expressions (smiling vs. neutral) altered landmark positions and
introduced noise in recognition.
Recognizing these constraints, the field advanced toward statistical shape
modeling. Techniques like the Active Shape Model (ASM) and its successor, the
Active Appearance Model (AAM), represented a major leap forward in the
1990s. Rather than relying solely on distances, ASM applied Principal
Component Analysis (PCA) to landmark coordinates gathered from annotated
training images. The result was a deformable shape model capable of adapting to
new faces by varying along statistically learned modes.
AAM further extended this approach by simultaneously modeling both shape
and texture. It aligned face images to a mean shape and then applied PCA to the
texture (i.e., pixel intensities) within the warped region. These models enabled
more robust face alignment and tracking, yet remained sensitive to initialization
errors and partial occlusions. For example, poor starting positions often led to
convergence on incorrect facial configurations.
Another significant contribution in this era was the Elastic Bunch Graph
Matching (EBGM) technique, which introduced a structured and flexible
representation of the face. EBGM constructed a graph of nodes anchored at
salient landmarks (e.g., eye corners, nose tip, mouth edges). Each node stored a
“jet”—a set of Gabor filter responses computed at multiple scales and
orientations—encapsulating both the topological layout and local texture. During
recognition, the algorithm searched for optimal correspondences between the
input face graph and those in the database, allowing for elastic deformation to
account for small changes in expression or pose.
Despite their historical impact, purely geometric models gradually declined in
popularity due to the increasing demand for robustness in real-world
applications. Their dependence on accurate landmark localization, particularly in
low-resolution or unconstrained imagery, limited their standalone effectiveness.


Nevertheless, geometric cues remain relevant today, often serving as pre-
processing steps for alignment or feature extraction in modern hybrid systems.

**2.2 Principal Component Analysis and Eigenfaces**
The Principal Component Analysis (PCA) technique revolutionised face
recognition by shifting the focus from measuring geometric distances to
analysing the entire face region. PCA is an unsupervised dimension reduction
technique that finds orthogonal directions (principal components) along which
the data vary the most. Given a set of centred training images (each reshaped into
a vector), one forms a covariance matrix and solves the eigenvalue problem. The
eigenvectors corresponding to the largest eigenvalues capture the directions of
greatest variance, and they form a basis for the eigenface subspace [ 1 ].
In practice, a face image with **M × N** pixels is converted into a vector of
length **MN**. Stacking all **p** training images into a matrix **_X_** ∈ **_R_** ( **_MN_** )× _p_ , one

computes the mean face **_x_** and subtracts it from each column:
**_X_** ~ = **_X_** − **_x_** 1 **_T_**
.
The covariance matrix is:

```
C =
```
```
1
p
```
```
X ~ X ~ T
(1)
```
Solving the eigenvalue problem **_Cvi_** = **𝛌** **_ivi_** yields the eigenfaces **_vi_**. Since
**_C_** is high‑dimensional, Turk and Pentland proposed computing eigenvectors of

the smaller matrix **_X_** ~ **_TX_** ~ and projecting them back to the original space using

**_ui_** = **_X_** ~ **_vi_**. Once the top **k** eigenfaces are obtained, a face image **x** is projected
into the subspace to obtain coefficients **_y_** ∈ **_Rk_** using

```
y = UT ( x − x ) (2)
```
where **U** contains the eigenfaces.
Classification can be performed by nearest neighbour search in this reduced
space or by measuring reconstruction errors. In a verification setting, one
computes the distance between the coefficients of two images and compares it to
a threshold.
PCA‑based methods were widely adopted because they require relatively few
samples to learn a compact representation and are computationally efficient [ 8 ].
Eigenfaces capture global appearance variations such as illumination changes
and coarse facial structure. However, because PCA does not consider class labels,
it emphasises features with large variance rather than those that discriminate
between individuals. Consequently, eigenfaces often include variations due to


lighting, pose and background, which may not be informative for identity.
Research showed that eigenfaces perform poorly under occlusions and
non‑frontal poses [ 9 ]. To address this, researchers developed methods to add
robustness, such as using multiple illumination subspaces or performing PCA on
local patches.

**2.3 Linear Discriminant Analysis (Fisherfaces)**
LDA, also Known as Fisher’s linear discriminant, is a supervised dimension
reduction technique. Unlike PCA, which maximises overall variance, LDA seeks
directions that maximise between‑class scatter while minimising within‑class
scatter. For face recognition, LDA is applied to find a projection that best
separates the classes (people) represented in the training set. The resulting basis
vectors are sometimes called Fisherfaces [ 2 ].
Let the training set contain images from **c** different people (classes) with **ni**

images for person **i**. The within‑class scatter matrix is defined as

```
SW =
```
```
c
∑
i = 1
```
```
∑
x ∈class i
```
```
( x − μi )( x − μi ) T , (3)
```
where **_μ_** is the mean of class **i**. The between‑class scatter matrix is

```
SB =
```
```
c
∑
i = 1
```
```
ni ( μi − μ )( μi − μ ) T , (4)
```
where **_μ_** is the overall mean. LDA finds a projection matrix **W** that maximises
the ratio of the determinants of the scatter matrices:

```
W =argmax
```
```
WTSBW
| WTSWW |
```
. (5)

The solution involves solving the generalized eigenvalue problem
_SBu_ = _λSWu_. The number of meaningful discriminant vectors is at most c − 1.
In practice, _SW_ is often singular because the dimensionality of images is much
larger than the number of samples; to mitigate this, Fisherfaces combine PCA and
LDA by projecting the images into a subspace of moderate dimension using
PCA, then applying LDA.
LDA typically provides better discrimination than PCA when multiple
samples per person are available and the training set captures various expressions
and lighting conditions. However, if the number of training images is small or the


number of classes is large, LDA may overfit the data. Furthermore, because LDA
learns a linear discriminant, it may not capture complex nonlinear variations.
Nonlinear variants such as Kernel Fisher Discriminant Analysis map the data into
a high‑dimensional space using kernels before performing LDA, enabling better
separation.

**2.4 Independent Component Analysis**
Independent Component Analysis (ICA) extends PCA by finding components
that are statistically independent rather than merely uncorrelated. While PCA
looks for orthogonal directions that capture maximum variance, ICA aims to
recover latent variables whose linear combinations produce the observed data.
Applying ICA to faces often yields basis images that resemble localized facial
features such as eyes, mouths and noses, which can capture high‑order statistics.
One formulation of ICA assumes that the observed data X (zero‑mean) is
generated by mixing independent sources S using an unknown mixing matrix
A:X = AS. The goal is to estimate an unmixing matrix W such that S = WX. ICA
algorithms, such as FastICA, use measures of non‑Gaussianity (e.g., kurtosis,
negentropy) or maximum likelihood estimation to find W. Bartlett, Movellan and
Sejnowski compared two architectures for ICA-based face recognition:
architecture I, which represents each face as a combination of basis images
(global features), and architecture II, which treats each basis image as a filter and
represents faces by filter responses (local features). Both architectures
outperformed eigenfaces on tests involving variations in expression and time [ 3 ].
ICA has several advantages over PCA and LDA. First, by exploiting
higher‑order statistics, it can capture independent features that are more
discriminative and it should handle faces lit from different angles and faces
showing different expressions without losing accuracy. Second, the locality of
ICA basis images resembles the receptive fields of neurons in the visual cortex,
suggesting a biological plausibility. However, ICA is computationally more
expensive than PCA, and the independence assumption may not hold in practice.
Moreover, ICA lacks a natural ranking of components by importance; therefore,
selecting an appropriate number of components is less straightforward than in
PCA (Fig. 2 ).


**_Fig. 2_** Visual comparisons: eigenfaces versus fisherfaces versus ICA basis images

**2.5 Other Subspace Methods**
In addition to PCA, LDA and ICA, researchers developed various other subspace
methods to improve face recognition performance under different conditions.
a. **Kernel PCA and Nonlinear Methods**

Kernel Principal Component Analysis (Kernel PCA) addresses the limitations of
linear PCA by performing PCA in a high‑dimensional feature space implicitly
defined by a kernel function. Using kernels such as the Gaussian radial basis
function or polynomial kernels, Kernel PCA can capture nonlinear variations in
facial appearance. Similarly, Kernel LDA applies LDA in the kernel space to find
nonlinear discriminant directions. These methods improve recognition
performance when the variations between different images of the same person are


nonlinear, but they require choosing appropriate kernel parameters and can suffer
from high computational cost.

b. **Random Subspace Methods and Ensemble Learning**

The Random Subspace Method (RSM) randomly selects subsets of features (e.g.,
pixels or eigenface coefficients) and trains multiple classifiers on these subsets.
The final decision is obtained by combining the outputs of individual classifiers
(e.g., via majority voting). RSM reduces the risk of overfitting and improves
robustness by diversifying feature sets. Researchers applied RSM to eigenfaces
and Fisherfaces, achieving improved recognition rates on the FERET and Yale
datasets. Bagging and Boosting are other ensemble techniques that combine
weak classifiers to form a strong classifier. AdaBoost has been used with Gabor
features to select the most discriminative features for face recognition.

c. **Local Subspace Analysis**

While PCA computes a single global subspace for all images, Local Subspace
Analysis builds multiple subspaces that capture different modes of variation such
as pose, illumination or expression. Another family of techniques falls under
manifold learning. The key idea is that even though a digital portrait is a very
high‑dimensional vector, the set of all possible faces lies on a much
lower‑dimensional, curved surface (a “manifold”) in that space. Methods such as
Isomap, Locally Linear Embedding (LLE) and Laplacian Eigenmaps attempt to
unfold that surface by examining how each face image relates to its closest
neighbours. By preserving these local relationships, they reveal the underlying
structure of facial variations like pose or expression. Once the manifold is
learned, new faces can be embedded and compared via geodesic distances. These
algorithms provide insights into the intrinsic dimensionality of face images but
are sensitive to noise and require sufficient sampling.

**2.6 Discussion**

Holistic subspace methods transform high‑dimensional image vectors into
compact representations that capture the most informative variations. PCA–based
eigenfaces established that faces can be represented using only a few hundred
coefficients, enabling real‑time recognition on modest hardware. LDA improved
discriminability by focusing on between‑class variance [ 2 ], and ICA captured
independent localized features [ 3 ]. These methods were widely used because they
are mathematically elegant and computationally efficient. However, they rely on


linear assumptions and often struggle with large pose variations and occlusions.
They also require careful alignment of faces; misaligned eyes or mouths can lead
to significant errors. To overcome these limitations, researchers turned to local
feature‑based approaches, which are the subject of the next section (Table 1 ).

**_Table 1_** Comparative recognition performance of classical face recognition methods

```
Method Approach
type
```
```
Main
feature
```
```
FERET
accuracy
(%)
```
```
ORL
accuracy
(%)
```
```
Yale
accuracy
(%)
```
```
Strengths Limitations
```
```
PCA
(eigenfaces)
```
```
Subspace Maximizes
variance
across all
samples
```
```
85–90 93–96 ~83 Simple, low
dimensionality
```
```
Sensitive to
lighting and
pose
```
```
LDA
(fisherfaces)
```
```
Subspace Maximizes
class
separability
```
```
88–94 96–98 ~86 Good class
discrimination
```
```
May overfit
with few
samples
ICA Subspace Captures
independent
sources,
localized
features
```
```
90–95 ~97 ~87 Robust to
lighting and
expression
```
```
Computationally
heavier
```
```
LBP Local
descriptor
```
```
Texture-
based local
patterns
```
```
92–96 ~98 2 Robust to
lighting and
occlusion
```
```
Sensitive to
noise and
misalignment
Gabor
filters
```
```
Local
descriptor
```
```
Multi-scale
and multi-
orientation
filtering
```
```
94–97 ~98 ~91 Excellent
robustness to
illumination
variation
```
```
High
computational
cost
```
###### 3 Local Feature‑Based Methods

Holistic methods summarise the entire face into a single vector, but this can make
them sensitive to global variations in illumination, pose and expression. Local
feature‑based methods address this by extracting features from small
neighbourhoods or keypoints. They focus on capturing fine texture details and
structural information that remains stable across varying conditions. Some of the
most influential local methods include Local Binary Patterns (LBP), Gabor
filters, Histogram of Oriented Gradients (HOG), Scale‑Invariant Feature
Transform (SIFT) and their combinations.

**3.1 Local Binary Patterns (LBP)**


Local Binary Patterns were introduced as a texture descriptor and later adapted
for face recognition [ 4 ]. Local descriptors such as Local Binary Patterns (LBP)
and Gabor wavelets gained popularity due to their strong resilience to
illumination variations and their computational efficiency. These methods extract
local texture or frequency information, allowing for stable recognition under
changes in lighting, pose, and expression. In contrast to holistic methods like
PCA and ICA, which rely on global appearance, local descriptors can capture
discriminative micro-patterns that remain consistent across sessions. The basic
LBP operator works on a 3 × 3 pixel block. For each pixel, its value is compared
to each of its eight neighbours. If the neighbour’s value is greater than or equal to
the center pixel, a value of 1 is assigned; otherwise, 0. Reading the binary values
from the neighbours in a clockwise order forms an 8‑bit binary pattern, which is
converted to a decimal number between 0 and 255. This number indicates the
local texture around the pixel. The LBP image for a face is built by computing
the LBP code at each pixel, and a histogram of these codes summarizes the
distribution of micro‑patterns across the face. Since LBP codes are invariant to
monotonic grayscale changes, they are robust to variations in illumination.
Several improvements to the basic LBP have been proposed. Uniform LBP
counts the number of transitions (0–1 or 1–0) in the pattern and labels patterns
with few transitions (e.g., 0–2) as uniform, assigning a single bin for all
non‑uniform patterns. This reduces the length of the histogram while preserving
discriminative power. Circular LBP extends the neighbourhood to any radius,
allowing for multi‑scale texture analysis. Rotation‑Invariant LBP rotates the
binary pattern to the minimum possible value to handle rotated faces. Another
powerful extension is Spatially Enhanced LBP, which divides the face image into
an array of subregions (e.g., 7 × 7) and computes a histogram for each region.
Concatenating these histograms forms a long feature vector that encodes both the
local texture and its spatial location. This approach improves recognition
accuracy because it preserves structural information [ 4 ].
To classify faces using LBP, one computes the LBP histograms of the training
images and then measures the Chi‑square distance or Histogram Intersection
distance between the histograms of a test image and those of gallery images. The
identity corresponding to the closest histogram is assigned. The simplicity and
efficiency of LBP make it suitable for real‑time applications, including face
liveness detection and expression recognition. However, LBP is sensitive to
image noise, particularly when neighbourhoods are small. It also struggles when
faces are misaligned or subject to extreme pose changes. Combining LBP with
robust alignment techniques or other descriptors can alleviate these issues.

**3.2 Gabor Filters and Wavelet Transform**


Gabor filters are linear filters that capture spatial frequency and orientation
information. A two‑dimensional Gabor filter is defined as a sinusoidal plane
wave multiplied by a Gaussian envelope. Gabor filters mimic the receptive fields
of simple cells in the human visual cortex. Applying a bank of Gabor filters at
multiple scales and orientations to a face image yields a set of Gabor wavelet
coefficients that represent local orientations and textures.
For face recognition, one typically uses a set of Gabor filters with 5 scales
and 8 orientations, resulting in 40 filter responses per pixel. The responses
around key facial points (such as eyes, nose and mouth) are concatenated to form
a feature vector. Wiskott et al. used Gabor jets in their EBGM approach and
showed that Gabor coefficients are more powerful than geometric distances for
face recognition [ 5 ]. Another popular approach is the Gabor Fisher classifier,
which applies PCA and LDA on the Gabor feature vectors to reduce
dimensionality and perform classification. Gabor features are robust to variations
in illumination and facial expressions and have been widely used in commercial
face recognition systems.
Despite their effectiveness, Gabor filters produce very high‑dimensional
feature vectors, leading to significant computational and storage requirements.
Dimensionality reduction techniques such as PCA, LDA and Generalized
Discriminant Analysis (GDA) are often employed to manage this complexity.
Additionally, because Gabor filters respond strongly to edges, they can be
sensitive to noise and alignment errors. Preprocessing steps like histogram
equalization and face alignment are commonly used before applying Gabor
filters.

**3.3 Histogram of Oriented Gradients (HOG)**
The Histogram of Oriented Gradients (HOG) descriptor was originally developed
for human detection but has been successfully applied to face recognition. HOG
divides an image into small cells (e.g., 8 × 8) and computes a histogram of
gradient orientations within each cell. The gradient magnitude is used as the
weight for each orientation. Adjacent cells are grouped into blocks for contrast
normalization, improving robustness to illumination changes. The concatenated
histograms of all blocks form the HOG descriptor. HOG captures the general
shape and structure of the face, such as the contour of the eyes, nose and mouth.
While HOG does not capture as detailed texture information as LBP or Gabor
features, it is robust to small deformations and lighting changes. Combining
HOG with other descriptors or machine learning classifiers (e.g., Support Vector
Machines) can yield high recognition accuracy.

**3.4 Scale‑Invariant Feature Transform (SIFT) and SURF**


The Scale‑Invariant Feature Transform (SIFT) is a keypoint‑based descriptor that
detects distinctive points in an image and describes them by gradient histograms.
SIFT keypoints are invariant to rotation and scale and robust to small affine
transformations. For face recognition, SIFT detects keypoints on salient facial
regions (e.g., eyes, nose corners, mouth corners). For each keypoint, a
128‑dimensional descriptor is computed by binning gradient orientations in a 4 ×
4 neighbourhood. To compare two faces, one matches SIFT descriptors between
the images using distance metrics and counts the number of matches.
Alternatively, one can build a Bag‑of‑Features (BoF) model that represents a face
by the frequency of visual words obtained by clustering SIFT descriptors. SIFT is
particularly useful for cross‑domain recognition, where faces may vary
significantly in scale and orientation.
Speeded Up Robust Features (SURF) is a faster alternative to SIFT. SURF
uses integral images to compute responses and approximates Gaussian
second‑order derivatives with box filters. SURF descriptors are shorter (64
dimensions) and have faster matching. In face recognition, SURF performs
similarly to SIFT but is less computationally intensive.

**3.5 Hybrid and Learned Local Representations**
Recognizing that different descriptors capture complementary information,
researchers combined multiple local features. Local Gabor Binary Pattern
(LGBP) concatenates LBP histograms computed on Gabor filter responses,
exploiting both orientation selectivity and binary encoding. HOG–LBP and
SIFT–LBP features similarly merge edge information and texture patterns. The
combination of descriptors often improves robustness to variations in pose and
illumination.
Another category of local methods involves learned descriptors. Before deep
neural networks, researchers used unsupervised learning methods such as Sparse
Coding and Local Non‑Negative Matrix Factorization to learn dictionary
elements from patches of face images. Each patch is represented by a sparse
combination of dictionary atoms, and these sparse codes are pooled and
concatenated to form the face descriptor. While not purely handcrafted, these
methods still fall under classical approaches because they do not involve
end‑to‑end learning on millions of images.

**3.6 Discussion**

Local feature‑based methods provide robustness against changes in lighting,
expression and pose because they capture fine details that remain consistent
across conditions. LBP is extremely simple and efficient [ 4 ], while Gabor filters
mimic the response of human visual cortical cells and capture multi‑scale,


multi‑orientation information [ 5 ]. HOG and SIFT were originally developed for
general object recognition but found application in faces due to their invariance
properties. Hybrid descriptors combine the strengths of individual methods at the
cost of longer feature vectors and more complex matching. One disadvantage
common to many local methods is their sensitivity to misalignment; accurate face
detection and alignment (e.g., eye normalization) are essential. Furthermore,
high‑dimensional feature vectors require dimensionality reduction or efficient
indexing for large galleries (Table 2 ).

**_Table 2_** Comparative characteristics of local versus holistic face recognition methods

```
Method Feature type Illumination
robustness
```
```
Computational
cost
```
```
Discriminative
power
```
```
Common use Cases
```
```
LBP Local binary
patterns
```
```
High Very low Moderate Real-time
systems
```
```
Mobile
devices
Gabor
filters
```
```
Frequency-
based local
textures
```
```
High Moderate High Expression-
variant
recognition
```
```
Facial
analysis
```
```
Eigenfaces
(PCA)
```
```
Global
appearance
```
```
Low Low Low to
moderate
```
```
Dimensionality
reduction
```
```
Baseline
comparison
Fisherfaces
(LDA)
```
```
Global +
class labels
```
```
Moderate Moderate Moderate to
high
```
```
ID verification Closed-set
classification
ICA Statistical
independence
of features
```
```
Moderate High High Robust
recognition
```
```
Under
occlusion
```
###### 4 Datasets and Evaluation Metrics

Benchmark datasets and standard metrics are critical for comparing face
recognition algorithms. Because performance can vary widely depending on
factors such as pose, illumination and occlusion, researchers use datasets with
different characteristics and report multiple metrics.

**4.1 Classic Datasets**

```
Yale Face Database: Picture a small group of 15 volunteers photographed in a
studio under 11 different lighting setups and wearing a variety of expressions.
Because everything else is controlled, this collection is ideal for showing how
algorithms like PCA, LDA and LBP handle changes in illumination and mood
[ 10 ].
ORL (AT&T) Face Database: This classic set contains 400 portraits of 40
people, each photographed 10 times. Subjects sit upright against a plain dark
```

```
background, occasionally wearing glasses or smiling. Its modest size and
consistency have made ORL a favourite for testing subspace methods and
teaching students the basics [ 11 ].
FERET: Commissioned by the U.S. Department of Defense in the 1990s, the
FERET program amassed more than 14,000 images of 1,199 individuals. It
introduced a formal gallery/probe testing protocol and includes frontal shots
alongside variations in pose and expression. The sheer variety in FERET
makes it a rigorous proving ground for classical algorithms [ 12 ].
Extended Yale B: This dataset zeroes in on the challenges of lighting.
Twenty‑eight people were photographed under 64 different light sources and
nine head poses, producing a catalogue of how shadows and highlights distort
facial appearance. Researchers often use Extended Yale B to test algorithms
that explicitly model illumination [ 13 ].
CMU PIE: Short for Pose, Illumination and Expression, this collection features
68 subjects captured under 13 different poses, 43 lighting conditions and 4
facial expressions. By mixing pose and lighting changes, PIE exposes the
weaknesses of methods that assume well‑lit, frontal faces [ 14 ].
Labeled Faces in the Wild (LFW) [ 6 ]: In contrast to the controlled sets above,
LFW contains over 13,000 photos of nearly 6,000 people scraped from the
internet. Faces are centred but appear in countless poses, lighting conditions
and backgrounds. LFW popularized two tasks—verification (do these two
photos show the same person?) and identification (who is this person?)—and
quickly became the standard benchmark for face recognition “in the wild.”
MegaFace, YouTube Faces and CASIA‑WebFace: Introduced during the
deep‑learning era, these massive datasets contain millions of images and
thousands of identities captured from online photos and videos. They illustrate
how classical methods struggle to scale to very large galleries or to cope with
the extreme variation found in real‑world media [ 15 ].
```
**4.2 Evaluation Metrics**
Face‑recognition technology can work in two fundamentally different modes.
Verification is like checking a passport: the system compares a presented face
against a single stored template to answer “is this the same person?”
Identification is more like searching through a crowd: the system scans a gallery
of templates to find the best match. How we judge performance depends on
which task we’re doing.
a. **Verification Metrics**


In verification, the system generates a similarity score for a pair of images and
compares that score to a threshold to decide if they belong to the same person.
We care about several types of errors:

```
False Acceptance Rate (FAR): the percentage of impostor attempts that are
incorrectly accepted as genuine. It is also called the False Match Rate (FMR).
A low FAR is important when security is critical [ 7 ].
False Rejection Rate (FRR): the percentage of genuine attempts that are
wrongly rejected, also known as the False Non‑Match Rate (FNMR). A low
FRR ensures that authorized users aren’t inconvenienced [ 7 ].
Equal Error Rate (EER): the point on a DET or ROC curve where the FAR and
FRR [ 16 ] are equal. It provides a single number summarizing the trade‑off
between the two types of errors; lower EER indicates a better‑balanced system.
Receiver Operating Characteristic (ROC) curve: a plot of the false rejection
rate (1 minus the true positive rate) against the false acceptance rate across
different thresholds. The area under the curve (AUC) is often used as an
aggregate measure of performance—the closer to 1, the better [ 17 ].
Detection Error Tradeoff (DET) curve: similar to the ROC curve but plotted on
a logarithmic scale. It highlights differences in error rates when FAR and FRR
vary across several orders of magnitude, making it useful for high‑security
applications [ 18 ].
```
b. **Identification Metrics**

In identification, the system searches a gallery of known identities for the best
match. Metrics include:

Rank‑1 Accuracy: the proportion of probes whose top match is the correct
identity. Higher rank‑1 accuracy indicates better performance [ 19 – 21 ].
Cumulative Match Characteristic (CMC) Curve: shows the probability that the
correct identity is within the top **k** matches, for k = 1, 2, .... It provides insight
into how quickly the recognition rate improves as more matches are
considered [ 19 – 21 ].
Identification Rate at Specified FAR: the probability of correct identification at
a given false acceptance threshold. This metric is important for applications
where security constraints limit the acceptable FAR [ 7 ].
Regardless of the metric, researchers often report results on multiple datasets
to demonstrate the generalizability of their algorithms. Some metrics, such as
FAR and FRR, depend on the chosen threshold; thus, presenting ROC or DET


curves allows readers to visualize the trade‑off and select an appropriate
operating point for their application.

**4.3 Experimental Protocols**
To ensure fair comparisons, datasets typically specify training and testing splits
and provide pre‑processed face images. For example, LFW defines View 1 for
model selection and View 2 for testing; researchers report results using tenfold
cross‑validation. The Blended Face Recognition (BFR) evaluation protocol
defines open‑set identification tasks (where the probe may not appear in the
gallery) and includes metrics like False Discovery Rate. Some protocols
emphasize cross‐pose or cross‐illumination evaluation. When reporting results on
classical methods, it is important to follow the prescribed protocol and clearly
state pre-processing steps (e.g., histogram equalization, face alignment).

**4.4 Biometric Evaluation Metrics**
In biometric systems, evaluating recognition performance involves quantifying
both the accuracy and the error trade-offs. Two fundamental error metrics are:

False Acceptance Rate (FAR): The probability that an unauthorized (impostor)
subject is incorrectly accepted by the system.
False Rejection Rate (FRR): The probability that an authorized (genuine)
subject is incorrectly rejected.
These two rates are typically evaluated across different threshold values to
generate the Receiver Operating Characteristic (ROC) curve, which plots FAR
against True Acceptance Rate (TAR = 1 − FRR). The ROC curve offers a
comprehensive view of the system’s trade-off between security and convenience.
Another key metric is the Equal Error Rate (EER), the point at which FAR
and FRR are equal. A lower EER indicates better overall performance.
In verification scenarios (1:1 matching), ROC curves and EER are standard.
In identification scenarios (1:N matching), metrics like Rank-1 accuracy and
Cumulative Match Characteristic (CMC) curves are also used to assess the
likelihood of the correct identity appearing in the top-k matches.

###### 5 Strengths and Limitations of Classical Methods

Classical face recognition methods possess several strengths that made them
popular for decades. They are interpretable; for example, eigenfaces can be
visualized as images that highlight the most significant variations in the dataset.
Subspace methods are computationally efficient, enabling real‑time recognition
on early hardware. Local descriptors like LBP and Gabor features are resilient to


moderate illumination changes and capture fine details that generalize across
images [ 4 , 5 ]. Because classical methods do not require massive amounts of
training data, they are suitable for applications with limited labelled faces or for
educational purposes.
However, these methods also have limitations. Linear assumptions restrict
subspace methods such as PCA, LDA and ICA, making them less effective under
large nonlinear variations in pose and illumination. Alignment sensitivity is a
major issue; misaligned eyes or mouth positions degrade recognition accuracy.
Global features like eigenfaces incorporate background information and lighting
changes that may not be relevant to identity [ 9 ]. Local descriptors mitigate some
of these problems but are still susceptible to noise, occlusion and extreme pose
changes. Furthermore, classical methods struggle to scale to millions of identities
because distance computations over large galleries become expensive and the
features may not be sufficiently discriminative. These limitations paved the way
for the transition to deep learning, which automatically learns hierarchical,
nonlinear features from large datasets.

###### 6 Conclusion and Outlook

This chapter has reviewed the historical evolution of classical face recognition
methods, tracing their journey from early geometric techniques to powerful local
descriptors. These approaches laid essential groundwork for today’s deep
learning-based systems, not only by offering insights into facial representation
but also by inspiring many core principles of modern computer vision.
Looking forward, classical methods retain value in edge computing scenarios
and hybrid architectures, offering computational efficiency and interpretability in
environments where deep models may be impractical or opaque. By blending
traditional and modern techniques, researchers can build more efficient, ethical,
and adaptable face recognition solutions.

###### References

1. Turk, M., Pentland, A.: Eigenfaces for recognition. J. Cogn. Neurosci. **3** (1), 71–86 (1991). https:// doi.
    org/ 10. 1162/ jocn. 1991. 3. 1. 71
    [Crossref]
2.
    Sirovich, L., Kirby, M.: Low-dimensional procedure for the characterization of human faces. J. Opt.
    Soc. Am. A **4** (3), 519 (1987). https:// doi. org/ 10. 1364/ JOSAA. 4. 000519
    [Crossref]
3.
    Bartlett, M.S., Movellan, J.R., Sejnowski, T.J.: Face recognition by independent component analysis.
    IEEE Trans. Neural Netw. Publ. IEEE Neural Netw. Counc. **13** (6), 1450–1464 (2002). https:// doi. org/ 10.


1109/ TNN. 2002. 804287
4.
Sedaghatjoo, Z., Hosseinzadeh, H., Bigham, B.S.: Local binary pattern (LBP) optimization for feature
extraction (2024). arXiv preprint arXiv: 2407. 18665. https:// doi. org/ 10. 48550/ arXiv. 2407. 18665
5.
Jemaa, Y.B., Khanfir, S.: Automatic local Gabor features extraction for face recognition (2009). arXiv
preprint arXiv: 0907. 4984. https:// doi. org/ 10. 48550/ arXiv. 0907. 4984
6.
Liu, C., Wechsler, H.: Gabor feature based classification using the enhanced fisher linear discriminant
model for face recognition. IEEE Trans. Image Process. **11** (4), 467–476 (2002). https:// doi. org/ 10. 1109/
TIP. 2002. 999679
[Crossref]
7.
Jain, A.K., Ross, A., Prabhakar, S.: An introduction to biometric recognition. IEEE Trans. Circuits Syst.
Video Technol. **14** (1), 4–20 (2004). https:// doi. org/ 10. 1109/ TCSVT. 2003. 818349
[Crossref]
8.
Huang, G.B., Ramesh, M., Berg, T., Learned-Miller, E.: Labeled Faces in the Wild: A Database for
Studying Face Recognition in Unconstrained Environments
9.
Min, R., Hadid, A., Dugelay, J.-L.: Efficient detection of occlusion prior to robust face recognition. Sci.
World J. **2014** , 1–10 (2014). https:// doi. org/ 10. 1155/ 2014/ 519158
[Crossref]
10.
Belhumeur, P.N., Hespanha, J.P., Kriegman, D.J.: Eigenfaces vs. fisherfaces: recognition using class
specific linear projection. IEEE Trans. Pattern Anal. Mach. Intell. **19** (7), 711–720 (1997). https:// doi. org/

10. 1109/ 34. 598228
[Crossref]
11.
Samaria, F.S., Harter, A.C.: Parameterisation of a stochastic model for human face identification. In:
Proceedings of 1994 IEEE Workshop on Applications of Computer Vision, pp. 138–142 (1994). https://
doi. org/ 10. 1109/ ACV. 1994. 341300
12.
Phillips, P.J., Moon, H., Rizvi, S.A., Rauss, P.J.: The FERET evaluation methodology for face-
recognition algorithms. IEEE Trans. Pattern Anal. Mach. Intell. **22** (10), 1090–1104 (2000). https:// doi.
org/ 10. 1109/ 34. 879790
[Crossref]
13.
Georghiades, A.S., Belhumeur, P.N., Kriegman, D.J.: From few to many: illumination cone models for
face recognition under variable lighting and pose. IEEE Trans. Pattern Anal. Mach. Intell. **23** (6), 643–
660 (2001). https:// doi. org/ 10. 1109/ 34. 927464
[Crossref]
14.
Sim, T., Baker, S., Bsat, M.: The CMU pose, illumination, and expression database. IEEE Trans. Pattern
Anal. Mach. Intell. **25** (12), 1615–1618 (2003). https:// doi. org/ 10. 1109/ TPAMI. 2003. 1251154
15.
The Megaface benchmark: 1 million faces for recognition at scale. IEEE Conference Publication. IEEE
Xplore. https:// ieeexplore. ieee. org/ document/ 7780896. Accessed 14 Sept. 2025
16.
Jonathon, P.: An Introduction to Evaluating Biometric Systems (2000)
17.
Fawcett, T.: An introduction to ROC analysis. Pattern Recognit. Lett. **27** (8), 861–874 (2006). https:// doi.
org/ 10. 1016/ j. patrec. 2005. 10. 010
[MathSciNet][Crossref]


18.
Martin, A., Doddington, G., Kamm, T., Ordowski, M., Przybocki, M.: The DET curve in assessment of
detection task performance. In: 5th European Conference on Speech Communication and Technology
(Eurospeech 1997), ISCA, pp. 1895–1898 (1997). https:// doi. org/ 10. 21437/ Eurospeech. 1997-504
19.
Alami, M.I., Ez-zahout, A., Omary, F.: Comparative study of person re-identification techniques based
on deep learning models. Информатика И Автоматизация **24** (3), Art. no. 3 (2025). https:// doi. org/ 10.
15622/ ia. 24. 3. 9
20.
Alami, M.I., Ez-Zahout, A., Omary, F.: Enhanced people re-identification in CCTV surveillance using
deep learning: a framework for real-world applications. Inform. Autom. **24** (02), 583–603 (2025). https://
doi. org/ 10. 15622/ ia. 24. 2. 8
[Crossref]
21.
Alami, M.I., Ez-zahout, A., Omary, F.: Impact of batch size on stability in novel re-identification model.
IAES Int. J. Artif. Intell. IJ-AI **14** (4), 2724–2733 (2025). https:// doi. org/ 10. 11591/ ijai. v14. i4. pp2724-
2733
[Crossref]
_OceanofPDF.com_


(1)

(2)

© The Author(s), under exclusive license to Springer Nature Switzerland AG 2026
K. Slimani et al. (eds.), _Emotion and Facial Recognition in Artificial Intelligence: Sustainable
Multidisciplinary Perspectives and Applications_ , Information Systems Engineering and Management
78
https://doi.org/10.1007/978-3-032-14778-3_4

## Integrating Acoustic Feature Extraction

## and LSTM Models for Emotion

## Classification in Speech

Charan Kumar Nunna^1 and Divya Meena Sundaram^2
School of Computer Science and Engineering, VIT-AP University,
Amaravati, India
School of Computer Science and Engineering, Vellore Institute of
Technology, Vellore, India

```
Divya Meena Sundaram
Email: divyameena.s@vit.ac.in
```
**Abstract**
Speech emotion recognition is a practice of integrating acoustic
characteristic of a spoken speech so as to identify and label the emotional
disposition of the speaker. In the present paper, an end-to-end architecture
of the speech emotion recognition is going to be presented using the
RAVDESS data set. This methodological integration includes natural
features extraction, high quality of audio processing, smart data
augmentation, and deep learning that enabled the successful categorization
of emotional speech signals. In this paper, the descriptions of the
arrangements performed on its RAVDESS dataset to arrange it by actor in
actor-specific subdirectories and the cautious readiness of the audio files are
presented as the basis of the paper. The key characteristics are obtained
through the assistance of Librosa library; they are provided to be zero-
crossing rate, root mean square energy, Mel spectrograms, and Mel-
frequency cepstral coefficients (MFCCs). The results show that the


combination of the traditional signal processing approaches and the modern
neural networks architecture, like Long Short Term Memory (LSTM) based
models, contributes to the high accuracy speech emotion recognition.

**Keywords** Speech emotion recognition – Acoustic feature extraction –
Spectrograms – Long short-term memory – Mel-frequency cepstral
coefficients

##### 1 Introduction

One of the fast-developing areas in affective computing is speech emotion
recognition. It enables machines to comprehend human emotional states
and express them based on voice [ 1 ]. This technology has the potential to
improve interactions between humans and computers, customize virtual
assistants, support mental health diagnostics, and adapt learning systems.
Since humans rely heavily on emotional signals in ordinary communication,
making machines able to recognize them will make machines more
empathetic and contextual [ 2 ].
Recent advancements in machine and deep learning have transformed
how emotion is identified. Past systems relied on manual features and rule-
based methods. These are now being replaced with data-driven models that
can uncover complex patterns in large datasets [ 3 ]. This paper offers a
detailed description of speech emotion recognition using one of the most
well-known databases—the RAVDESS (Ryerson Audio-Visual Database of
Emotional Speech and Song) [ 4 ]. The continued high-quality audio
recordings of professional actors in RAVDESS, covering a wide range of
emotions, make it a preferred choice for building stable speech emotion
recognition systems.
Data preparation is the first step in the approach. The RAVDESS dataset
is arranged in actor-specific directories to simplify preprocessing. Later, the
Librosa library is used to extract acoustic features such as zero-crossing
rate, root mean square energy, Mel spectrograms, and Mel-frequency
cepstral coefficients (MFCCs). These features describe both the time-
varying and spectral properties of speech, providing a snapshot of
emotional cues. To account for inconsistencies in real-world audio, the
framework applies data augmentation techniques such as noise injection,
pitch shifting, time stretching, and signal shifting. These methods create


varied forms of the original recordings, simulating different acoustic
conditions and introducing diversity to the training set. This helps the deep
learning model generalize better to realistic sounds.
The extracted features are then fed into a deep neural network. The
architecture combines convolutional layers, which capture local features,
with a bidirectional LSTM, which models long-term temporal relationships.
This hybrid model effectively captures the dynamics of speech signals and
classifies audio inputs into eight emotion categories. It provides a deeper
analysis of the speaker’s emotional state. The framework can also leverage
parallel processing during feature extraction to improve computational
efficiency and scalability, making it more feasible for real-world
applications.
In short, this study proposes a complete pipeline that combines
advanced signal processing, robust data augmentation, and progressive deep
learning methods for effective speech emotion recognition. Using the
RAVDESS dataset and this methodology, the study contributes
meaningfully to affective computing research and brings us closer to
systems that can understand and respond to human emotions.

##### 2 Literature Review

New breakthroughs in deep learning and signal processing have greatly
advanced the area of speech emotion recognition. In early research, the
focus was on hand-engineered features such as Mel-frequency cepstral
coefficients (MFCCs), zero-crossing rates, and other spectral
characteristics. These were extracted from speech signals and then
classified using common methods such as support vector machines (SVMs)
or Gaussian mixture models (GMMs) [ 5 ]. Although these approaches laid
the foundation for emotion recognition, they failed to capture fine time-
scale variations and the context-dependent nature of emotions.
The invention of deep learning changed this paradigm. Spectrograms
combined with Convolutional Neural Networks (CNNs) have been
successful in learning spatial hierarchies of features. Recurrent Neural
Networks (RNNs), particularly Long Short-Term Memory (LSTM)
networks, have been effective in modeling temporal patterns in speech [ 6 ].
Hybrid CNN-Bidirectional LSTM architectures yield better results by
capturing local spectral trends together with long-term temporal


connections [ 7 ]. This combination has led to significant improvements in
emotion classification accuracy on datasets such as RAVDESS.
Despite these encouraging developments, several research gaps remain.
One major limitation is the reliance on relatively small, controlled datasets.
Many datasets lack diversity, as they are often created in artificial
conditions. This restricts the robustness and generalization of models when
applied to real-world scenarios, where factors such as background noise,
accents, and recording conditions can heavily affect performance [ 8 , 9 ].
Another issue lies in data augmentation. Most studies use simple
techniques such as noise injection, pitch shifting, time stretching, and signal
shifting. However, little research has explored systematic methods to
combine and optimize these techniques [ 10 ]. Using them in isolation often
limits the model’s adaptability and can reduce performance in unforeseen
scenarios [ 11 ]. Thus, current augmentation practices lack a holistic
framework for enhancing data effectively.
Computational efficiency and scalability also remain central challenges.
Traditional pipelines process audio files sequentially, which is time-
consuming when handling large datasets. Although some studies have
experimented with parallel processing, comprehensive application of such
techniques in both feature extraction and model training is rare. This limits
the feasibility of deploying emotion recognition systems in real-time
applications [ 12 , 13 ]. Another important but underexplored area is
interpretability. Deep learning systems often act as black boxes [ 14 ]. They
achieve high accuracy but provide little insight into how decisions are
made. This lack of transparency is problematic in domains such as
healthcare or service delivery, where the reasoning behind predictions is
crucial [ 15 ].
Our proposed research addresses these gaps with an integrated
approach. First, we adopt a systematic data augmentation strategy. Instead
of using techniques individually, our framework combines noise injection,
pitch shifting, time stretching, and signal shifting in a structured way. This
creates more diverse and realistic training data, making the models more
robust. Second, we employ parallel processing for feature extraction. By
leveraging multi-core processors, our system minimizes computational
overhead, scales efficiently to large datasets, and significantly reduces
training and deployment time. This makes real-time execution more
practical [ 16 ]. Third, our framework introduces a hybrid model composed


of convolutional layers and Bidirectional LSTMs. This design captures both
complex frequency patterns and temporal information, allowing the model
to learn robust representations of emotional speech. As a result, it achieves
state-of-the-art performance on the RAVDESS dataset.
Finally, our framework improves interpretability. By combining
systematic feature extraction with visualization techniques, we provide
valuable insights into how the model reaches its decisions. This
transparency is critical for building trust in automated speech emotion
recognition systems, especially in sensitive applications. Overall, our
approach not only enhances performance but also addresses key challenges
of data diversity, computational efficiency, and model interpretability.

##### 3 Proposed Methodology

It describes a holistic architecture of the speech emotion recognition in
voice that would begin with data collection and preprocessing, be followed
by extraction and augmentation of features, and conclude by the derivation,
training, and testing of a deep learning model. In the following pages, the
methodology that is used is explained in details, providing a description of
each stage of the research process.

**3.1 Data Acquisition and Setup**

In our work, the collection of RAVDESS (Ryerson Audio-Visual Database
of Emotional Speech and Song) with quality and well-annotated recording
and the diversity of shown emotions served as base of our research. It was
downloaded using special interface KaggleHub that makes it reproducible
and simple to get the source material. On the download page the data was
made available in directories by actor complete with a number of audio
files. Such an organization is necessary because it will provide a way to
traverse the dataset systematically and process all available audio samples
accordingly. Then a target folder (in this case, /content/ravdess_dataset) was
created and the data was moved in it. This will alleviate the dataset away to
be processed hence ensuring a controlled environment which file paths can
be well dealt with. The code lists and checks the directories to ascertain the
integrity of the dataset and to make sure there is a number of folders, as one
bit of assessment on completeness before the actual work starts on the


dataset. Figure 1 presents the count of emotions in the RAVDESS dataset,
which highlights the dataset’s coverage of diverse emotional categories.

**_Fig. 1_** Distribution of emotions in the RAVDESS dataset, showing the count of audio samples across
eight emotion categories

**3.2 Audio Preprocessing**

The data is preprocessed before the feature extraction is performed to
ascertain that information in raw files is in an analyzable format. Librosa is
a widely used Python library to perform audio analysis and all the audio
files can be read by this library. A variety of steps are significant during the
procedure of preprocessing.

**File organization and filtering** : The system goes through each folder and
reads each file, and hence it retrieves the emotion label based on the file
name. The file naming convention of RAVDESS also codes the label of an
emotion as the third item in case the name is segmented by hyphens. The
files that are not created following the required naming structure are not
examined and thus, only valid data is processed.

**Silence and Normalization** : Every audio file then gets loaded and its
silence parts cut off with the help of Librosa trimming functions. This
makes it so that the analysis is done on the signal part that contains the


emotional content. In addition, normalization of the data where required is
also done so as to ensure that there is consistency among the samples.
Figure 2 illustrates a trimmed raw audio sample, which represents the
cleaned waveform after preprocessing.

**_Fig. 2_** Example of a trimmed raw audio sample from the RAVDESS dataset, showing the waveform
after removing silence segments for preprocessing

**Debugging with Visualization** : In order to have a check of the accuracy of
the preprocessing that has taken place, a couple of plots are generated.
Those are the waveforms of both the raw and trimmed audio, zoomed-in
sections to take a closer look, and spectrograms. Besides confirming the
efficiency of preprocessing, the use of such visualizations provides an
insight into the acoustics of the provided samples of speeches. Figure 3
shows a magnified portion of the raw audio signal, which highlights subtle
waveform details used in feature extraction.


**_Fig. 3_** Zoomed-in view of a raw audio waveform from the RAVDESS dataset, highlighting finer
temporal variations in the speech signal for detailed analysis

**3.3 Feature Extraction and Augmentation**

The feature extraction and feature augmentation stage is also a critical
component of the workflow where we train audio data to measure the
machine learning model as far as speech recognition is concerned. The
initial layer is the extraction of meaningful features based on raw audio data
which contains a lot of acoustic features. These are attributes such as Zero-
Crossing Rate (ZCR) which is the amount of times that a signal crosses
between positive and negative position and gives information as to the
frequency contents and noise of the signal. The second helpful tool, Root
Mean Square Energy (RMSE), measures the general intensity or the
loudness of the signal audio as time flows. The Mel Spectrogram that shows
the range of frequencies composing a sound in a graphical form is
especially convenient because it corresponds to how human ears perceive
the sound. The given feature gives a closeout to the allocation of energy of
the signal in various frequencies and time. They are otherwise known as
MFCCs, and in turn are calculated on the basis of the Mel Spectrogram and
give a concise squashing of the spectral envelope of a talk signal. Speech
recognition MFCCs are common because they provide the most useful
information of a human speech and eliminate other non-essential data.


Figure 4 illustrates the Mel spectrogram of a RAVDESS audio sample,
highlighting how frequency components vary across time.

**_Fig. 4_** Example of a Mel spectrogram generated from a RAVDESS audio sample, showing the
distribution of frequency components over time after applying the Mel filter bank
Other extractable features are spectral centroid that indicates the
position of the center of mass of spectral power distribution and spectral
flux which quantifies the rapidity of the change of power spectrum of a
signal. One can also extract the formant frequencies which are the resonant
frequencies of the vocal tract to give a measure of the phonetic contents of
speech. The features extract various aspects of audio including noise,
intensity, aspects of spectral properties, and aspects of temporal properties.
When combined, features outline various cues of the speech, and this is in a
closer resemblance to the human hearing system and therefore useful in
training and implementation of other machine learning systems whose task
entails recognition and comprehension of the speech as patterns.
In order to strengthen the competence and universalization potential of
the machine learning model, the original audio samples are subjected to
data augmentation strategies. The latter methods artificially increase the
size of the data by generating altered versions of the current samples, in
addition to exposing the model to a more comprehensive set of possible
variations in speech as it may bear in practice. Noise augmentation This


may be the most common form of augmentation, namely adding noise of
various kinds to clean speech samples. This mimics the diversity of
acoustics such as cafes, streets, or an office, that puts the model more
resistant to the background noise. With the noise type and level
manipulated, it is possible to develop a wide range of augmented samples.
Another augmentation process which is very important is pitch shifting
which shifts the fundamental frequency of the speech signal. This method
simulates fluctuations in the intonations of a speaker considering the role
that some differences in the pitch may have as a result of gender, age, or
emotions. By being subjected to transmission of several pitch-shifted copies
of the same utterance, the model is in turn trained to extract the content of
speech, despite the change of the pitch.
Different speech rates are considered by means of time stretching. It is
used as a means of speeding up or slowing down an audio without varying
the pitch. It aids the model to adjust to users who speak with varying
speeds, either, being fast (swift) or, being slower and measured speech.
Signal shifting creates variations in time by shifting all the sounds of the
audio signal either forward or backward in time. That aids in making the
model insensitive to the absolute timing of the speech events within the
audio frame, which makes it better at recognizing speech even when there
were minor time disparities between speech events. Figure 5 shows
examples of audio augmentation, where noise addition and pitch shifting
are used to simulate real-world acoustic conditions.

**_Fig. 5_** Examples of data augmentation applied to RAVDESS audio samples: **a** audio signal with
added background noise, and **b** audio signal with pitch shifting, used to increase variability and
robustness of the dataset

Other augmentation methods can involve volume perturbation that
models change in recording level or speaker location, room impulse
response that employs reverberation delays to show the appearance of


various acoustic surroundings. Through the implementation of these
augmentation strategies, the data would be extended and varied sufficiently.
A single audio sample can have many augmented copies, which means that
the training material will be expandable and more diverse. When subjected
to the same feature extraction pipeline, this augmented dataset retains a
consistent feature space and, in combination with dramatically increasing
the model capabilities to process a wide variety of inputs with different
speech, serves as a method to significantly increase the strength of the
model.
The use of the extensive feature extraction and data augmentation leads
to a very strong basis to train the speech recognition models. The benefit of
this strategy is not only an increase in the effectiveness of the model on the
expanded test input set to a wider scope of real-world speech but also the
better overall generalization, being robust to variability in speaker
attributes, acoustics and recording conditions. Consequently, the trained
model is more capable of managing the nuances and variations associated
with human speech when it comes to practical application. Since the
process of extracting features of hundreds of augmented audio samples is
rather computationally demanding, the methodology has had to incorporate
parallel processing to increase efficiency. The joblib library is applied to
carry out the processing of feature extraction through all accessible CPU
cores. All the audio files are processed with its feature extracted by a
process each, therefore, reducing the overall running time. The parallel
processing strategy will play a critical role in scalability because it will
allow the framework to render bigger datasets and more sophisticated
augmentation schemes without being a bottleneck.

**Model Architecture**

The architecture of the model of the presented speech emotion recognition
system is built based on the combination of the elements of convolutional
and recurrent neural networks to process the speech signal efficiently and
classify it on the basis of the extracted emotion. The convolutional layers
identify local structures in the spectral representation of audio; they identify
the key aspects such as formants and harmonic structure. It is then subjected
to max pooling and dropout layers so as to diminish the information and
focus on the leading features and avoid the over translation of information.


Backwards-forwards LSTM cell layers have the capability to make long-
range temporal relations in processing an input sweep in both directions and
thus they present a more in-depth understanding of temporal coherence
dynamics as necessity in emotion classification.
The final level of the model part is the dense layers which are fully
connected and assemble the characteristics of the previous level. Softmax
activation is used on its output layer to give the probability values of eight
classes of emotions, hence one can easily make predictions. The
implementation of Adam optimizer and a loss discipline such as category
cross entropy would be used to optimize this type of a hybrid architecture
and help us to ensure that we do not lose out on smaller details in the
spectrum and overall dynamics in sounding the speech. The use of
convolutional and recurrent layers forms a strong network that can detect
the emotional words in speech using a high recognition rate.

**3.4 Training and Evaluation**

The extracted features and the enhanced features are added to the training
set to ensure that it is enhanced. The processing of the data is organized to
be done in batches, where the set size is 128 deciding on the empirical
observations. This is a compromise of the utilization of memory and
computational speed and enables a stable and fast convergence. The
validation set is applied to ensure that the model is not overfitting and is
checked in the middle of the training, by checking its performance. Training
accuracy and loss as well as validation accuracy and loss are also monitored
giving a chance to fine tune the hyper parameters in case it is necessary.
Displays of accuracy and loss as a function of the training steps provide
good perspectives on the model training. These visualizations play a
paramount role in determining any problem like overfitting or under fitting
and help in further improvement of the architecture or training protocol.
The last model is stored to be deployed and analyzed at a later stage after
training. Such a saved model can be used to categorize audio samples that it
is not aware of, hence it can be applied to real-time applications in the field
of emotion detection.

##### 4 Results and Discussion


The results section presents the detailed overview of the experimental
results, with a number of significant elements of the given study. It starts
with the description of the dataset features and preprocessing procedures
covering how the information was structured and ready to be used in
training. The section proceeds to review the methods used to conduct
feature extraction and data augmentation that prioritize the need to
emphasize on the model to enhance its capability to pick the emotional
connotation in speech. The behaviour of the models is studied with the help
of several evaluation tools, including accuracy and loss, giving a clear view
on predictive abilities of the developed system. A review of the
computational efficiency is also carried out, especially in regards to the
scalability and performance of the system when used in real-time.
Moreover, a comparative study of the existing approaches offered in the
literature is also made, the advantages of the offered model are pointed out,
and the directions which should be enhanced are identified.
The experimental data explained the fact that the defined CNN + LSTM
based architecture is efficient in the framework of speech emotion
recognition. We had training and validation data of 20 epochs on the model
too. Training and validation accuracy had continuously increased over the
epochs, with the last one above 92% and 89%, respectively, as shown in
Fig. 6. At the same time, the loss of the model decreased steadily, which
means that it learned well and did not overfit.

**_Fig. 6_** Training and validation performance of the proposed speech emotion recognition model on
the RAVDESS dataset, showing accuracy and loss trends over multiple epochs

In evaluating the generalization and robustness, we used a number of
important performance measures: accuracy, precision, recall, and F1-score


applied on the model. These measures were contrasted to the ones of the
base models, such as SVM, CNN, GRU, and Random Forest classifiers.
Results indicated, Table 1 , that the proposed CNN + LSTM architecture was
the best performer reporting the highest value of accuracy (90.6%),
precision (91.1%), recall (90.3%), and F1-score (90.7%).

**_Table 1_** Comparative performance of models on RAVDESS dataset

**Model Accuracy (%) Precision (%) Recall (%) F1-Score (%)
SVM** 72.5 71.2 70.9 71.0
**CNN** 85.3 84.7 84.2 84.4
**GRU** 84.1 82.9 83.0 82.9
**Random forest** 77.8 76.0 75.5 75.7
**CNN + LSTM (ours)** 90.6 91.1 90.3 90.7
The act of combining rich acoustic features (MFCCs, Mel spectrograms,
ZCR, RMSE), extensive data augmentation measures (noise, pitch, time and
shift) along with the combination of spectral and temporal aspects through
the hybrid neural architecture are credited with the observed gains. In
addition, parallelized feature extraction saved much preprocessing time and
the framework could be applied to a scalable and real-time application.
Efficiently, the outcomes establish the fact that combining the traditional
signal processing and deep neural modeling provides a flexible, adaptive,
and extremely precise system of speech emotion recognition that has
surpassed known methods both in functional capacity and flexibility.
In spite of such encouraging findings, some limitations still exist.
Despite being useful, the RAVDESS dataset is recorded in laboratory
conditions and may, therefore, fail to replicate the full nature of the range of
variability that may be witnessed in the real world. In a future study the
consideration of the pipeline run on a broader range of different noisy
datasets should be taken into consideration. Also, due to the complexity of
deep neural networks, using machine learning, it may not be feasible to
determine the effects of specific features therein. The explanation of the
model by offering some post-hoc or attention can give additional
information in the decision-making process.

##### 5 Conclusion and Future Work


In this research work, a unified and robust system of speech emotion
recognition has been proposed using superior acoustic feature extraction,
wide coverage of data augmentation methods, and a combined system of
CNN layers with bidirectional LSTM layers expressing a hybrid deep
learning pipeline without any shortcomings. The system produced high
accuracy in classification tasks by using RAVDESS dataset, and at the same
time it was efficient by using parallel processing. The addition of the
variety of augmentation methods increased the generalization ability of the
model and in turn offers more resistance to acoustic variability in the real
world. What is more, comparative tests revealed that the suggested model is
better than conventional classifiers and stand-alone neural networks with a
specific focus on accountability of not only the spectral, but also of the
temporal activity of the emotional speech. ROC curves and confusion
matrix proved the relatively uniform recognition of expressions in the wide
range of emotions with few false classifications. Even though there is still a
difference between the highest and actualized performance parameters, the
outcomes confirm the practical efficiency of the framework in use.
Future studies could explore real-time application to the embedded
system, and scale up the data to multilingual and spontaneous speech, and
incorporate attention mechanisms onto the model in order to be able to
interpret the output better. The identity of human emotion is discussed by
way of speech, facial expression, body language and physiological
expressions. Additional spatial and textual data, such as an audio file with
visual and text data, have the potential to create a more detailed picture on
emotional states and contribute to better accuracy and situational awareness
of an speech emotion recognition system. One of the good ways is multi-
modal aggregation. The future work should strive to produce a model that
can be run on the edge devices and that the model can be massively
lightweight and efficient, yet not at the cost of the accuracy.

##### References

1. Hassan, A., Masood, T., Ahmed, H.A., Shahzad, H.M., Tayyab Khushi, H.M.: Benchmarking
    pretrained models for speech emotion recognition: a focus on Xception. Computers **13** (12), 315
    (2024)
2.
    Xu, C., Liu, Y., Song, W., Liang, Z., Chen, X.: A new network structure for speech emotion
    recognition research. Sensors (Basel) (2024)


3.
Singh, A. et al.: Analyzing the recent advancements for speech emotion recognition using
machine learning techniques. Afr. J. Biomed. Res. **27** (4S) (2024)
4.
Livingstone, S.R., Peck, K., Russo, F.A.: RAVDESS: the Ryerson audio-visual database of
emotional speech and song. In: Annual Meeting of the Canadian Society for Brain, Behaviour
and Cognitive Science, pp. 205–211 (2012)
5.
Mol George, S., Ilyas, P.M.: A review on speech emotion recognition: a survey, recent advances,
challenges, and the influence of noise. Neurocomputing **568** , 127015 (2024)
[Crossref]
6.
Rajapakshe, T., Rana, R., Riaz, F., Schuller, B.: Parameterized Quantum Circuits for Novel
Representation Learning in Speech Emotion Recognition (2025) [Preprint]
7.
Shi, H., Zhang, X., Cheng, N., Wang, J.: Enhancing Emotion Recognition in Conversation
Through Emotional Cross-Modal Fusion and Inter–Class Contrastive Learning (2024) [Preprint]
8.
He, Z.: Research Advanced in Speech Emotion Recognition Based on Deep Learning (2025)
9.
Kingeski, R., Henning, E., Paterno, A.: Fusion of PCA and ICA in Statistical Subset Analysis for
Speech Emotion Recognition (2024)
10.
An, Y., Kolanupaka, S., An, J., Smith, B.: Is the Lecture Engaging for Learning? Lecture Voice
Sentiment Analysis for Knowledge Graph–Supported Intelligent Lecturing Assistant (ILA)
System (2024)
11.
Hamza, H., Gafoor, F., Sithara, F., Anil, G., Anoop, V.S.: EmoDiarize: Speaker Diarization and
Emotion Identification from Speech Signals using Convolutional Neural Networks (2023)
[Preprint]
12.
Singh, A., Gupta, A.: Decoding Emotions: A Comprehensive Multilingual Study of Speech
Models for Speech Emotion Recognition (2023) [Preprint]
13.
Wu, H., Chou, H.-C., Chang, K.-W., Goncalves, L., Du, J., Jang, J.R., Lee, H.-Y.: EMO–
SUPERB: An In-depth Look at Speech Emotion Recognition (2024) [Preprint]
14.
Choi, J. et al.: Speech Emotion Recognition Systems and Their Security Aspects (2024)
15.
Patamia, R.A., Santos, P.E., Acheampong, K.N., Ekong, F., Sarpong, K., Kun, S.: Multimodal
Speech Emotion Recognition Using Modality–Specific Self–Supervised Frameworks (2023).
arXiv preprint arXiv: 2312. 01568
16.
Abdelhamid, A.A., El-kenawy, E.M., Alotaibi, B., Eid, M.: Robust Speech Emotion Recognition
Using CNN+LSTM Based on Stochastic Fractal Search Optimization Algorithm (2022)
_OceanofPDF.com_


(1)

(2)

© The Author(s), under exclusive license to Springer Nature Switzerland AG 2026
K. Slimani et al. (eds.), _Emotion and Facial Recognition in Artificial Intelligence: Sustainable
Multidisciplinary Perspectives and Applications_ , Information Systems Engineering and Management
78
https://doi.org/10.1007/978-3-032-14778-3_5

## Emotion Detection in Human-Machine

## Interaction Using ML Techniques

R. Karthick Manoj^1 and S. Aasha Nandhini^2
Department of Electrical and Electronics Engineering, AMET Deemed
to be University, Kanathur, India
Department of Electronics and Communication Engineering, Sri
Sivasubramaniya Nadar College of Engineering, Kalavakkam, India

```
R. Karthick Manoj
Email: Karthickmanoj.r@gmail.com
```
**Abstract**
This chapter offers a comprehensive examination of automated human
emotion recognition in the context of human–machine interaction. The
objective is to evaluate the efficacy of machine learning and deep learning
in detecting emotions in words, textual content, and facial images. To test
and assess convolutional neural networks for vision, Bi-LSTM models for
audio, and transformer encoders (BERT) for text, we employed benchmark
datasets (FER-2013, CK+, RAVDESS, TESS, ISEAR, and AffectNet).
Additionally, using hybrid early-late fusion, we enhanced a multimodal
FusionNet that integrates many modalities. FusionNet achieves an accuracy
of 94.5% and a macro-F1 score of 0.92 in testing, surpassing unimodal
baselines by up to 9%. Research utilizing confusion matrices indicates that
the system is proficient at identifying nuanced emotions such as fear and
disgust. User evaluations indicate that 88% of individuals concur with the
system's forecasts. Future initiatives must prioritize explainable AI to
improve transparency, cross-cultural generalization to reduce demographic
bias, and the integration of physiological data (e.g., EEG, heart rate) for


more thorough multimodal inference. These principles will facilitate the
safe, ethical, and efficient utilization of emotion-aware systems in
healthcare, education, and interactive technology.

**Keywords** Emotion detection – Human–machine interaction – Machine
learning – Deep learning – Facial recognition – Speech emotion recognition

- Multimodal systems – Artificial intelligence

##### 1 Introduction

According to Moerland et al. [ 1 ], the ability of robots to understand and
respond to human emotions has gone from being a theoretical notion to a
technologically mature area of active research in the last few years. This is a
major step forward in what we know about how individuals feel. This
capacity, also known as affective computing or emotional detection, has
helped a lot to improve the human–machine interface (HMI). As digital
technology becomes increasingly widespread in everyday life, robots that
can comprehend emotions will become more vital. This standard includes a
variety of various aspects, such safety, education, entertainment, and
customer service. These algorithms must possess the capability to
comprehend human speech and behavior in relation to emotional states.
This might result in meetings that are more beneficial, easier to understand,
and more focused.
Emotion is a multifaceted psychological and physiological condition
that influences human perception, cognition, and action. It may be
expressed through several modalities: facial expressions, vocal tone, body
language, and both written and spoken content. In contrast to traditional
computing systems that rely solely on logical or factual data, emotionally
aware systems aim to interpret these intricate signals and utilize them to
provide appropriate responses [ 2 ]. Such technologies can transform a
simple user interface into a flexible, context-sensitive assistant capable of
fostering trust and engagement. Emotion detection systems are mostly built
with machine learning (ML). Machine learning methodologies enable
computers to identify patterns throughout extensive datasets and provide
intelligent predictions or classifications without the need for explicit
programming for each scenario [ 3 ]. Initial emotion identification tasks have
demonstrated significant efficacy for traditional algorithms such as Support


Vector Machines (SVM), k-Nearest Neighbors (k-NN), and Decision Trees,
especially when integrated with manually crafted features from audio,
visual, or textual data. These models often falter when confronted with
extremely variable, large-scale, high-dimensional factors that characterize
real-world emotional expression.
Deep learning, particularly models such as Convolutional Neural
Networks (CNNs), Long Short-Term Memory (LSTM) networks, and
Transformers, has radically transformed the domain of emotion recognition
[ 4 ]. These systems greatly increase the accuracy and resilience of emotion
detection systems by automatically learning hierarchical characteristics
from raw data and being able to predict complicated temporal and
geographical connections. While LSTMs and Transformers are well-suited
for voice and text-based emotional detection, managing the sequential
structure of audio signals and linguistic information, CNNs shine in
evaluating facial expression data, catching small changes in muscle action.
Usually, emotion detection follows a disciplined pipeline of data collection,
preprocessing, feature extraction, classification, and post-processing or
decision fusion, an efficient emotion detecting system.
The performance of a model depends much on the dataset used. Often
used as benchmarks in the field, publicly available datasets including FER-
2013 (Facial Expression Recognition), CK (Cohn-Kanade), RAVDESS
(Ryerson Audio-Visual Database of Emotional Speech and Song), and
AffectNet provide labeled examples of various emotional states [ 5 ]. These
databases provide a broad spectrum of emotional expressions across many
populations and contexts: happy, sorrow, rage, fear, contempt, surprise, and
neutrality. Important processes that greatly affect the capacity of the model
to learn meaningful patterns are data preparation and feature extraction.
Normalization, noise reduction, face alignment, and audio feature extraction
(such Mel-frequency cepstral coefficients or MFCCs) are all common ways
to normalize input data and reduce unneeded variation [ 6 ]. Advanced
models may enhance feature relevance by employing domain-specific
embeddings or attention mechanisms.
Even with significant advancements, it is still a difficult task to
understand how another person is feeling. Data imbalance is a significant
issue; some emotions, such as happiness and neutrality, are overrepresented
in training data, but other emotions, such as fear or disgust, which are more
subtle or culturally suppressed, show up less frequently in training data [ 7 ].


It is possible that this discrepancy will result in models that are less
accurate, which would be unsatisfactory to groups that are not well
represented. Inter-subject variability, which refers to the fact that different
people react differently to the same stimuli owing to factors such as age,
gender, ethnicity, and personality, is a problem that continues to be a
problem. When it comes to real-time applications, something that makes
things a lot more challenging is the requirement for both low-latency
processing and excellent accuracy. Some examples of models that are
capable of accomplishing this in the actual world are chatbots and robotic
help, both of which are able to detect emotions in milliseconds and maintain
low costs. A great number of individuals are discussing lightweight designs,
model pruning, and hardware acceleration technologies like as graphics
processing units (GPUs) and edge-area artificial intelligence devices as
potential solutions to fulfill this demand.
Multimodal emotion detection is another trend that is growing in this
sector. It uses many forms of data, such as facial expressions, noises, and
text, to make classification more accurate and trustworthy [ 8 ]. Multimodal
models can aid when one modality isn’t working well. For example, verbal
cues can still tell you how someone feels even when face recognition
doesn’t work well in low light. Fusion approaches may be divided into three
groups: early (feature-level), late (decision-level), and hybrid, which
combines data from various sources. Along with concerns with technology,
we need to think carefully about the moral issues that come with emotional
AI. If people don't know who is in charge of development and execution,
prejudice might get worse, user privacy could be violated, or individuals
could update systems to spy on others and find out how they feel.
One concern is that a poorly trained model may misinterpret emotions
in a culturally inappropriate way. Another potentiality is that a business will
use emotional data to simply target ads to people. These problems make it
especially important to be ethical, transparent, or to obtain consent to create
organizations that engage with people's feelings. Currently, the focus is on
sustainable AI. This is an beneficial to the model impact on the environment
and highlights the importance of not using an inordinate amount of power,
which will simplify inference pipelines, encourage carbon reduction
strategies, and alleviate the cost of running massive deep learning models
on processing units.


The analysis of emotions in the context of human-technology
interaction is an emerging and expanding field. There have been
advancements, as a result of machine learning and deep learning, and in
relation to multimodal data processing, enabling computers to more
accurately understand and react to people's emotions. Therefore, it would be
a more exact statement to say that machines are now better able to detect
and respond to human emotions. As systems that make use of these
technologies become more sophisticated and commonplace, the people who
build these systems still face multiple ethical, social, and technological
challenges. For the purposes of this chapter, we will analyze the various
machine learning models for assessing emotions, examine their
effectiveness, discuss theoretical and practical challenges, and in
conclusion, make recommendations for further work in this important area.

##### 2 Related Work

In the past five years, developments in machine learning (ML) have
advanced significantly. Understanding human emotions is protected today
than ever as advances in human–machine interactions (HMI). This section
discusses this development through a brief synopsis of research from
multiple disciplines, which focuses on the importance of ML in social
interaction recognitions, including the promise of HMI with actionable
solutions. A person’s face is the best indicator of their emotional state at a
given moment in time. Recently, using deep learning algorithms improved
the accuracy of facial emotion recognition (FER). Diwan et al. [ 9 ] released
a review article that extensively investigated and reviewed state-of-the-art
methods of machine learning techniques to autonomously detect human
emotions (AHER). The authors aim to report the use of diverse techniques,
including FER.
A recent study by Khare et al. [ 10 ] about state-of-the-art technology for
emotion recognition stresses the importance of deep learning approaches for
increasing detection through facial and body language analysis. Speech can
use tone, pitch, rhythm, and a combination of other factors to represent an
emotion in an individual. Karthik et al. [ 11 ] provide a detailed review of
work conducted using machine learning methods in speech emotional
recognition (SER) systems. Important parts of this work focused on data
processing, feature extraction, and voice classification. Geetha et al. [ 12 ]


conducted research that examined the effects of data augmentation methods
and classification techniques on the ability for the emotional recognition of
speech. They identified the combination of each method that produced the
most efficient method for recognized accuracy. A fundamental
understanding of human emotions may also be acquired by examining text
data, including chat conversations and social media posts.
Kuo et al. [ 13 ] conducted a thorough literature review of DL algorithms
for emotional detection in text. The study's findings show that transformer-
based models—like BERT—perform better than conventional machine
learning methods. The work makes it abundantly evident that we must
research low-resource languages and develop methods for rapidly
identifying emotions in textual input. Computer systems that use a wide
range of data types may make it easier for people to identify emotions. In
their article, Patel and Annavarapu [ 14 ] discuss a range of approaches to
understanding emotions and examine how integrating different senses could
aid in more precise identification. A multimodal emotional recognition
model that combines audio and facial inputs is presented in a recent work
[ 15 ]. The study shows that the model is more accurate than unimodal
systems.
Direct measurement of brain activity related with emotions is provided
by electroencephalography (EEG). Using Tsallis entropy as a feature for
enhanced classification, a recent work in Ab Wahab et al. [ 16 ] examines the
use of supervised machine learning models in EEG-based emotional
detection. The study emphasizes how well EEG signals may be used to
create passive brain-computer interfaces for emotional recognition. Useful
applications depend on the development of real-time emotional recognition
systems. Emphasizing the integration of face and voice modalities, a paper
in Salloum et al. [ 17 ] addresses the development of a multimodal intelligent
HMI system capable of real-time emotional recognition. Furthermore, a
paper in proposes a new emotion identification system for human–robot
interaction using EfficientNet with transfer learning to train convolutional
neural networks for real-time uses. In both healthcare and education,
emotional recognition technologies have major ramifications. Aiming to
improve learning experiences, a research investigates how artificial
intelligence may recognize and react to student emotions. As described in
many research, emotional recognition systems can help in healthcare to
monitor patient well-being and enhance human–robot interactions.


Significant challenges persist in affective recognition research, despite
advancements. Data asymmetry, cultural differences, and the need of real-
time processing remain main difficulties. Important challenges are also
ethical ones like privacy concerns and possibly artificial intelligence
framework bias. Future subjects of research include explainable artificial
intelligence models, sustainable, energy-efficient emotion recognition
systems, and low-resource settings’ transfer learning.

##### 3 Proposed Methodology

A comprehensive framework has been developed to capture, process, and
classify human emotional states using machine learning (ML) and deep
learning techniques in the proposed methodology for emotion detection in
human–machine interaction (HMI). Emphasizing the utilization of
multimodal data to improve accuracy and resilience, this part describes
every stage of the pipeline, from data collecting to model deployment.
Appropriate for adaptive systems in sectors like healthcare, education,
customer service, and smart environments, the approach enables both real-
time and offline applications.
Figure 1 Displays End-to-end FusionNet design showing the parallel
pipelines for processing audio, video, and text, with a late-fusion layer for
detecting emotions in several modes. It begins with data capture, which
includes collecting text data, physiological indications, facial expressions,
voice, and vocal cues, among other types of data. Data preprocessing
pipelines for every type of data send these sources through to clean,
standardize, and get them ready for analysis. In the feature extraction and
representation process, machine learning and deep learning approaches are
used to get spatial, temporal, and contextual aspects. During the Model
Architecture and Training stage, different types of models are used for
different types of data. For example, CNNs are used for facial expressions,
LSTMs are used for speech, and BERT is used for text. Multimodal fusion
makes classification stronger by merging information or judgments from all
streams. Next is model evaluation, which uses confusion matrices, the F1-
score, and accuracy to check the system. Finally, the trained and optimized
model is hosted in a Real-Time Implementation environment for practical
HMI usage.



**_Fig. 1_** End-to-end FusionNet architecture showing parallel vision, audio, and text pipelines with
late-fusion layer
This work used publically available sources including Kaggle for
datasets. These consist of Emotion-Stimulus and ISEAR for text-based
emotional annotation; FER-2013 and CK+ for facial expressions;
RAVDESS and TESS for speech. AffectNet and SEMAINE were used for
multimodal integration, therefore ensuring that the system is evaluated on a
spectrum of representative emotional data.

**3.1 Data Acquisition**

Emotion identification systems start with multimodal data collecting as
human emotions are communicated through several channels including
facial expressions, voice, text, and physiological signs. While speech cues
including pitch, tone, and rhythm are collected using microphones visual
data including facial expressions is gathered using cameras and image
sensors. Written materials include chat logs, transcripts, or user-generated
content offer textual data which provides insight into emotional intent from
which to draw. Though optional, physiological data including EEG or heart
rate offer further levels of emotional information and can be obtained using
biosensors. We propose to use a variety of well-known benchmark datasets
to guarantee the evolution of a strong and varied emotion identification
model. FER-2013 and CK+ give labeled visual emotion data for facial
expressions; extensive audio samples are available for speech-based
emotional recognition from datasets such RAVDESS, TESS, and Emo-DB.
While AffectNet and SEMAINE offer multimodal resources integrating
face, auditory, and occasionally textual inputs, text-based emotion data may
be obtained via databases such as ISEAR and Emotion-Stimulus. Usually
comprising happy, sadness, anger, fear, disgust, surprise, and neutrality,
these datasets are labeled with a standard set of emotional categories,
therefore providing the basis for model training and assessment.
Table 1 illustrates that no individual dataset fulfills all criteria for
comprehensive multimodal emotion identification. AffectNet and FER-
2013 are extensive, real-world datasets that offer diversity but are plagued
by class imbalance and label noise, while CK+ and RAVDESS give clean,
high-quality samples with restricted demographic variety. Text corpora
(ISEAR, Emotion-Stimulus) include sophisticated semantics yet exhibit
geographical biases. Collectively, these datasets provide complementary
advantages extensive coverage, high accuracy, and diverse modalities


justifying the necessity for data fusion, stratified sampling, and class
weighting in the proposed approach.

**_Table 1_** Benchmark datasets used for multimodal emotion detection

```
Dataset Modality Size Rationale for use Key limitations
FER-2013 Facial
images
```
```
35,000 Large-scale, diverse
expressions for CNN
training
```
```
48 × 48 low-resolution
images, class imbalance
```
```
CK+ Facial
images
```
```
593 High-quality posed
sequences for fine-grained
facial cues
```
```
Small size, lab-controlled,
limited spontaneity
```
```
RAVDESS Speech 1,440 clips Studio-quality emotional
speech with balanced
classes
```
```
English only, actor-
induced emotions
```
```
TESS Speech 2,800 clips Clear articulation across age
groups
```
```
Limited to older female
speakers
ISEAR Text 7,600
sentences
```
```
Well-annotated emotional
sentences for text modelling
```
```
European cultural bias
```
```
Emotion-
stimulus
```
```
Text 2,500
sentences
```
```
Context-rich textual
emotions
```
```
Smaller size, annotation
subjectivity
AffectNet Multimodal 1 M
images
```
```
Rich, in-the-wild facial
expressions
```
```
Imbalanced categories,
noisy web labels
SEMAINE Audio–
visual
```
```
150
sessions
```
```
Natural dyadic
conversations for fusion
testing
```
```
Complex annotations,
higher preprocessing
demand
```
**3.2 Data Preprocessing**

Raw multimodal data is gathered then preprocessed to improve quality and
guarantee consistency among several sources. Preprocessing for facial
pictures consists in face identification, alignment, resizing usually to 48 ×
48 pixels grayscale conversion, and normalizing. These processes
standardize input dimensions for deep learning models and assist to lower
noise. Rotation, flipping, and scaling are among the augmenting methods
used to vary the dataset and enhance model generalization. Noise filtering
and voice activity identification separate emotional speech segments in
audio processing. MFCCs, chroma vectors, and energy levels are extracted
to show emotional prosody. Preprocessing text requires tokenizing,
lemmatizing, and stop word deletion. Cleaned text is turned into numerical


vectors via Word2Vec, GloVe, transformer-based approaches include BERT.
These preprocessing actions guarantee that the classification pipeline gains
from every modality clean, relevant, interpretable qualities.

**3.3 Feature Extraction and Representation**

Feature extraction creates high-level representations that let us understand
emotional patterns in raw data. Convolutional neural networks (CNNs)
automatically learn spatial information from facial photos in the visual
modality. This allows them to find patterns that match certain emotions,
such as wrinkled brows, grins, or frowns. In the audio domain, sequential
models that use LSTM or GRU networks look at time-dependent speech
variables including MFCCs, pitch, and rhythm. This lets them capture vocal
subtleties that are linked to emotional states. BERT and RoBERTa are two
modern language models that provide rich contextual embeddings that help
you write in a way that shows your mood, tone, and intent. Dense brain
layers send these representations to improve qualities that are specific to
emotions. The algorithm creates a more complete and accurate emotional
representation by integrating learning embeddings with characteristics that
were made by hand.

**3.4 Model Architecture and Training**

The models for classification are designed to take use of the strengths of
each type of data. We use deep CNN models like VGGNet or ResNet to
look at the picture's attributes and figure out how it makes us feel. We
employ a softmax layer to figure out what emotions are on a person's face.
Bi-directional LSTM networks are used to look at audio data across time.
This enables speech emotion detection figure out how emotions vary in
speech. For emotion classification tasks, tagged text data is utilized to
improve transformer models like BERT in the text-based pipeline. We train
these models with a cross-entropy loss function and then improve them with
techniques like Adam or SGD. When training on datasets with labels,
stratified sampling is utilized to resolve class imbalance. Using
performance metrics like accuracy, F1-score, and loss curves over several
epochs helps make sure that the models converge and can be employed in
future contexts.

**3.5 Multimodal Fusion**


Multimodal fusion techniques combine data from several sources to
increase the accuracy and dependability of systems. When early fusion is
enabled, feature vectors from text, audio, and visual inputs can be combined
to create composite classifiers. This method uses a majority vote or a
weighted average to aggregate forecasts. In contrast, late fusion affects each
modality separately. The best aspects of both technologies are combined in
hybrid fusion systems. First, they generate intermediate predictions, and
then they employ ensemble models or meta-learners to refine those
predictions. Two examples of sophisticated frameworks that aid in our
understanding of how various types of data interact are Multimodal
Transformers and Fusion Neural Networks. This enables the system to
understand people's emotions even in situations when one mode provides
insufficient or inconsistent information. Fusion gives you a great deal of
control over the settings and allows you to build systems that can change
how things feel.

**3.6 Model Evaluation**

The assessment of the emotion detection system is primarily defined by
standard classification metrics, including accuracy, precision, recall, and
F1-score. This confusion matrix was developed to assist in identifying often
conflated emotions such as fear, surprise, sorrow, and neutrality.
Consequently, it is permissible to evaluate the model's efficacy throughout
the whole spectrum of emotions. Alternative cross-validation techniques,
such as K-fold validation, ensure the model's robustness and assist in
mitigating overfitting. In both optimal (clean) and practical (noisy or cross-
subject) circumstances, the decision-making process almost achieves
generalization across several contexts and diverse populations. In the
context of multimodal systems, both general and specialized performance
evaluations validate the contribution of each unique input channel. In
addition to ensuring the system operates correctly, the evaluation is
responsible for fostering innovative developments in preprocessing, feature
extraction, and fusion methodologies.

##### 4 Results and Discussion

This section outlines the outcomes of training and assessing several
machine-learning and deep-learning models utilizing facial images, audio,


text, and multimodal data. The numerical results are very important, but the
analysis that follows shows the differences in performance and emphasizes
the effects on real-world use.

**4.1 Model Performance Overview**

Table 2 shows the F1-scores and classification accuracy for all the machine-
learning and deep-learning models that were tested. In every case, deep
architectures consistently beat traditional methods. This shows that data-
driven representation learning is better at finding emotions. The ResNet-18
network got 89.3% correct on the FER-2013 dataset for recognizing facial
expressions, whereas the VGG-16 network got 92.1% correct and 0.90%
correct on the CK+ dataset. These improvements happen because
convolutional layers automatically find hierarchical spatial information,
such little eyebrow twitches, tiny lip movements, and shade changes, that
hand-made descriptors or shallow networks usually miss. Because CNNs
can pick up on small details in local structure, they can handle mild
variations in lighting and head attitude, which are both typical in real-world
video streams.

**_Table 2_** Performance comparison of emotion detection models across modalities

```
Modality Dataset Model used Accuracy (%) F1-score
Facial images FER-2013 CNN (ResNet-18) 89.3 0.87
Facial images CK+ CNN (VGG-16) 92.1 0.90
Speech RAVDESS Bi-LSTM 87.4 0.85
Speech TESS LSTM + MFCC90.2 0.88
Text ISEAR BERT 91.0 0.89
Text Emotion-stimulus Bi-GRU 88.6 0.86
Multimodal fusion AffectNet + RAVDESSFusionNet 94.5 0.92
```
The Bi-LSTM and LSTM + MFCC models got 87–90% of the speech
emotion identification right. Their recurrent designs are great at modeling
long-term relationships between pitch, energy, and spectral dynamics. This
kind of modeling makes it possible to tell the difference between emotions
with similar global pitch contours, like sadness and rage, by using more
subtle temporal clues. This capability is very crucial for contact center
analytics or telehealth apps, because background noise or speaker


fluctuation typically hides simpler acoustic data. BERT got 91% accuracy
and an F1 score of 0.89 for text-based emotion recognition, which shows
how powerful deep contextual embeddings are. Transformer attention
methods capture long-range semantic links, sarcasm, and negation—
phenomena that bag-of-words models or simple recurrent networks handle
poorly. This capacity is very important for analyzing the feelings of social
media postings or conversational actors where the emotional purpose is
often oblique.
Lastly, the FusionNet multimodal model had the best overall
performance, with 94.5% accuracy and 0.92 macro-F1. This was because it
used both facial, audio, and textual inputs using early- and late-fusion
techniques. This integration makes the system more robust in case one
channel fails. For example, when faces are only partially visible or the
lighting is bad, prosodic or textual information can still help with proper
categorization. FusionNet's better findings show that cross-modal
reinforcement is necessary for reliable emotion identification in real-world
situations including healthcare monitoring, adaptive learning environments,
and customer service systems.

**4.2 Confusion Matrix Analysis**

Figure 2 Normalized confusion matrix for FusionNet predictions on the
FER-2013 test set. Darker diagonal cells indicate higher correct-
classification rates, highlighting how the multimodal approach reduces
misclassification of emotions such as fear and surprise. The genuine
emotion labels are in the rows, and the anticipated labels are in the columns.
The darker diagonal cells show better correct-classification rates. Fear and
Surprise are the two emotions that are most often confused.


**_Fig. 2_** Confusion matrix of FusionNet predictions on FER-2013 test set, normalized per class
They both include facial indicators like wide eyes and elevated
eyebrows, which makes it hard to tell them apart. Smaller overlaps show up
between Neutral and Sadness, which shows little variances in how stiff the
facial muscles are and where the gaze is directed. The speech channel
sometimes mixes up Disgust and Anger since both have low-frequency,
tense voice patterns. Text analysis reveals infrequent confusions between
Happiness and Anger in caustic remarks. FusionNet cuts down on these off-
diagonal mistakes by combining facial, verbal, and textual clues. When one
mode is unreliable, such low-light video or poor audio, information from
other channels can give clear proof. This cross-modal reinforcement makes
the model strong enough to be used in real life, such as in healthcare
monitoring, adaptive learning platforms, and virtual assistants that can
recognize emotions.


**4.3 Comparative Evaluation with Baseline Models**

To benchmark the system's performance, comparisons were made against
classical machine learning models. Table 3 displays side-by-side
comparisons that indicate a big difference between deep-learning models
and typical machine-learning baselines. SVM, k-Nearest Neighbours, and
Naïve Bayes are some classic algorithms that use hand-crafted features.
They employ edge detectors for pictures, MFCCs for audio, and bag-of-
words for text. These features don’t help us understand the complex, non-
linear nature of emotional impulses very effectively. That means that on
average, they are right around 65–74% of the time. On the other hand, deep
networks construct hierarchical representations straight from the data.

**_Table 3_** Comparative performance of traditional
and deep learning models for emotion detection

```
Model Accuracy (Avg) (%)
SVM (Visual/text) 74.3
k-NN (Speech/text) 70.2
Naive Bayes (text) 65.8
Shallow ANN 82.4
Proposed DL models 94
```
The convolutional layers of the neural network search for tiny spatial
signals in face photographs, the recurrent layers search for long-term
patterns in audio, and the transformers investigate the meaning of text
through its context. The CNNs, LSTMs, and BERT all saw a 15–25%
improvement as a result of this innovation. There are a number of
significant trade-offs associated with deep models, including the need for
extensive datasets that are well-annotated, a significant amount of
processing capacity, and stringent regularization. It is also difficult to
comprehend the process by which they make decisions. Because deep
learning is more accurate and usable for a larger variety of data sources, the
prices are acceptable for applications that need sentiment analysis, such as
health monitoring or customer service assessment. Deep learning is also
helpful for a wider range of data sources. When you install the system on
edge devices, you will only be able to accomplish this if you make use of


explainability tools and quick inference approaches such as model trimming
and quantization.
Using technology that can tell how people feel raises a lot of ethical
problems. These fears are far more than just privacy concerns. Bias is a
major issue in medicine, and it's crucial to remember that. Models mostly
trained on Western facial datasets may struggle to accurately identify the
emotions of individuals who are underrepresented in these datasets. This
may result in the exclusion of adverse emotions such as pain, worry, or
despair, with a postponement in the development of appropriate therapeutic
strategies. When the cultural norms of the individuals being evaluated do
not align with the training data, computerized assessment systems may
unjustly disadvantage individuals in educational or professional settings.
For instance, kids from countries where showing feelings isn't as frequent
could be thought to be emotionally apathetic since they don't show their
feelings.
It is feasible that real-time recording of people's facial or verbal
expressions might lead to intrusive surveillance in public places or when
someone is in charge. This would help governments or businesses find out
about political opposition or personal flaws without asking the people they
are watching for permission. To completely protect against these dangers,
developers need to do demographic performance audits, gather data, and
make judgments using technology that keeps users’ information safe. They
also need to acquire permission in a way that is clear and open, and people
should be able to say no to taking part. It's important to be honest about the
model's boundaries and undertake unbiased study to make sure that new
technologies improve people's lives instead of making it easier to
discriminate or compel people to do things. This is the only method to make
sure that new technologies are good for people.

##### 5 Conclusion and Future Work

This chapter shows that combining convolutional neural networks, LSTM
architectures, and transformer models like BERT may accurately and
quickly figure out how someone feels based on their face, voice, or text.
The proposed fusion architecture achieved 94.5% accuracy and a 0.92 F1-
score, surpassing robust unimodal benchmarks. It also addressed issues
about data imbalance, inter-subject variability, and the inherent difficulty of


articulating emotions. The next research should have three key aims. For
people to trust AI and make sure that model thinking is explicit, especially
in sensitive sectors like healthcare or education, explainable AI (XAI) is
very important. To ensure uniform performance across diverse populations,
cultural and language generalization must be pursued via domain adaptation
and transfer learning. Physiological integration, which includes things like
heart-rate variability, galvanic skin reaction, and EEG, is a way to find
subtle internal states that facial expressions, vocalizations, or written
communication can't show. We still need to deploy in a way that is
sustainable. Models that are resource-efficient, safeguard privacy, and can
operate on edge devices will make it safe to use in the real world. If
emotion-aware computing focuses on these areas, it might become an
ethical, intelligible, and generally used technology that improves everyday
interactions between humans and machines better.

##### References

1. Moerland, T.M., Broekens, J., Jonker, C.M.: Emotion in reinforcement learning agents and
    robots: a survey. Mach. Learn. **107** (2), 443–480 (2018)
    [MathSciNet][Crossref]
2.
    Bhuyan, B.P., Ramdane-Cherif, A., Singh, T.P., Tomar, R.: Common sense reasoning for neuro-
    symbolic AI. In: Neuro-Symbolic Artificial Intelligence: bridging Logic and Learning, pp. 271–
    290. Springer Nature, Singapore (2024)
3.
    Samal, P., Hashmi, M.F.: Role of machine learning and deep learning techniques in EEG-based
    BCI emotion recognition system: a review. Artif. Intell. Rev. **57** (3), 50 (2024)
    [Crossref]
4.
    Hazmoune, S., Bougamouza, F.: Using transformers for multimodal emotion recognition:
    taxonomies and state-of-the-art review. Eng. Appl. Artif. Intell. **133** , 108339 (2024)
    [Crossref]
5.
    Gursesli, M.C., Lombardi, S., Duradoni, M., Bocchi, L., Guazzini, A., Lanata, A.: Facial emotion
    recognition (FER) through custom lightweight CNN model: performance evaluation in public
    datasets. IEEE Access **12** , 45543–45559 (2024)
    [Crossref]
6.
    Tiwari, M., Verma, D.K.: Gender recognition in text-independent speaker identification using
    MFCC, spectrogram, Bi-LSTM, and rat swarm evolutionary algorithm optimization. Int. J.
    Speech Technol. **28** (1), 245–260 (2025)
    [Crossref]


7.
Lomas, T., Nilsson, A.H., Kjell, O., Niemiec, R., Pawelski, J.O., Padgett, R.N., VanderWeele,
T.J.: Differentiating balance and harmony through natural language analysis: a cross-national
exploration of two understudied wellbeing-related concepts. J. Positive Psychol. 1–19 (2025)
8.
Al-Saadawi, H.F.T., Das, B., Das, R.: A systematic review of trimodal affective computing
approaches: text, audio, and visual integration in emotion recognition and sentiment analysis.
Expert Syst. Appl. **255** , 124852 (2024)
[Crossref]
9.
Diwan, A., Sunil, R., Mer, P., Mahadeva, R., Patole, S.P.: Advancements in emotion
classification via facial and body gesture analysis: a survey. Expert. Syst. **42** (2), e13759 (2025)
[Crossref]
10.
Khare, S.K., Blanes-Vidal, V., Nadimi, E.S., Acharya, U.R.: Emotion recognition and artificial
intelligence: a systematic review (2014–2023) and research recommendations. Inf. Fusion **102** ,
102019 (2024)
[Crossref]
11.
Karthik, A., Manikandan, M., Dinesh, P., Mugilan, A., Muhannadu Haarish, S.: Advancing
human–computer interaction through a hybrid EEG-based emotion recognition system. In:
Proceedings of the 2024 Third International Conference on Distributed Computing and Electrical
Circuits and Electronics (ICDCECE), pp. 1–6. IEEE (2024)
12.
Geetha, A.V., Mala, T., Priyanka, D., Uma, E.: Multimodal emotion recognition with deep
learning: advancements, challenges, and future directions. Inf. Fusion **105** , 102218 (2024)
[Crossref]
13.
Kuo, J.Y., Hsieh, T.F., Lin, T.Y.: Constructing multi-modal emotion recognition model based on
convolutional neural network. Multimedia Tools Appl. 1–26 (2024)
14.
Patel, P., Annavarapu, R.N.: Application of supervised machine learning models in human
emotion classification using Tsallis entropy as a feature. J. Big Data **12** (1), 1–18 (2025)
[Crossref]
15.
Sharma, A., Sharma, K., Kumar, A.: Real-time emotional health detection using fine-tuned
transfer networks with multimodal fusion. Neural Comput. Appl. **35** (31), 22935–22948 (2023)
[Crossref]
16.
Ab Wahab, M.N., Nazir, A., Ren, A.T.Z., Noor, M.H.M., Akbar, M.F., Mohamed, A.S.A.:
EfficientNet-Lite and hybrid CNN-KNN implementation for facial expression recognition on
Raspberry Pi. IEEE Access **9** , 134065–134080 (2021)
[Crossref]
17.
Salloum, S.A., Alomari, K.M., Alfaisal, A.M., Aljanada, R.A., Basiouni, A.: Emotion recognition
for enhanced learning: using AI to detect students’ emotions and adjust teaching methods. Smart
Learn. Environ. **12** (1), 21 (2025)
[Crossref]
_OceanofPDF.com_


(1)

© The Author(s), under exclusive license to Springer Nature Switzerland AG 2026
K. Slimani et al. (eds.), _Emotion and Facial Recognition in Artificial Intelligence: Sustainable
Multidisciplinary Perspectives and Applications_ , Information Systems Engineering and Management
78
https://doi.org/10.1007/978-3-032-14778-3_6

## Real Time: 3D Facial Expression

## Recognition Using Improved AlexNet

## Convolutional Network via Deep-Emotion

Narimane Saad^1
Department of Computer Science, LIMED Laboratory, Bejaia, Algeria

```
Narimane Saad
Email: narimane.saad@univ-bejaia.dz
```
**Abstract**
In the early stages of human emotion processing, Facial Expression
Recognition (FER) plays a crucial role. A new model for face expression
recognition (FER) is presented in our work, using recent success researches
of deep learning method utilizing the AlexNet CNN, which is trained on
key facial features and significantly outperforms earlier models trained on
key datasets, particularly JAFFE, FER2013 and CK+. Multi block Local
Binary Patterns (MB_LBP) and CNN are approaches used to implement
extracting features and reduction space of expression class identification
respectively, followed by a classifier the Support Vector Machine (SVM)
for the first approach and the integrated Soft Max classifier for the CNN.
Then, a visual method is employed for the purpose of identifying and
detecting regions of interest on the face to recognize different emotions, as
determined by the classifier's output according based on our findings; we
demonstrate that different facial features can elicit different emotional
responses. The average accuracy using datasets JAFFE, FER2013 and CK+
is 98.86%, 93.76% and 97.88% respectively.


**Keywords** Facial expression recognition (FER) – AlexNet – Deep emotion

- Convolutional networks

##### 1 Introduction

One of the most studied biometrics, Face Expression Recognition (FER)
has been increasingly popular recently in everything from security cameras
in public spaces and airports to everyday products like Apple's FaceID. In
the past, a wide range of traditional hand-crafted feature-based techniques
that can identify emotional expressions on a person's face and micro-
expressions. These approaches typically rely on extracting informative
spatial and temporal patterns from facial images or video sequences. For
instance, local binary patterns (LBP) capture local texture features by
encoding neighborhood intensity differences, making them robust to
lighting variations. The scale-invariant feature transform (SIFT) retrieves
distinctive, scale- and rotation-invariant keypoints useful for describing
salient facial structures. Histograms of Oriented Gradients (HOG) analyze
the distribution of orientations of edges to depict the form and look of facial
features. Additionally, Gabor wavelet transforms are utilized to model
spatial frequency characteristics and multi-resolution facial texture
information. Collectively, these hand-crafted descriptors have provided the
foundation for traditional facial expression recognition systems by enabling
effective characterization of subtle facial changes associated with different
emotional states. But these methods only extract superficial information
from the movie, which isn't enough for abstract feature representation, and
that's the fundamental drawback. Most studies have concentrated on 2D
FER up until now, but more recent ones have turned their attention to 3D
face recognition. An ever-changing face, whether as a result of age or other
external elements (such as a person's voice, health, physiological signals,
expressions, etc.), makes facial recognition a difficult task, especially when
it comes to identifying emotions. After establishing the significance of FER
models based on deep learning, we will proceed to present a few of the
most popular FER datasets.
Recognizing emotions, communicating nonverbally, and identifying
persons are all greatly aided by facial expressions. Along with vocal
intonation, when it comes to the common display of emotion, they are vital.
They also serve as a means of emotional expression, enabling men to show


their true feelings. When someone is feeling emotional, it shows to
everyone around them. Consequently, FER models based on deep learning
generally make use of facial expression data for automated emotion
recognition [ 1 ].
Software emotion recognition allows programs to “examine” a person's
facial expressions using advanced image processing techniques. Thanks to
recent advancements in technology, emotion recognition software has
become quite proficient. In addition to its ability to monitor initial facial
emotion recognition software is able to discern subtle body language
signals known as “micro-expressions” that indicate emotions like
happiness, sadness, surprise, anger, etc.., which could disclose someone's
emotional state even when they aren't aware of it [ 2 ].
Emotion recognition is compatible with several biometric picture
identification systems and facial recognition algorithms. Countless security
scenarios can benefit from these two categories of technologies. “Emotion
recognition” is able to use a variety of features, including facial expressions
[ 3 – 5 ], voice [ 6 , 7 ], EEG [ 8 ], and text [ 9 ]. Facial expressions rank high
among these features because they are easy to gather a big dataset of
(compared to other methods for human recognition), they are observable,
and they contain many relevant aspects for emotion recognition [ 3 , 10 – 12 ].
While 2D FER has come a long way, it still can't handle the two biggest
issues: changes in lighting and variations in poses—when applied to data in
three dimensions, both static and dynamic. The 3D Face shape model,
which incorporates depth information for 3D FER, is able to capture even
the most minute of facial deformities. These models are inherently resistant
to fluctuations in illumination and position [ 13 ]. In reason, the input color
as part of the pretreatment process, the image needs to be grayscaled. Image
preprocessing, feature extraction, and expression classification are the three
main components of the traditional FER method (Fig. 1 a), while Fig. 1 b
shows an example of a CNN-based FER procedure [ 14 ].


**_Fig. 1_** **a** Procedure in convolutional FER approaches. **b** Procedure of CNN-based FER approach
[ 14 ]
All data collecting has been done our method seeks to address the
challenge of creating a system based on deep learning while remaining in
one place and subjected to same illumination conditions that can detect a
person's 3D face and recognize its expression. We can't reach our target
without the aforementioned observation and attention mechanism that
enables us to zero up on the face's most notable characteristics. We show
that a convolutional neural network can achieve a very crucial accuracy rate
even with less than 10 layers. These works function immediately on image
databases captured in controlled environments, but also abort more complex
datasets with variations in face pose and occlusion. Our research presents
the next contributions [ 12 ]:

```
Focusing on the face's feature-rich regions is made possible by our
proposed method, based on an AlexNet help to obtain models with high
capacity without overfitting there by enhancing the FER performance,
and still outperforms exceptional recent researches in precision.
In second, we use a new algorithm of Deep Learning explodes the
records! To demonstrate the image's most important regions, or the
regions that influence the classifier's output the greatest. In Fig. 1 a, we
can see patterns of the regions seen as important for different emotions.
In Sect. 2 , we begin to compile a literature review by going over the
following sections. In Sect. 3 , we go over the architecture of the model
```

```
and the suggested framework. The outcomes of the experiments, the main
databases used for the research, and the model visualization will be
described in Sect. 4. Lastly, Sect. 5 brings this effort to a close this work
in Sect. 5.
```
##### 2 Related Works

Facial expression recognition in its early stages relied heavily on manually
created features. The computer vision community has embraced deep
learning following AlexNet's victory as part of the ILSVRC, which stands
to the 2013 FER Challenge on Facial Expression Recognition, which
showcased preliminary attempts to suggest deep learning methods for
emotion identification, and to the ImageNet Large Scale Visual Recognition
Challenge. Contrary to what one might expect, a deep convolutional neural
network took first competing in the 2013 FER Challenge, the top
handcrafted model placed a very disappointing fourth. Most recent efforts
in facial emotion recognition have relied on deep learning, with a handful of
notable outliers. To attain superior outcomes, some of these new studies
suggested training a network of convolutional neural networks, while others
integrated deep features with custom-made features like SIFT or
Histograms of Oriented Gradients (HOG). Facial expression identification
in video has been the subject of a small number of studies, although the
majority of them have focused on static photographs [ 15 ].
In addition, there are significant restrictions to be addressed when using
deep learning based methods, even if studies with learnt features regularly
show better recognition rates than handcrafted ones. Deep networks are
extremely susceptible to overfitting, hence training them with a large
dataset is essential in practice.
Given the scarcity of 3D face datasets—which contain a limited
quantity of 3D scans—this criterion could prove to be particularly
important in 3D FER. This means that deep learning-based 3D face
solutions have more options for training their feature extraction network,
such as using pre-trained deep models or boosting their data. Furthermore,
those deep networks frequently use as input a number of geometrical
representations of the same 3D scan, such as a 3D face scan [ 16 ].
Ekman and Friesen [ 17 ] showed the presence of seven facial
expressions that are surprise, fear, disgust, anger, joy, sadness and


neutrality. The first six expressions are called the six universal ones and
they are identifiable by any person regardless of their culture. They
invented a system of manual codification of facial expressions known as
FACS [ 18 ] thus establishing the standard for subsequent applications on
emotion recognition based on this idea. Later, the six facial expressions
(besides neutral) integrated across the majority of human recognition
datasets, seven fundamental emotions often follow. Emotions examples
from three datasets (JAFFE, FER and CK+) are visualized in Fig. 2.

**_Fig. 2_** The seven main feelings. Rows one through three display photos from the JAFFE, FER, and
CK+ databases, correspondingly
Most recent efforts in this area rely on the time-honored two-step
machine learning procedure, which involves feature extraction from images
as the first step and emotion detection using a classifier (e.g., random forest,
SVM, or neural network). Results from unimodal systems are the basis of
these works. Emotion prediction machines that rely solely on facial
expressions [ 19 ] or vocal sounds [ 6 , 20 ] no longer work.
Multimodal systems, which incorporate more than one factor into the
emotion prediction process, eventually outperform single-feature systems.
Thereafter, characteristics like audio-visual emotions, electroencephalogram
(EEG), and body movements have been utilized in conjunction. Neural
networks and other smart technologies power the emotion recognition
system.
According to Shiqing et al. [ 21 ], multimodal recognition is superior to
unimodal systems. Recent studies have shown that DNNs are capable of
producing discriminative features that closely mimic the original set's


complicated non-linear dependencies [ 20 ]. Facial emotion detection often
makes use of hand-crafted features for example; the histogram of oriented
gradients (HOG) [ 22 , 23 ], scale invariant feature transform (SIFT) [ 24 ],
local binary patterns (LBP) and mapped LBP [ 25 ], Haar features [ 26 ] and
Gabor wavelets [ 27 ].
The picture would then be given the most suitable emotion by a
classifier. These algorithms performed admirably on simpler datasets,
however, these shortcomings become obvious when dealing with more
complicated datasets (exhibiting greater intra-class variation). In order to
improve some defiance, we mention the problems of occlusion where the
face can be occluded (eye-glasses, hat, hand or hair...) (Fig. 3 ).

**_Fig. 3_** The face occlusion problem

```
The Table 1 presents the different popular datasets widely used for FER.
```
**_Table 1_** The most relevant datasets of facial expression recognition

```
Name datasets Database size Description
Japanese female
facial expression
“JAFFE” (1998)
```
- There are a total of 213 photos in the
database, featuring 10 Japanese female
models posing for 7 different face
expressions (1 neutral and 6 basic)
    - On a scale from 1 to 6, 60
    Japanese volunteers scored each
    image

```
Cohn-Kanade,
CK (2000)
```
- Contains 100 university students – Participants’ ages varied from
    eighteen to thirty. There were 65%
    females, 15% blacks, and 3%
    people of Asian or Latino descent
**BU-3DFE
Database (Static
Data) (2006)**
- Has one hundred participants and two
thousand five hundred models of facial
expressions expressions. The 100
participants’ ages range from 18 to 70 plus,
with 56% being female and 44% being
male
- Researchers can access the BU-
3DFE database (e.g., computer
vision, human computer
interaction)


```
Name datasets Database size Description
The Bosphorus
(2008)
```
- The 105 people in the 3D database are
represented in a wide range of poses,
expressions, and occlusion circumstances
    - Most of the participants are in
    the 25–35 age range. In total, there
    are 45 ladies and 60 males
**CK+ (2010)** – Contains f 593 images in total from 123
subjects
- Participants ranged in age from
18 to 50, with 69% being female,
81% being Euro-American, 13%
being Afro-American, and 6%
belonging to other groups
**BU-4DFE
database
(Dynamic Data)
(2010)**
- There are 43 male and 58 female
participants in the final database. 606 sets
of three-dimensional facial expressions
from 101 people are included in the
database
- A video the recording rate for
here are 25 frames per second for
the 3D facial expressions. There
are six typical face expressions for
every subject
**The FER (2013)** – Contains 35,887 face images distributed
across 28709 training sets, 3589
verification sets, and 589 test sets
- Each image is 48 pixels × 48
pixels in grayscale

```
CK+ (2024) – Approximately 10.000 images and after
augmentation between 50.000 and 100.000
images
```
- It emphasizes how crucial data
augmentation methods are for
enhancing model performance,
especially when there is a shortage
of labeled training data
**CK+, FER2013
(2025)**
- Approximately between 1.000 and 50.000
samples (images/videos)
- A deep learning framework that
unites multimodal data fusion with
CNN to enhance emotion
recognition systems
In our study, we have chosen FER2013, JAFFE, and CK+ datasets, this
selection on facial expression recognition (FER) is motivated by their
complementary characteristics and widespread adoption in all researches.
Using these datasets in combination affords a well-balanced evaluation
framework encompassing data diversity, experimental control, and realism.
This integrated approach enables thorough and reliable assessment of
automatic facial expression recognition systems.
For studies including deep learning and vision, the “convolutional
neural networks” (CNNs) topology has established as the gold standard.
The top three competitors in the 2014 ImageNet object identification
competition all used CNN methodologies; among them, the GoogLeNet
architecture achieved an outstanding classification error rate of 6.66% [ 27 ,
28 ]. The GoogLeNet architecture employs an innovative multi-scale
methodology by integrating many classifier architectures alongside
different sources for backpropagation. Reducing back-propagation before


reaching the system's initial layers causes a number of issues, and this
design solves those problems. Supplementary layers that diminish
dimensionality enable GoogLeNet to expand in both breadth and depth
without considerable drawbacks, thereby advancing towards the intricate
network-in-network structures initially articulated by Lin et al. [ 29 ]. The
design consists of several “Inception” layers, The design is able to conduct
increasingly complex decisions because each node functions as a micro-
network within the larger network [ 4 ].
Also, convolutional neural network (CNN) designs have shown
remarkable results. The Convolutional CNN layered structure is the basis of
AlexNet's architecture. The layers begin with convolutional layers, further
on, the stack consists of rectified linear units (ReLUs), max-pooling layers,
and numerous fully connected levels. The ILSVRC-2012 competition
changed our view of CNN effectiveness with their leading error rate of
15.3%. This network was among the pioneers in using the “dropout”
technique to address the overfitting issue, as proposed by Hinton et al. [ 30 ],
which was essential in the advancement of massive neural networks. A
significant problem in employing CNN designs that are commonly used are
their computational complexity and depth. The complete network executes
approximately 100 million operations per iteration, whereas Deep neural
networks and SVM both utilize a lot less operations to develop an adequate
model. This renders Convolutional CNNs challenging to implement in time-
sensitive situations [ 4 ].
There have been outstanding outcomes with more conventional CNN
designs as well. AlexNet [ 31 ] follows the traditional layered architecture of
convolutional neural networks (CNNs), which starts with a large number of
fully connected layers at the top and continues through max-pooling layers
and rectified linear units (ReLUs) below. They completely changed our
perspective on CNN effectiveness with has a top-five mistake rate of 15.3%
at the ILSVRC-2012 event. Additionally, this network played a crucial role
in the development of massive neural networks by being one of the first to
use the “dropout” method, which was proposed by Hinton et al. [ 30 ] and
was used to solve the overfitting problem. Traditional convolutional neural
network (CNN) designs are notoriously difficult to implement due to their
computational complexity and depth. When it comes to creating a good
model, the entire network does something like 100 million operations in
one iteration, whereas SVM and shallow neural networks do significantly


less operations. Because of this, using conventional CNNs in situations
where time is of the essence is extremely challenging [ 4 ]. Essential
information describing the geometric relationships of the face can be found
in depth photos and films, which record the intensity of facial pixels as a
function of distance from a depth camera.
For instance, in order to recognize face expressions, a CNN was used to
unregistered facial depth pictures acquired from a Kinect depth sensor that
had gradient direction information [ 32 ] retrieved a number of prominent
Deep learning networks (such as CNN and DBN) were used in conjunction
with characteristics extracted from depth films to perform facial emotion
recognition (FER). When it comes to Li et al. [ 33 ] investigate the 4D Facial
Expression Recognition (3D FER utilizing dynamic data) in order to draw
attention to the changing patterns of distortion caused by changes in face
expressions using a dynamic geometrical image network. Bypassing the
need for facial landmark recognition, computing 3D expression coefficients
from images intensities using CNN was proposed by Chang et al. [ 34 ]. As a
result, the model is very resistant to severe appearance modifications, such
as occlusions, size changes, and out-of-plane head rotations.
More and more research is looking at ways to improve performance by
combining 2D and 3D data. Concurrent feature extraction from RGB and
depth map latent modalities was achieved by Oyedotun et al. [ 35 ] using
CNN. In order to study multi-modal 2D + 3D face emotion recognition
(FER), Li et al. [ 36 ] implemented a deep fusion convolutional neural
network (DF-CNN). To find the best weight combination for 2D and 3D
face representations, as a first step, six distinct 2D attribute maps were
derived from 3D face scans with textures: dimensions, texture, curvature,
and the x, y, and z-axis normal components. These maps were then input
into the networks that deal with fusing and extracting features.
Using texture and depth pictures to identify different facial regions [ 37 ],
suggested extracting deep features and then merging them to create links
through feedback. Using unsupervised domain adaption methods, Wei et al.
[ 38 ] dug further into the 2D + 3D FER data bias problem [ 13 ]. Recently,
Deep CNN (DCNN) is becoming the mainstream in face expression
recognition there are several methods based on for FER, we adopt the
(DCNN) architecture in our research, with review of some successful deep
learning applications in FER. In Through the efforts of several researchers,
we could obtain a good performance on these ideal datasets learning


methods, we have based on the researches of Shervin Minaee, Amirali
Abdolrashidi in 2019 used the Deep learning methodology utilizing an
attentional convolutional network with FER2013, CK, JAFEE, FERG
datasets Achieved accuracy rate about: 70.02%, 98.0%, 92.8% and 99.3%
respectively with different datasets [ 12 ].
Additionally, In 2020, Sahil Sharma and Vijay Kumar used sequential
deep learning to apply voxel-based 3D face reconstruction to FER, they
achieved accuracy rate about 90.01%, 78.21%, and 85.68% respectively
with different datasets; Bosphorus, UMBDB, KinectFaceDB [ 39 ]. Recently,
in February 2021, Nayaneesh Kumar Mishra and Satish Kumar Singh use
the 3D CNNs for FER video datasets; CVBL, LFW, YTF achieved accuracy
rate about: 97%, 99.42% and 95.0% respectively [ 40 ]. The significant
accomplishment of deep learning, namely convolutional neural networks,
pertains to image categorization and other visual problems [ 17 , 41 – 45 ],
Numerous groups established deep learning-based models for facial
emotion recognition (FER).
The following table presents the different approach and successes of
emotion recognition [ 4 ]. Those approaches perform important
improvements compared to the previous approaches in emotion recognition,
without attaining the problem of emotion detection respecting the important
face regions (Table 2 ).

**_Table 2_** Different approach and successes of emotion recognition [ 4 , 12 , 20 , 40 , 46 – 49 ]

```
Type of
method
```
```
Reference
and year
```
```
Approach and
method
```
```
Data sets Performance
```

**Type of
method**

```
Reference
and year
```
```
Approach and
method
```
```
Data sets Performance
```
**Handcrafted
features
methods**

- Hu et al.
(2019) [ 46 ]
    - Fusion features for
    center-symmetric
    local signal
    magnitude pattern-
    based face
    expression
    recognition
       - JAFFE, CK+ – Accuracy rate f 80 and
          97.67% with the two
          datasets respectively
- He and Chen
(2020) [ 48 ]
    - Higher-order
    singular-value
    decomposition and
    improved local
    binary pattern
    underpin person-
    independent face
    expression
    recognition
       - CK+, Oulu-
       CASIA
       NIR&VIS
          - The best average
          accuracy obtained are
          82%, 77.5%
          respectively with
          different datasets

**Deep
learning
methods**

- Khorrami et
al. (2015) [ 50 ]
    - Master the use of
    facial action units in
    CNN-based
    expression
    recognition
       - Cohn-Kanade
       dataset (CK+).
       Toronto Face
       Dataset (TFD)
          - High accuracy in
          emotionrecognition,
          zero-bias CNN
-
Mollahosseini
et al. (2016)
[ 4 ]
    - Deep neural
    network facial
    expression
    recognition
       - MultiPIE,
       MMI, CK+,
       DISFA, FERA,
       SFEW, and
       FER2013
       datasets
          - More effective and
          faster to train than
          conventional
          convolutional neural
          networks


**Type of
method**

```
Reference
and year
```
```
Approach and
method
```
```
Data sets Performance
```
**Deep
learning
methods**

- Meng et al.
(2017) [ 51 ]
    - A convolutional
    neural network for
    facial expression
    recognition with
    identity awareness
    (IACNN)
       - CK+ and MMI
       datasets
          - Achieved accuracy
          rate about: 71.29%,
          55.41% compared with
          the two datasets
          respectively
- Shima and
Omori
(2018) [ 52 ]
    - Image
    augmentation for
    classifying facial
    expression images
    by using deep neural
    network pretrained
    with object image
    database
       - ATR facial
       expression image
       database (DB99)
          - Average recognition
          rate of 97.92%
- Minaee et al.
(2019) [ 12 ]
    - A method for deep
    learning that utilizes
    attentional
    convolutional
    networks
       - FER2013, CK,
       JAFEE, FERG
          - Achieved accuracy
          rate about: 70.02%,
          98.0%, 92.8% and
          99.3% respectively with
          different datasets
- Sharma and
Kumar (2020)
[ 47 ]
    - Using sequential
    deep learning, voxel-
    based 3D face
    reconstruction can be
    applied to facial
    expression
    recognition (FER)
       - Bosphorus,
       UMBDB,
       KinectFaceDB
          - Achieved accuracy
          rate about 90.01%,
          78.21%, and 85.68%
          respectively with
          different datasets
- Sharma and
Kumar (2021)
[ 49 ]
    - 3D landmark based
    face restoration for
    recognition utilizing
    variational
    autoencoder and
    triplet loss
       - Bosphorus,
       UMBDB,
       KinectFaceDB
          - Achieved accuracy
          rate 83.3%, 77.2% and
          80.9% respectively with
          different datasets


```
Type of
method
```
```
Reference
and year
```
```
Approach and
method
```
```
Data sets Performance
```
```
Deep
learning
methods
```
- Mishra and
Singh (2021)
[ 40 ]
    - 3D FER using 3D
    CNNs using video
    data
       - CVBL, LFW,
       YTF
          - Achieved accuracy
          rate about: 97%,
          99.42% and 95.0%
          respectively with video
          datasets
Ezerceli and
Eskil (2022)
[ 53 ]
    - For the FER-2013
    dataset, a system
    based on
    convolutional neural
    network (CNN)
    algorithm for facial
    emotion recognition
    (FER)
       - The
       combination of
       FER2013 and
       CK+ datasets
          - Achieved 93.7%
          accuracy rate

```
Bapat et al.
(2023) [ 54 ]
```
- Building databases
and using deep
learning for facial
expression
recognition
    - CK+ and
    FER2013

```
Attained 93.94%
accuracy for the CK+
dataset and 67.18% for
the FER2013 dataset,
respectively
Grover and
Bansal (2024)
[ 55 ]
```
- A lightweight
convolutional neural
network approach
for efficient facial
expression
recognition on public
datasets
    - FER-2013,
    CK+, RAF-DB,
    KDFE
       - Achieved 70, 99.2,
       84.4, and 94% for,
       FER2013, CK+, RAF-
       DB and KDFEdatasets
       respectively

```
Chindiyababy
et al. (2025)
[ 56 ]
```
- Advanced human–
computer interaction
using deep learning-
based facial emotion
recognition
    - FER2013,
    AffectNet, and
    CK+ dataset
       - Achived 92.5%
       accuracy rate for
       emotion recognition

##### 3 The Proposed Model Architecture

**3.1 Overview [ 57 ]**

We had to do some digging to figure out what was going on and how to
improve our results after the jobs were operational. Here, we'll break down
the FER system into its component parts; Fig. 4 shows our proposed
system.


**_Fig. 4_** Outline of our proposed approach [ 58 ]
The following encapsulates our overarching methodology.

**_3.1.1 Construct Dataset_**

We gathered labeled facial expression datasets from various sources and
standardized their labels and photos into a uniform format. Subsequently,
we incorporated personalized photographs of ourselves and a companion to
enhance the data collection. In this study, we register facial images
throughout each database utilizing established research methodologies. To
ascertain the expression of the individual before the Kinect, we initially
evaluated the image database. The “Viola-Jones” approach was introduced


for detecting faces in digital images or video sequences, and a “Haar
Cascade Detector” implementation of this method is available in the
OpenCV package. We decided to establish our own repository of
expressions to circumvent the limitations of classification amid interclass
variability linked to significant fluctuations in light within the same
category. Finally, the program uses machine learning techniques to try to
group the given face picture into seven different basic emotion groups.
Support Vector Machines (SVMs) are predominantly employed for facial
expression recognition applications. Based on our findings, facial
registration improved the FER algorithms’ accuracy by 37%, implying that,
registration is a crucial component of every FER method, much like
normalization in conventional issues [ 4 ].

```
Pre-process Images : In order to reduce the impact of poor data quality
on the network, preprocessing is carried out. A point cloud representing
information pertaining to the surface of a three-dimensional face
coordinates (x, y, z) is the input to our suggested network. At first, we
sub-sample points uniformly on 3D facial surfaces to remove the off-face
area and bring the partial point clouds’ dimensionality reduced to N
points per sample. We standardize the data on a scale from 0 to 1 so we
can make a consistent point cloud [ 59 ].
Augmentation [ 59 ]: We utilized facial-detection algorithms to isolate the
face in each photograph. Subsequently, we re-scaled the cropping and
manually removed substandard photos as a preliminary procedure for the
CNN. The Using more training data can improve the neural network's
effectiveness. One well-established way to improve a network's
generalizability is to use data augmentation procedures. Data
augmentation involves generating additional samples for each class
through various transformations while maintaining exactly the same
amount of courses. Its purpose is to avoid overfitting and improve
training robustness:
```
- On the vertical axis, the three-dimensional face is tilted at 90°.
- To improve the data, a small amount of random point rotation is
    applied (sigma = 0.06 and angle-clip = 0.18).
- Using 0.02 as the mean and 0.06 as the standard deviation for the
    Gaussian noise to disturb the location of each point cloud.


- Performing all of the aforementioned methods for enhancing data at
    the level of the point cloud, including rearranging the sequence of
    points within each cloud and applying random translations and scaling
    to the data within the clouds.
- Rearranging the sequence in each point cloud, using a combination of
    the aforementioned data augmentation approaches applied to the point
    cloud data through random translations and scaling.

**_3.1.2 Feature Extraction_**

Is an important step because it determine the result we use one of most
popular for face analysis MB_LBP which use the texture feature that is
robust to misalignment and the variation of illumination. LBP original can
be considered like a particular case of MB_LBP [ 58 , 60 ], we require a
Support Vector Machine (SVM) classifier to categorize the extracted
characteristics [ 61 , 62 ]. The Fig. 5 showed an example of image and its
different intensities.

**_Fig. 5_** Example of image and its different intensities [ 58 ]

**Feature\Pooling dimension reduction** ; Construct CNN: The objective
of CNN design is to resize the image to a format that can be efficiently
processed without sacrificing essential features to get accurate
predictions [ 61 ]. The CNN method operates by transmitting the input
image via a series stacked layers, which include the fully connected layer,
pooling layer, rectified linear unit, and convolution layer, to yield an
accurate result. The CNN methodology incorporates an integrated
classifier known as Soft Max for picture classification.
The objective of CNN was to convert the input set into accurate and
valid outcomes [ 61 ]. Resilient to variations in location and scale invariance
in facial expression recognition, CNN comprises five layers: convolutional
layers (used to diminish the spatial dimensions of the features), pooling


layers, and fully connected layers. Average pooling and max pooling are the
two most prevalent nonlinear down sampling techniques employed for
achieving translation invariance [ 13 ]. We employed pre-trained variants of
AlexNet, re-training the initial and final layers. We needed to explore
several learning rate techniques and parameters to produce a non-divergent
model.

**_3.1.3 Create an Interactive User Interface_**

OpenCV enabled us to acquire images from our trials; we utilized Microsoft
Kinect for 3D facial modeling mostly due to its affordability and ease of
use. The Kinect possesses a limited scanning resolution, although it
maintains a comparatively high image registration rate of 30 frames per
second. It is equipped with an infrared emitter and dual cameras. One
camera captures visible light, while the other functions in the infrared
spectrum and is utilized for depth measurement [ 63 ]. Infrared photons
reflected from the user's body facilitate the creation of a 3D facial model.
The model (Candid3 [ 64 ]) is founded on 121 distinct facial points, captured
by the Kinect gadget. These points are positioned at distinctive locations on
the face, including the corners of the mouth, cheekbones, nose and
eyebrows. Figure 6 illustrates a collection of distinctive facial points
recorded in 2D space (a) and the coordinate system (x, y, z) as delineated by
the Kinect device (b). Subsequently, we isolated the face as previously, pre-
processed the image for the CNN, and transmitted it to AWS. A script on the
server would process the image through the CNN, obtain a prediction, and
retrieve the results locally [ 1 ].

**_Fig. 6_** **a** Feature points on the face, **b** kinect coordinate system

**3.2 CNN Construction [ 4 ]**


The advancement of neural network architectures relies on optimizing
either the amount of neurons or layers, allowing the network to learn
intricate functions. But problems like rising CPU requirements and training
data overfitting become more apparent as topologies become deeper and
more complicated.
Building deep sparse networks with strong theoretical foundations and
biological inspiration is the easy way to tackle the problem of dense
networks, as described by Arora et al. [ 65 ]. Actually, computing processes
over infrequent networks are beyond the capabilities of modern GPUs and
CPUs. By assessing sparse networks that take use of the advantages stated
in theory by Arora et al., with the dense architecture necessary for effective
computation preserved [ 4 ], the first layer offered in [ 63 ] resolves these
issues. Several convolutional neural network (CNN) models were cited:
AlexNet (2012), GoogleNet (2014), VGGNet (2014) and ResNet (2015).
As shown in [ 66 , 67 ], the Inception layer has shown outstanding results
when used to Deep Neural Network applications. Consequently, it makes
sense to apply the sophisticated methods used for object recognition to the
problem of facial expression recognition (FER). In addition to the
theoretical benefits of a sparse and relatively deep network, the first layer
uses smaller convolutions to improve local feature recognition and larger
convolutions to approximatively represent global features. There seems to
be a reasonable correspondence between the way people express their
feelings and the improved local performance. Most facial expressions,
especially those involving the eyes and lips, are easily identifiable to
humans [ 27 ].
Without guidance to concentrate on certain regional characteristics,
children with autism may have difficulty correctly identifying emotions.
Our expectation is that there will be notable improvements in local feature
performance, which should lead to better FER results, while applying Lin et
al.’s [ 68 ] network-in-network theory [ 68 ] and use the first layer structure.
The network-in-network approach has many benefits, one of which is a
decreased vulnerability to overfitting thanks to improved local and global
pooling performance. Because the network is resistant to overfitting, we can
increase its depth without worrying about the small image dataset we have
for the FER task. Section 3.2.3 lays out the principles of AlexNet topologies
that informed the research presented in this paper.


```
Our network comprises two components: it includes two Convolutional
CNN modules, each one including a max pooling layer after a
convolutional one. The activation function of a rectified linear unit
(ReLU) is f(x) = max(0, x), where x is the input to a neuron, and both
modules use this function. The vanishing gradient problem is mitigated
when using the ReLU activation function in conjunction with alternate
activation functions (for more information, see [ 29 ]).
```
**_3.2.1 Application of Deep Convolutional Neural Networks for_**

**_Classification_**

Classification and grouping of images based on similarities is the primary
use of artificial neural networks that use convolutional neural networks as
their basis. An method with many potential uses is the convolutional neural
network (CNN), including character recognition, anatomy, tumor detection,
street sign recognition, and many more. Discrete Convolutional Neural
Networks Emotion detection using visual analysis is fundamentally a
challenge with picture categorization. So, it stands to reason that a top-tier
DCNN model with strong picture classification capabilities would also
demonstrate strong face expression recognition capabilities. The initial
ConvL C1 uses a 5 × 5 convolution kernel, however it is 3 × 3 for the C2
and C3 foundation layers. The reason for the difference is that the size of
the convolution kernel for the latter two convolutions is 3 × 3, which leads
to better results. This is because using two 3 × 3 enhances the network's
capacity for non-linear operations, leading to a more discriminative
decision function. However, if the initial 3 × 3 layer is used,
underperforming because the parameters of the network model are
undersized. The layer's mathematical expression is given by [ 39 ]:

```
xli = f ∑
i ∈ Mjl ∘^1
```
```
xli −^1 Kilj − 1 Kijl+ blj (1)
```
According to [ 39 ], the variables l, f, k, b, and Mj stand for the layer,
activation function, convolution kernel, bias, and feature map, respectively.
We experimented with many DCNN models, including customized
iterations of AlexNet, VGG, GoogLeNet, and ResNet. This research is not
aim to compare other DCNN models; hence, we utilize a proprietary


AlexNet network to illustrate emotion identification efficacy on the FER
dataset [ 69 ]. Discrete wavelet transform was used to extract features of the
human face’s local texture. The deep convolutional neural network takes the
output as its input. This research proposes a network architecture with three
convolutional layers, two pooling layers, and one fully connected layer, as
illustrated in Fig. 7 [ 39 ].

**_Fig. 7_** La structure convolution network proposée

**_3.2.2 Max Pooling_**

Max pooling addresses two issues for convolutional neural networks
(CNNs); it reduces dimensionality by reducing each layer's output size and
the resources available to the network with translational invariance. The
latter is significant because if a feature is displaced marginally by a few
pixels, it will be aggregated in a “max pool” and subsequently sent to later
layers of a broader framework. This has demonstrated superior performance
compared to other pooling strategies, such as average pooling. Figure 8
illustrates an example of this process [ 70 ].

**_Fig. 8_** Technique of maxpooling [ 70 ]


**_3.2.3 AlexNet_**

This is the shallowest network we trained and tested, consisting of only 5
layers. The complete architecture and adaptation are illustrated in Fig. 9.
The Feature Selection Network (FSN) integrates a feature selection
mechanism within AlexNet, which autonomously eliminates irrelevant
features and highlights linked features based on learnt feature maps of facial
expressions [ 13 ].

**_Fig. 9_** AlexNet architecture [ 13 ]

After these modules, we build the network using its techniques and
include two “Inception” type modules with convolution layers that are 1 ×
1, 3 × 3, and 5 × 5, all of which are connected using ReLU. To combine two
completely linked layers, the layers as output used for classification via
ReLU activation. The study's network architecture is shown in Fig. 10.

**_Fig. 10_** Overview of CNN design. Fully linked expression recognition layers, max pooling layers,
and the convolutional layers (Conv) are identified by n = 5 layers, with each layer labeled


accordingly

**_3.2.4 Classification Expression_**

In the last step, the deep network performs facial emotion recognition
(FER) to assign each given face a fundamental emotion. To reduce the
dispersion of the expected class probabilities throughout the distribution of
the ground truth, CNNs mainly employ the Softmax loss function. Further,
another method calls for separate classifiers to be applied once deep neural
networks have been used to extract features [ 13 ]. Using the softmax
regression classifier, as described in this work, the classifier layer functions
as the CNN's output layer. Using the data x as input for a particular training
session, the softmax function acts as a multi-class classifier in the network's
final layer, boasting robust non-linear classification capabilities. The set {1,
2,..., k} includes the output category y, which means there are a total of k
classes; for the purposes of this article, we will define k as 10. The
following is the probability distribution of the class y = i that it belongs to,
assuming that the input data x is defined: In equation [ 71 ], for instance, “i”
denotes the parameters that require estimation, “e” is the base of the natural
logarithm, and “t” denotes the transposition. “What is the meaning of:”

```
P( y = i |x;Θ) (2)
```
The input data x's chance of belonging to each class i takes on values
between 1 and k [ 39 ].

```
P ( Cj =
```
```
1
x
```
```
)=
```
```
ew
```
```
tjx
```
```
∑ ki = 1 ew
```
```
tjx (3)
```
##### 4 Experimental Results

Extensive experimental testing of tested our model on various databases for
facial expression recognition is presented in this part. We begin with a brief
summary of the databases used in this research; next, here, we describe in
detail how well our models performed across three databases, comparing
the results to those of other renowned prior research. We subsequently


present the prominent locations identified by the use of a visualization
method by our trained model.

**4.1 Face Databases**

A number of notable, freely accessible databases of facial expressions are
used to evaluate the proposed methodology. These databases include
JAFFE, CK+, and the Facial Expression Research Group Database
(FER2013) [ 11 ].

```
JAFFE : Ten Japanese female models display seven different facial
expressions in this dataset's 213 photos. Sixty Japanese volunteers rated
each image using six emotional adjectives [ 10 ] (see Fig. 2 the images in
the first).
CK+ : Each of the 593 images in this collection—taken by 123 different
individuals—depicts a human face expression that the subject has
assigned to one of the seven basic emotions. From 18 to 50 years old,
69% of the participants are female, 81% are of Euro-American descent,
13% are of Afro-American descent, and 6% are of some other ethnic
group. Pixel arrays ranging from 640 × 490 to 640 × 480 were created
from image sequences that were intended for frontal and 30° viewpoints.
You can see the database in action in Fig. 11 [ 72 , 73 ].
```

**_Fig. 11_** Examples among CK+ dataset [ 73 ]
**FER2013 Face Dataset** : The FER dataset is specifically given for a
Kaggle competition. The dataset comprises 35,887 facial images,
consisting of 28,709 training samples, 3,589 verification samples, and
589 test samples, all of which are grayscale images measuring 48 pixels
by 48 pixels. The samples are categorized into seven groups based on a
generally uniform distribution: furious, unpleasant, terrified, pleased,
neutral, sad, and amazed. Each sample in the collection exhibits
significant variability in age, facial orientation, and other characteristics,
closely resembling real-world conditions [ 11 , 74 ] (Fig. 12 ).


**_Fig. 12_** FER2013 face dataset [ 11 ]
The following table summarizes our experimental results on the datasets
already mentioned with the seven standard expressions (Table 3 ).

**_Table 3_** Our experimental results using the datasets “ **JAFFE, FER2013, CK+** ”

```
Dataset Dataset size Number of training images Testing images Accuracy (%)
JAFFE 213 images 163 24 98.86
FER2013 35,887 images 28,709 15,11 93.76
CK+ 593 images 732 347 97.88
```
**4.2 Comparison and Analysis of Experiments**

Using the aforementioned datasets, we will now show that the proposed
deep neural network architecture is effective. Each case involves using a
subset of the dataset for model training, a separate set for validation, and a
test set for accuracy reporting. There is complete object-agnostic
categorization of databases into test, validation, and training sets. To
evaluate the results, K-fold cross-validation, with K = 5, was employed.
When it comes to JAFEE and CK+, in the release of the database, the
training and test sets are specified. Instead of employing K-fold cross-
validation, the results are evaluated on the selected test set. The training,
validation, and test sets show small differences in sample sizes across each
fold because different databases have different sample sizes per mood and
participant.
Approximately 21,315 photos (or about 70% of the total) are used for
training, whereas 1,530 images (or about 30% of the total) are used for


testing. Two thousand images are chosen at random for the purpose of
assessing each facial emotion. We achieved a precision level of around
93.76%.
The comparison between the proposed technique and other prior
research on the FER2013 dataset is presented in the Table 4.

**_Table 4_** Demonstrates the efficacy of our suggested algorithm in comparison to
alternative methodologies on FER2013 dataset

```
Approach Average accuracy percentage (%)
CNN (2023) [ 54 ] 67.18
Lightweight CNN (2024) [ 55 ] 70
Multimodal data fusion with CNN (2025) [ 56 ] 92.5
The proposed algorithm 93.76
```
In the JAFFE dataset, we employed 138 photos for training, accounting
for roughly 70% of the total, and 53 test photos, or 30% of the total. The
complete accuracy of this dataset is approximately 98.86%. Table 5 presents
a comparison between the proposed technique and several precedent
researches on the JAFFE dataset.

**_Table 5_** Demonstrates the efficacy of our proposed algorithm in
comparison to alternative methodologies utilizing the JAFFE
dataset

```
Approach Average accuracy percentage (%)
Fusion-CNN (2023) [ 75 ] 93.07
CNN + SVM(2024) [ 76 ] 89.23
Deep CNN (2025) [ 77 ] 98.12
The proposed algorithm 98.86
```
In the CK+ dataset, we employ 380 photos for training, comprising
approximately 70% of the total comprises 163 images designated for
testing, or roughly 30% of the whole dataset. The total correctness of this
dataset is approximately 97.88%. Table 6 presents a comparison between
the proposed approach and several preceding efforts on the CK+ dataset.


**_Table 6_** Demonstrates the efficacy of our suggested algorithm
in comparison to alternative methodologies on the CK+ dataset

**Approach Average accuracy percentage (%)
Fusion-CNN (2023) [ 75 ]** 98.22
**CNN + SVM (2024) [ 76 ]** 96.19
**Deep CNN (2025) [ 77 ]** 99.3
**The proposed algorithm** 97.88
We note that with the three datasets proposed our performance,
producing satisfactory outcomes, with an average accuracy rate of
approximately 96.83% in comparison to the other datasets.

**4.3 Evaluation of AlexNet on the Test Set**

Upon training AlexNet, we attained the subsequent accuracies across all
categories (Fig. 13 ).

**_Fig. 13_** Accuracies across all classes AlexNet

Overall, our network achieved an average accuracy of 87% across all
categories, with the highest accuracy in the neutral category and the lowest
in the sad category.


##### 5 Conclusion

Using a convolutional network built on the AlexNet architecture, this study
introduces a new framework for deep neural networks that can
automatically identify facial expressions in real-time. Each of the five
convolutional layers in the suggested network is followed by max pooling,
and then there are five fully linked layers. Extensive experimental testing
on the three most popular facial expression recognition databases, namely
JAFEE, FER2013, and CK+, revealed encouraging outcomes. The results
show that our network is better than existing state-of-the-art methods,
which usually maximize the organized features and classifier parameters on
the datasets. By minimizing operational requirements for network training
and improving classification accuracy in subject-specific and cross-database
evaluation settings, the proposed method outperforms existing CNN
algorithms.

**Acknowledgements**
We would like to express our gratitude for the help provided by the
following: a 64-bit system, an NVIDIA GeForce GTX 1070T Gamer, an
Intel Core i7-9700K with 3.6 GHz and 16 GB of RAM, and the
construction of our solution in C++ using the OpenCV library.

##### References

1. Tarnowski, P., Ko lodziej, M., Majkowski, A., Rak, R.J.: Emotion recognition using facial
    expressions. Procedia Comput. Sci. **108** , 1175–1184 (2017)
2.
    Everything about emotion recognition. https:// sightcorp. com/ knowledge-base/ emotion-
    recognition/. Accessed 20 Feb. 2021
3.
    Aneja, D., Colburn, A., Faigin, G., Shapiro, L., Mones, B.: Modeling stylized character
    expressions via deep learning. In: Asian Conference on Computer Vision, pp. 136–153. Springer
    (2016)
4.
    Mollahosseini, A., Chan, D., Mahoor, M.H.: Going deeper in facial expression recognition using
    deep neural networks. In: 2016 IEEE Winter Conference on Applications of Computer Vision
    (WACV), pp. 1–10. IEEE (2016)
5.
    Liu, P., Han, S., Meng, Z., Tong, Y.: Facial expression recognition via a boosted deep belief
    network. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    pp. 1805–1812 (2014)


6.
Han, K., Yu, D., Tashev, I.: Speech emotion recognition using deep neural network and extreme
learning machine. In: Fifteenth Annual Conference of the International Speech Communication
Association (2014)
7.
Clavel, C., Vasilescu, I., Devillers, L., Richard, G., Ehrette, T.: Fear-type emotion recognition for
future audio-based surveillance systems. Speech Commun. **50** (6), 487–503 (2008)
[Crossref]
8.
Petrantonakis, P.C., Hadjileontiadis, L.J.: Emotion recognition from EEG using higher order
crossings. IEEE Trans. Inf Technol. Biomed. **14** (2), 186–197 (2009)
[Crossref]
9.
Wu, C.-H., Chuang, Z.-J., Lin, Y.-C.: Emotion recognition from text using semantic labels and
separable mixture models. ACM Trans. Asian Language Inf. Process. (TALIP) **5** (2), 165–183
(2006)
10.
Lyons, M., Kamachi, M., Gyoba, J.: Japanese female facial expression (jaffe) database (2017)
11.
Goodfellow, I.J., Erhan, D., Carrier, P.L., Courville, A., Mirza, M., Hamner, B., Cukierski, W.,
Tang, Y., Thaler, D., Lee, D.-H., et al.: Challenges in representation learning: A report on three
machine learning contests. In: International Conference on Neural Information Processing, pp.
117–124. Springer (2013)
12.
Minaee, S., Minaei, M., Abdolrashidi, A.: Deep-emotion: facial expression recognition using
attentional convolutional network. Sensors **21** (9), 3046 (2021)
[Crossref]
13.
Han, X., Zhong, Y., Cao, L., Zhang, L.: Pre-trained alexnet architecture with pyramid pooling
and supervision for high spatial resolution remote sensing image scene classification. Remote
Sens. **9** (8), 848 (2017)
14.
Huang, Y., Chen, F., Lv, S., Wang, X.: Facial expression recognition: a survey. Symmetry **11** (10),
1189 (2019)
15.
Georgescu, M.-I., Ionescu, R.T., Popescu, M.: Local learning with deep and handcrafted features
for facial expression recognition. IEEE Access **7** , 64827–64836 (2019)
[Crossref]
16.
Alexandre, G.R., Soares, J.M., Thé, G.A.P.: Systematic review of 3d facial expression
recognition methods. Pattern Recogn. **100** , 107–108 (2020)
[Crossref]
17.
Ekman, P., Friesen, W.V.: Constants across cultures in the face and emotion. J. Personal. Soc.
Psychol. **17** (2), 124 (1971)
18.
Friesen, E., Ekman, P.: Facial action coding system: a technique for the measurement of facial
movement. Palo Alto **3** (2), 5 (1978)
19.
Levi, G., Hassner, T.: Emotion recognition in the wild via convolutional neural networks and
mapped binary patterns. In: Proceedings of the 2015 ACM on International Conference on
Multimodal Interaction, pp. 503–510 (2015)


20.
Khan, R., Sharif, O.: A literature review on emotion recognition using various methods. Glob. J.
Comput. Sci. Technol. (2017)
21.
Zhang, S., Wang, X., Zhang, G., Zhao, X.: Multimodal emotion recognition integrating affective
speech with facial expression. WSEAS Trans. Signal Process. **10** (2014), 526–537 (2014)
22.
Hough, P.V.: Method and means for recognizing complex patterns, US Patent 3,069,654 (1962)
23.
Chen, J., Chen, Z., Chi, Z., Fu, H., et al.: Facial expression recognition based on facial
components detection and hog features. In: International Workshops on Electrical and Computer
Engineering Subfields, pp. 884–888 (2014)
24.
Shan, C., Gong, S., McOwan, P.W.: Facial expression recognition based on local binary patterns:
a comprehensive study. Image Vis. Comput. **27** (6), 803–816 (2009)
25.
Whitehill, J., Omlin, C.W.: Haar features for facs au recognition. In: 7th International
Conference on Automatic Face and Gesture Recognition (FGR06), p. 5. IEEE (2006)
26.
Bartlett, M.S., Littlewort, G., Frank, M., Lainscsek, C., Fasel, I., Movellan, J.: Recognizing
facial expression: machine learning and application to spontaneous behavior. In: 2005 IEEE
Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’05), vol. 2,
pp. 568–573. IEEE (2005)
27.
Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V.,
Rabinovich, A.: Going deeper with convolutions. In: Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 1–9 (2015)
28.
Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A.,
Khosla, A., Bernstein, M., et al.: Imagenet large scale visual recognition challenge. Int. J.
Comput. Vis. **115** (3), 211–252 (2015)
[MathSciNet][Crossref]
29.
Lin, M., Chen, Q., Yan, S.: Network in network (2013). arXiv: 1312. 4400
30.
Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., Salakhutdinov, R.: Dropout: a simple
way to prevent neural networks from overfitting. J. Mach. Learn. Res. **15** (1), 1929–1958 (2014)
[MathSciNet]
31.
Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with deep convolutional
neural networks. Adv. Neural Inf. Process. Syst. **25** , 1097–1105 (2012)
32.
Uddin, M.Z., Hassan, M.M., Almogren, A., Zuair, M., Fortino, G., Torresen, J.: A facial
expression recognition system using robust face features from depth videos and deep learning.
Comput. Electr. Eng. **63** , 114–125 (2017)
[Crossref]
33.
Li, W., Huang, D., Li, H., Wang, Y.: Automatic 4d facial expression recognition using dynamic
geometrical image network. In: 2018 13th IEEE International Conference on Automatic Face &
Gesture Recognition (FG 2018), pp. 24–30. IEEE (2018)


34.
Chang, F.-J., Tran, A.T., Hassner, T., Masi, I., Nevatia, R., Medioni, G.: Expnet: Landmark-free,
deep, 3d facial expressions. In: 2018 13th IEEE International Conference on Automatic Face &
Gesture Recognition (FG 2018), pp. 122–129. IEEE (2018)
35.
Oyedotun, O.K., Demisse, G., El Rahman Shabayek, A., Aouada, D., Ottersten, B.: Facial
expression recognition via joint deep learning of rgb-depth map latent representations. In:
Proceedings of the IEEE International Conference on Computer Vision Workshops, pp. 3161–
3168 (2017)
36.
Li, H., Sun, J., Xu, Z., Chen, L.: Multimodal 2d+ 3d facial expression recognition with deep
fusion convolutional neural network. IEEE Trans. Multimedia **19** (12), 2816–2831 (2017)
37.
Jan, A., Ding, H., Meng, H., Chen, L., Li, H.: Accurate facial parts localization and deep learning
for 3d facial expression recognition. In: 2018 13th IEEE International Conference on Automatic
Face & Gesture Recognition (FG 2018), pp. 466–472. IEEE (2018)
38.
Wei, X., Li, H., Sun, J., Chen, L.: Unsupervised domain adaptation with regularized optimal
transport for multimodal 2d+ 3d facial expression recognition. In: 2018 13th IEEE International
Conference on Automatic Face & Gesture Recognition (FG 2018), pp. 31–37, IEEE (2018)
39.
Bendjillali, R.I., Beladgham, M., Merit, K., Taleb-Ahmed, A.: Improved facial expression
recognition based on dwt feature for deep CNN. Electronics **8** (3), 324 (2019)
[Crossref]
40.
Mishra, N.K., Singh, S.K.: Face recognition using 3d CNNS (2021). arXiv:2102.01441
41.
Dhall, A., Goecke, R., Joshi, J., Wagner, M., Gedeon, T.: Emotion recognition in the wild
challenge 2013. In: Proceedings of the 15th ACM on International Conference on Multimodal
Interaction, pp. 509–516 (2013)
42.
Dhall, A., Goecke, R., Lucey, S., Gedeon, T.: Static facial expression analysis in tough
conditions: data, evaluation protocol and benchmark. In: 2011 IEEE International Conference on
Computer Vision Workshops (ICCV Workshops), pp. 2106–2112, IEEE (2011)
43.
Ekman, P., Freisen, W.V., Ancoli, S.: Facial signs of emotional experience. J. Personal. Soc.
Psychol. **39** (6), 1125 (1980)
44.
Eleftheriadis, S., Rudovic, O., Pantic, M.: Discriminative shared Gaussian processes for
multiview and view-invariant facial expression recognition. IEEE Trans. Image Process. **24** (1),
189–204 (2014)
[MathSciNet][Crossref]
45.
Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R., Guadarrama, S., Darrell,
T.: Caffe: convolutional architecture for fast feature embedding. In: Proceedings of the 22nd
ACM International Conference on Multimedia, pp. 675–678 (2014)
46.
Hu, M., Yang, C., Zheng, Y., Wang, X., He, L., Ren, F.: Facial expression recognition based on
fusion features of center-symmetric local signal magnitude pattern. IEEE Access **7** , 118435–
118445 (2019)
[Crossref]


47.
Sharma, S., Kumar, V.: Voxel-based 3d face reconstruction and its application to face recognition
using sequential deep learning. Multimedia Tools Appl. 1–28 (2020)
48.
Ahmed, F., Bari, H., Hossain, E.: Person-independent facial expression recognition based on
compound local binary pattern (clbp). Int. Arab J. Inf. Technol. **11** (2), 195–203 (2014)
49.
Sharma, S., Kumar, V.: 3d landmark-based face restoration for recognition using variational
autoencoder and triplet loss. IET Biom. **10** (1), 87–98 (2021)
[Crossref]
50.
Khorrami, P., Le Paine, T., Huang, T.S.: Do deep neural networks learn facial action units when
doing expression recognition?. Comput. Vis. Pattern Recogn. (2015). arXiv: 1510. 02969
51.
Meng, Z., Liu, P., Cai, J., Han, Sh., Tong, Y.: Identity-aware convolutional neural network for
facial expression recognition. In: 2017 12th IEEE International Conference on Automatic Face &
Gesture Recognition (FG 2017). IEEE (2017)
52.
Shima, Y., Omori, Y.: Image augmentation for classifying facial expression images by using deep
neural network pre-trained with object image database. In: Proceedings of the 2018 the 3rd
International Conference on Robotics, Control and Automation, pp. 140–146 (2018)
53.
Ezerceli, Ö., Eskil, M.T.: Convolutional neural network (CNN) algorithm based facial emotion
recognition (FER) system for FER-2013 dataset. In: 2022 International Conference on Electrical,
Computer, Communications and Mechatronics Engineering (ICECCME). IEEE, Maldives
(2022)
54.
Bapat, M.M., Patil, C.H., Mali, S.M.: Database development and recognition of facial expression
using deep learning. Res. Sq. **17** , 1–20 (2023)
55.
Grover, R., Bansal, S.: Efficient facial expression recognition through lightweight CNN
technique on public datasets. SN Comput. Sci. (Springer Nature Link) **6** (15), 19 (2024)
56.
Chindiyababy, U., Kakkar, P., Vedula, J., Yunus, J., Umidbek, A., Sharma, S.: Deep learning-
based facial emotion recognition for advanced human-computer interaction. In: 2025 First
International Conference on Advances in Computer Science, Electrical, Electronics, and
Communication Technologies (CE2CT). IEEE, Nainital, India (2025)
57.
Ahlberg, J.: Candide-3-an updated parameterised face (2001)
58.
Saad, N.: Reconnaissance tridimensionnelle de visage. Ph.D. thesis, Université Mohamed Khider
de Biskra (2018)
59.
Bhople, A.R., Shrivastava, A.M., Prakash, S.: Point cloud based deep convolutional neural
network for 3d face recognition. Multimedia Tools Appl. 1–23 (2020)
60.
Saad, N., Djedi, N.: 3d face recognition related with facial expressions based on MB-LBP
method. Courier de savoir **25** , 93–102 (2018)
61.
Ravi, R., Yadhukrishna, S., et al.: A face expression recognition using CNN & LBP. In: 2020
Fourth International Conference on Computing, Methodologies and Communication (ICCMC),
pp. 684–689. IEEE (2020)


62.
Saad, N., Djedi, N.: Recognition of 3d faces with missing parts based on SIFT and LBP methods.
In: Biometric Security and Privacy, pp. 273–297. Springer (2017)
63.
Gross, R., Matthews, I., Cohn, J., Kanade, T., Baker, S.: Multi-pie. Image Vis. Comput. **28** (5),
807–813 (2010)
[Crossref]
64.
Microsoft kinect. https:// msdn. microsoft. com/ en-us/ library/ jj131033. aspx. Accessed 20 Feb.
2021
65.
Gilligan, T., Akis, B.: Emotion AI real-time emotion detection using CNN. Stanford University
(2016); Microsoft kinect. https:// web. stanford. edu/ class/ cs231a/ prev_ projects_ 2016/ emotion-ai-
real. pdf. Accessed 13 Nov. 2023
66.
Arora, S., Bhaskara, A., Ge, R., Ma, T.: Provable bounds for learning some deep representations.
In: International Conference on Machine Learning, PMLR, pp. 584–592 (2014)
67.
Sun, Y., Liang, D., Wang, X., Tang, X.: Deepid3: face recognition with very deep neural
networks (2015). arXiv: 1502. 00873
68.
Bal, E., Harden, E., Lamb, D., Van Hecke, A.V., Denver, J.W., Porges, S.W.: Emotion
recognition in children with autism spectrum disorders: relations to eye gaze and autonomic
state. J. Autism Dev. Disord. **40** (3), 358–370 (2010)
[Crossref]
69.
Barsoum, E., Zhang, C., Ferrer, C.C., Zhang, Z.: Training deep networks for facial expression
recognition with crowd-sourced label distribution. In: Proceedings of the 18th ACM
International Conference on Multimodal Interaction, pp. 279–283 (2016)
70.
Orozco, D., Lee, C., Arabadzhi, Y., Gupta, D.: Transfer learning for facial expression
recognition, Florida State Univ., Tallahassee, FL, USA, Tech. Rep 7 (2018)
71.
Melinte, D.O., Vladareanu, L.: Facial expressions recognition for human–robot interaction using
deep convolutional neural networks with rectified adam optimizer, **20** (8), 2393 (2020)
72.
Li, K., Jin, Y., Akram, M.W., Han, R., Chen, J.: Facial expression recognition with convolutional
neural networks via a new face cropping and rotation strategy. Vis. Comput. **36** (2), 391–404
(2020)
[Crossref]
73.
Chew, S.W., Rana, R., Lucey, P., Lucey, S., Sridharan, S.: Sparse temporal representations for
facial expression recognition, PSIVT 2011, Part II, pp. 311–322. Springer, Berlin Heidelberg,
LNCS 7088 (2011)
74.
Agrawal, A., Mittal, N.: Using CNN for facial expression recognition: a study of the effects of
kernel size and number of filters on accuracy. Vis. Comput. **36** (2), 405–412 (2020)
[Crossref]
75.
Jabbooree, A.I., Khanli, L.M., Salehpour, P., Pourbahrami, S.: A novel facial expression
recognition algorithm using geometry β-skeleton in fusion based on deep CNN. Image Vis.
Comput. J. (Elsevier) **134** (2023)


76.
Jabbooree, A.I., Khanli, L.M., Salehpour, P., Pourbahrami, S.: Geometrical facial expression
recognition approach based on fusion CNN-SVM. Int. J. Intell. Eng. Syst. **17** (1) (2024)
77.
Elsheikh, R.A., Mohamed, M.A., Abou-Taleb, A.M., Ata, M.M.: Improving deep feature
adequacy for facial emotionrecognition: the impact of anti‑aliasing on landmark‑based and
pixel‑based approaches. Multimedia Tools Appl. J. (Springer) (2025)
_OceanofPDF.com_


(1)
(2)
(3)

© The Author(s), under exclusive license to Springer Nature Switzerland AG 2026
K. Slimani et al. (eds.), _Emotion and Facial Recognition in Artificial Intelligence: Sustainable Multidisciplinary
Perspectives and Applications_ , Information Systems Engineering and Management 78
https://doi.org/10.1007/978-3-032-14778-3_7

#### Feature Aggregation for Efficient Continual

#### Learning of Complex Facial Expressions

Thibault Geoffroy^1 , Myriam Maumy^2 and Lionel Prevost^1 ,^3
Learning Data Robotics (LDR) esieaLab, ESIEA, Paris, France
Laboratoire Arènes, UMR CNRS 6051, équipe RSMS EHESP, Paris, France
CRI, Universite Paris 1 Panthéon-Sorbonne, Paris, France

```
Thibault Geoffroy (Corresponding author)
Email: thibault.geoffroy@esiea.fr
```
```
Myriam Maumy
Email: myriam.maumy@ehesp.fr
```
```
Lionel Prevost
Email: lionel.prevost@esiea.fr
```
**Abstract**
As artificial intelligence (AI) systems become increasingly embedded in our daily life,
the ability to recognize and adapt to human emotions is essential for effective human–
computer interaction. Facial expression recognition (FER) provides a primary channel
for inferring affective states, but the dynamic and culturally nuanced nature of
emotions requires models that can learn continuously without forgetting prior
knowledge. In this work, we propose a hybrid framework for FER in a continual
learning setting that mitigates catastrophic forgetting. Our approach integrates two
complementary modalities: deep convolutional features and facial Action Units (AUs)
derived from the Facial Action Coding System (FACS). The combined representation is
modelled through Bayesian Gaussian Mixture Models (BGMMs), which provide a
lightweight, probabilistic solution that avoids retraining while offering strong
discriminative power. Using the Compound Facial Expression of Emotion (CFEE)
dataset, we show that our model can first learn basic expressions and then
progressively recognize compound expressions. Experiments demonstrate improved
accuracy, stronger knowledge retention, and reduced forgetting. This framework
contributes to the development of emotionally intelligent AI systems with applications
in education, healthcare, and adaptive user interfaces.


**Keywords** Facial expression recognition – Continual learning – Feature fusion –
Action units – Probabilistic models

**1 Introduction**

Advances in Artificial Intelligence (AI) have made intelligent systems an integral part
of our daily lives. With increasing integration into our technological environment, the
ability of AI to assess and distinguish between different emotional states has become
essential for improving human–computer interaction [ 1 , 2 ]. Facial Expression
Recognition (FER) provides a channel to infer, analyze, and study how individuals
express their emotional states through facial expressions.
The most common categorization of emotions is Ekman’s basic emotions [ 3 ],
which consist of six basic categories: anger, fear, disgust, happiness, sadness, and
surprise (see Fig. 1 ), along with the neutral state representing the absence of emotion.
This representation has become a baseline for emotion analysis. Its simplicity and
universality have enabled the creation of numerous datasets in the FER context, and
most FER algorithms rely on Ekman’s annotation as a basis for classification. While
this categorization is useful, in real-life contexts it can be limiting due to the
complexity and subtlety of emotions. For this reason, other annotation systems were
developed to analyze emotions in specific contexts such as learning. For example,
Arthur Graesser and Sidney D’Mello [ 4 ] used a set of annotations for emotions
occurring during the learning of difficult material. Such context-specific annotations
can be very useful to study emotional states in activities like learning, job interviews,
or clinical settings. However, creating datasets for such subtle expressions is both
complex and time-consuming, which also complicates the development of FER
systems for their classification. An alternative approach is to study compound
expressions (Fig. 2 ) which represent facial expressions formed by the combination of
two or more basic emotions. These expressions are not represented by simple
combined activations of the facial muscles from the separate expressions but are
considered distinct and recognizable affective states which allow us to move beyond
the basic expressions and study more nuanced and complex expressions which are
more representative of real-life interactions.


**_Fig. 1_** Ekman's 6 emotional expressions [ 5 ]


**_Fig. 2_** Compound facial expressions [ 6 ]
In addition to end-to-end systems such as Convolutional Neural Networks (CNNs),
which learn features jointly with expression classification, other methods can be used
to extract features for FER. Facial Action Units (AUs), introduced by Ekman and
Friesen [ 7 ], categorize facial muscle activations, and provide a structured and objective
representation of facial expressions. Integrating multiple feature types for classification
has already proven effective [ 8 ], particularly in scenarios with limited training data,
where continual learning is especially challenging. Although CNN-extracted features
are powerful for the tasks they are trained on, their generalization is reduced on unseen
tasks, and continual training of CNNs can increase catastrophic forgetting [ 9 ].
Continual learning is a machine learning paradigm that enables models to learn
continuously during deployment. In this setting, data become available sequentially,
either one instance at a time or in batches, and the model is trained with little or no
access to previous data. Without appropriate strategies, this leads to significant
knowledge loss on previously learned tasks, a phenomenon known as catastrophic
forgetting. This issue is one of the main challenges in continual learning and has been
extensively studied [ 9 – 11 ]. The objective of continual learning is to acquire new
information while retaining old knowledge. Some methods address this by storing parts
of the dataset, but this increases memory requirements and raises privacy concerns. For
this reason, example-free methods are increasingly studied [ 12 , 13 ].
Many applications of continual learning involve learning new classes of objects.
Datasets such as CIFAR-100 [ 14 ] and ImageNet [ 15 ] are benchmarks for such tasks,
where the model must recognize very different objects, ranging from apples to cars or
even porcupines. These diverse recognition tasks impose high complexity, as the model
must make accurate predictions without suffering catastrophic forgetting. In the FER
context, however, continual learning tasks are not completely disjoint: all data consist
of faces, and the goal is to detect and classify new emotional expressions. For example,
the Compound Facial Expressions of Emotion (CFEE) dataset [ 6 ] contains both
Ekman’s basic emotions and compound expressions. These compound expressions are
distinct and culturally consistent facial patterns formed by the combination of two or
more basic emotions, such as “happily surprised” or “sadly fearful.”
Several approaches have been proposed for such tasks, including regularization,
and retraining of the model, but they perform poorly in this context. Moreover, since
compound expressions are less frequent, the available labeled data are much more
limited than for basic expressions, making neural network retraining difficult.
Since Action Units alone are not sufficient to classify compound expressions, and
continuously retraining CNN feature extractors leads to catastrophic forgetting, our
approach combines low-level features from a pre-trained CNN [ 16 ] with high-level
features derived from Action Units. This hybrid representation enhances accuracy in
compound expression recognition.
For the inference itself, most state-of-the-art FER approaches rely on deep neural
networks, but these models are not always appropriate for continual learning.
Probabilistic models offer several advantages: their ability to model class-conditional


distributions allows class-by-class analysis and reduces vulnerability to catastrophic
forgetting. Gaussian Mixture Models (GMMs) have shown strong performance in
modeling data distributions, as FER classes often form clusters in the feature space.
However, classical GMMs require fixing the number of mixture components in
advance, which is a costly hyperparameter to optimize. To address this limitation, we
employ Bayesian Gaussian Mixture Models (BGMMs), which automatically infer the
optimal number of mixture components. This enables the model to adapt its complexity
to the data and represent class distributions more robustly.
Our approach combines multimodal feature vectors with Bayesian Gaussian
Mixture Models to achieve strong performance in facial expression recognition within
a continual learning framework.
The two main contributions of this work are:
Demonstrating the utility of multimodal feature fusion for accurately classifying
both basic and compound facial expressions.
Introducing an efficient probabilistic model that enables continual learning with
minimal training overhead while effectively mitigating catastrophic forgetting
compared to CNN-based approaches.

**1.1 Related Works
Facial Expression Recognition**
Facial Expression Recognition (FER) is a pivotal domain in affective computing,
aimed at automatically identifying human emotional expressions through facial cues.
Traditional FER systems have historically relied on geometric or appearance-based
features such as Active Shape Models (ASM) [ 17 ], Local Binary Patterns (LBP) [ 18 ],
or Gabor filters [ 19 ]. Though robust, these methods struggle with generalization when
applied to unconstrained data [ 20 ].
With the rise of deep learning, Convolutional Neural Networks (CNNs) [ 21 ] and
their extensions such as Long Short-Term Memory (LSTM) models [ 22 ] have
significantly boosted FER performance, especially on datasets such as CK + [ 23 ],
FER2013 [ 24 ], or RAF-DB [ 25 ]. These models are often fine-tuned from pretrained
image classification architectures (e.g., VGG, ResNet, EfficientNet) and consistently
outperform classical methods by learning discriminative features in an end-to-end
fashion [ 20 ].
However, these deep learning approaches are typically trained in classical setting
where the whole set of training data arrives all at once and are prone to catastrophic
forgetting when required to adapt to new distributions of data or new classes. This
makes them not suitable for continual learning.

**Continual Learning**
Continual learning has emerged as a necessary framework for machine learning models
that must adapt over time to new data distributions and classes without forgetting
previously acquired knowledge. The field categorizes approaches into regularization-


based methods (EWC [ 9 ]), replay-based strategies (FearNet [ 26 ]), architecture-based
expansions (ZOO [ 27 ]) representation-based (DualNet [ 28 ]) or optimization-based
methods (GEM [ 13 ]).
Unlike generic object classification benchmarks such as CIFAR-100 or ImageNet
[ 15 ], FER tasks involve overlapping domains (all tasks concern facial data) but shifting
label semantics (e.g., from basic to compound expressions). This makes continual
learning in FER a more structured and domain-specific challenge. The Compound
Facial Expression of Emotion (CFEE) dataset, which includes both basic and
compound expressions, is particularly well suited for investigating these dynamics
thanks to its natural separation between simple and more complex emotions.
Yet, most continual learning methods are evaluated on object recognition datasets
with clearly distinct tasks and classes. In contrast, FER involves different types of
expressions derived from the same facial images, meaning that all tasks share a
common basis of recognizing facial features. This strong similarity between tasks calls
for approaches that can better exploit the inherent structure of FER data while
remaining computationally efficient.

**Generative and Probabilistic Models**
Generative models such as GANs offer a solution to catastrophic forgetting by
approximating past data distributions, enabling the virtual replay of old task data.
However, these methods can be resource-intensive and remain prone to forgetting
when continuously trained [ 29 ]. An alternative lies in probabilistic models, which
explicitly model data distributions. By modeling each class distribution separately, they
mitigate catastrophic forgetting by preventing interference between new and old
classes.
Among these, Gaussian Mixture Models (GMMs) [ 30 ] and Bayesian Gaussian
Mixture Models (BGMMs) [ 31 ] have demonstrated strong discriminative capabilities
in FER [ 32 ]. In FER, where class distributions are naturally clustered yet sometimes
highly overlapping (particularly for compound emotions), GMM-based approaches
provide a lightweight and interpretable solution. Our work integrates BGMMs trained
conditionally on each class within a multimodal representation space (combining
CNN-extracted features and Action Units), enabling continual acquisition of
knowledge on increasingly complex expressions while mitigating catastrophic
forgetting.
Prior FER studies applying GMM or BGMM have mostly done so in classical
settings and typically rely on a single feature modality, limiting their capabilities for
complex tasks like recognizing compound expressions. Our work addresses these
limitations by using BGMMs with several feature vectors extracted from the images
combining CNN-extracted features and Action Units. This approach uses both low-
level and high-level information, enabling more resilient continual learning and
reducing catastrophic forgetting through the continual learning process.


**2 Main Content**

**2.1 Method**

**_2.1.1 Intuition_**
The main idea behind our model is the following: since we are working on a set of
related tasks centered on facial image analysis, we chose to specialize the model within
this specific domain rather than extend it to unrelated tasks. In this context, combining
different types of features, with high-level features such as Action Units and low-level
features extracted by a CNN-based encoder, enables the model to classify not only
basic expressions but also more subtle ones, including compound expressions.

**_2.1.2 Architecture_**
Our model consists of three parts: an ensemble of feature extractors, a feature
aggregator, and a probabilistic classifier. The feature extractors can be of different
types (CNN-based features, Action Units, etc.) and are designed to compute
complementary information, so that one can help compensate for the other’s errors. In
our implementation, we employ two feature extractors. The first is a CNN trained
exclusively on basic emotions, from which we use the features extracted at the
penultimate layer. The second is an Action Units extractor [ 33 ]. These two sources of
information, one providing a low-level feature vector trained on the dataset and the
other a high-level feature vector, together form a strong representation of the subject’s
face and emotional expression.
The second component of the model is a feature aggregator that combines the
outputs of both extractors. In this initial experiment, the aggregation strategy is limited
to a simple concatenation of the two feature vectors, namely those extracted from the
CNN and the Action Units. This serves as a first step to evaluate the viability of the
approach. Future work will explore more advanced aggregation strategies, such as
weighted fusion or Principal Component Analysis (PCA), to construct a more robust
and discriminative feature representation.
The final part of the model is a set of K Gaussian mixtures, where K corresponds to
the number of classes. Each mixture contains one or more Gaussian distributions
_Gk_ =( _μk_ ,Σ _k_ ). To determine the optimal number of components, we rely on the
Bayesian Gaussian Mixture approach [ 31 ], which prunes redundant components during
training [ 34 ]. Each conditional Gaussian mixture is initialized when its class appears
and is trained to learn the conditional distribution of its class. Thus, each Gaussian
mixture acts as an expert specialized in its own class (Fig. 3 ).


**_Fig. 3_** Model architecture for 2 tasks (task1 = class1 + class2 and task2 = class3)

**_2.1.3 Algorithm_**
During training on the initial task, the different modalities are aggregated into a feature
vector that is used to train the Gaussian mixtures. For this concatenation, we adopt a
basic strategy: the low-level feature vectors are normalized between 0 and 1 and then
combined with the high-level features to form a single vector containing both
modalities.
After aggregation, the training dataset is divided into n subsets, each corresponding
to one class of the initial task. These subsets are then used to train Gaussian mixtures.


Each mixture is trained with the Bayesian Gaussian Mixture approach [ 31 ]. We
observed that using diagonal covariances matrices gives the best performance when
using Bayesian Gaussian Mixtures as shown in Table 1. A complete covariance matrix
overfit strongly and is not able to perform with the testing set. In constrast a simple
variance producing spherical representation of data shows lower performances and the
best performances are achieved with a diagonal covariance matrix.

**_Table 1_** Accuracy of the combined features with different type of covariances, the diagonal covariances
show the best performances for these features

**Accuracy (mean ± std)Simple variance Diagonal covariance matrix General covariance matrix
Train** 0. 5838 ± 0. 0087 0. 6067 ± 0. 003 1.0000
**Test** 0. 5540 ± 0. 0047 **0. 5850** ± **0. 01** 0.3745
For subsequent tasks, the same process is repeated while applying the same
aggregation strategy. The Gaussian mixtures specialized in the new tasks are trained in
the same way as for the initial task.
During inference, each facial image is processed by the feature extractors and
aggregated following the same strategy as in training. The resulting feature vector is
then passed through all Gaussian mixtures to compute the corresponding log-
likelihoods. The mixture that produces the highest log-likelihood is considered the best
match, and its associated class is returned as the model’s prediction.
The training and inference algorithms of this architecture are presented below:



**2.2 Experimental Protocol**

**_2.2.1 Continual Learning_**
Continual learning can be separated into different types of scenarios depending on how
the set of labels and the amount of data points provided during each new task vary. In
this study, we focus on a common scenario called Class-Incremental Learning. In this
setting, tasks are presented to the model sequentially, and the model learns each task
without access to the data from previous ones. Furthermore, the labels for each task are
provided to the model for training but not in the inference phase.
Formally, the training set for a task _t_ can be defined as _D_ t={ _X_ t, _Y_ t} where _X_ t
represents the data of a given task and _Y_ t the corresponding labels. We assume that
each task follows a distribution _D_ t:= _p_ ( _X_ t, _Y_ t) and that there is no difference in
distribution between training and testing sets. In this study, each task’s data is assumed
to arrive in a single batch.
Since our work focuses on facial expression classification, we divided the dataset
into two parts. The initial task, used to train the feature extractor and initialize the
Gaussian mixtures, consists of Ekman’s basic expressions (joy, sadness, surprise, fear,


anger, disgust, and neutral). To organize the remaining classes into multiple tasks, we
grouped the compound expressions according to their emotional proximity. We created
incremental tasks corresponding to compound variants of joy, sadness, fear, anger, and
disgust. This design allows us to evaluate how our architecture handles each family of
basic emotions throughout the continual learning process. Based on this principle, the
incremental tasks are defined as follows:

```
Compound joy : happily surprised, happily disgusted, awed.
Compound sadness : sadly fearful, sadly angry, sadly surprised, sadly disgusted.
Compound fear : fearfully angry, fearfully surprised, fearfully disgusted.
Compound anger : angrily surprised, angrily disgusted, hatred.
Compound disgust : disgustedly surprised, appalled.
```
**_2.2.2 Datasets_**
To enable continual learning across a wide range of emotional expressions, we require
an annotated dataset containing the target categories. For this purpose, we use the
Compound Facial Expressions of Emotions (CFEE) dataset. It contains 5,060 images
from 230 subjects, distributed across 22 classes. The images were collected in a
controlled laboratory setting: each face is centered against a white background, and
each subject displays several expressions. The subjects vary in both gender and
ethnicity.
In terms of labeling, the dataset includes seven basic emotion classes (including
neutral) and twelve compound emotion classes such as happily-surprised or angrily-
disgusted. Although compound expressions are composed of multiple basic emotions,
they are not merely the mechanical combination of facial muscle activations from their
components. Instead, they are considered distinct, recognizable expressions that can be
reliably used for model inference.
In addition to these nineteen basic and compound expressions, the dataset also
contains three so-called complex emotions: appalled, hatred, and awed. While not
formally categorized as compound expressions, they can be interpreted as
combinations of multiple affective states. For instance, appalled may be described as
fear mixed with disgust, hatred as strong anger combined with disgust, and awed as
intense joy blended with fear. These associations are derived from the semantic
meaning of the original English terms.

**_2.2.3 Metrics_**
Continual learning performance needs to be evaluated from three perspectives: overall
performance across all tasks, memory stability on previously learned tasks, and
learning plasticity on new tasks [ 35 ]. To address these aspects, we rely on three
metrics.
The overall performance is assessed using Average Accuracy (AA) and Average
Incremental Accuracy (AIA). Memory stability is measured using the Forgetting
Measure (FM), which quantifies the average loss of performance on past tasks. Finally,


learning plasticity is measured using the Intransigence Measure (IM), which captures
the impact of learning new tasks on model performance.
Average Accuracy (AA) at task k evaluates model performance across all tasks
learned so far. It is defined as the mean test accuracy on all k tasks:

```
AAk =
```
```
1
k
```
```
k
∑
j = 1
```
```
ak , j , (1)
```
where _a_ k,j∈[ 0 , 1 ] represents the accuracy on the test set of task _j_ after training on task
_k_ .:
The Average Incremental Accuracy (AIA) represent a cumulative view of how AA
evolves over time. It is defined as:

```
AIAk =
```
```
1
k
```
```
k
∑
i = 1
```
```
AAi , (2)
```
The stability of the model during continual learning is measured by the Forgetting
Measure (FM), which evaluates how much accuracy the model has lost on previous
tasks. It is computed by comparing the maximum performance of the model compared
to the current performance.
For task _j_ at step _k_ , forgetting is computed as:

```
fj , k =( ai , j − ak , j ), ∀ j < k. (3)
```
```
The FM at the k-th task is defined as:
```
```
FMk =
```
```
1
k − 1
```
```
k − 1
∑
j = 1
```
```
fj , k. (4)
```
A lower FM indicates a better ability to retain past knowledge and resist
catastrophic forgetting.
The learning plasticity of the model, i.e. its ability to acquire new tasks, is
evaluated using the Intransigence Measure (IM). This metric is defined as the
difference between the performance of a jointly trained reference model and the
performance of the continual learner:

```
IMk = a ∗ k − ak , k (5)
```
where _a_ ∗ _k_ represents the accuracy of randomly initialized reference model trained on

the joint dataset of all the previous tasks ⋃ _kj_ = 1 _Dj_.


A lower IM represents a model that is able to not lose much accuracy between its
continual learning performances and its normal learning performances.
All those metrics are shown in Table 2.

**_Table 2_** Continual learning metrics

```
Measure Role RangeOptimization
goal
Average accuracy Measure the average performance so far [0, 1] Maximize
Average incremental
accuracy
```
```
Measure the trend of improvement through the tasks [0, 1] Maximize
```
```
Forgetting measure Measure how much the old knowledge is forgotten [0, 1] Minimize
Intransigence measure Measure difficulty in learning new tasks compared to
classical training
```
```
[0, 1] Minimize
```
**_2.2.4 Protocol_**
For this experimentation, the first set of features, referred to as the deep features, is
computed using the MobileNet model [ 16 ]. The architecture is modified so that the
penultimate layer outputs a feature vector of 512 dimensions. The model is trained on
the basic emotions using cross-entropy loss. After training the model to its best
accuracy, the entire dataset is passed through the network to extract the embeddings
from this layer.
The second set of features, referred to as the Action Unit (AU) features, is
computed using the OpenFace tool [ 33 ], which provides 17 Action Units representing
the activation of specific facial muscles for each subject.
The two feature vectors are concatenated to generate a third feature vector, referred
to as the merged features. This merged vector is the primary representation used in our
experiments, although we also evaluate the two individual feature sets to compare
performance across modalities.

**2.3 Experimental Results**
In our approach, the initial task of emotion recognition contains the seven basic
emotions. The accuracy of each feature type on these basic emotions is reported in
Table 2 and shows that the performances are competitive with other models.
Throughout the continual learning process, as illustrated in Figs. 4 , 5 , and 6 , accuracy
decreases across all three feature representations. However, the merged feature vector
combining the Action Unit features with the deep features extracted from the CNN
consistently provides better performance. This concatenation yields a relative
improvement of approximately 8% in classification accuracy across tasks, increasing
from 0.530 with action units features alone to 0.575 with merged features, as shown in
Table 3.


**_Fig. 4_** Average accuracy of each task throughout the continual learning of the model trained on the deep feature
vectors, showing the loss of accuracy on the older tasks

**_Fig. 5_** Average accuracy of each task throughout the continual learning of the model trained on action units
features, showing the loss of accuracy on the older tasks


**_Fig. 6_** Average Accuracy (AA) of each task throughout the continual learning of the model trained on merged
feature s showing the improvement of the merged features in comparison to the sole Action Units or deep features

**_Table 3_** Accuracy performances for each task at the end of the training

```
Accuracy
(mean ±
std)
```
```
Basic
expressions
```
```
Compound
joy
```
```
Compound
sadness
```
```
Compound
fear
```
```
Compound
anger
```
```
Compound
disgust
```
```
All
```
```
Deep
features
```
```
0. 654 ± 0. 09 0. 635 ± 0. 14 0. 382 ± 0. 1 0. 384 ± 0. 11 0. 358 ± 0. 07 0. 496 ± 0. 12 0. 511 ± 0. 05
```
```
AU
features
```
```
0. 653 ± 0. 1 0. 693 ± 0. 06 0. 405 ± 0. 09 0. 427 ± 0. 07 0. 352 ± 0. 08 0.527 ± 0.09 0. 530 ± 0. 05
```
```
Merged
features
```
```
0.744 ± 0.02 0.705 ± 0.03 0.467 ± 0.030.441 ± 0.04 0.378 ± 0.03 0.496 ± 0.04 0.575 ± 0.01
```
```
Relative
evolution
```
```
+ 13% + 1% + 15% + 3% + 5% –5% + 8%
```
Figures 7 , 8 , and 9 present the performance of the model using three incremental
metrics: Average Incremental Accuracy, Forgetting Measure, and Intransigence
Measure. These metrics respectively capture the evolution of accuracy during continual
learning, the stability of the model in retaining knowledge from previous tasks, and the
ability of the model to acquire new information during training. Across all three graphs,
the concatenation of deep features and Action Unit features consistently yields the best
results.


**_Fig. 7_** Average Accuracy (AA) and Average Incremental Accuracy (AIA) throughout the continual learning process
for the three feature vectors. Showing the best performances for the merged features in Average Accuracy and
Average Incremental Accuracy


**_Fig. 8_** Forgetting Measure (FM) across the continual learning process for the three feature vectors. The results
indicate that the merged feature representation mitigates forgetting more effectively than individual feature types


**_Fig. 9_** Intransigence Measure (IM) across the continual learning process for the three feature vectors, indicating that
the addition of new tasks has a negligible effect on the model’s performance
In Fig. 7 , we report both the Average Accuracy and Average Incremental Accuracy
of the three feature representations. The concatenated features show clear
improvements in both metrics. While the deep features achieve strong performance on
the initial task, their accuracy declines steadily during continual learning. This behavior
is expected, since the deep features were extracted from a model trained exclusively on
the initial task and thus generalize poorly to new tasks. In contrast, the Action Unit
features start with lower performance on the initial task but degrade less across
incremental tasks. The combined features, however, capture the strengths of both
representations: they perform well on the initial task and retain stronger performance
throughout continual learning. This demonstrates that the BGMM can effectively
leverage the merged feature vector to infer the correct labels, even when the features
are not produced by a model specialized for those classes.
Figure 8 shows the average forgetting of our approach on the three feature vectors.
We observe lower forgetting with the concatenated feature vector, which indicates less
performance loss on previous tasks during the continual learning process. The purpose
of this metric is to highlight the stability of the proposed architecture. We can see that
the merged features start with the same level of forgetting as the deep features on the
first incremental task but gradually forget less than the two individual feature vectors


as training progresses. This demonstrates that the combined features are more resilient
than the separate vectors throughout the continual learning process.
Figure 9 shows the intransigence performance of the model during continual
learning across different tasks. Since our approach learns each class probability
distribution separately, it does not suffer from accuracy loss due to the continual
learning process. As a result, the Intransigence Measure remains equal to or close to
zero for all feature vectors. This reflects the ability of the proposed architecture to
maintain separate representations for each class. Consequently, the number of tasks and
their order do not influence the performance of the architecture. The only task with a
strong impact on overall performance is the initial task, because the CNN used for deep
feature extraction is trained on this smaller set of labels. A reduced label set lowers the
quality of the extracted feature vector and thus affects the full model performance.
For each incremental metric, our approach achieves better performance than the
individual feature vectors. The combined feature representation is able to compensate
for the weaknesses of each separate modality. The proposed architecture suffers less
from catastrophic forgetting, maintains higher accuracy, and does not experience
performance degradation due to the continual learning setting. These results
demonstrate the capability of our approach to effectively address the continual learning
problem in Facial Expression Recognition.

**3 Discussion**

Each feature type exhibits different strengths and weaknesses. Figure 1 illustrates
which features perform better for each expression. In the last row, negative values
correspond to classes where the Action Unit features predicted the correct label while
the deep features did not, and positive values indicate the opposite case. We observe
that the deep features perform better on the basic expressions of the initial task but
show weaker results on compound expressions. In contrast, the Action Unit features
generally perform better on compound expressions, apart from the “appalled” class.
This outcome is consistent with expectations: deep CNN features, trained
specifically on basic expressions, achieve higher accuracy on those categories but
generalize poorly to compound ones. Conversely, Action Unit vectors, which encode
facial muscle activations, demonstrate more stable performance across tasks (Fig. 10 ).


**_Fig. 10_** Difference in correctly recognized test samples between the deep feature model and the AU model on the
CFEE dataset, highlighting that deep features perform better on basic expressions, while AU features show superior
performance on compound expressions
We also evaluated how our approach compares to an idealized scenario in which,
between the two feature sets, the model always selects the best-performing one. To
simulate this, predictions were generated using both models, and a sample was
considered correct if either of the feature-based models produced the right label. As
shown in Table 4 , while the merged features outperform each feature set individually,
they still fall short of the performance achieved by this best potential baseline. This gap
is likely due to the simplicity of the concatenation strategy used in our current
implementation.

**_Table 4_** Final and potential best accuracy of the model through different features

```
Metric Merged features Deep features Action unit features Potential best features
Test accuracy (mean ± std) 0. 614 ± 0. 01 0. 552 ± 0. 02 0. 545 ± 0. 04 0. 738 ± 0. 03
```
**4 Limitations and Future Work**

Our current architecture has several limitations. First, it relies on a large initial task to
train the CNN feature extractor. This dependency reduces the flexibility of the
continual learning analysis, since performance in later tasks is strongly influenced by
the first training step. In future work, we plan to explore other types of feature
extractors and descriptors, such as Histogram of Oriented Gradients (HOG) or Local
Binary Patterns (LBP), to reduce this reliance and diversify the extracted
representations.
A second limitation is the feature fusion strategy, which is currently restricted to
simple concatenation. While this provides an initial proof of concept, it does not fully
exploit the complementarity between modalities. Future work will therefore investigate
more advanced fusion methods, such as dimensionality reduction (e.g., PCA) or
dynamic weighting mechanisms, to better emphasize the most informative components


of the feature vectors. These strategies may further improve performance, particularly
for continually learning on complex compound expressions.
Finally, our experiments were conducted on the CFEE dataset, which contains
numerous labels and was collected under controlled conditions, making training and
evaluation easier. However, this dataset does not capture the variability of real-world
scenarios. As a next step, we plan to extend our experiments to more challenging
datasets such as RAF-DB, which include greater diversity in pose, illumination, and
occlusion, thereby allowing us to evaluate our approach under more realistic
conditions.

**5 Conclusion**

In this work, we introduced a modular architecture for facial expression recognition
(FER) in a continual learning setting. Our approach combines deep features extracted
from a CNN with Action Units (AUs) to build a hybrid representation of facial
expressions. This is then modeled using an ensemble of Bayesian Gaussian Mixture
Models with each one conditionally trained on an emotion class. This architecture
allows the system to incrementally acquire new emotional categories, including
compound expressions, while effectively mitigating catastrophic forgetting, which is a
core challenge in continual learning.
Through experiments conducted on expression-specific datasets Compound Facial
Expression of Emotion (CFEE), we showed that aggregating different feature types
leads to more accurate performance during the continual learning process. The merged
feature consistently outperformed models based on individual feature types across all
evaluated metrics. While our current concatenation strategy provides encouraging
results, it does not yet match the potential upper bound where the most informative
modality is always selected.
This work contributes to the development of adaptive and emotionally intelligent
systems that can evolve without the need for retraining on all past data. In future work,
we aim to explore more advanced feature aggregation mechanisms to further enhance
performance in dynamic and unconstrained real-world environments.

**References**

1. Cowie, R., Douglas-Cowie, E., Tsapatsoulis, N., Votsis, G., Kollias, S., Fellenz, W., Taylor, J.G.: Emotion
    recognition in human-computer interaction. IEEE Signal Process. Mag. **18** , 32–80 (2001). https:// doi. org/ 10.
    1109/ 79. 911197
    [Crossref]
2.
    Picard, R.W.: Affective computing. MIT press (2000)
3.
    Ekman, P., Friesen, W.V.: Constants across cultures in the face and emotion. J. Pers. Soc. Psychol. **17** , 124–129
    (1971). https:// doi. org/ 10. 1037/ h0030377
    [Crossref]


4.
Graesser, A.C., D’Mello, S.: Emotions during the learning of difficult material. Psychol. Learn. Motiv.-Adv.
Res. Theory (2012). https:// doi. org/ 10. 1016/ B978-0-12-394293-7. 00005-4
[Crossref]
5.
Geslin, E.: Process of inducing emotions in virtual environments and video games (2013)
6.
Du, S., Tao, Y., Martinez, A.M.: Compound facial expressions of emotion. Proc. Natl. Acad. Sci. U. S. A. **111**
(2014). https:// doi. org/ 10. 1073/ pnas. 13223551 11
7.
Ekman, P., Friesen, W.V.: Facial action coding system. Environ. Psychol. Nonverbal Behav (1978)
8.
Castellano, G., Kessous, L., Caridakis, G., 2008. Emotion recognition through multiple modalities: face, body
gesture, speech. In: Peter, C., Beale, R. (eds.) Affect and Emotion in Human-Computer Interaction, Lecture
Notes in Computer Science. Springer Berlin Heidelberg, Berlin, Heidelberg, pp. 92–103 (2008). https:// doi. org/

10. 1007/ 978-3-540-85099-1_ 8
9.
Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins, G., Rusu, A.A., Milan, K., Quan, J.,
Ramalho, T., Grabska-Barwinska, A., Hassabis, D., Clopath, C., Kumaran, D., Hadsell, R.: Overcoming
catastrophic forgetting in neural networks. Proc. Natl. Acad. Sci. **114** , 3521–3526 (2017). https:// doi. org/ 10.
1073/ pnas. 16118351 14
[MathSciNet][Crossref]
10.
French, R.M.: Catastrophic forgetting in connectionist networks. Trends Cogn. Sci. **3** , 128–135 (1999). https://
doi. org/ 10. 1016/ S1364-6613(99)01294-2
[Crossref]
11.
Robins, A.: Catastrophic forgetting, rehearsal and pseudorehearsal. Connect. Sci. **7** , 123–146 (1995). https:// doi.
org/ 10. 1080/ 0954009955003931 8
[Crossref]
12.
Chaudhry, A., Ranzato, M., Rohrbach, M., Elhoseiny, M.: Efficient Lifelong Learning with A-GEM (2019).
https:// doi. org/ 10. 48550/ arXiv. 1812. 00420
13.
Lopez-Paz, D., Ranzato, M.: Gradient episodic memory for continual learning (2017). https:// doi. org/ 10. 48550/
arXiv. 1706. 08840
14.
Krizhevsky, A., Hinton, G.: Learning multiple layers of features from tiny images (2009)
15.
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., Fei-Fei, L.: ImageNet: a large-scale hierarchical image
database. In: 2009 IEEE Conference on Computer Vision and Pattern Recognition. Presented at the 2009 IEEE
Conference on Computer Vision and Pattern Recognition, pp. 248–255 (2009). https:// doi. org/ 10. 1109/ CVPR.
2009. 5206848
16.
Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., Andreetto, M., Adam, H.:
MobileNets: efficient convolutional neural networks for mobile vision applications (2017). https:// arxiv. org/ abs/
1704. 04861v1. Last accessed 19 Jun 2025
17.
Cootes, T.F., Taylor, C.J., Cooper, D.H., Graham, J.: Active shape models-their training and application.
Comput. Vis. Image Underst. **61** , 38–59 (1995). https:// doi. org/ 10. 1006/ cviu. 1995. 1004
[Crossref]
18.
Zhang, Z., Lyons, M., Schuster, M., Akamatsu, S.: Comparison between geometry-based and gabor wavelets-
based facial expression recognition using multi-layer perceptron, textordmasculine. In: Proceedings of
International Conference on Automatic Face and Gesture Recognition (1998). https:// doi. org/ 10. 1109/ AFGR.
1998. 670990
19.
Abhishree, T.M., Latha, J., Manikantan, K., Ramachandran, S.: Face recognition using gabor filter based feature
extraction with anisotropic diffusion as a pre-processing technique. Procedia Comput. Sci. Int. Conf. Adv.
Comput. Technol. Appl. (ICACTA) **45** , 312–321 (2015). https:// doi. org/ 10. 1016/ j. procs. 2015. 03. 149


20.
Kopalidis, T., Solachidis, V., Vretos, N., Daras, P.: Advances in facial expression recognition: a survey of
methods, benchmarks, models, and datasets. Information **15** , 135 (2024). https:// doi. org/ 10. 3390/ info15030135
[Crossref]
21.
Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A.,
Al-Amidie, M., Farhan, L.: Review of deep learning: concepts, CNN architectures, challenges, applications,
future directions. J. Big Data **8** , 53 (2021). https:// doi. org/ 10. 1186/ s40537-021-00444-8
[Crossref]
22.
Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural Comput. **9** , 1735–1780 (1997). https:// doi.
org/ 10. 1162/ neco. 1997. 9. 8. 1735
[Crossref]
23.
Lucey, P., Cohn, J.F., Kanade, T., Saragih, J., Ambadar, Z., Matthews, I., Ave, F.: The extended Cohn-Kanade
dataset (CK+): a complete dataset for action unit and emotion-specified expression, pp. 94–101 (2010)
24.
Goodfellow, I.J., Erhan, D., Carrier, P.L., Courville, A., Mirza, M., Hamner, B., Cukierski, W., Tang, Y., Thaler,
D., Lee, D.-H., Zhou, Y., Ramaiah, C., Feng, F., Li, R., Wang, X., Athanasakis, D., Shawe-Taylor, J., Milakov,
M., Park, J., Ionescu, R., Popescu, M., Grozea, C., Bergstra, J., Xie, J., Romaszko, L., Xu, B., Chuang, Z.,
Bengio, Y.: Challenges in representation learning: a report on three machine learning contests. In: Lee, M.,
Hirose, A., Hou, Z.-G., Kil, R.M. (eds.), Neural information processing. Springer, Berlin, Heidelberg, pp. 117–
124 (2013). https:// doi. org/ 10. 1007/ 978-3-642-42051-1_ 16
25.
Li, S., Deng, W., Du, J.: Reliable crowdsourcing and deep locality-preserving learning for expression
recognition in the wild. In: 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR).
Presented at the 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), IEEE, Honolulu,
HI, pp. 2584–2593 (2017). https:// doi. org/ 10. 1109/ CVPR. 2017. 277
26.
Kemker, R., Kanan, C.: FearNet: brain-inspired model for incremental learning. 6th Int. Conf. Learn. Represent.
ICLR 2018—Conf. Track Proc. 1–16 (2018)
27.
Ramesh, R., Chaudhari, P.: Model zoo: a growing “brain” that learns continually (2022). https:// doi. org/ 10.
48550/ arXiv. 2106. 03027
28.
Pham, Q., Liu, C., Hoi, S.: DualNet: continual learning, fast and slow (2021). https:// doi. org/ 10. 48550/ arXiv.

2110. 00175
29.
Wang, L., Zhang, X., Su, H., Zhu, J.: A comprehensive survey of continual learning: theory, method and
application (2024). https:// doi. org/ 10. 48550/ arXiv. 2302. 00487
30.
Dempster, A.P., Laird, N.M., Rubin, D.B.: Maximum likelihood from incomplete data via the EM algorithm. J.
R. Stat. Soc. Ser. B Stat. Methodol. **39** , 1–22 (1977). https:// doi. org/ 10. 1111/ j. 2517-6161. 1977. tb01600. x
31.
Corduneanu, A., Bishop, C.: Variational Bayesian model selection for mixture distribution. Artif. Intell. Stat. **18** ,
27–34 (2001)
32.
Tariq, U., Yang, J., Huang, T.S.: Maximum margin GMM learning for facial expression recognition. In: 2013
10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG).
Presented at the 2013 10th IEEE International Conference and Workshops on Automatic Face and Gesture
Recognition (FG), pp. 1–6 (2013). https:// doi. org/ 10. 1109/ FG. 2013. 6553794
33.
Baltrusaitis, T., Zadeh, A., Lim, Y.C., Morency, L.-P.: OpenFace 2.0: facial behavior analysis toolkit. in: 2018
13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018). Presented at the
2018 13th IEEE International Conference on Automatic Face AND Gesture Recognition (FG 2018), IEEE,
Xi’an, pp. 59–66 (2018). https:// doi. org/ 10. 1109/ FG. 2018. 00019
34.
Lu, J.: A survey on Bayesian inference for Gaussian mixture model (2021). https:// doi. org/ 10. 48550/ arXiv. 2108.
11753


35.
Wang, L., Zhang, X., Su, H., Zhu, J.: A comprehensive survey of continual learning: theory, method and
application (2023)
_OceanofPDF.com_


(1)

(2)

(3)

© The Author(s), under exclusive license to Springer Nature Switzerland AG 2026
K. Slimani et al. (eds.), _Emotion and Facial Recognition in Artificial Intelligence: Sustainable
Multidisciplinary Perspectives and Applications_ , Information Systems Engineering and Management
78
https://doi.org/10.1007/978-3-032-14778-3_8

## Advanced Techniques in Facial Landmark

## Detection and Feature Extraction for

## Emotion-Aware AI Systems

Shaik Khaja Mohiddin^1 , Shaik Sharmila^2 and Khadija Slimani^3
Department of CSE, Siddhartha Academy of Higher Education,
Deemed to Be University, Vijayawada, Andhra Pradesh, India
Department of IT, Vignan’s Nirula Institute of Technology and Science
for Women, PedaPalakalur, Guntur, Andhra Pradesh, India
esieaLab LDR, Higher School of Computer Science Electronics and
Automation (ESIEA), Paris, France

```
Khadija Slimani
Email: Khadija.slimani@esiea.fr
```
**Abstract**
Emotion-Aware AI systems, behaviour perception and understanding
leveraging large-scale facial data to include learning-based Facial Landmark
detection; Attribute Error: ‘None Type’ object has no attribute ‘shape’. This
chapter discusses the state-of-the-art advanced techniques to find these
important facial landmark points such as corners of eyes; tip of nose and
different lip contours used for emotion modelling. We conduct an extensive
survey of traditional as well as modern localisation methods with the help of
deep learning such as Active Shape Models, Active Appearance Models
(AAM), Convolutional Neural Networks for heatmap regression and
attention mechanism to enhance the efficacy in performing localisation.
Likewise, the chapter presents classical feature requirements such as LBP
and HOG as reverse roles of low-level visual processing while also


describing recent deep embeddings for facial texture, shape and dynamics
depiction. Emphasis is placed on cross-condition performance testing
illumination, head pose and occlusion, ethnicity. We also discuss the real-
time use-cases, and associated implementation challenges and optimization
approaches for achieving this empirical deployment across edge-AI
scenarios/concerns, small device mobile environments. Furthermore, a
review of present benchmark datasets and benchmarks is included and how
to implement trustworthy emotion-capable AI pipelines. Combining
traditional approaches with the contemporary deep learning models the
explanations in this chapter provide specific guidance to researchers aiming
at building efficient and reliable emotion-recognizing systems.

**Keywords** CNN-based detection – Emotion recognition – Feature
extraction – Facial landmarks – Heatmap regression – Pose invariance

##### 1 Introduction

**1.1 Importance of Facial Analysis in Emotion-Aware AI**

Facial analysis also plays a key role that enables the machines to identify
and perceive human emotions without being invasive and in real time.
Human face micro expressions are rich affective cues that reflect micro
movements, muscle activities and dynamic formations that are strongly
associated with interior emotional states. Past studies have shown that non-
verbal communication forms a significant portion of human communication
and facial expression plays a crucial role in providing critical context to
verbal communication [ 1 ]. Facial signals can be applied in the domain of AI
so as to better and customized human–machine interactions, combined with
the advantages of naturalness, compassion, and adaptability, and play a key
role in the domains of assistive technologies, driver monitoring, and social
robotics.
Moreover, facial recognition is more direct visual as compared to speech
or physiological detectors, and this makes it less intrusive and scalable.
Advances in and computer vision and deep learning have achieved levels of
facial landmark detection and feature extraction to such high standards, that
we can now make strong inferences of emotion even in rather poor
conditions [ 2 ]. As cameras (e.g., in smartphone, vehicle, or IoT devices) are
more widespread, facial analysis is a feasible situational awareness


modality, emotion-driven personalisation, as well as context-sensitive
system signals, in both consumer and industrial contexts.

**1.2 Objectives of the Study**

The aim of this study is to provide a critical and detailed and systematic
review of advanced methodology of face landmark detectors and feature
extractors specifically to support the emotion conscious AI systems. Both
classical approaches, as well as novel deep architecture, are created and
trained. The second objective is to present performance trade-offs and field
experience and integration strategies to the Indian pipelines of emotion
recognition.
The study is also intended to offer guidance to the researcher and
practitioner by identifying the best practices, paradigm comparison, and
unresolved issues. We aim to provide the appropriate datasets,
benchmarking procedures, and evaluation measures to ensure that the
readers can reasonably evaluate, as well as enhance state-of-the-art systems.
Lastly, we will work on promoting better comprehension of how these
systems can be deployed to real-world applications with limitations like
computational resources, privacy, and cross-cultural variability.

**1.3 Applications in Real-World Systems**

The facial landmark recognition and emotion-aware AI have been used in
numerous fields. Automatic emotion recognition is applied in health care to
either track patient mood, indicate depression or aid in autism therapy with
feedback about emotional conditions [ 3 ]. Facial analysis work may also be
utilized in mental health analytics, such as to continuously measure the
emotional valence in a therapy session or telehealth context [ 4 ]. In learning
environments, where the frustration, engagement or boredom of learners is
identified, adaptive tutoring systems can modify the pace of pedagogy or
content [ 5 ].
In smart self-driving cars and autonomous vehicles, facial-read driver
stress, fatigue, or distraction can be detected, and, in autonomous driving,
safety measures can be taken around drivers, as well as human connection to
the vehicles. Facial emotion AI finds its application in marketing and
advertising as well: analysing the reactions of the audience in real-time,
systems can personalize the content or measure the success of the campaign
[ 6 ]. Besides, facial analysis can be used in entertainment (VR/AR, gaming,


social robots); to make the avatars or agents act according to the mood of the
users. This is because machine identity verification in the wild will
persistently require the combination of face landmark recognition and
effective feature integration to create responsive and context-sensitive AI
systems [ 7 ].

##### 2 Fundamentals of Facial Landmark Detection

**2.1 Advanced Techniques in Facial Landmark Detection**

Facial landmark detection can be described as the process of locating certain
key points (landmarks) on a human face including the corners of the eyes,
tip of the nose, and corners of the mouth to obtain facial geometry in a small
and structured way. Such landmarks have been used as anchors to
downstream tasks like face alignment, expression analysis, head pose
estimation and emotion inference. Facial landmarks can offer a semantic
cognizant scaffold that allows algorithms to make inferences about shape
transformations, relative motion and local appearance variations. Recent
papers have provided increasingly accurate and efficient landmark detection
models: e.g., Hong and Lin [ 8 ] introduce a knowledge distillation method
that replenishes the knowledge of the large models to small ones without
introducing the detection precision in small gadgets [ 8 ]. Equally, another
model called Efficient Facial Landmark Detection (EFLD) was proposed by
Wu, who achieved a high level of landmark localization specifically to work
with embedded systems through a lightweight backbone and cross-format
training approach [ 9 ]. Collectively, the above developments underscore the
changing significance of landmark detection as the initial step in the current
facial analysis pipelines (Fig. 1 ).


**_Fig. 1_** Advanced techniques in facial landmark detection and feature extraction for emotion aware AI
systems
The figure depicts the top-level view of the end-to-end pipeline of
emotion-conscious AI systems, in which it has been highlighted the role of
the facial landmark detecting and feature extraction in identifying human
emotions. It commences with an acquired facial image which is usually
through camera vision systems. The stage of landmark detection on the face
refers to determining the most important structural features (such as the
eyes, nose, lips and jawline) to project the distinct geometry of the face.
These landmarks serve as important reference signals to the next stage of
feature extraction, where pixel or region-based features are calculated,
typically through geometric, appearance-based or deep learning features, to
compute non-trivial representations of facial expression. These features are
lastly inputted into an emotion recognition model that then is used to
classify the seen expression (e.g., “Happy” through trained classifiers or
deep networks). This modular flow is not only capable of interpreting
emotions in real time but is also robust and adaptable to be applied in
human–computer interaction, affective computing and intelligent
surveillance. The illustration is successful in producing the contribution of
organized face recognition to emotionally intelligent AI systems.

**2.2 Common Landmark Points (e.g., Eyes, Nose, Mouth)**


Some common sets of points are often labelled in datasets and models in
facial landmark detection, often including the eyes (inner/outer corners,
eyelids), eyebrows (arch line), nose (bridge, tip, nostril boundaries), lips
(corners and contours), and the jawline or chin. These reference landmarks
allow a system of consistency in reference between faces. To illustrate this,
Huang et al. [ 10 ] report the lightweight key point detector that uses
regression and heatmap supervision to precisely detect such typical facial
key points in highly misleading scenarios such as masking or changes in
pose [ 10 ]. Their findings reveal the extent to which the strength of
localization of landmarks in these major regions of the face can be beneficial
in downstream tasks such as recognition of expressions or face alignment.
Besides, according to a recent report of surgical cases evaluation in
changing conditions provided by Frajtag et al. [ 11 ], the algorithms were
shown to be reliable in detecting landmarks such as ears corners and nose tip
to allow its usage safely in the future [ 11 ]. Through that, these conventional
landmarks remain the crucial element in the process of designing and
benchmarking facial analysis systems (Table 1 ).

**_Table 1_** Key facial landmarks and their functional relevance in emotion analysis

```
Region Landmark
points
```
```
Description Associated
emotions
```
```
Relevance score
(High/Med/Low)
```
```
Application
context
Eyes Inner/outer
canthus,
eyelids
```
```
Detects
blinking, gaze
shifts
```
```
Surprise, fear,
happiness
```
```
High Eye tracking,
fatigue
detection
Eyebrows Arch,
inner/outer
brow
```
```
Indicates
stress or
surprise
```
```
Anger, sadness,
surprise
```
```
High Emotion
labeling, HCI
```
```
Nose Nose tip,
nostril edges
```
```
Minor
expression
role
```
```
Disgust, neutral Low Face
alignment,
identity
verification
Lips/mouth Corners,
upper/lower
lips
```
```
Critical for
smile/frown
cues
```
```
Joy, sadness,
disgust
```
```
High Emotion
scoring, speech
sync
Jawline Chin tip, jaw
angle
```
```
Affects facial
geometry
```
```
Tension, fear,
determination
```
```
Medium Head pose
estimation
Forehead Mid-brow,
upper brow
line
```
```
Shows
wrinkle
patterns
```
```
Surprise,
confusion
```
```
Medium Stress detection
```

**2.3 Dataset Annotations and Landmark Standards**

With facial landmark detection problems, the datasets must be familiar and
trusted in terms of their annotations that indicate exactly the location of each
landmark within facial geometry. A typical procedure is that a fixed set of
landmarks (e.g. 68-point, 98-point or 106-point scheme) is mended and
uniformly applied in all images so that identities can be extrapolated using
models. When consensus correspondence (e.g. with Procrustes analysis) is
used to reduce inter-annotator error, they are typically customarily marked
out by expert manual labelling or semi-automatic methods. Some datasets
like 300-W, WFLW use the 68-point scheme, and some extend to profile
views using defined landmarks such as Menpo. Metadata relating to
visibility, occlusion and landmark confidence are also a good annotation
standard. These annotation conventions allow the use of consistent
evaluation metrics (including normalized mean error, area under curve, and
failure rate) between studies and make it possible to benchmark and
compare methods. The annotation standard consistency is what allows one
to compare fairness among algorithms and transfers between datasets.

##### 3 Classical Approaches to Landmark Detection

**3.1 Active Shape Models (ASM)**

One of the early statistical models of shape modelling and fitting (such as
faces) is Active Shape Models (ASM), which trains a point distribution
model (PDM) based on annotated training shapes and then refits the shape to
match image features. ASM uses local adjustment of all landmarks (e.g.
along a normal profile or gradient) and controls them constrained with a set
of global shape variables to stay within plausible variation based on PCA.
The tradeoff between local feature matching and global shape constraints
enables ASM to impose consistent facial geometry to the image data fitting.
There has been more recent investment in the revisiting of the concept in a
parametric or hybrid form; a recent 2025 paper has a parametric ASM
implementation which encloses cascade regression into the ASM model in
order to enhance efficiency and compactness in facial landmark detection
[ 12 ]. Although it has benefits, ASM may not be able to cope with significant
changes of pose or elaborate variation of texture.

**3.2 Active Appearance Models (AAM)**


With the Active Appearance Models (AAM), ASM is extended to model
shape and texture (appearance) change in the joint generative model. Both
the shape of landmarks and the form of the distorted texture (to a canonical
shape) are parameterized in AAM and the parameters are updated iteratively
(often by gradient descent or Gauss Newton) to reduce the discrepancy
between the appearance of the image and the appearance of the model. This
joint modelling has more powerful constraints and may have more precise
fits particularly in rather controlled imaging situations. Since shading,
texture and variance of intensity are conveyed through the appearance
component, AAM can be better adapted to detail of subtle surface features in
the face [ 13 ]. Nevertheless, AAM fitting is computationally intensive and is
susceptible to both initialisation and light variations.

**3.3 Constrained Local Models (CLM)**

Constrained Local Models (CLMs) combine global shape constraints with
local appearance models (patch experts) that evaluate likelihoods of
landmark positions in local neighbourhoods. Each local patch detector
proposes candidate positions, and these local responses are combined under
a global shape prior (often via optimization such as mean-shift) to enforce
anatomical consistency. CLMs thus enjoy robustness to local variations such
as partial occlusions or local texture noise while maintaining structural
coherence. An enhanced example is the Convolutional Experts CLM (CE-
CLM), which integrates convolutional local detectors with expert mixtures
within the CLM framework, achieving state-of-the-art alignment especially
for profile faces [ 14 ].

**3.4 Limitations of Classical Methods**

Traditional landmark classification techniques like ASM, AAM and CLM
encounter a number of constraints on their in-the-wild application. In the
first place, they are limited to linear PCA models and, as such, cannot
capture complex non-linear facial deformations, as are found in different
expressions and extreme poses. Secondly, they are usually sensitive to good
initialization and are slow to draw near the true face in cases that the initial
guess is distant, and hence are fragile in practice. Third, they are more
susceptible to changes in illumination, occlusions (e.g., hair, glasses) and
texture noise as they do not behave well in those cases due to the
vulnerability of their local detectors or profile matching. Inter-demographic


and inter-imaging weaknesses Generalization to diverse demographics and
imaging conditions is another weakness: classical methods typically cannot
be easily scaled to large and diverse data without hand-tuning [ 15 ]. Lastly,
their respective iterative fitting is computationally expensive, so they will
not be well suited to run in real time or with limited resources.
A holistic hierarchical view of the complexes that are normally
encountered in the facial landmark detection systems and the mitigation
measures that can be adopted is provided in the below figure christened
Common Challenges in Facial Landmark Detection and Suggested
Mitigations. The roofline lists five major barriers: occlusion, head position
variability, ethnic and cultural bias, real time options and low light/noise- all
high impact items to the nature and strength of the localization of landmarks
in uncontrolled environments. These barriers result in a more central block
that can be referred to as Mitigation Techniques which highlights the need to
have a combined strategic response. According to this block, the diagram is
divided into four large areas of solution, i.e., Data Augmentation, Robust
Algorithms, Multi-modal Fusion and Hardware Optimization. These areas
are also further broken down into techniques that can be applied individually
such as image improvement, ensemble techniques, 3-dimensional modelling
and adaptive thresholding which are effective tools that can be employed to
improve model performance and accuracy. This visual overlay highlights the
depth of face landmark issues in the real world, and why algorithmic,
architectural and data-centric refinements must be made in order to establish
workable and trustworthy facial analysis systems (Fig. 2 ).


**_Fig. 2_** Typical issues with facial landmark detection and recommended mitigation

##### 4 Deep Learning-Based Landmark Detection

**4.1 Convolutional Neural Networks (CNNs)**

Since then been replaced by convolutional Neural Networks (CNNs) that
have become the de facto minimal design architecture of facial landmark
detection compared to older regression or template-based methods. To learn
hierarchical features (that is, first the edges, then the facial structures) CNNs
are trained on raw patches of images or on whole crops of face using several
convolutional and pooling layers. These trained features are next projected
to land mark coordinates by fully connected layers or regression heads. The
deep features are able to record strong patterns of appearance variation
based on changes in lighting, pose, and expression. An example is a study of


facial landmarks detection and image processing which showed that
convolutional frameworks, particularly the variant of ResNet, are more
accurate compared to cascade regression in adverse environmental factors.
CNNs are still popular in mobile or real-time systems due to their ability to
be optimized using quantization, pruning, or compact backbones without
leading to misleading landmark localizations when used in highly
unconstrained in the wild scenarios.

**4.2 Heatmap Regression Models**

Heatmap regression has become an effective face landmark detection
paradigm that learns regression of wards into dense pixel-by-pixel
prediction. Instead of simply producing the values of coordinates, the
network produces a heatmap at a single location per landmark containing the
intensity of the pixel indicating the likelihood of the landmark at that
location. After that, the coordinate is chosen as the position of the largest
response (argmax) or through a differentiable approximation such as Soft
argmax. Another recent work Heatmap Regression without Soft-Argmax to
do facial landmark detection [ 17 ] suggests a different structured prediction
objective that uses no Soft argmax nor requires Soft-argmax to converge to
the state-of-the-art on tasks like WFLW and 300 W [ 16 ]. Heatmap
approaches are outstanding in the modelling of uncertain spatial
information, aesthetic neighborhood and provide more localization
consistency within a network of occlusions or subtle variations compared to
direct regression networks.

**4.3 Attention Mechanisms in Facial Detection**

The attention mechanisms have also been used to enhance the facial
landmark network by making the models focus on the spatial regions or
channels selectively that can be used to localize the Landmarks. Spatial or
channel attention modules can dynamically weight features in landmark
detection tasks so that such important parts of the face as lips, eyes or nose
are emphasized and decoded at different conditions. In general, Attention-
based Face Alignment investigates the benefits of attention modules in
convolutional models in improving localization accuracy in the presence of
pose and lighting change. The other method is Selective Cascaded
Regression with Patch Attention that applies patch-level attention to regress
more challenging landmarks by focusing on the most-informative local


patches at successive steps. These attention-based mechanisms enhance
robustness to background clutter, occlusion and misalignment by adjusting
identically with highlighting relevant parts and eliminating irrelevant details
on a layer-by-layer basis.

**4.4 Lightweight and Real-Time Models**

The application of facial landmark detection in real-time systems like
mobile, embedded or edge devices needs models to be computationally
efficient, low-latency and light on memory without compromising on
accuracy. Some of the methods that contribute to this trade-off include
model pruning, quantization, knowledge distillation, and efficient
architecture design (e.g. MobileNet, EfficientNet backbones). The hybrid
architecture that shares CNNs with lightweight architecture or coarse-to-fine
prediction heads have also been discovered to be promising. One such work
is the piece by CNNs and Markov-like Models to Facial Landmark
Detection: a hybrid model that restricts the number of landmarks in the
model as well as adding the spatial consistency penalty to simplify the
model without compromising accuracy. More intelligent heatmap-based
models like PIPNet can eliminate the inference bottleneck by inference and
score at sub-resolution feature maps, so no high up sampling rates are
needed and can run at tens of frames per second on both GPUs and CPUs.
The viability of precise and real time identification of facial features in
resource-limited environments is brought to light by designs such as these
[ 17 ].

##### 5 Feature Extraction Techniques

**5.1 Geometric-Based Feature Extraction**

A geometric-based feature extraction emphasizes on the spatial association
of facial landmarks (distances, angles, ratios, triangulations) to encode the
shape and relative arrangement of facial features. These characteristics are
necessarily interpretable, compact and robust to the changes of moderate
light conditions and they are useful in emotion recognition systems. As an
example, Murugappan et al. apply to the feature sets based on the landmark
a triangulation method by constructing triangles between selected Action
Units (AUs) and calculating their inscribed circle area and triangle area to
differentiate the feelings in real-time. PLOS.


**5.2 Appearance-Based Techniques (HOG, LBP)**

Appearance-based feature extraction computes local texture or gradient
patterns of patches on face images to extract descriptors, to describe fine-
grained texture and shading variation related to motion of facial muscles.
There are two popular methods: Histogram of Oriented Gradients (HOG)
and Local Binary Patterns (LBP). The HOG descriptor also quantifies the
gradient orientation distributions within local cells which makes it resistant
to illumination, local replacements and illumination variations.

**5.3 Deep Feature Embeddings (e.g., VGGFace, FaceNet)**

Deep feature embeddings are vectors of high dimensionality, semantically
meaningful feature vectors encoding both facial identity, texture, and
structural details by using pre-trained deep neural networks (which are most
frequently face recognizing). They can also be extended by such
embeddings that can be fine tuned to perform tasks like emotion recognition,
or they can be combined with landmark features. As an example, in Deep
learning of facial embeddings and facial landmark points to recognise
emotion the authors examine how FaceNet embeddings can be used in
conjunction with landmark features to achieve more accurate results in
emotion recognition.

**5.4 Temporal Feature Extraction for Video Streams**

Temporal feature extraction encodes the dynamic changes in face
expressions over video frames to allow the emotion recognizers to utilize
motion, onset, and time attributes of a face instead of analyzing each frame
separately. TCNs processing sequences of frame-wise facial embeddings are
one of the effective methods. In the example of Video-based Facial
Expression Recognition, EmotiEffNet and Temporal Convolutional
Networks use a pre-trained facial feature extractor frame by frame and
subsequently input the frame features to a TCN to learn temporal features
and enhance the classification (Table 2 ).

**_Table 2_** System pipeline from face input to emotion output

```
Step
No
```
```
Stage name Input Process
description
```
```
Output Tools/techniques
used
```
```
Real-time
suitability
1 Face
detection
```
```
Raw
image/video
```
```
Detect face
using
```
```
Cropped face
area
```
```
Viola-Jones,
MTCNN
```
```
Yes
```

```
Step
No
```
```
Stage name Input Process
description
```
```
Output Tools/techniques
used
```
```
Real-time
suitability
bounding
boxes
2 Landmark
detection
```
```
Cropped
face
```
```
Localize key
facial points
```
```
Facial key
points
```
```
CNN, heatmaps,
Dlib
```
```
Yes
(lightweight
CNN)
3 Feature
extraction
```
```
Key
points/ROI
```
```
Extract
geometric or
texture
features
```
```
Feature
vector
```
```
HOG, LBP, deep
embedding
```
```
Partial
```
```
4 Emotion
classification
```
```
Feature
vector
```
```
Classify into
predefined
emotions
```
```
Emotion label SVM, RNN,
transformers
```
```
Yes
```
```
5 Post-
processing
```
```
Emotion
label
```
```
Refine
output with
context
```
```
Annotated
emotion
```
```
Temporal
smoothing
```
```
Yes
```
```
6 Output
integration
```
```
Final label Display or
action-trigger
```
```
HCI
response/alert
```
```
GUI/API/actuators Yes
```
##### 6 Integration into Emotion-Aware AI Pipelines

**6.1 Preprocessing and Normalization**

The initial steps of preprocessing and normalizing represent the most
important part of the process of shaping the raw facial image data to serve
the subsequent processing. These measures can avoid variations occurring
by lighting, scale, orientation, and noise to enhance the strength and stability
of landmark detection and feature extraction. Popular preprocessing tasks
encompass image resizing, color space conversion, histogram equalization
or lightness correction, denoising, face alignment (rotation/translation) to a
canonical pose. Recent reviews underline the significance of these steps to
facilitate a general performance in solving the tasks of facial analysis.
Normalization of landmark coordinates (e.g. scaled by interocular distance)
is frequently carried out in landmark detection systems to ensure that
landmarks are insensitive to face size and distance.
There are also modern implementations of the normalization step with
the detection architecture. An example of this is the case of continuous
landmark detectors that employ a spatial transformer module to learn jointly
how to best warp/crop the face region to predict landmarks, rather than


applying predictive heuristics. The result of this combined learning can be
increased temporal stability and correspondence with downstream tasks. In
practice, to use fast routines to scale, crop, and normalize, it is convenient to
have a light preprocessing pipeline on the edge (e.g. in mobile or embedded
systems) to run in real-time.

**6.2 Feature Fusion for Emotion Recognition**

Once the feature sets of individual features have been extracted (geometric,
appearance-based, and learned embeddings), an effective emotional AI
system can combine multiple modalities or types of features to gain
increased robustness and accuracy. Broad categories of strategies of feature
fusion include early fusion (concatenation of features prior to classification),
late fusion (combining the outputs of independent classifiers), and hybrid
fusion. In emotion recognition, early fusion may take advantage of
complement between texture and landmark feature and enhance strength of
discrimination. As one example, HOG descriptors using geometric landmark
distances tend to work better together than individually.
In more sophisticated forms of fusion, attention mechanisms or graph-
based fusion are applied where the contribution of features is dynamically
weighted. Among recent works, there is a suggestion to apply the cross-
modal transformer-based fusion technique (MemoCMT) that uses features
of two different modalities (e.g., visual + audio) and produces more
comprehensive embeddings. Other literature discusses spatiotemporal
fusion, which introduces time variations in expressions to a spatial measure
in order to more effectively represent emotion change. In the design of
fusion, there is a tradeoff between diameters of features that can be
mismatched, synchronization and the chance of overfitting on the basis of
high dimensional concatenation.

**6.3 Real-Time System Deployment**

Real time deployment of emotion aware AI (e.g., in mobile apps, assistive
robots, driver monitoring, etc.) results in a limited latency, compute and
memory requirements. End-to-end optimization of the pipeline should be
made efficient without adversely affecting accuracy. Face alignment and
landmarks detectors have to be able to make rapid inference with low jitter,
and remain within camera motion for sudden head motions. Face alignment
is also applicable to real-time devices to overcome these problems by


optimal model architectures and utilization of parallelism or hardware
acceleration.
Real-time deployment strategies have been proposed, such as model
pruning, quantization, lightweight architectures (e.g. MobileNet, efficient
CNN variants) and pipelining stages to asynchronise computation and I/O.
Temporal smoothing/filtering of landmark locations between frames can
also be done to minimize jitter and abrupt changes. Other systems use frame
skipping or region-of-interest tracking in order to prevent unnecessary full-
frame processing. Moreover, real-time robustness requires partial-occlusion
or landmark-loss fallbacks, dynamic-background or lighting adverse
adjustment. This profiling method and hardware-software co-design must be
followed with careful consideration so that the system can be responsive and
reliable with the actual requirement of the real world (Table 3 ).

**_Table 3_** Challenges in real-time deployment of emotion-aware AI systems

```
Challenge
type
```
```
Cause/scenario Affected
components
```
```
Impact
on
accuracy
```
```
Example
case
```
```
Mitigation
strategy
```
```
Tools/models
used
```
```
Occlusion Hand, glasses,
mask covering
face
```
```
Landmark
detection
```
```
High Face with
mask
```
```
Context-
aware
models,
GANs
```
```
DenseReg,
robust CNNs
```
```
Head pose Non-frontal
face angles
```
```
Eye/nose/mouth
alignment
```
```
Medium–
high
```
```
Profile
view
```
```
Multi-view
datasets, 3D
models
```
```
PRNet, FPN
```
```
Lighting
variation
```
```
Dim or uneven
lighting
```
```
Feature
extraction
```
```
Medium Nighttime
image
```
```
Histogram
equalization,
training
augmentation
```
```
LBP,
CLAHE
```
```
Expression
intensity
```
```
Subtle or
exaggerated
emotion
```
```
Emotion
classification
```
```
High Micro-
expressions
```
```
Temporal
modeling,
multi-frame
input
```
```
LSTM,
optical flow
```
```
Dataset
bias
```
```
Imbalanced
demographics
```
```
Generalization High Poor
accuracy
on
minorities
```
```
Fair
sampling,
domain
adaptation
```
```
Fair face,
transfer
learning
```
```
Processing
time
```
```
High model
complexity
```
```
Real-time
responsiveness
```
```
Medium Mobile
deployment
```
```
Model
pruning,
quantization
```
```
Mobile net,
ONNX
```

##### 7 Future Directions

**7.1 Explainable Landmark Detection**

With more elaborate emotion-aware AI systems, the need to explain them is
increasing, especially in facial landmark detection. Conventional deep
learning models can be used as a black box, and do not give much
information about how or why particular landmarks are selected in the
process of prediction. Using techniques including attention map, saliency
visualization, or rule-based landmark reasoning to interpret the decisions
made by the model, Explainable Landmark Detection (XLD) seeks to close
this gap. As an example, by imagining which parts of the face were most
important to a given emotion prediction, or understanding whether the
system made an error in part alignment because of occlusions, they can be in
a better position to trust the system. Particularly important to combine
explainability is in contexts where the stakes of a high-stakes application,
e.g. in mental health diagnostics, or surveillance, are of grave importance in
the event of failure to understand the outcome. Scholars are looking at
hybrid paradigms; a collection of statistical algorithms that incorporate
neural networks which can preserve some trace of transparency. In addition,
the XAI (eXplainable AI) principles, Layer-wise Relevance Propagation
(LRP) and SHAP values are currently being applied to spatial model types
such as CNNs in face recognition tasks. The methods not only raise the
confidence of the users but also facilitate debugging and regulatory
compliance as well as improved auditing of the data sets.

**7.2 Multimodal Emotion Analysis**

The next step in the development of holistic and context sensitive affective
computing systems is multimodal emotion analysis. Adding audio samples,
text, physiological response (heat rate or EEG) and context are also
important in the identification of emotions despite the importance of facial
features and visual markers. Every modality has its own understanding-
facial expressions show evident effect, voice is used to indicate tone, pitch
and stress, and text and bio signals express more informative cognitive and
affective information. Recently, multimodal transformers, and graph fusion
networks have made possible sophisticated asynchronous and heterogeneous
integrations between data streams. The models can also dynamically re-
examine the relevance of each modality, with increased functionality


provided in complex real-life situations. One such case is where the face is
partially blocked, voice or entry of texts can be utilized to compensate for
the missing modality. These collections of datasets CMU-MOSEI and
AffWild2 have facilitated the research in this area by offering corresponding
annotations of multimodal emotion. Nonetheless, some difficulties persist,
e.g.: matching data through time, processing missing capabilities, and
computational cost in real-time architecture. However, multimodal emotion
AI is likely to develop more human-aware and context-aware emotionally
intelligent systems that will be more accurate as well.

**7.3 Ethical and Privacy Considerations**

The introduction of facial emotion recognition technologies presents very
important ethical and privacy considerations that it is important to mitigate
before it is too late. To begin with, facial data is sensitive by nature and will
be able to demonstrate intimate details of mood, mental state, and identity of
people. In the absence of powerful regulatory tools and open access to
consent, these systems can easily infringe personal privacy and establish a
surveillance-based system of control. Discriminatory models also may give
unfair results, especially against an underrepresented demographic, where
training data demographics is often non-demographically varied. There is
also a question of an incorrect application of emotion AI in areas like hiring,
police force, or politics where the perceived emotion is not necessarily the
intended or real emotion based on context. These concerns will require such
privacy-preserving approaches in machine learning as federated learning, as
well as differential privacy and secure multiparty computation. Moreover,
we need to formulate data governance policies, ethical review policies, as
well as policies of public accountability. It also needs to be explainable and
has the option of withdrawing and particularly when it is an application that
has been made public. As AI with emotionally-aware minds gradually rise in
numbers, integrating equity, openness, and responsibility within the design
of the system will ensure that people remain publicly trusting and socially
agreeable.

##### 8 Conclusion

The chapter provided a detailed description of the advanced methods of
facial landmark detection and feature extraction, which are key elements of


emotion-aware AI-based systems. We examined both classical and deep
learning methods of detecting face landmarks, such as ASM, AAM, CNNs,
and heatmap regression models, their abilities, and limitations in different
scenarios, such as occlusion, lighting, and pose change. We also divided
feature extraction strategies into geometric type, appearance-based type and
learned feature types, and their involvement in accurately detecting
emotions. The different system pipelines, datasets and benchmarking
practices were also addressed that provide background information on how
to create a robust, real-time facial analysis system.
To develop effective and ethical emotion-sensitive AI systems, the
practitioners are encouraged to embrace hybrid solutions to integrate the
advantages of both a traditional and modern system. To enhance
performance and generalizability, lightweight models trained on
demographically diverse data sets can be used and edge deployment can be
optimized. To deal with ethical issues, particularly in real-time models, it is
important to incorporate privacy-sensitive mechanisms and explainable AI
models that can be trained on sensitive data. The ongoing assessment with
the real world and the integration of other research fields like Psychologists
and Ethicists will even better provide the system developed by AI to be
correct, responsible, and aligned with human-centred values.

##### References

1. Kumar, A., Kumar, A., Gupta, S.: Machine learning-driven emotion recognition through facial
    landmark analysis. SN Comput. Sci. **6** (2), 120 (2025)
    [Crossref]
2.
    Jagadiswary, D., Venkiteswaran, A.: AI pioneering ethical, analytical and real time emotional
    recognition in dynamic human expressions. In: 2025 International Conference on Data Science,
    Agents & Artificial Intelligence (ICDSAAI). IEEE (2025)
3.
    Rajesh, S.G., et al.: Enhancement of virtual assistants through multimodal AI for emotion
    recognition. IEEE Access (2025)
4.
    Ribeiro, B., et al.: ISR-FABEL: a landmark-based dataset and multimodal emotion recognition
    framework for child-robot interaction. IEEE Access (2025)
5.
    Slimani, K., Ruichek, Y., Messoussi, R.: Compound facial emotional expression recognition using
    CNN deep features. Eng. Lett. **30** (4), 1402–1416 (2022)
6.
    Kar, I., et al.: Supervised learning-driven emotion recognition through facial landmark trajectories
    enhanced by SMOTE methodology. In: 2025 AI-Driven Smart Healthcare for Society 5.0. IEEE


(2025)
7.
Krishnasamy, N., et al.: Ensemble deep learning framework for hybrid facial datasets using
landmark detection: state-of-the-art tools. J. Comput. Cogn. Eng. (2025)
8.
Hong, Z.W., Lin, Y.C.: Improving facial landmark detection accuracy and efficiency with
knowledge distillation (2024). arXiv: 2404. 06029
9.
Wu, J.J.: Efficient facial landmark detection for embedded systems (2024). arXiv: 2407. 10228
10.
Huang, Y.: A robust and efficient method for effective facial keypoint detection. Appl. Sci. (2024)
11.
Frajtag, I., Švaco, M., Šuligoj, F.: Evaluation of facial landmark localization performance in a
surgical setting (2025). arXiv: 2507. 18248
12.
Ravikiran, D.N., Kumar, S.R., Singh, A.P.: Parametric facial landmark detection using active
shape models. Int. J. Modern Trends Sci. Technol. **11** (4), 45–52 (2025)
13.
Cootes, T.F., Edwards, G.J., Taylor, C.J.: Active appearance models. IEEE Trans. Pattern Anal.
Mach. Intell. **23** (6), 681–685 (2001)
[Crossref]
14.
Zadeh, A., Baltrušaitis, T., Morency, L.-P.: Convolutional experts constrained local model for
facial landmark detection (2016). arXiv: 1602. 04114
15.
Johnston, B., Macho, D., Bronlund, J.E.: A review of image-based automatic facial landmarking.
EURASIP J. Image Video Process. **2018** (1), 1–20 (2018)
[Crossref]
16.
Jin, H., Liao, S., Shao, L.: Pixel-in-pixel net: towards efficient facial landmark detection in the
wild (2020). arXiv: 2003. 03771
17.
Yang, C.A., Yeh, R.A.: Heatmap regression without soft-argmax for facial landmark detection
(2025). arXiv: 2508. 14929v1
_OceanofPDF.com_


(1)
(2)

© The Author(s), under exclusive license to Springer Nature Switzerland AG 2026
K. Slimani et al. (eds.), _Emotion and Facial Recognition in Artificial Intelligence: Sustainable
Multidisciplinary Perspectives and Applications_ , Information Systems Engineering and Management
78
https://doi.org/10.1007/978-3-032-14778-3_9

## Detection of Microexpressions and Subtle

## Emotions

Roopali Pahwa^1 and Vinayak Gupta^2
Digital Analyst, Cyber Forensic Laboratory, Panchkula, Haryana, India
Department of Forensic Science, School of Bioengineering and
Biosciences, Lovely Professional University, Phagwara, Punjab, India

```
Vinayak Gupta
Email: vinayakgupta224@gmail.com
Email: vinayak.32860@lpu.co.in
```
**Abstract**
Microexpressions are swift, involuntary facial movements that reveal
genuine emotions that people may attempt to conceal. These expressions
differ from macroexpressions, which are more pronounced or more
unmistakable. Subtle emotions are low-intensity, nuanced feelings that may
not be overtly expressed or easily recognised. These last for a fraction of a
second, making them difficult to detect. Microexpressions and subtle
emotions can indicate concealed emotions, making them beneficial for
mental health assessments, security inspection, lie detection, and
understanding social interactions. Both are universal across nations,
cultures and sometimes even in individuals, and may reveal emotions even
when someone tries to obscure them. AI-based detection of these emotions
relies on a combination of sophisticated feature extraction, deep learning,
multimodal data integration, and access to rich, annotated datasets,
delivering high accuracy and real-time performance for both research and
commercial applications. Various approaches and techniques exists that can
be used to detect microexpressions and subtle emotions. Artificial


Intelligence bridge the gap between emotional credibility and technological
reconstruction, which results in more successful facial reconstruction or
recognition. The use of artificial intelligence to investigate subjects
microexpressions and subtle emotions has expanded the capabilities of
investigation and criminal profiling. Besides criminal investigation, other
fields such as customer service, deceit detection, psychotherapy, and
human–computer interaction may also utilise this technology. In this
chapter, the integration of AI into microexpression and subtle emotion
detection is a significant leap forward, offering a powerful tool. As research
and development continue, enhanced understanding of human behaviour
and improved interactions across diverse applications are promised by AI-
driven emotion recognition.

**Keywords** Facial recognition – Microexpression – Subtle emotions –
Artificial intelligence – Machine learning – Forensic sciences

##### 1 Introduction

Facial expressions are the postures and movements of the muscles beneath
the skin's surface that represent an individual's emotional state [ 63 ]. The
facial nerve primarily controls the 43 facial muscles that work in unison to
produce these expressions. According to Charan [ 9 ], facial expressions
serve as a universal nonverbal communication system that conveys a
person's emotional state. It can be produced involuntarily, that is, by
expressing true emotions, or consciously, frequently impacted by social or
cultural norms [ 63 ]. These are important ways to communicate one's
feelings. The three general categories of facial expressions are
macroexpressions, microexpressions, and subtle expressions. Each category
has a distinct duration, level of awareness, and emotional significance [ 20 ].
Macroexpressions are noticeable facial expressions that remain from about
0.5 to 4 s only. They are displayed consciously or semi-consciously and
generally match the context of the verbal communication. These form the
basis of everyday emotional communication and are easily perceived by
others (Montanha 2025). On the other hand, microexpressions are very
brief, spontaneous facial expressions that reveal a person's true feelings,
often when they are trying to hide or conceal their feelings. These have a
maximum duration of approximately 200 ms, and usually stay for 1/25 and


1/5 of a second [ 16 ]. Subtle expressions are less obvious and less intense
expressions, and they can be a sign of the beginning or end of an emotion.
Although they are longer than microexpressions, they are also frequently
more difficult to spot than macroexpressions [ 59 ].
To purposefully express the feelings, people typically use easily
recognised macroexpressions that last only a few seconds. However, a
quick and delicate facial movement known as a microexpression may reveal
true feelings beneath the surface. For example, consider a child trying to lie
or fake a story to parents, peers, etc. A distinct kind of microexpression, in
addition to macroexpressions may be a reason of concealed feeling, which a
person could not express. Controlling microexpressions is almost
impossible. This can be used in a variety of contexts, such as business
relationships, clinical diagnosis, teaching, and interrogation. Mostly people
lack emotional intelligence, thus, microexpression analysis is so useful in
our daily lives [ 70 ]. Depending on how expression is altered,
microexpression can be categorised as follows:

1. Simulated expression: A fleeting, fake expression that lacks a true
    emotion is called a “simulated expression.” The neutral state of the face
    is rapidly restored.
2. Neutralized expression: When a true emotional expression is repressed,
    the face appears neutral. It’s possible that the microexpression is so
    well hidden that it is practically undetectable.
3. Masked expression: A false expression totally obscures a true emotion.

# For instance, a person may use a smile to cover up their anger [ 2 ].^

Low-intensity emotional states known as subtle emotions are
characterised by subtle or minimal facial expressions, which frequently
make it challenging to identify and differentiate them from more overt or
intense emotions. When it comes to facial expression, subtle emotions are
those that don't involve strong muscle movements and that both people and
automated systems may easily miss [ 59 ]. Both more complex affective
states and the universal core emotions in their milder manifestations can be
classified as subtle emotions.
Examples of subtle emotions include:


1. Guilt, shame, pride, envy, love, excitement, and boredom are examples
    of mixed and complex subtle emotions that call for introspection and
    social awareness
2. Basic emotions such as subtle joy, subtle sadness, subtle anger, subtle

# fear, subtle disgust, and subtle surprise can all be subtle variations.^

3. Ambivalence and other subtle emotions like nostalgia, relief, trust,

# interest, etc. are context-dependent and culturally shaped.^

4. Subjective nuance in classification, which is further subdivided into

# cultural influence and individual differences [ 54 ].^

Subtle emotions and microexpressions are real and we classify them
under Paul [ 20 ] original conditions. People from any culture (and isolated
indigenous cultures and blind born people) perceive these emotions in their
biological form. Analysis will be objective because there are movements in
human facial musculature associated with specific movements associated
with specific emotions (Table 1 ). Ekman's paradigm is fundamental, but it
has flaws. While some studies show cultural differences in the intensity of
expression, more recent research proposes broader emotional categories,
such as pride and humiliation [ 12 , 13 ].

**_Table 1_** Identifies seven universal microexpressions, each
corresponding to a core emotion

```
Emotions Typical facial indicators
Disgust Wrinkled nose, raised upper lip
Anger Lowered brows, glaring eyes, pressed lips
Fear Raised eyebrows, wide eyes, open mouth
Sadness Drooping eyelids, downturned lips, raised inner eyebrows
Happiness Smiling mouth, crow’s feet wrinkles around eyes
Surprise Raised eyebrows, wide eyes, open mouth
Contempt One side of the mouth raised (asymmetrical expression)
```
In a videotaped interview from 1969, Ekman and Friesen saw a brief
full-face emotional expression that indicated a significant negative mood
that a mental patient was attempting to conceal from her psychiatrist to


persuade him that she was no longer suicidal. When the interview footage
was viewed in slow motion, it was discovered that the patient was
displaying a fake smile that lasted longer than two frames (1/12 s) after a
very brief sad face [ 41 ]. Thus, microexpressions, although not discriminant
enough for facial recognition, provide additional information that aids
traditional facial recognition. Initially, attempts were made to integrate
appearance-based facial characteristics, such as facial marks, skin colour,
and hair colour/style. However, more recently, behaviour-based facial
characteristics, such as head dynamics, visual speech, and facial
expressions, have also been studied [ 37 ]. Microexpressions can be taught to
humans, although the results are usually substandard. It is necessary to
create computer programs to recognise microexpressions because humans
are not sufficiently capable of doing so. Automatic microexpression
recognition refers to the use of computer-based technologies to recognise
microexpressions. Sadness, joy, fear, anger, surprise, contempt, and hatred
are the seven categories of both macro and microexpressions [ 1 , 38 ].
Because it contains a wealth of psychological information that, when
recognised, may be helpful in the areas of lie detection, criminal
investigation, neuromarketing, psychotherapy, and teaching and learning,
experts have recently focused more on the significance of microexpression
[ 47 ]. Thus, this chapter focuses on subtle emotions and microexpressions
rather than macroexpressions, which could reveal the actual emotions that
people often try to hide. Further, the utility of microexpressions and subtle
expressions using artificial intelligence and technological advancements for
facial recognition has been investigated.

##### 2 Differentiating Between Subtle Emotions and

##### Microexpressions

It takes more than technical definitions to distinguish between subtle
emotions and microexpressions. It entails identifying how someone acts,
looks, and is perceived in interpersonal situations. Microexpressions are
short, uncontrollable facial expressions that happen when someone is trying
to hide or repress a strong emotion. These brief expressions, which usually
last between 1/25th and 1/2 of a second, are usually uncontrollable. They
are brought on by strong emotions that temporarily overcome an


individual's deliberate attempts to keep a calm or neutral appearance.
Microexpressions are often overlooked in daily interactions because of their
speed and subtlety. Seeing someone's true feelings before they are quickly
hidden is what it's like to watch a microexpression. For instance, just before
a forced smile or a neutral expression is returned, one may notice a flash of
anger or panic in the person's mouth or eyes. The emotional context of
microexpressions is based on concealment, and the person actively tries to
hide these feelings. But the emotion is intense enough to temporarily
overcome their self-control. Since microexpressions are easily missed in
casual conversation, detecting them requires keen observation. Finding one
offers a unique and candid glimpse into an individual's inner emotional state
and can be compared to seeing a secret slip through the cracks [ 13 ].
Subtle emotions, on the other hand, are low-intensity emotional
experiences that are typified by soft and delicate facial muscle movements.
It's easy to overlook these expressions because they're so subtle. They
frequently outlive microexpressions, though. Subtle emotions are not the
product of concealment. Instead, they happen when a sensation is just
starting, disappearing, or never getting strong enough to come out
completely. When someone is feeling a subtle emotion, they don't intend to
hide it; it's just too subtle to be noticeable. Subtle emotions are compared to
a calm undercurrent rather than an abrupt wave in terms of subjective
experience. There may be a subtle hint of sadness in someone's eyes or a
barely noticeable smile of satisfaction—emotions that are present but
neither overpowering nor purposefully concealed. Sensitivity to context and
an acute awareness of subtle behavioural cues are necessary for the
detection of subtle emotions. The identification of subtle emotions is more
about noticing subtle changes or undertones in a person's behaviour than it
is about spotting a fleeting “leak” of emotion, as is the case with
microexpressions. This recognition necessitates an appreciation for the
subtle cues that can reveal one's actual emotions, even when those emotions
are not intense or purposefully concealed [ 20 , 53 ].

##### 3 Traditional Methods of Detection of

##### Microexpressions and Subtle Emotions


Prior to the development of artificial intelligence (AI) and computer-based
analysis, human observation, psychological frameworks, and subjective
evaluation methods were used to identify microexpressions and subtle
emotions. Although these techniques are fundamental, these are constrained
by human perception and biases. Such techniques are widely applicable in a
variety of clinical and practical contexts [ 58 ]. Some of the conventionally
important techniques for identifying subtle emotions and microexpressions
are discussed below.

**3.1 Training and Direct Human Observation**

Observation is the most common technique for examining emotions.
Although people may try to hide their emotions, skilled observers, such as
psychologists or interrogators, will look for signs of fleeting facial
expressions that convey genuine emotions. The most common way of doing
this is to make the person comfortable while talking. This technique was
developed in 1978 by Dr. Paul Ekman, who used slow-motion video
playback to identify microexpressions. Thus, the Facial Action Coding
System (FACS) was developed, which categorised facial muscle
movements associated with specific emotions. As microexpressions are so
subtle and transient, specialised training was required for accurate
recognition [ 18 ].

**3.2 Facial Action Coding System (FACS)**

FACS is a manual method to study the facial movements. All potential
facial movements based on the muscles that produce them are
comprehensively analysed. Observers use FACS to deduce meaning from
the underlying emotion by systematically noting which facial muscles
contract [ 58 ]. It is founded on the fundamental ideas of universality,
validity, action units (AUs), and objective measurements.

**_3.2.1 Objective Measurement_**

FACS, which describes facial movements without interpreting emotions,
guarantees analytical neutrality. Coders note obvious changes, like skin
bulges or furrows, to infer muscle activation. For example, zygomatic major
(AU12) raises the corners of the mouth, while orbicularis oculi (AU6)
causes crow's feet wrinkles.


**_3.2.2 Action Units_**

An action unit is a code that denotes a particular facial muscle movement or
set of muscles. More than 40 AUs that describe facial movements are used
to represent individual or combined muscle movements. Each AU is given a
code that contains timing (onset, offset, at peak), symmetry (left, right, or
centre), and intensity (graded from A to E) parameters. AUs can be used to
distinguish between different facial expressions. For instance, the
simultaneous activation of AU6 (cheek raiser) and AU12 (lip corner puller)
indicates a sincere (Duchenne) smile, which entails true happiness
involving both the mouth and the eyes. A Pan-Am (posed or fake) smile, on
the other hand, usually only uses AU12 and does not move the muscles
surrounding the eyes, which makes it seem less real.

**_3.2.3 Validity and Universality_**

According to FACS, there are seven universal emotions: joy, grief, surprise,
fear, anger, disgust, and contempt. All ages, genders, and cultural
backgrounds can relate to these. Its accuracy in detecting subtle differences,
like genuine versus fake smiles or painful emotions, has been demonstrated
by research [ 64 ].

**3.3 Self-assessment Techniques**

Certain questionnaires, rating scales like Likert scales, or visual aids like
the self-assessment manikin (SAM) are utilised by the people to analyse
their emotional states [ 24 ]. Attitudes, opinions, and perceptions are
measured using the Likert scale. On a predetermined scale with three to
seven level points, respondents are asked to indicate how much they agree
or disagree with a statement. This also enables the researchers to measure
subjective data and identify patterns in a persons behaviour and thoughts.
Various domains including forensic sciences, education, psychology, and
social sciences require understanding of human reactions and facial
expressions. Self-assessment techniques can be helpful as these are non-
invasiveness.
Self-assessment manikin (SAM) is a non-lexical as it estimates an
individuals emotional reaction to different stimuli, like images, or events
and can be called as a visual self-assessment tool. SAM scales are used by
the participants to rate their emotional reactions to each stimulus. These are
provided with a series of visual manikins which represents different


landmark on each dimension. Usually for a predetermined period of time,
one stimulus is provided at a time and the participants are encouraged to
focus on each stimulus when it is provided. Participants rate their emotional
reactions to each experience using three distinct SAM measures:

1. Valence: Participants choose between a happy, smiling manikin
    (pleasant) and a frowning, unhappy manikin (unpleasant). An image of
    a snake, for instance, might produce a low pleasure rating (frowning
    figure).
2. Arousal: Participants choose a figure that is enthusiastic and wide-eyed
    (high arousal) or calm and sleepy (low arousal). A dramatic car chase
    scene, for instance, would have a high arousal rating.
3. Dominance: A big, strong figure that feels in control or a small,
    submissive figure that feels controlled are both possible. For instance,
    facing a spider could lead to poor dominance (small figure).

Each scale has a point range of 5 or 9, and the participant marks the
point that best captures their feelings as part of the SAM process. The
process is repeated for each stimulus in the set [ 8 ]. The stimuli's order can
be changed to prevent order effects. Both paper forms and electronic
(automated) forms are used to record the ratings for each stimulus and
dimension. Although these methods are simple to use, their dependability
may be impacted because people may not recognise or express their
emotions correctly, especially when they are subtle or socially
unacceptable.

**3.4 Behavioural Observation**

The majority of the emotional information in interpersonal communication
is conveyed by body posture, gestures, speech content, and vocal tone in
addition to facial expressions [ 24 ]. The screening of passengers by
observation techniques (SPOT) methodology is used in security contexts.
This approach comprises trained officials systematically observing
passengers for behavioural cues and microexpressions that may disclose
concealed feelings or intentions. In checklist-based observation, observers
use standardised checklists to record the frequency, duration, and context of
observed microexpressions or subliminal emotional cues [ 49 ]. The


subjective interpretation of these cues may vary depending on the
individual and culture [ 24 ].

##### 4 Difficulties in Identifying Subtle Emotions and

##### Microexpressions

The low intensity and fleeting nature of microexpressions and subtle
emotions, their overlap with other facial gestures, the challenge of robust
feature extraction, and the limitations of human perception and data make
them challenging to detect. Both automated and traditional approaches still
face these challenges, though some are gradually being overcome by
emerging technologies. The following lists some of the difficulties that arise
when analysing facial expressions.

**4.1 Overlapping Facial Movements**

Overlapping microexpressions present significant challenges for detecting
and recognising facial movements in both automated systems and manual
observations by humans. Overlapping facial movements arise when
multiple microexpressions occur quickly, one after another or at the same
time, blending facial muscle movements and making them difficult to
identify distinct emotional cues. The facial gesture that arises from the
overlay of microexpressions may not correctly reflected distinct emotions.
On the other hand, traits of ambivalence might lead to a confusion to the
spectator. This in turn leads to obscurity and increases the likelihood of a
misclassification [ 19 ]. Impulsive overlap of microexpressions due to lip
movements and eye blinking simultaneous is one such examples. Explicit
detection is obstructed by irrelevant facial movements in real-time or video
observations [ 41 ].

**4.2 Low Intensity and Short Duration**

Microexpressions are difficult to identify and differentiate from other facial
movements due to extreme brief time span (less than 0.2 s) and low
intensity. Human spectator, who are highly skilled, and certain algorithms
might overlook or misunderstand certain microexpressions because of their
complexity [ 34 ]. Electromyography (EMG) reveals that microexpressions
contain 7–9% maximal voluntary contractions that are hidden in full


expressions. Subtle microexpressions are difficult to detect as that have
lower intensities. Microexpressions can easily go unnoticed or mistaken as
a non-emotional facial movements due to these characteristics. Spectator’s
inconsistent detection and annotation are exacerbated by individual
variances in perception, vigilance, and instinctive definitions [ 32 ].

**4.3 Feature Extraction Difficulties**

The feature extraction process is essential for recognizing microexpressions
and subtle emotions. However, due to the distinctive traits of these
microexpressions and subtle emotions, it constitutes a number of
drawbacks. Customary methods rely on manually generated features, which
are time-consuming, precise, and may not notice dimunitive changes. Due
to their ingenious nature, these usually contain segments that are inadequate
for accurate recognition [ 73 ]. Motion feature extraction of microexpressions
requires sundering of the remarkable facial muscle movements from
expendable or overlapping motions. Methods like action unit analysis or
optical flow often require additional calculations. These may still not detect
the subtle, transient features that define microexpressions. Feature
extraction algorithms may find it difficult to avoid overoptimization and
generalise effectively because of the small sample size and lack of diversity
of microexpression datasets. It is even more challenging to train
unimpeachable feature extractors due to the absence of tagged
microexpression samples [ 33 ].

**4.4 Human Factors**

Human factors influence the correctness and dependability of remembrance.
These present multiple obstacles to identifying subtle emotions and
microexpressions. One of the obstacles is when considerable facial
movements occur simultaneously or shortly after each other. For instance, a
change in the facial movements, such as a smile, can mask or obstruct the
detection of an eye microexpression. This might significantly reduce hit
rates and make subtle hints difficult to spot. This masking effect is
especially problematic in natural settings where facial emotions are
dynamic and rarely isolated [ 28 ]. Because human observers are susceptible
to bias, fatigue, and perceptual limitations, consistent and accurate detection
is challenging without extensive training. Because dishonest people can
deliberately manipulate or hide obvious behaviours, it can be more


challenging to identify them in high-stakes situations [ 40 ]. Distractions,
shifting lighting, and the presence of additional nonverbal cues make real-
life detection more difficult. Training and assessment paradigms often lack
ecological validity because laboratory settings usually fall short of
capturing the complexity of real-world social contexts. Subjective human
interpretations of subtle emotions and microexpressions can lead to
differences between observers. Cultural norms, expectations, and personal
biases can all affect how something is perceived and understood [ 49 ].

**4.5 Technical and Methodological Limitations**

Both humans and machines are affected by the substantial challenges posed
by technical and methodological limitations when it comes to recognising
microexpressions and subtle emotions. These restrictions stem from the
transient and low-intensity nature of microexpressions, the variety of
human expression, and the boundaries of technology and research. Pre-
established window intervals and detection thresholds that don't apply to
different video frame rates or subjects may cause false positives or missed
detections. Over-magnification methods can produce noise, while under-
magnification methods might totally miss minute movements.

**4.6 Data Limitations**

The lack of large, spontaneous, and dynamic microexpression databases
hinders the development and validation of detection methods. Variations in
frame rates, video lengths, and recording settings complicate the creation of
consistent datasets and benchmarks [ 41 ]. Because actual microexpressions
typically take place in emotionally charged, high-stakes situations that are
hard to replicate in lab settings, it can be difficult to create realistic, high-
quality databases of them. There may be biases in training and evaluation
because posed expressions, which are very different from real
microexpressions, are used in many recent studies [ 69 ].

##### 5 AI-Based Methods for the Detection of

##### Microexpressions and Subtle Emotions

Machine learning and deep learning algorithms are used in AI-based
microexpression and subtle emotion recognition. These can help to analyse


physiological data, speech, text, and facial movements. The correctness,
momentum, and scalability of emotion recognition have improved
tremendously using these methods. Artificial Intelligence paradigms are
useful for small and low-intensity expressions that are difficult for humans
to understand [ 10 ].

**5.1 Preprocessing and Frame Selection**

The most essential steps for detecting microexpressions and subtle emotions
include preprocessing and frame selection. These amplify the correctness
and effectiveness of the recognition algorithms. These methods associate
important frames that capture small muscle movements and suitable facial
regions to prepare raw data. It involves the following procedures.

**_5.1.1 Face Detection and Alignment_**

The face in each video frame is recognised and aligned to a canonical
posture in the first step of preprocessing. It aids to diminish deviations
caused by movements of head or camera angles. Face landmarks are
positioned consistently across frames, which increases the consistency of
feature extraction. Background noise and unnecessary details can be
removed by aligning and cropping the image [ 1 ].

**_5.1.2 Head Movement and Eye Blink Removal_**

Eye flutter, avert your glance, and head movement may introduce noise into
feature extraction. Common preprocessing methods and filters are used to
reinforce the face region. Facial features that are less affected by these
artefacts, such as the cheeks or lips can also be selected [ 65 ].

**_5.1.3 Data Labelling and Frame Division_**

Clips are divided into individual frames. Facial action units (AUs) are
assigned very carefully to each one of them. Key frames are marked as
‘onset’ for the emergence of expression change, ‘apex’ for the peak
expression, and ‘offset’ for the return to neutral expression. The first
deviation from non-involvement occurs in the onset frame.

**_5.1.4 Optical Flow and Motion Feature Extraction_**

TV-L1 and other optical flow algorithms used to record small muscle
movements and the variations in pixel intensity between onset and apex


frames. Deep learning models uses these motion features as inputs and are
critical for fetching attention to subtle movements that may be missed by
the human eye [ 35 ].

**5.2 Feature Extraction Techniques**

Microexpressions and subtle emotions detection depends on extracting
features that capture the short, low-intensity, and localised facial muscle
movements. This can be achieved by the following techniques.

**_5.2.1 Local Binary Pattern_**

Local Binary Pattern (LBP) refers to a common texture descriptor that
encodes local spatial patterns in pixel intensities. This is frequently used for
facial microexpression identification in apex frames, which contain the
most discriminative information [ 1 ]. It is based on the ideas of Local
Texture Encoding. Comparison of pixel intensity to neighbours is
performed and differences are converted into binary information [ 1 ]. Local
Binary Pattern on Three Orthogonal Planes (LBP-TOP) analyses texture
dynamics across three planes to provide spatiotemporal analysis for video.
The Spatial texture within frames are represented by XY plane. XT plane
represents the horizontal-temporal motion, which changes in row order over
time. The YT plane represents vertical-temporal motion, that is, column-
wise changes over time. Both methods are perfect for real-time applications
because they are computationally efficient and resistant to monotonic
greyscale changes [ 11 ].
Face alignment is carried out during the preprocessing step of the
process and procedure, where facial regions are identified and cropped to
guarantee consistent analysis. Then, using temporal segmentation, video
segments with microexpressions are separated [ 41 ]. LBP is calculated for
static images during feature extraction. The image is separated into tiny
areas (such as 16 × 16 blocks) for every frame. Each block's LBP codes are
calculated for every pixel, and histograms of the codes are created for each
block before being concatenated. LBP-TOP is used for video sequences.
The video is handled like a three-dimensional volume (X, Y, T). Processing
specific to the plane is then carried out. Each frame of spatial texture is
extracted in the XY plane. Horizontal motion, like lip movements, is
recorded in the XT plane, while vertical motion, like brow raises, is


recorded in the YT plane. LBP histogram of each frame is calculated for
encoding of the feature [ 25 ].
Further, a single feature vector is created by linking the histograms. To
reduce the feature extent while maintaining biased information,
dimensionality reduction is carried out. The best features can be chosen
using PCA or genetic algorithm. SVM, k-NN, or Random Forests classifiers
are trained for classification using labelled microexpression data. Public
databases like CASME II, SMIC, or SAMM are often used. Finally, metric
techniques such as accuracy, F1-score, and leave-one-subject-out cross-
validation (LOSO-CV) can be used for evaluation [ 36 ].

**_5.2.2 Local Non-negative Matrix Factorisation_**

Local non-negative matrix factorisation (LNMF) breaksdown and captures
small, facial images into localised, non-negative traits from the apex frame.
These characteristics signify movements of facial muscles. It is also
possible to turn the macroexpression data microexpression samples by
using it with a macro-to-micro (MtM) transformation technique [ 21 ]. The
pixels in a face image are represented by a non-negative data matrix (X),
which breaks down into two non-negative matrices. First, localised face
components are represented by the basis matrix (W), such as movements of
the mouth and eyes. Second, the coefficient matrix (H). Weights for
combining bases into phrases are encoded in the coefficient matrix (H). X ≈
WH represents this decomposition. Non-negativity ensures interpretability
because facial features cannot have negative values. LNMF enforces
locality and sparsity constraints. Spatial sparsity occurs when certain facial
regions are restricted to bases (for example, crow’s feet are used to
symbolise happiness). Mutually orthogonal bases isolate particular muscle
activities and reduce redundancy. This represents tiny, fleeting movements
that are linked to microexpressions. Unlike holistic approaches like PCA,
facial expressions are broken down into muscle activities that correlate with
their anatomical foundation [ 21 ].
Apex frames (peak intensity) from microexpression videos are
preprocessed as input at the start of the process. Face cropping, alignment,
and normalisation are used to remove changes in pose and illumination.
Each frame is then vectorised into a column of X, forming a matrix. Next,
feature extraction is performed via LNMF. The factorisation involves

solving the optimisation problem: minimise ||X-WH||^2 concerning W and H,


subject to W, H ≥ 0, with added locality constraints. Sparsity in W is
maximised, and redundancy in H is minimised. Basis images (e.g., eyebrow
raise, lip corner pull) are contained in the output W, while H provides
weights per expression, encoding the intensity of muscle actions. To address
the challenge of limited microexpression data, macro-to-micro
augmentation is employed. macroexpressions (e.g., from the CK+ dataset)
are decomposed using LNMF. Microexpressions are then synthesised by
reducing the intensity of coefficients H in WH and applying temporal
subsampling to shorten the duration. This results in expanded, realistic
training data, preserving non-negativity. Finally, LNMF features (columns
of H) perform the classification for each sample as input. Features and
emotions are plotted using a classifier and cross-validation is performed on
datasets like CASME II or SAMM for its evaluation [ 21 ].

**_5.2.3 Motion Feature Extraction (MoExt)_**

MoExt produces motion properties by linking the microexpression’s onset
and apex frames. Motion features that are distinctive to microexpressions
are outlined by separating and then merging the shape and texture features
from both frames. The model's durability and generalization are increased
by pre-training with macroexpression data and assorted losses. Relevant
motion can be focused and removal of irrelevant information can thus be
performed [ 33 ]. Motion extraction is based on looking at and isolating the
small, quick changes in facial muscle movements between certain frames.
These frames are usually the start and peak of an expression. This method is
important because microexpressions are quick and often too subtle for static
texture analysis. The main clue is to get dynamic features, which shows real
changes in emotions while getting rid of texture or identity information that
is irrelevant [ 33 , 39 ].
The onset and apex frames of facial movements are identified and
extracted from video sequences, capturing the spasm and maximum
intensity of microexpressions after data preparation. To ensure consistency
throughout samples, facial regions are cropped, aligned, and normalised.
Further, feature segregation is performed in which shape and texture
features are independently extracted from both onset and apex frames using
a feature segregator. This aids in isolating the actual muscle movement from
static facial characteristics. Motion feature extraction then takes place and is
specific to the microexpressions that are derived by calculating the


difference or transformation between the shape features of the onset and
apex frames.
The main principle is to collect stimuli which present dynamic
properties and embody substantial emotional variations, but to omit purely
irrelevant texture or identity content. This is essential since
microexpressions may be short-latent and very minute, and therefore
unsuitable to analyze by texture alone [ 33 , 39 ].
This is initiated by data preparation in which onset (initial) and apex
(peak) frames are identified and extracted on facial video sequences to
capture the start as well as the most intense microexpression. Facial areas
are adjusted including alignment, cropping and normalisation so as to
ensure similarity in different samples. After that, the separation of features
is carried out. Both onset and apex are separated using feature separator in
order to extract shape and texture features separately. Making this
separation at all helps separate the real motion of muscles helping
distinguish them than the facial static features. Afterwards, motion feature
extraction occurs Motion features that are specialized to the
microexpression are obtained as a difference or transformation of the shape
features of the onset frame and the apex frame.
Advanced methods, such as motion flow generators, are employed to
analyse pixel intensity changes across frames, highlighting expressive
regions where micro-movements occur. Dynamic maps or optical flow
might be used to visualise and quantify subtle motion over time. For
advanced deep learning approaches, pre-training and reconstruction are
utilised. To overcome limited microexpression data, models are pre-trained
on larger macroexpression datasets, enabling the network to learn general
motion patterns before fine-tuning on subtle microexpressions. The model
may be trained to reconstruct the apex frame from the onset frame using
extracted motion and texture features, ensuring that motion features are
representative and disentangled from irrelevant information. Contrastive
loss is used during pre-training to maximise the distinction between
relevant motion features and noise. Integration and fine-tuning are next,
which incorporates the motion extractor and feature separator into the
primary microexpression recognition network. All sets within the network
are further enhanced by microexpression datasets to maximise the accuracy
of identifying subtle, real-world expressions. A last task is its classification.
Motion features extracted are fed into classifiers to classify the type of


emotion or microexpression. The assessment compares the effectiveness of
the methods based on general metrics (accuracy, F1-score) and their ability
to cross-authenticate on standard datasets [ 39 , 56 ].

**5.3 Machine Learning Classifiers**

This approach is universal in the detection of micro expressions and the
subtle emotions, as well as classic methods of deep learning and the
combination of specialized feature extraction. Some of the machine
learning classifier techniques are explained here.

**_5.3.1 Support Vector Machine_**

Support Vector Machine (SVM), a handheld machine learning algorithm,
was used to classify microexpressions and subtle emotions because of its
ability to handle high-dimensional, subtle and complex data patterns. SVM
is commonly adopted as a standard classifier in microexpression
identification due to its high data efficiency in high dimensions, and its
stability. Performance is increased by associating it with manually designed
features, such as Local Binary Pattern (LBP) on apex frames or Local
Binary Pattern on Three Orthogonal Planes (LBP-TOP) on clip sequences
[ 29 ]. Types of kernel functions can be used to optimise the classification,
including the polynomial and radial basis functions. The results found in
surveys indicate that SVM can classify the microexpressions with a
considerable level of accuracy based on the datasets such as CASME II and
SMIC [ 48 ]. The SVM is grounded in the principle of maximal margin
classification that attempts to find the best hyperplane between the data
items in different classes (e.g., emotions) with the maximum margin, which
can improve generalisation and robustness. SVM uses kernel functions,
namely, radial basis functions and polynomials, to transform non-linearly
separable data to higher-dimensional versions to segregate them. It can be
used in binary and multiclass problems, including, e.g., the classification of
multiple emotions, using one-vs-one or one-vs-all strategies [ 21 ]. Shu-Xin
Du et al. came up with a better support vector machine in classification,
where there are distinct costs that can be acquired when entities are
misclassified. To test the diagnosis of breast cancer, two weighted support
vector machines weighted C-SVM and V-SVM have been designed, which
portrays the success of these techniques. The fact that the advantage thus
gained comes at a price of potential decrease in accuracy of the smaller


training size class and in combined accuracy over all classes was mentioned
[ 61 ].

**_5.3.2 Extreme Learning Machine_**

Extreme Learning Machine (ELM) is a form of single hidden layer
feedforward neural network (SLFN) and was popularised due to its fast
learning, together with quick generalisation capabilities. LM has found an
application in microexpression and subtle attitude recognition due to its
efficiency and effectiveness in data of large dimensions and small data
sample sizes [ 1 ]. It is demonstrated to be more accurate and faster in
training than SVM in microexpression recognition using LB and LBP-TOP
features. As an example, ELM attained 97.54% at best on CASME II apex
frames of microexpressions in terms of the training accuracy [ 45 ]. LM
works on the notion of randomised weights, where the connections between
the hidden layer input units and output units are assigned randomly and
held fixed, whereas only the connection coefficients of the output units are
learnt. The analysis eliminates the iterative backpropagation that is required
in traditional backpropagation by using the weighted output weights
through a single step least-squares solution. Fast training ELM is the
structure that permits training speeds that are far swifter than the classic
distance across the board, with the added advantage of evading local
minima, and therefore ELM proves to be suitable in real-time operations
and enormous-scale undertakings [ 45 ].

**5.4 Deep Learning Models**

The automatisation of extracting features as well as accuracy in the
recognition of microexpressions and faint emotions have been
revolutionised through deep learning. Some of the most important models,
techniques, and datasets of the domain are described below.

**_5.4.1 Convolutional Neural Networks_**

Convolutional Neural Networks (CNNs) are specialised deep learning
networks that analyse grid-based data with photos, videos and audio being
some examples. It is a cutting-edge measure of identifying
microexpressions and minor emotions because it has the capability to
automatically collect complicated spatial and temporal items presented in
facial image sequences. It is exceptional at spatial hierarchy and pattern


recognition tasks, which include identification of face recognition, object
detection, and picture classification as it automates feature extraction and
eliminates manual preprocessing [ 57 ].
These three can be considered the background of this approach.
Hierarchical feature learning is one of such principles, with low-level-to-
high-level features being trained by CNN on input images or in video data
of microexpressions and minor emotions respectively, their spatial
(appearance) and timescale presence (motion). The second principle is end-
to-end learning in which the CNNs are trained on manually labeled datasets
allowing them to discover an information-preserving mapping between raw
input features and emotional categories, without the intervention of manual
feature engineering. The third principle is spatiotemporal modelling, where
sequences of frames are processed by 3D CNNs and hybrid attention
models, building on standard CNNs and capturing the transient and delicate
nature of microexpressions [ 27 , 52 ].
CNNs work by detecting features, whereby filters (or kernels) skim
facial images to recognise crucial patterns, like edges or low-profile curves.
These networks learn in a hierarchical manner, with the early layers
learning low-level features such as edges and corners, and the deeper ones
combining them to form more complex facial structures and expressions.
With the same kernels being used to detect features throughout the whole
image, some position invariance is achieved, which is important for
detecting moving facial microexpressions. Pooling operation then enhances
and summarises the hierarchical features with special focus on localised
area, maintaining vital information and eliminating dimensionality. Such an
effective extraction and interpretation of facial characteristics enables
CNN as a powerful approach in forensic facial reconstruction using subtle
emotional expressions [ 23 ].
For example, sequences are processed at several temporal scales by the
Dual Temporal Scale CNN (DTSCNN) to learn discriminative features
based on expression durations [ 43 ]. Spatial-temporal features are extracted
more effectively by Hybrid Attention-3DNet through the combination of
dual-path 3D CNNs with attention modules and squeeze-and-excitation
blocks [ 27 ]. Features at several scales are integrated by the Multi-Scale 3D
Residual CNN, and attention is employed for contextual awareness,
resulting in more subtle feature detection [ 30 ]. 3D CNNs for spatiotemporal


characteristics are combined with ANNs for inter-region relationships by
Deep3DCANN, employing a fusion technique for robust prediction.

**_5.4.2 Fully Connected Neural Networks_**

Fully Connected Neural Networks (FCNNs), also known as dense or
feedforward neural networks, are fundamental deep learning designs in
which every neuron in one layer connects to every neuron in the following
layer. It is frequently employed as the final classification step in
microexpression and subtle emotion detection systems, usually after feature
extraction layers like convolutional or recurrent layers. Retrieved data is
converted into emotion categories by FCNNs through the learning of
weighted connections between all neurons in adjacent layers [ 66 ]. They
excel at learning complicated patterns from structured data. However, these
have problems with high-dimensional inputs such as photographs. It is used
for categorisation after feature extraction, sometimes attaining extremely
high accuracy in controlled datasets.
Fully connected neural networks (FCNN) are based on four
fundamental principles.

i. Dense Connectivity: Communication between each neuron in one
layer and every neuron in the next layer occurs, allowing complicated,
non-linear correlations between input properties and output classes to
be learned by the network.

ii. Feature Mapping: High-dimensional feature vectors (derived from
images or videos) are converted into probability distributions for
emotion classes by FCNNs.

iii. Parameter Learning: Weights are adjusted by the network using
backpropagation and gradient descent to reduce classification error on
training data.

iv. Integration with Deep Models: FC layers frequently follow
convolutional layers (CNNs) or other feature extractors, acting as the
decision-making component for interpreting learnt features [ 7 ].

Reconstruction of the face on the basis of the analysis of the
microexpression and the slightest emotions is properly to use the Fully


Connected Neural Networks (FCNNs). Each neuron performs the forward
propagation of weighted combination of inputs with the following formula
z = W x + b where W is the weight matrix, a column vector of the input x,
and the bias b. The learning capability is achieved by the model by the need
to apply the non-linear activation functions such as ReLU to the result of
the sum (a = f(z)). The input is featured by 9 dimensions, and the output by
4 dimensions due to a 9A times 4 weight matrix. The result from the
network is the facial input data fac-simile, which is fed to the
backpropagation algorithm in calculating the error in prediction against the
actual values. As used in the network, optimization algorithms such as
Stochastic Gradient Descent (SGD) and Adam simply adjusts weights
through the process of error propagation, known to begin with the output
layer. The training procedure enables the FCNN to tune its internal
parameters, and this improves its detection and reconstruction of facial
expression of microexpressions of emotions [ 55 ].
The deep pre-trained neural networks used two final fully connected
layers to detect microexpressions. For example, AlexNet reached an
accuracy of up to 99.84% after freezing 50% of the learnable layers in the
transfer learning. These convolutional networks are learned to make fine-
grained transfers to the target emotion classes of subtle facial emissions
through the FC layers thereby achieving automatic recognition of the subtle
emotions. CNN output feature vectors are fed into FC neurons, and frames
are also classified as microexpression or neutral, which assists with
microexpression temporal localisation [ 7 ].

**_5.4.3 Multi-task Learning_**

Multi-task learning (MTL) plays an important role in making use of
common representations across similar tasks to facilitate generalisation and
performance. In tackling microexpression and challenges in subtle emotion
recognition, e.g., scarcity of data and subtle signal extraction, TRL resorts
to additional tasks that provide complementary information simultaneously
[ 26 ]. Learning architectures that jointly learn to identify landmarks on the
face and to classify subtle emotions, to help understand nuanced
expressions and states of the human mind [ 26 ]. The term TL is a machine
learning paradigm where a single model is trained to perform many related
tasks simultaneously, in order to utilise shared representations to the
advantage of generalisation and efficiency. The combination of


complementary tasks to facial landmark detection, action unit (AU)
identification, and valence-arousal estimate is used in TDL to enhance the
accuracy of recognition of microexpressions and subtle emotion recognition
[ 71 ]. Applying a shared backbone network (e.g., CNN) to extract the
common feature also relevant in more than two tasks also falls inside MTL,
due to practical elimination of redundancy and increased efficiency. Facial
landmark detection or action unit recognition are also used as auxiliary
tasks to provide contextual priors that enhance the most significant activity
of emotion recognition. The overfitting is avoided by means of the joint
training by limiting the model to learn features that are specific across tasks,
which yields a regularisation effect important to small microexpression
datasets [ 26 ].

**5.5 Micro-mimics and Emotional AI**

A person’s genuine emotional or psychological state can be revealed by
micro-mimics, which are incredibly small, fleeting facial muscle
movements that are frequently imperceptible to the unaided eye. In high-
stakes situations where people may feel under pressure to hide their
emotions, such as negotiations, interviews, and mental health evaluations,
these micro-mimics, often referred to as microexpressions, are essential.
Micro-mimics offer a direct, instinctive window into real emotions, setting
them apart from more overt or deliberately controlled displays, making their
analysis more relevant [ 67 ]. Artificial intelligence systems that use a
combination of sophisticated sensory and computational methods to
identify, understand, and react to human emotions are referred to as
emotional AI. Emotional AI makes use of a number of crucial technologies
in relation to micro-mimics and nuanced emotions, which are as follows:

i. Neural networks and deep learning are algorithms that analyse
enormous collections of facial pictures and video data to find nuances
and ephemeral expressions that convey emotional states. The ability of
deep learning models to differentiate between micro-mimics and
typical face motions is particularly strong.

ii. Computer Vision uses frameworks such as the Facial Action Coding
System (FACS), AI employs computer vision techniques to examine
facial landmarks and muscle movements frame-by-frame and correlate
them to particular emotions [ 50 ].


iii. Multimodal Fusion: To create a thorough emotional profile, emotional
AI frequently combines visual (facial), auditory (voice), textual
(language), and physiological (heart rate, skin conductance) data
streams. Accuracy is increased by this fusion because signals from one
modality can reinforce minor indications from another.

iv. Explainable AI (XAI): Some systems employ explainable AI
technologies (such as integrated gradients or LIME) to make it clearer
how judgments are made about emotion identification to foster trust
and transparency. This is particularly crucial in delicate applications
like healthcare or education [ 68 ].

**5.6 Real-Time AI Emotion Recognition Software**

Using the most advanced computer vision, deep learning, and multi-modal
analysis, real-time AI emotion identification software has made tremendous
progress in detecting subtle emotions as well as microexpressions. An
outline of the major technologies, cutting-edge solutions, and contemporary
environment is provided below:
i. Advanced Computer Vision and Deep Learning: These systems
examine real-time video streams for face landmarks, muscle
movements, and micro-movements using deep neural networks that
have frequently been trained on enormous datasets. Because
algorithms are designed to be correct and swift and they can provide
real-time feedback on emotional states.

ii. Microexpression Detection: Trained models are able to identify tiny,
involuntary facial movements, or microexpressions, that occurs
frequently. This calls for sensitive feature extraction and high frame-
rate video analysis, frequently using recurrent neural networks (RNNs)
and convolutional neural networks (CNNs) [ 44 ].

iii. Subtle Emotion Detection: By investigating subtle differences in facial
muscle tension, eye movement, and micro-gestures, these models are
able to identify tiny, low-intensity emotional indicators that go beyond
unconcealed expressions. For a more thorough emotional profile, some
solutions additionally combine text and speech analysis.


iv. Multi-Modal Emotion Recognition: Prominent platforms include facial
analysis, voice stress detection, and text sentiment analysis. For
instance, Kodexo Labs combines facial expressions, verbal clues, and
vocal emphasis to provide strong, context-aware emotion
identification [ 62 ].

**5.7 Dataset and Annotation Advances**

Recent advancements in datasets and annotation methodologies have
significantly improved the detection of microexpressions and subtle
emotions. Following are the detailed breakdown of key developments.

**_5.7.1 Micro and Macroexpression Warehouse_**

Micro and macroexpression warehouse (MMEW) are extensive, high-
resolution facial expression dataset designed to facilitate the detection and
recognition of both microexpressions and subtle emotions. It is distinctive
in providing paired micro and macroexpressions from the same subjects,
permitting robust algorithm development and cross-modal analysis [ 72 ].
More than 300 clip samples covering seven emotion categories like
happiness, surprise, disgust, fear, sorrow, rage, and contempt are included in
MMEW, which was first released in 2022. MMEW is built on the unified
dataset approach, which provides both micro and macroexpressions,
allowing researchers to investigate the link between subtle (micro) and
overt (macro) emotional displays in the same individuals. It comprises
subject-independent evaluation procedures to replicate real-world situations
in which models come into contact with invisible people, and contains the
same subject's micro and macroexpressions, allowing for comparison [ 5 ].

**_5.7.2 High-Quality Annotation_**

Individual sample is annotated with onset, apex, and offset frames, as well
as facial action coding system (FACS) action units, providing exact
temporal and muscle movement data for algorithm training and evaluation.
MMEW's rich emotion groups provide a greater range of emotion classes
(happy, surprise, rage, disgust, fear, sadness, others) than many earlier
datasets, allowing for sophisticated emotion analysis [ 5 ].

**_5.7.3 Child Microexpression Dataset_**


Child Microexpression Dataset (CMED) is the first dataset precisely
designed to capture and analyse unpremeditated microexpressions in
children, addressing a significant gap in affective computing research,
which has largely concentrated on adults. The first dataset fills a significant
void in developmental psychology and education by concentrating on
unplanned child microexpressions. It captures children's tiny emotional
indicators, which vary in duration and intensity from those of adults.
CMED captures spontaneous microexpressions in ecologically valid
settings, such as virtual psychotherapy group activities, rather than
controlled laboratory elicitation. It enables the study and automated
detection of subtle emotional expressions in children, which differ in
characteristics from adult’s expressions. Child-specific focus recognises
that children's facial expressions are more unpredictable, intense, and
dynamic than adult’s, inducing the creation of a separate dataset. Multi-
stage annotation combines automated and manual processing, as well as a
rigorous three-stage labelling process, to ensure accurate microexpression
identification. Baseline establishment produces baseline findings for
automatic detection and recognition by combining handmade features and
deep learning algorithms [ 42 ].

**_5.7.4 Spontaneous Action Micro-movement_**

The Spontaneous Action Micro-Movement (SAMM) dataset is a primary
resource for researchers studying the detection of microexpressions and
subtle emotions. SAMM is based on the principles of spontaneity and
ecological validity. It captures true micro-facial movements that occur when
people suppress or hide their emotions, especially in high-stakes or
emotionally charged circumstances. It is particularly developed to conquer
the difficulties of collecting, annotating, and interpreting spontaneous,
involuntary facial movements that reveal hidden emotions [ 14 , 51 ]. It
includes 159 microexpression samples from 32 participants that have been
labelled with AUs and seven emotions. A high resolution of 2048 × 1088
and a high frame rate of 200 frames per second enable the accurate analysis
of transient expressions [ 5 ]. In AU coding, all facial motions are labelled
with the FACS, which connects precise muscle movements to emotional
states, allowing for objective and fine-grained analysis. The dataset
overcomes past constraints by incorporating varied people and high-quality


video recordings, ensuring resilience and generalizability in emotion
recognition studies [ 51 ].

##### 6 Role of Studying Microexpressions and Subtle

##### Emotions in Emotion and Facial Recognition

Microexpressions improve facial emotion recognition accuracy because
they are difficult to fake and provide authentic emotional information [ 53 ].
Training programs, such as the Ekman SETT and METT tools, have been
shown to significantly enhance an individual’s ability to recognise subtle
emotions and microexpressions, ultimately leading to improved emotion
recognition in real-world situations. Expression duration affects recognition
accuracy, and microexpressions need quick processing. With practice,
humans can become more proficient at recognising even extremely short
exposures, as low as 40 ms [ 49 ]. Emotionally aware AI systems use
microexpression detection to respond empathetically and adaptively to
users [ 53 ]. Advanced deep learning architectures, like Hybrid Deep Neural
Networks (HDNNs), have outperformed traditional CNN models by
capturing complex hierarchical facial features and integrating multimodal
cues like pupil size variation. HDNNs have shown superior performance in
detecting microexpressions for applications like deception detection, with
accuracy rates up to 91% [ 40 ]. Comparably, convolutional neural networks
combined with optimisation methods have improved classification accuracy
in certain face microexpression detection tasks to around 99% [ 3 ].
These advancements in microexpression detection enhance facial
emotion identification in general by offering more accurate and detailed
emotional information, which is essential for applications ranging from
healthcare and human–computer interaction to security and law
enforcement. Incorporating microexpression analysis can greatly reduce the
gap toward human-level emotion identification accuracy (~90%), whereas
typical face emotion recognition software only achieves about 75–80%
accuracy [ 6 ].

##### 7 Application of AI-Based Microexpression and

##### Subtle Emotion Detection


AI-based detection of microexpressions and mild emotions has found wide
and impactful uses in a variety of disciplines, capitalising on the capacity of
these transitory and low-intensity facial clues to disclose true emotions that
people often conceal. A detailed outline of the main application areas and
examples is listed below.

**7.1 Security, Law Enforcement, and National Security**

Microexpressions are useful in lie detection and threat assessment because
they are trustworthy markers of hidden emotions like dishonesty, fear, or
violence. Artificial intelligence (AI) systems with microexpression
recognition capabilities can examine live video feeds or surveillance
footage to identify suspicious or hostile activity in border control, airports,
and interrogation situations. Security inspections are more definite because
of AI-based microexpression detection, which is barely invasive and more
problematic for people to manipulate than traditional polygraphs. For
instance, deep learning models such as Tiefes FCNN support real-time
deception spotting in law enforcement by achieving over 99% precision in
microexpression recognition [ 18 ].

**7.2 Marketing, Customer Experience, and Consumer Insights**

Artifical Intelligence manages emotion detection utilises subtle emotional
prompts and facial microexpressions to record genuine customer reactions
to services, products, and advertisements. Because of this, sellers can better
forecast customer satisfaction, optimize user experience, and customise the
advertisements, then they could with traditional surveys. Examining
emotional reactions during brand tracking or product testing helps identify
which elements or messages evoke either positive or negative emotions,
thereby directing ongoing innovation and improvement. Real-time feedback
on brand perception can be obtained by combining facial expression
detection with sentiment analysis from social media [ 60 ]. For example,
Artificial Intelligence can detect minute facial expressions in response to
commercials or advertisements, enabling dynamic marketing strategy
adjustments based on emotional engagement.

**7.3 Clinical Diagnostics and Mental Health**

AI-powered microexpression analysis benefits psychotherapists to better
diagnose and treat conditions like depression, PTSD, or autism spectrum


disorders, which helps them identify transient emotional responses that may
be missed during sessions, such as brief expressions of disgust or anxiety.
AI-powered tools and techniques offer objective, scalable assessments that
supplement human judgment, enabling continuous monitoring and early
intervention. For instance, artificially intelligent face detection systems
have shown sensitivity comparable to trained therapists in detecting
millisecond-range microexpressions during psychotherapy.

**7.4 Social Robots and Human-Computer Interaction**

Human-computer interaction (HCI) and social robots have the ability to
acknowledge minor emotions and microexpressions variations, by allowing
systems to understand real human emotional states, react sympathetically,
and modify interactions in real time. Robots perceive delicate emotions by
combining facial analysis with contextual information and physiological
signs, such as heart rate and EEG. For instance, systems based on Wavelet
Transforms analyse biosignal and face data to identify low-intensity
emotions, such as mild anxiety, allowing for more nuanced reactions [ 46 ].
Microexpression detection is used by HCI systems to dynamically alter user
interfaces. For example, virtual assistants change to encouraging tones
when they sense user annoyance, and learning platforms modify the level of
difficulty of the information if confusion is identified. Over time, systems
can improve their ability to recognise emotions through machine learning
algorithms. By examining user-specific feedback, robots such as those
tested with Wavelet Transform enhance the recognition of subtle emotions
and allow for more individualised interactions [ 22 ]. Examples include
platforms integrating facial microexpression analysis with voice and text
modalities that provide adaptive, context-aware interactions [ 50 ].

**7.5 Law, Investigation, and the Judicial System**

When determining whether witnesses, suspects, or defendants are telling the
truth, detectives and legal professionals can use microexpressions to
uncover hidden emotions. Artificial intelligence (AI) has been created to
assess these transient expressions, often surpassing humans in their
recognition, particularly in high-stakes, unexpected scenarios like criminal
investigations [ 37 ]. Law enforcement organisations are testing AI-based
tools that monitor facial expressions during interviews or border checks,
such as the Automatic Deception Detection System (ADDS) and


iBorderCtrl in the EU. In order to detect duplicity or hidden intent without
depending entirely on verbal clues or polygraphs, these systems employ
artificial agents to non-disruptive profile psychological states. By
identifying signs of tension, anxiety, or rage, companies like Oxygen
Forensics supply police with emotion-detecting software that they say
offers important insights in substantial investigations. Artificial intelligence
(AI)-based emotion identification systems can help jurors and judges
analyse the reliability of witnesses by pointing out emotional outbreak that
might be indication of stress, anxiety, or dishonesty. There have been
requests for regulation due to the quick uptake of these technologies. For
example, the European Parliament has suggested designating emotion
detection as a high-risk AI use, and some are calling for complete
prohibitions in judicial decision-making because of worries about bias,
accuracy, and fundamental rights [ 31 ].

**7.6 Forensic Science**

Forensic science uses microexpressions and subtle emotions to reveal
concealed emotional states during suspect interrogations, criminal
investigations, and behavioural analysis. Forensic Science helps to spot
deceit, predict behaviour, and analyse emotional reactions to specific
stimuli.
(i) In suspect interrogations, concealed emotions can be revealed
through microexpressions detection when discussing if the crime
were done or not. Systems like the Real-Time Lie Detector are being
developed to integrate these cues with physiological data for
credibility assessment. For instance, a suspect might display
microfacial expressions of disgust while denying knowledge of a
victim, potentially indicating deception.

(ii) Forensic sketch analysis involves analysing sketches drawn from
eyewitness accounts for emotional cues, such as a tense jaw
indicating anger or widened eyes suggesting fear. Based on these
features, Artifical Intelligence based models are being developed to
predict suspect behaviour [ 17 ].

(iii) In courtroom monitoring, AI frameworks are employed to monitor

# accused for concealed emotions, such as micro-smirks during victim^


testimony, in order to assess repentance or deceit.
However, several challenges and limitations exist. The momentary and
subjective nature of microexpressions leads to reliability concerns, with
interpretations varying across individuals and contexts. Ethical concerns
emerge from the potential for algorithmic bias or privacy violations if
contradicts use is made in legal settings. Furthermore, technical barriers,
such as low-intensity expressions and head movements in real-world
settings, reduce observation accuracy.

##### 8 Ethical Considerations

**8.1 Consent and Privacy**

Detecting microexpressions and delicate emotions requires convention and
analysing extraordinary sensitive affective data, which might provide
personal information about a person's goals, emotional state, or
psychological health. Before any emotional information is gathered or
examined, people must be fully enlightened and provide their express
consent. This is particularly important in settings where power dynamics
may compel people to join, such as law enforcement, security, or the
workplace [ 4 ].

**8.2 Validity of Science and Accuracy**

The validity of microexpressions and subtle emotions as markers of deceit
or real affect is a topic of continuous scholarly discussion. According to
research, there is no clear connection between lying and microexpressions,
and how they are interpreted can vary depending on the situation. When
these cues are used to make important decisions (as in court or security
screenings), there is a chance of false positives and negatives, which could
result in unjust outcomes or prejudice [ 15 ].

**8.3 Misuse Possibility**

Concerns regarding manipulation, depict, and monitoring are raised by the
usage of emotion recognition technology, especially those driven by
artificial intelligence. Organisations or authorities run the threat of using
these tools to secretly monitor or control people, violating their right to
privacy and autonomy [ 4 ].


**8.4 Fairness and Bias**

Extremely when applied to varied cultures, genders, or neurodiverse
groups, emotion recognition agenda may reflect or magnify original biases
in their training data, producing unjust or discriminatory results. Cultural
and isolated differences can influence how emotions are displayed and
recognised, making it coherent that people will misjudge microexpressions
[ 4 , 15 ].

**8.5 Accountability and Transparency**

Clear accountability methods should be in place to manoeuvre any abuse,
mistakes, or damages brought on by these technologies. Transparency
regarding the collection, analysis, and usage of data is essential for both
developers and users of emotion detection systems [ 4 , 28 ].

**8.6 Contextual Awareness**

Microexpressions, in specific, are very circumstantial-dependent emotional
expressions. Context, goal, and possible outcomes must all be carefully
considered before making inferences from emotional data in order to use it
ethically. It is possible to make serious mistakes if you interpret them
without taking the context into account [ 28 ].

##### 9 Conclusion

Artificial Intelligence (AI)-based microexpression detection has extensive
potential in a variety of fields by discovering primary emotional prompts
that might improve apprehension and communication. Several prospective
approaches to automatic microexpression detection and recognition have
attracted interest in the affective and visual computing sectors. Since a
number of new extemporaneous facial microexpression databases were
made available to facilitate automatic analysis of microexpressions, there
has been considerable development in machine analysis of facial
microexpressions in recent years. Facial expressions, specifically
microexpressions and subtle emotions, provide a rich window into a
person's genuine emotional state, often disclosing feelings that people try to
conceal. While macroexpressions form the basis of everyday emotional
communication, the ephemeral nature of microexpressions and the low
intensity of delicate emotions pose distinct obstacles and opportunities for


research. Though humans frequently struggle to correctly identify
microexpressions, approach in computer-based technologies are paving the
way for automated microexpression recognition, which promises to unlock
the wealth of psychological information hidden within these transient facial
movements while also improving traditional facial recognition techniques.
The ability to notice and interpret these small indications has important
applications in a variety of domains, including deception detection, criminal
investigation, and mental health. This chapter emphasises the importance of
focusing on microexpressions and subtle emotions, as well as the possibility
for artificial intelligence to tackle their power to gain a better understanding
of human behaviour.

**Key Takeways**

These findings point to microexpressions and subtle emotions as carriers of
the deepest meaning related to actual human feelings, which are oftentimes
imperceptible to untrained human observation. The premises upon which
traditional techniques relied are being surpassed by AI methods that enable
the feasible detection of emotions in a more accurate way, and, perhaps, in
real time. Despite the pending issues—namely, the technical ones and the
ethical ones, including dataset quality, cultural bias, and privacy—AI-based
emotion detection represents a significant and innovative way to
understanding and applying emotional intelligence to various fields.

**Reflection/Discussion Question**
As more applications for AI are being developed, especially for the
detection of micro-expressions and subtle emotions, there is no end for the
development. Yet now the question is, to what extent can society balance
the advantages of the use of such technology-for instance, lie detection,
security, or even mental health-with all the ethical issues it raises in terms
of privacy, misuse in surveillance, cultural bias, and possible
misinterpretation of human emotions?

##### References

1. Adegun, I.P., Vadapalli, H.B.: Facial microexpression recognition: a machine learning approach.
    Sci. Afr. **8** (2020)


2.
Aiken, S., Gonzalez, K.: Microexpression: definition, types and examples. Study.com. Accessed
14 June 2025 (2023)
3.
Arun, A.N., Maheswaravenkatesh, P., Jayasankar, T.: Facial microemotion detection and
classification using swarm intelligence-based modified convolutional network. Expert Syst.
Appl. **233** (2023)
4.
Barker, D., Reddy Tippireddy, M.K., Farhan, A., Ahmed, B.: Ethical considerations in emotion
recognition research. Psychol. Int. **7** (2), 43 (2025)
[Crossref]
5.
Ben, X., Ren, Y., Zhang, J., Wang, S.J., Kpalma, K., Meng, W., Liu, Y.J.: Video-based facial
microexpression analysis: a survey of datasets, features and algorithms. IEEE Trans. Pattern
Anal. Mach. Intell. **44** (9) (2021)
6.
Beltramin, A.: How accurate is facial emotion recognition (FER)?. Morphcast: advanced facial
emotion AI. Accessed 16 June 2025 (2023)
7.
Borza, D., Itu, R., Danescu, R.: Micro expression detection and recognition from high speed
cameras using convolutional neural networks. In: VISIGRAPP (5: VISAPP): international
Conference on Computer Vision Theory and Applications 201–208 (2018)
8.
Bradley, M.M., Lang, P.J.: Measuring emotion: the self-assessment manikin and the semantic
differential. J. Behav. Ther. Exp. Psychiatry **25** (1), 49–59 (1994)
[Crossref]
9.
Charan, A.: Marketing Analytics Practitioner’s Guide: product, Advertising, Packaging,
Biometrics, Price and Promotion 2. World Scientific (2023)
[Crossref]
10.
Chaudhari, A., Bhatt, C., Nguyen, T.T., Patel, N., Chavda, K., Sarda, K.: Emotion recognition
system via facial expressions and speech using machine learning and deep learning techniques.
SN Comput. Sci. **4** (4) (2023)
11.
Chengeta, K., Viriri, S.: Facial expression detection for video sequences using local feature
extraction algorithms. Signal Image Process. Int. J. **10** , 27–41 (2019)
12.
Coppini, S., Lucifora, C., Vicario, C.M., Gangemi, A.: Experiments on real-life emotions
challenge Ekman’s model. Sci. Rep. **13** (1) (2023)
13.
Cunic, A.: How to read facial expressions: you can improve your ability to read others’ emotions.
verywellmind. Accessed 17 June 2025 (2024)
14.
Davison, A.K., Lansley, C., Costen, N., Tan, K., Yap, M.H.: Samm: a spontaneous micro-facial
movement dataset. IEEE Trans. Affect. Comput. **9** (1), 116–129 (2016)
[Crossref]
15.
Döllinger, L., Laukka, P., Högman, L.B., Bänziger, T., Makower, I., Fischer, H., Hau, S.:
Training emotion recognition accuracy: results for multimodal expressions and facial micro
expressions. Front. Psychol. **12** (2021)


16.
Dong, Z., Wang, G., Lu, S., Li, J., Yan, W., Wang, S.J.: Spontaneous facial expressions and
micro-expressions coding: from brain to face. Front. Psychol. 12 (2022)
17.
Dubey, N., Uprety, D.C.: A review on criminal facial emotion recognition and detection. Int. J.
Adv. Eng. Manag **5** (8) (2023)
18.
Durga, B.K., Rajesh, V., Jagannadham, S., Kumar, P.S., Rashed, A.N.Z., Saikumar, K.: Deep
learning-based micro facial expression recognition using an adaptive Tiefes FCNN model.
Traitement du Signal **40** (3) (2023)
19.
Dwivedi, R., Kumar, D.: Challenges of facial microexpression detection and recognition: a
survey. In: The International Conference on Neural Information Processing, pp. 483–492.
Springer Nature, Singapore (2022)
20.
Ekman, P.: Lie catching and microexpressions. The Philos. Decept. **1** (2), 5 (2009)
21.
Gao, J., Chen, H., Zhang, X., Guo, J., Liang, W.: A new feature extraction and recognition
method for microexpression based on local non-negative matrix factorization. Front. Neurorobot.
**14** (2020)
22.
Gayathri, R., Uma, S.: Social robot face emotion recognition using wavelet transformation.
Educ. Adm.: Theory Pract. **30** (6) (2024)
23.
Gurucharan, M.K.: Basic CNN architecture: a detailed explanation of the 5 layers in
convolutional neural networks. upGrad Education Private Limited. (2025)
24.
Hamrouni, A., Bendella, F.: Emotion recognition using various measures and computational
methods. Iraqi J. Comput. Sci. Math. **5** (3), 34 (2024)
[Crossref]
25.
Hong X, Xu Y, Zhao G (2017) Lbp-top: a tensor unfolding revisit. In: Computer Vision-ACCV,
2016 Workshops: ACCV 2016 International Workshops, vol. 13, pp. 513–527, Taipei, Taiwan
26.
Hu, G., Liu, L., Yuan, Y., Yu, Z., Hua, Y., Zhang, Z., Yang, Y.: Deep multi-task learning to
recognise subtle facial expressions of mental states. In: Proceedings of the European Conference
on Computer Vision (ECCV), pp. 103–119 (2018)
27.
Irawan, B., Munir, R., Utama, N.P., Purwarianti, A.: Enhancing microexpression recognition: a
novel approach with hybrid attention-3DNET. Jordanian J. Comput. Inf. Technol. **11** (1) (2025)
28.
Iwasaki, M., Noguchi, Y.: Hiding true emotions: microexpressions in eyes retrospectively
concealed by mouth movements. Sci. Rep **6** (1) (2016)
29.
Jia, Y.W.Y., Turk, C.H.M.: Fisher non-negative matrix factorization for learning local features.
In: Proceedings of the Asian Conference on Computer Vision, pp. 27–30 (2004)
30.
Jin, H., He, N., Li, Z., Yang, P.: Microexpression recognition based on multi-scale 3D residual
convolutional neural network. Math. Biosci. Eng. **21** (4), 5007–5031 (2024)
[Crossref]


31.
Kelion, L.: Emotion-detecting tech should be restricted by law-AI now (2019). Accessed 18 June
2025
32.
Li, J., Lu, S., Wang, Y., Dong, Z., Wang, S.J., Fu, X.: Could microexpressions be quantified?
Electromyography gives affirmative evidence. IEEE Trans. Affect. Comput. (2025)
33.
Li, R., Wang, L., Yang, T., Xu, L., Ma, B., Li, Y., Wei, H.: microexpression recognition by
motion feature extraction based on pre-training (2024). arXiv: 2407. 07345
34.
Li, J., Wang, T., Wang, S.J.: Facial microexpression recognition based on deep local-holistic
network. Appl. Sci. **12** (9) (2022)
35.
Liu, Y., Li, Y., Yi, X., Hu, Z., Zhang, H., Liu, Y.: Microexpression recognition model based on
TV-L1 optical flow method and improved ShuffleNet. Sci. Rep. **12** (1) (2022)
36.
Lu, G., Yang, C., Yang, W., Yan, J., Li, H.: Microexpression recognition based on LBP-TOP
features. Nanjing Youdian Daxue Xuebao (Ziran Kexue Ban)/J. Nanjing Univ. Posts
Telecommun. (Nat. Sci.) **37** (6), 1–7 (2017)
37.
Malik, P., Singh, J.: Microexpression recognition-Contemporary applications and algorithms.
AIP Conf. Proc. **2916** (1) (2023)
38.
Montanha, R.H., Raupp, G.N., Schmitt, A.C.P., de Andrade Araujo, V.F., Musse, S.R.: Micro and
macro facial expressions by driven animations in realistic virtual humans. Entertain. Comput. **52**
(2025)
39.
Muhammad Buhari, A., Ooi, C.P., Baskaran, V.M., Tan, W.H.: Motion and geometric feature
analysis for real-time automatic microexpression recognition systems. F1000Research **10** (2021)
40.
Nikbin, S., Qu, Y.: A study on the accuracy of micro expression based deception detection with
hybrid deep neural network models. Eur. J. Electr. Eng. Comput. Sci. **8** (3), 14–20 (2024)
[Crossref]
41.
Oh, Y.H., See, J., Le Ngo, A.C., Phan, R.C.W., Baskaran, V.M.: A survey of automatic facial
microexpression analysis: databases, methods, and challenges. Front. Psychol. **9** (2018)
42.
Pasha, M., PengWong, K.: CMED: a child microexpression dataset (2025). arXiv: 2503. 21690
43.
Peng, M., Wang, C., Chen, T., Liu, G., Fu, X.: Dual temporal scale convolutional neural network
for microexpression recognition. Front. Psychol. **8** (2017)
44.
Qian, C., Marques, J.A.L., de Alexandria, A.R.: Real-time emotion recognition based on facial
expressions using artificial intelligence techniques: a review and future directions. Multidiscip.
Rev. **8** (10)
45.
Raghava, M., Chandan, M.M., Sahithi, M., Kumar, H.M., Aravind, M.: Micro expression using
machine learning approach. Int. J. Adv. Res. Sci. Commun. Technol. **2** (1) (2022)
46.
Reed, V.: Can robots learn social cues? Embodied AI and social intelligence. AI Competence
(2024)


47.
Saeed, U.: Facial microexpressions as a soft biometric for person recognition. Pattern Recogn.
Lett. **143** , 95–103 (2021)
[Crossref]
48.
Sharma, P., Coleman, S., Yogarajah, P., Taggart, L.: Micro expression classification accuracy
assessment (2019)
49.
Shen, X., Wu, Q.: Recognizing microexpression: an interdisciplinary perspective. Front. Psychol.
(2019)
50.
Srinivas, R.: AI and emotion recognition: understanding human feelings through technology
(2025)
51.
Stofa, M.M., Zulkifley, M.A., Zainuri, M.A.A.M.: Micro-expression-based emotion recognition
using waterfall atrous spatial pyramid pooling networks. Sensors **22** (12) (2022)
52.
Talluri, K.K., Fiedler, M.A., Al-Hamadi, A.: Deep 3d convolutional neural network for facial
microexpression analysis from video images. Appl. Sci. **12** (21) (2022)
53.
Tomasi, C.: Understanding microexpressions and their role in facial emotion recognition:
Morphcast. Advanced Facial Emotion AI (2024)
54.
Umair, M., Rashid, N., Khan, U.S., Hamza, A., Iqbal, J.: Emotion fusion-sense (Emo Fu-sense)–
a novel multimodal emotion classification technique. Biomed. Signal Process. Control **94** (2024)
55.
Unzueta, D.: Fully connected layer versus convolutional layer: explained (2022). https:// builtin.
com/ machine-learning/ fully-connected-layer
56.
Verma, M., Vipparthi, S.: A motion flow guided micronet framework for micro expression
recognition (2023). SSRN:501 1543
57.
Venkatesan, R., Li, B.: Convolutional Neural Networks in Visual Computing: a Concise Guide.
CRC Press (2017)
[Crossref]
58.
Venneti, S.: Revealing true emotions through microexpressions: a machine learning approach
(2018)
59.
Wang, S., Yuan, Y., Zheng, X., Lu, X.: Local and correlation attention learning for subtle facial
expression recognition. Neurocomputing **453** , 742–753 (2021)
[Crossref]
60.
Wezowski, K., Penton-Voak, I.: Associations between workplace emotional intelligence and
micro expression recognition. Acta Psychol. **240** (2023)
61.
Wei, W., Jia, Q.: Weighted feature Gaussian kernel SVM for emotion recognition. Comput.
Intell. Neurosci. (1) (2016)
62.
William, B.: Software that uses AI to detect voice stress and emotion (2025)


63.
Wingenbach, T.S.: Facial EMG–investigating the interplay of facial muscles and emotions. In:
Social and affective neuroscience of everyday human interaction, vol. 283 (2023)
64.
Witkower, Z., Hill, A.K., Koster, J., Tracy, J.L.: Beyond face value: evidence for the universality
of bodily expressions of emotion. Affect. Sci. **2** (3), 221–229 (2021)
[Crossref]
65.
Yadav, R., Kacker, P.: Efficient methods for facial microexpressions detection and classification.
Indian J. Comput. Sci. Eng. **12** (5) (2021)
66.
Yang, H., Sun, S., Chen, J.: Deep learning-based microexpression recognition algorithm
research. Int. J. Comput. Sci. Inf. Technol. **2** (1), 59–70 (2024)
67.
Yaremchenko, O., Pukach, P.: Investigation and comparative analysis of algorithms about
recognition of micro mimics for analysis of person using emotional AI. Lviv Polytechnic
National University, Bandera str. **12** (4), 563–577 (2017)
68.
Xie, X., Fang, Z.: Multi-modal emotional understanding in AI virtual characters: integrating
microexpression-driven feedback within context-aware facial microexpression processing
systems. J. Wirel. Mob. Netw. Ubiquitous Comput. Dependable Appl. **15** (3), 474–500 (2024)
69.
Zhang, L., Arandjelović, O.: Review of automatic microexpression recognition in the past
decade. Mach. Learn. Knowl. Extract. **3** (2), 414–434 (2021)
[Crossref]
70.
Zhao, G., Li, X., Li, Y., Pietikäinen, M.: Facial microexpressions: an overview. Proc. IEEE
**111** (10), 1215–1235 (2023)
[Crossref]
71.
Zheng, H., Wang, R., Ji, W., Zong, M., Wong, W.K., Lai, Z., Lv, H.: Discriminative deep multi-
task learning for facial expression recognition. Inf. Sci. **533** , 60–71 (2020)
[Crossref]
72.
Zheng, Y., Blasch, E.: Facial microexpression recognition enhanced by score fusion and a hybrid
model from convolutional LSTM and vision transformer. Sensors **23** (12) (2023)
73.
Zhi, R., Hu, J., Wan, F.: Microexpression recognition with supervised contrastive learning.
Pattern Recogn. Lett. **163** , 25–31 (2022)
[Crossref]
_OceanofPDF.com_


(1)

(2)

(3)

© The Author(s), under exclusive license to Springer Nature Switzerland AG 2026
K. Slimani et al. (eds.), _Emotion and Facial Recognition in Artificial Intelligence: Sustainable
Multidisciplinary Perspectives and Applications_ , Information Systems Engineering and Management
78
https://doi.org/10.1007/978-3-032-14778-3_10

## Navigating the Future of Emotion AI:

## Technical Barriers, Ethical Concerns, and

## Sustainable Advancements

Shaik Khaja Mohiddin^1 , Shaik Sharmila^2 and Khadija Slimani^3
Department of CSE, Siddhartha Academy of Higher Education,
Deemed to be University, Vijayawada, Andhra Pradesh, India
Department of IT, Vignan’s Nirula Institute of Technology and Science
for Women, Peda Palakalur, Guntur, Andhra Pradesh, India
esieaLab LDR, Higher School of Computer Science Electronics and
Automation (ESIEA), Paris, France

```
Shaik Khaja Mohiddin (Corr esponding author)
Email: mail2mohiddin@gmail.com
```
```
Shaik Sharmila
Email: mail2shaiksharmila@gmail.com
```
```
Khadija Slimani
Email: Khadija.slimani@esiea.fr
```
**Abstract**
Nowadays emotion AI is emerging as a forum of outrageously disruptive
technology in the industry of artificial intelligence with consequences to
distant as mental health diagnosis, teaching interfaces, testing vehicles,
customer-focused and even human–computer interfaces. Going further, it is
a technology encompassed in a web of technical obstacles and ethical and
social measures. In this chapter, the problems associated with the modelling


techniques include difficulty in modelling generalization, biases in datasets,
cross-cultural variation by individuals on the expression of emotions in real
time, performance restrictions and ability to capture subtle contextual
differences of emotions. Besides, numerous ethical problems, including
invasion of privacy, emotional surveillance, right to consents and
manipulability of feelings and secrets during decision making processes
etc., are critically analyzed proving that attempts to create interpretable and
transparent AI systems become inevitable. To address these multi-
dimensional issues, the chapter explores the future-forward solutions such
as federated learning systems, edge computing systems coupled with
privacy-sensitive AI applications, emotion-aware multimodal fusion models
to meet individual user/ inter-user needs and culturally adaptive emotion-
based models. It points to the necessity to have interdisciplinary synergy
which is a combination of views based on the fields of psychology,
computer science, ethics and policy as a guide to responsible use of
Emotion AI all over the world. This chapter attempts to chart an
architectural path on how to create Emotion AI systems that would be fair,
transparent, circumstantial, as well as human-centric throughout the global
paradigm by outlining current traps, and future various innovations.

**Keywords** Bias mitigation – Ethical AI – Explainability – Future trends –
Multimodal – Integration – Privacy-preserving learning

##### 1 Introduction

Emotion Artificial Intelligence (EAI), as it is widely referred to as the
Artificial Intelligence (AI), represents the paradigm shift in the world of
technology and the psychology of people. It enables machines to recognize,
decode and respond to human emotions through multimodal expression
such as face expression, voice intonation, and in the case of physiological
expression. Its innovation is revolutionizing various areas including health
care and education, consumer relationships and self-driving systems since
they give compassionate relations with the reflection of human compassion
and intuition. By the manner in which adaptive decision-making systems
have the power to influence, emotion AI to this day is embarking on a new
line of evolution besides sentiment analysis into elaborate reasoning of


affective abilities with the deployment of AI that is based on deep learning,
computer vision, and natural language processing.
However, in this case, with the Emotion AI development, the situation is
different as the technical, ethical, and social concerns are mixed, which
necessitates a responsible innovation approach. The issues of technical
limitations, such as model generalization, bias, and cross-cultural
inconsistencies still do not allow accuracy and inclusivity. At the same time,
the lack of transparency and accountability in AI-supported decision
systems is questioned due to the lack of privacy and consent, as well as
manipulation of emotions, which introduce ethical concerns. Therefore, to
ensure that Emotion AI evolves in a sustainable manner, it is necessary to
install such interdisciplinary insights in computer science, psychology,
ethics, and policy fields in this chapter. The discussion forms the basis of
the definition of conscious systems through emotion that will be just,
understanding, culturally competent as well as in agreement with human
ethics in a more AI integrated world.

**1.1 Evolution of Emotion Recognition Systems**

Emotion Artificial Intelligence (AI) is slowly changing the ways of
incorporating machines and human emotions and paves the way to the
evolution of different industries. Emotion AI is often related to affective
computing, which aims at developing human emotion recognition and
interpretation and responding to computer systems in the manner human
beings do. The decision to use emotion AI is founded on human computer
interaction, information retrieval skills, multimodal signal processing to
streamline the feelings of individuals toward the vast array of social
information [ 1 ]. Also, emerging cognitive AI is one of the fields that are
drawing significant attention since it aims at enabling systems to think,
reason and make human decisions. This change represents the primary
success in creating human-like, intelligent systems which are required to
create an era where AI is becoming more integrated into the daily human
processes and operations [ 2 ].
The differentiated application of the artificial intelligence is the step
towards the future of Emotion AI, and possibly one of the solutions of the
creation of the emotional performance within the learning environment. The
learning activities and adaptive learning proceed to scan the emotional
conditions of the students and record the information using facial


recognition and machine learning [ 3 ]. As the application of AI to the work
of businesses is increasing, the necessity to implement emotional
intelligence arises. As a complementary application to traditional artificial
intelligence, emotion AI may heavily shake the status quo in the workplace,
making the procedures of the digital interaction with humans significantly
more closely related to human emotions [ 4 ]. The Future of Emotion AI path
illuminates the exciting and problematic prospect of emotional intelligence
and machine learning integration thereby showing the staggering potential
in personal growth and better machine-human interactions.

**1.2 Current Scope and Applications in Society**

The field of emotion AI has developed rapidly in regard to the expansion of
experimental studies to the practical applications in most fields hence,
emotional intelligence is an attainable part of the modern artificial systems.
Emotion AI application in the healthcare field is to diagnose mental
diseases, such as depression and anxiety using the voice tones, facial
expression, and physiological signs. The adaptive learning may be noted in
the educational environment, where emotional recognition will be presented
so that pedagogic methods could be adjusted according to the emotional
activity of the students and the degree of stress. Another aspect that has
been enhanced with the technology is the customer service which AI-
assisted chatbots and virtual assistants can detect the emotions of the user
and respond with empathy which results in increased level of customer
satisfaction and loyalty to the brand. Tissue-sensitive technology to enhance
the quality of content recommendations in the entertainment sphere
analyses the reaction of users, and in the automotive market, fatigue, and
stress detecting emotion-recognition algorithms enhance the safety of the
drivers.
Over these fields, Emotion AI is emerging as a necessity unit to increase
efficiency and human–computer collaboration at the workplace. Emotional
sensitive analytics are enabling companies to have insights into the well-
being of staff members, automate the hiring process, and employ real-time
emotional data to integrate workgroups. The use of affective computing by
social media networks and the marketing agencies is in such a way that they
apply the technology in learning how consumers act and subsequently draw
up programs that resonate with the emotional motive. Despite its growing
strength, such applications also demand more ethical regulation to put in


check any potential misuse, the effect of emotions or the tiny, manipulated
judgments. The current Emotion AI size can thus be defined as an empathy-
enabling technology, and even a way of regulating the reality that
everything that the technology is subjected to in the society is clear,
inclusive and humane.

##### 2 Foundations of Emotion Modelling

**2.1 Understanding Emotional Cues: Facial, Vocal, and**

**Physiological Signals**

It is an inevitable component of interpersonal relationships and
communication, which is the process of perception of emotional signals
depending on the facial, vocal, physiological indicators. The infancy had
shown the facial forms and vocalization as the strong behaviour regulators
that showcase the importance of emotional development. Using the case of
infants, infants can be capable of responding to fearful vocal expressions
without the involvement of their body, studies have indicated that infants as
young as 12 months can be capable of reacting to vocal signals that produce
fear on their own, as a result they make behavioural changes [ 5 ]. This
signifies that, at such a young age, auditory modality is applicable with this
experience developing the foundation in the complex emotional modulation
and thereby socializing in the more advanced stage in maturity (as children
grow) [ 6 ]. These abilities at such a tender age point to the specialty of the
vocal cues in the socialization process and are telling that they bear strong
roots in the evolutionary background of humankind.
On the same line, the processing of facial cues and vocal cues has been
demonstrated by studies done to determine the neural processes of
emotional perception. Amygdala is a brain component, which is relevant in
the processing of emotions and the neurons in this part react to both the
visual and auditory emotional stimulus and this entails that the two are
coupled up to form an emotional perception [ 7 ]. Other researchers that
studied the emotion judgment by considering only vocal expressions have
demonstrated that the emotion judging can utilize areas involving
contribution, as well as working memory, which is the cognitive load of
processing those emotions that are auditory [ 8 ]. They are all in support of
the narrowing of the interplay of various types of sensory modalities in the


interpretation of the emotion and the fact that both the voice and the face
expression have a substantial role in accomplishing the task of identifying
the emotion. They as well suggest the necessity to integrate these cues in
developing systems to detect emotions in artificial intelligence.
The strategic roadmap to sustainable and ethical Emotion AI is the
specific as shown in Fig. 1 , that represents the holistic approach toward the
development of the Emotion AI that is sustainable and ethically sound in its
nature. Its fundamental meaning is embodied in Sustainable and Ethical
Emotion AI, which is represented as a heart within a lightbulb, as a sign of
innovation based on empathy and responsibility. Alongside this core
objective are four key elements namely; Ethical Design Principles, which
advances the idea of fairness and integrity in the creation of the models;
Global Guidelines and Standards, which fosters an internationally
consistent governance; Privacy-Sustaining Techniques, which asserts the
development of a model with a trait of confidentiality; and
Multidisciplinary Collaboration, which entails the incorporation of insights
across the fields of psychology, ethics, computer science and policy. These
elements are interconnected and together they form a complete ecosystem
where ethical governance, technical accuracy and societal values become
one to create AI-based Emotion form systems that are transparent and
human-centric, as well as socially advantageous.

**_Fig. 1_** A strategic roadmap for sustainable and ethical emotional AI


**2.2 Deep Learning and Multimodal Fusion in Emotion**

**Recognition**

The field of emotion recognition through the assistance of deep learning has
been very successful in the past few years, especially when using
multimodal datasets. Multimodal fusion is the combination of different
types of data audio, visual and physiological to enhance the performance
and the quality of emotion recognition systems. One of these approaches is
the hierarchical fusion convolutional neural network (CNN) that generates
multiscale features of multimodal data to generate a combined feature. It
has proven to be more successful in the emotion classification and has high
accuracy on the datasets, such as the DEAP and the MAHNOB-HCI [ 9 ].
The other solution involves the use of a multi-modal deep belief network
(MDBN), which involves the integration of effects of psychophysiological
signals coupled with a bimodal network of video features to classify
emotions more accurately than the traditional techniques [ 10 ].
The aptitude of deep learning in the transformation of the emotional gap
in emotion recognition tasks is likewise portended by the combination of
audio-visual representation using deep convolutional neural networks. The
style can integrate both audio and visual properties to produce combined
depiction that enhances the outcomes of categorization of emotions. Tests
that have been conducted on databases such as RML, have performed
nicely, which compel the researchers to conduct research in the area [ 11 ].
Other contemporary methodology alterations will also encompass tactical
use of the application of spiking neural networks (SNNs) to emotion
recognition, which is effective in the processing of spatio-temporal data.
The competitive ability of research that employs NeuCube and multimodal
data, without being utilised by the normal EEG, implies the scalability and
adaptability of the SNNs in affective computing [ 12 ]. This kind of
development introduces the relevance of deep learning and the multimodal
fusion towards propelling the boundaries of the emotion recognition
technology.

##### 3 Technical Barriers in Emotion AI

**3.1 Lack of Model Generalization and Cross-Cultural**

**Variability**


The issue of model extrapolation and cross-cultural diffusiveness within AI
systems is still a big challenge. AI systems, especially those that have been
trained using a small set of data or those systems trained on a culturally
homogeneous data set, cannot generalize far better in cultural contexts. This
results in models that can unintentionally replicate bias or cannot interpret
and react to cultural nuances during emotion recognition in a way that
would be accurate. Of importance is that AI should break cultural barriers
particularly when using AI in emotion recognition since when cultural
differences are subtle among societies, the meaning of expression may
differ significantly [ 13 ]. To solve these problems, various sets of training
must be provided with a large variety of different cultural situations and
manifestations so that AI models could become more robust and unbiased.
In addition, due to lack of generalizations, AI systems are incredibly
prone to overfitting whereby it works well with the training data but fails
with new or unseen data. This fact is heightened in a high-dimensional
dataset, which is the root of AI tasks that require dealing with emotion
recognition wherein cultural peculiarity might result in crucial variations in
the patterns of the data. The lack of model generalization can be alleviated
by adopting strategies like federated learning in which local training of
models on various datasets and pooling them into a model are done
globally. This strategy will allow more diversity in the cultural expressions
that models are exposed to, and this enhances their capabilities to generalize
in more populations [ 14 , 15 ] (Table 1 ).

**_Table 1_** Strategic roadmap for sustainable and ethical emotion AI

```
Strategic pillar Objective Implementation
strategy
```
```
Tools and
methods
```
```
Performance
indicators
```
```
Long-term
impact
Ethical
governance
```
```
Ensure
transparency
and
accountability
```
```
Establish AI
ethics boards and
audit systems
```
```
IEEE P7000
standards,
model
explainability
```
```
Ethical
compliance
ratio, public
trust index
```
```
Responsible
AI
ecosystem
```
```
Technical
innovation
```
```
Overcome
real-time
emotion
detection
limits
```
```
Optimize data
pipelines, reduce
latency
```
```
Edge AI, cloud
integration
```
```
Processing
speed,
accuracy rate
```
```
Efficient
real-time
emotion AI
```
```
Privacy
preservation
```
```
Protect
emotional and
biometric data
```
```
Employ
federated and
```
```
Homomorphic
encryption,
secure FL
```
```
Data leakage
incidents,
trust metrics
```
```
User data
sovereignty
```

```
Strategic pillar Objective Implementation
strategy
```
```
Tools and
methods
```
```
Performance
indicators
```
```
Long-term
impact
encrypted
learning
Sustainable AI
design
```
```
Reduce
energy and
hardware
dependency
```
```
Integrate energy-
aware ML
models
```
```
Green AI
frameworks,
low-power
chips
```
```
Energy
consumption
index
```
```
Eco-friendly
AI
deployment
```
```
Interdisciplinary
collaboration
```
```
Merge ethics,
psychology,
and
computing
```
```
Create cross-
domain research
labs
```
```
Joint research
platforms,
open datasets
```
```
Research
output
diversity
```
```
Holistic
emotion AI
innovation
```
```
Global
standardization
```
```
Achieve
uniform
global AI
practices
```
```
Develop shared
AI ethics
benchmarks
```
```
ISO/IEC
23,053, global
AI alliance
```
```
Compliance
level across
regions
```
```
Harmonized
global AI
regulation
```
**3.2 Dataset Biases and Limited Diversity**

The issue of bias in data sets and lack of diversity is a critical problem in
the creation of equitable and non-discriminatory AI solutions. Existing
datasets tend to be not as diverse, which leads to an AI model that fails to
cover the minority populations or the cultural manifestations. This
imbalance may create models that do not perform equally across various
groups of people, hence maintaining more biases and inequalities in the
application of AI [ 16 ]. Synthetic data generation has been suggested as a
remedy to counter biases in data due to its ability to produce artificial data
that balances out underrepresented demographics thus boosting model
fairness without reducing performance. Moreover, remedies to data bias
necessitate a thorough insight into the origins and consequences of the bias.
It entails the enumeration of biases at data collection, data annotation, and
training of the model. Fairness-conscious approaches to machine learning,
e.g. re-weighting or re-sampling to deal with imbalanced data, are essential
steps towards fairness. AI systems. However, these measures should be
assessed on a regular basis and best practices published to provide not just
models that perform well but also fairly to all the user demographics [ 17 ].

**3.3 Real-Time Processing Constraints and Latency Issues**

The most important in the deployment of AI systems are the latency and
inability of real time processing, when dealing with time-sensitive


applications, where an instance of timely decision making is necessary.
Devices or autonomous vehicles with AI systems such as the Internet of
Things (IoT) have a condition that they need to analyse data quickly to
implement their behaviour. These processes may have extremely high
computation demands that may be incapable of being computed on the
traditional hardware that has created delays that could become a
constraining factor to the system performance. The solutions to these
problems include but are not limited to new hardware accelerators, edge
computing and algorithmic solutions to simplify the processing speeds, but
at the same time, the accuracy must not be compromised [ 18 ]. Edge
computing also comes with a solution in which the data is processed closer
to the point where it originates and thus reduces the latency and the
bandwidth usage. In addition to this, AI algorithms can be more effectively
compressed using model compression algorithms like, but not limited to,
pruning and quantization, which can significantly reduce the amount of
computation, as well as reduce the processing time. The key modification in
building such technologies is imperative to transform AI utilization in real-
time environments and guarantee feasible implementation in overall and
complex working conditions [ 19 ] (Table 2 ).

**_Table 2_** Technical, ethical, and sustainable dimensions of emotion AI

```
Aspect Focus area Challenges
identified
```
```
Proposed
solutions
```
```
Key
technologies
```
```
Expected
outcomes
Technical Model
generalization
```
```
Dataset bias and
cross-cultural
variance
```
```
Federated
learning and
diverse data
inclusion
```
```
Deep neural
networks
(DNN),
CNNs
```
```
Improved
accuracy
across global
users
Ethical Data privacy Unauthorized
emotional
surveillance
```
```
Differential
privacy,
anonymization
```
```
Edge AI,
blockchain
```
```
Enhanced
trust and
transparency
Sustainability Energy
efficiency
```
```
High
computational
cost
```
```
Edge
computing,
lightweight AI
models
```
```
Quantum ML,
energy-aware
frameworks
```
```
Reduced
carbon
footprint
```
```
Social Human-AI
interaction
```
```
Lack of emotional
alignment
```
```
Context-aware
sentiment
mapping
```
```
NLP,
multimodal
fusion models
```
```
Enhanced
empathy in
AI systems
Policy Global
regulation
```
```
Absence of
standardized
```
```
International AI
policy
```
```
Explainable
AI (XAI),
```
```
Accountable
governance
```

```
Aspect Focus area Challenges
identified
```
```
Proposed
solutions
```
```
Key
technologies
```
```
Expected
outcomes
ethics frameworks legal AI mechanisms
Cultural Inclusivity
and bias
```
```
Misinterpretation
of cultural
emotions
```
```
Culturally
adaptive
emotion
datasets
```
```
Transfer
learning,
domain
adaptation
```
```
Fair and
culturally
sensitive AI
models
```
**3.4 Contextual and Temporal Ambiguities in Emotional**

**Interpretation**

The presence of contextual and time ambiguities also becomes a major
challenge in the emotional interpretation provided by AI because the AI
system cannot easily handle the dynamic nature of human emotions in
different scenarios and across different time periods. All emotions are
dynamic, and they depend on situational factors, so it cannot canonically
determine or predict them with the help of AI models unless an extensive
contextual knowledge is provided. This difference is also made difficult
when emotions have to be perceived with varying modalities like taking
audio, visual and news information together to have a deeper interpretation
of the context. Emotional interpretation could be enhanced by making AI
models incorporate respect to time and care about the context. It needs
complex models that are able to learn about emotion trajectories and
contextual changes across time that might use recurrent neural networks or
temporal fusion processes. Further methods such as sequence-to-sequence
learning and attention systems play a crucial role in ensuring that these
models are more sensitive to the change of time in emotion data, which
eventually enhances the accuracy of AI systems to interpret and react to
human emotions [ 20 ] (Fig. 2 ).


**_Fig. 2_** Navigating the future of emotion AI technical barriers, ethical concerns, and sustainable
advancements
The dimensions included in the diagram named Navigating the Future
of Emotion AI are the ones that are core in the responsible construction of
the Emotion AI systems. It is divided into three main sections, such as
Technical Barriers, Ethical Concerns and Sustainable Advancements. Such
problems as the generalization of models, the bias and real time factors,
being taken into light in the Technical Barriers branch, undermine the flex
and scale of affective computing models. The Ethical Concerns section
ponders on such issues as emotional manipulation, breach of privacy and
the lack of transparency that indicates the ethical issues of the emotion-
cantered data analysis and the automated decisions. The basis is Sustainable
Advancements that suggests such important enablers to the future
development as energy efficiency, scalable solution, and long-term effect on
the environment. The compass in the middle represents the proportionality


of navigation which must exist between technology, ethics and
sustainability to make sure that Emotion AI is a transparent, inclusive, and
green innovation that positively impacts on people and the society.

##### 4 Ethical and Privacy Challenges

Due to the growth of Emotion AI, the scope of privacy and consent breach
has become a major issue and particularly in those instances where
emotional conditions are observed without the consent of the users. Activity
Sometimes, in business and government contexts, the deployment of
emotion recognition technology, like facial coding or voice recognition
[ 21 ], is deployed without people having full information about the use or
storage of their emotional data. This scenario poses a perceived attempt of
consent violation since they might have not been given a chance to choose
and make an informed consent thereby violating their privacy rights and
causing ethical issues related to emotional surveillance [ 22 ].
Also, these anxieties are compounded by the adoption of Emotion AI in
the learning setting. The introduction of emotional surveillance systems at
the classroom level, such as the use of facial recognition, poses a question
of the validity and the agreement of tracking student emotions as a learning
analytics tool. Such surveillance is likely to have unintended consequences,
including stigmatization or profiling particularly when the emotional data
are utilized to reach a conclusion regarding the capabilities of students or
their potential [ 23 ]. As a responsible reaction to these problems, it is
essential to implement robust regulations frameworks that put the emphasis
on transparency and the consent of users.

**4.1 Data Ownership, Manipulation, and Trust Deficit**

The issue of data ownership is a major problem of the world of emotion AI,
where emotional data are frequently commoditized, and no ownership
rights are defined. Such data ownership confusion may bring about
manipulation and exploitation of the emotional data hence resulting in lack
of trust among the users. When AI based systems can use emotional data to
predict or make decisions that impact them without clearly communicating
on how the data was used, the trust of the user is also damaged. These
problems can be alleviated by means of transparent and fair data
governance frameworks to ensure that users have confidence once again.


By making sure that users can access, manage, and learn more about how
their emotional information is being processed, one will be empowered to
make wise decisions regarding the interactions with AI systems. There must
also be mechanisms of redressing any violation to the rights of data
ownership or privacy and hold organisations responsible in ensuring trusts
and integrity in their data management policies [ 24 ].

**4.2 Emotional Exploitation in Marketing and Behaviour**

**Prediction**

Emotion AI in marketing has its managers with enormous opportunities of
personalization, as well as threats of exploiting the feelings of the users.
Advertising machines that interpret emotions to keep the advert relevant are
ethically dangerous because they can control consumer behaviour or pre-
empt vulnerable people with emotionally charged material. The practices
may greatly influence how people make their decisions, and the result is
some sort of psychological manipulation that appeals to the emotional
weaknesses of users [ 25 ]. Moreover, emotionally powered behaviour
prediction models can take advantage of consumer vulnerabilities hence the
issue regarding consent and autonomy is an ethical issue. It is essential to
enforce the ethical principles and consumer protection to make sure that the
use of Emotion AI in marketing does not interfere with the autonomy of
people and avoids exploitative actions. Such policies ought to be directed at
creating transparency and user consent to create trust in emotionally
adaptive AI technologies applied in the consumer-facing settings [ 26 ].

**4.3 The Need for Transparent and Explainable Systems**

The complexity and opaqueness of the Emotion AI models make it
unnecessary to be developed to be transparent and explainable and install
trust and responsibility. Explainability is important, not only in the manner
of knowing how these systems work, but it must be audited and questioned
on the issue of fairness and accuracy. To effectively gain meaningful
explainability, interdisciplinary work is necessary to create AI systems that
do not just work effectively but also respond to societal values and ethics.
Furthermore, transparent AI systems empower users due to the insights
received on the decision-making processes affecting personal and financial
results [ 27 ]. This is particularly relevant in the use of predictive modelling
and decision-making where mistrust and scepticism towards AI


technologies can be reduced through the knowledge of algorithmic
processes by the users. The responsible use of Emotion AI can be
encouraged by encouragement of transparency and accountability where the
application of Emotion AI is used to advantage both the individual and the
community in an ethical way [ 28 ].

##### 5 Interdisciplinary Perspectives for Responsible

##### Emotion AI

**5.1 Bridging Psychology, Computer Science, and Ethics**

Psychology, computer science, and ethics are significant to the creation of
ethically responsible AI systems. The multidisciplinary approach mentioned
would ensure that AI technologies would not only be sound technically but
also morally functional and aligned with human values. Computational
ethics is a discipline that is geared toward solving the convergence of the
two; that is, delivering human ethical decisions to the AI systems to
increase moral soundness and credibility. The computational ethics
framework entails integrated ethical conceptualization ideas and technical
design through the augmentation of the capabilities of the AI to behave in
ethically contentious situations and the development of the inter-
disciplinary research to facilitate the ethical AI development [ 29 ]. Further,
this integration has been justified by the fact that AI is being applied in
several fields, hence one must have a detailed understanding of how AI
systems are applied at the educational and social levels. Education
Curricula and courses that can educate future AI professionals on the
ethical, fair and safe use of AI are becoming more developed. The above
academic curriculum emphasizes the role of psychological savvy to the AI
system by granting the students the competence to meet the moral and
technical challenges that come with AI [ 30 ].

**5.2 Policy Frameworks and International Governance Models**

These issues conceptualized as frameworks and international governing
models will be required to ensure the ethical and council concerns disposed
of by them is secured. Management AI is likened to the act of shepherding
cats, whereby the various stakeholder interests and technological advances
cannot be synchronized. Even good models of governance must deploy


interdisciplinary decisions by reflecting the merits of AI, as well as the
dangers and identify regulations that would contribute to responsible
innovation [ 31 ]. At the global level, cooperation needs to take place to
resolve AI policies in various regulatory locations. This also involves the
provision of ethical issues like algorithm bias and data privacy and easing
transparency through explainable AI (XAI). The new global regimes that
prioritize these characteristics would make sure that the application of AI
technologies would not take advantage of human rights and other
conditions leading to the development of distrust in people [ 32 ]. The
policymakers are asked to consider the technical solutions and overall, the
impacts of the social lives creating AI legislations and state the necessity of
a changeable system of governance that could be changed to the speed of AI
technology changes [ 33 ].
The Fig. 3 entitled as Navigating the Future of Emotion AI is a
theoretical map leading the exploration of Emotion AI in four important
domains: Technical Challenges, Ethical Concerns, Sustainable Practices,
and Interdisciplinary Approaches. In the middle of it, there is the golden
compass arrow, which represents direction and balance, pointing to
responsible innovation. The Technical Challenges quadrant emphasizes the
obstacles of the generalization of the models, variability of the data, and
processing efficiency in the real time, which limit the scaling capability of
Emotion AI. Privacy issues, emotional manipulation, and lack of
transparency are covered under the Ethical Concerns section of the policy,
and the three areas where the principles of governance and accountability
are required. Sustainable Practices, in their turn, become focused on energy
efficiency, scalability, and long-term ecological impact, which guarantees
that Emotion AI will develop with the minimum environmental footprint.
Lastly, the Interdisciplinary Approach emphasizes that ethics, psychology,
and computer science should be combined to produce ethically sensitive but
morally compliant systems. Combined, the diagram becomes a visual
roadmap that shows that the real progress in Emotion AI is not merely about
the high level of technology but is to be made in order to balance the
concepts of innovation and ethical and sustainable values.


**_Fig. 3_** Navigating the future of emotion AI—A compass for ethical and sustainable innovation

##### 6 Sustainable Advancements in Emotion AI

This table identifies key areas of sustainable progress in Emotion AI with a
primary focus on approaches balancing ethical concerns and privacy issues
with scalability. In each row, the major authors, the nature of progress, its
description, practical uses, future trends, and recurrent challenges have been
described. Decentralized training in federated learning guarantees privacy
of data, whereas edge computing enhances computations in real-time


(secured and efficient). Multimodal fusion promotes emotional
comprehension with the help of a wide variety of data, and culturally
adaptive models promote inclusivity and situational correctness. A
combination of these developments will lead to the development of human-
centric, transparent, and sustainable Emotion AI systems to deploy in the
future (Table 3 ).

**_Table 3_** Summary of sustainable advancements in emotion AI research and implementation

```
Author(s) Kind of
sustainable
advancement
```
```
Overview Application Recent trends Challenges
```
```
Korkmaz et
al. [ 34 ];
Yurdem et
al. [ 35 ]
```
```
Federated and
privacy-
preserving
learning
approaches
```
```
Federated
Learning (FL)
is a distributed
machine
learning
paradigm where
multiple devices
collaboratively
train a shared
model without
sharing their
raw data,
ensuring
privacy and
reducing data
transfer costs
```
```
Used in
healthcare for
training models
on patient data
across hospitals
without sharing
sensitive records
and in mobile
devices for
personalized AI
(e.g., keyboard
suggestions)
```
```
Blockchain
integration in
Chain FL
mitigates
centralization
risks, and
reference
architectures
aim to
standardize FL
systems for
scalability and
security
```
```
Managing data
security,
adversarial
attacks, and
non-IID data;
designing
incentive
mechanisms
for
participants
```
```
Jin et al.
[ 36 ] Al-
Doghman et
al. [ 37 ];
Alrowaily
and Lu [ 38 ]
```
```
Edge
computing for
secure and
decentralized
emotion
analysis
```
```
Edge computing
processes data
close to the
source,
reducing
latency, saving
bandwidth, and
enhancing
privacy—ideal
for emotion
analysis
applications
```
```
Applied in real-
time emotion
recognition
systems to
deliver low-
latency analysis
in smart devices
and autonomous
systems
```
```
Incorporation
of
microservices
for modular
and scalable
edge
architectures;
emphasis on
distributed
frameworks
for enhanced
security
```
```
Balancing
resource
constraints and
security;
managing
distributed
authentication
and secure
data access
```
```
Zhu et al.
[ 39 ]; Liu et
al. (2018);
Nemati et al.
[ 40 ]; Sahu
and
```
```
Emotion-
aware
multimodal
fusion
architectures
```
```
Combines
multiple data
modalities such
as audio, visual,
and textual
inputs for
```
```
Utilized in
healthcare for
emotion-based
diagnostics and
in personalized
content
```
```
Use of self-
attention
mechanisms
and low-rank
tensor
techniques for
```
```
Addressing
data
heterogeneity,
ensuring
model
robustness,
```

```
Author(s) Kind of
sustainable
advancement
```
```
Overview Application Recent trends Challenges
```
```
Vechtomova
[ 41 ];
Gaonkar et
al. [ 42 ]
```
```
comprehensive
emotion
understanding
and analysis
```
```
recommendation
systems
```
```
efficiency
while
retaining data
integrity
```
```
and managing
differing input
quality across
modalities
Zhao et al.
[ 43 ]; De
Leersnyder
et al. [ 44 ];
Wortman
and Wang
[ 45 ]
```
```
Culturally
adaptive
emotion
models
```
```
Develops AI
systems that
interpret and
respond to
human
emotions while
considering
cultural nuances
and personal
variations
```
```
Deployed
socially
interactive robots
and adaptive
virtual assistants
to enhance user
engagement and
satisfaction
```
```
Incorporating
cultural
regulation
patterns and
relational
dynamics into
adaptive
emotional
intelligence
frameworks
```
```
Integrating
diverse
cultural inputs
without
compromising
accuracy in
emotion
recognition
```
##### 7 Future Directions and Research Opportunities

**7.1 Toward Human-Centric and Context-Aware Systems**

Emotion recognition systems are becoming increasingly human and
adaptive to the requirements on the accurate capture of a multifaceted
emotional state. Because of the diversity of people, culture, and the
situations, emotions are always subtle and dependent on a plethora of
influences. The complexities present a great difficulty in coming up with
systems that have capabilities of understanding and analysing emotions to a
high degree of accuracy. Developments in deep learning have led to more
successful emotion recognition by allowing the extraction of features more
effectively in a complex form like audio and video [ 46 ]. To ensure more
situational awareness, the tendency among scholars is to think in the
multimode where information on several various aspects, such as facial
expressions, voice tone, even physiological cues is considered. This will not
just increase the accuracy of the emotional recognition process but also a
more consolidated description of emotional conditions. Such systems can
further be complemented with Natural Language Processing (NLP) in
understanding the emotional meaning of a spoken or written word regarding
the application of the language and context.

**7.2 Benchmarking Fairness, Accountability, and Transparency**


As the emotion recognition systems are progressively integrated into the
scope of various applications, it is of impeccable importance to make them
balanced, responsible, and open. These systems should be designed
considering the ethical aspect such as privacy, and any biasness made in the
data representation. Machine learning models will inevitably reproduce the
biases that are inherent in their training information and will treat members
of a particular demographic unfairly. To solve the problems, it is necessary
to prepare a variety of datasets that would be representative in terms of the
emotional manifestations of different populations [ 47 ]. Moreover, it is
important that the way such models operate and make decisions is
transparent so as to gain trust in the users. This involves approaching the
technicalities as well as clarifying to the stakeholders the potential effects
and limitations of the technologies of emotion recognition. Accountability
and transparency will enable the developers to design ethically right
systems and in harmony with their idealistic values of respective societies.

##### 8 Conclusion

This is even ahead in relation to Emotion AI on the right balance of
creation, ethical management and humanity. With the usage of Emotion AI
beginning to look more of a Real-Life system rather than an Enhanced
Proposal, it ought to be regulated with more transparency and inclusiveness
and fairness. Not only is a sustainable Emotion AI necessary for contacting
a trust, but it must follow the ethical design aspect of the life cycle of AI
that can be realized through data collection and the training of the model, its
implementation, and decision-making. The inclusion of privacy preserving
measures in the model such as federated learning and secure edge
computing is to be assured that feelings information is secret and
confidential. Correspondingly, the proposition of explainable AI systems
will empower users and policymakers to actualize the cycle of emotional
inferences and consequently generate a feeling of accountability and
prevent abuse or misuse.
The strategy of the AI Emotion should, thus, be aimed at
multidisciplinary collaboration between engineers, ethics, psychologists,
and legislators. This will involve the establishment of universal standards of
conduct that outline what can be carried out concerning the use of
emotional data, fairness standards in addition to audit systems. The


promotion of datasets, inclusiveness to cultural classification, the model and
the environment will help in removing biasness and increase the validity of
familiarity recognition models in the different societies. Lastly, Emotion AI
needs to become a life-altering disruptor that complements empathy and not
exploit it-developing a future where technology has a beneficial effect on
cognition and emotion sensitivity, interpersonal relationships and ethical
human-AI co-existence.

##### References

1. Cambria, E.: Affective computing and sentiment analysis. IEEE Intell. Syst. **31** (2), 102–107
    (2016). https:// doi. org/ 10. 1109/ mis. 2016. 31
    [Crossref]
2.
    Zhao, G., Li, Y., Xu, Q.: From emotion AI to cognitive AI. Int. J. Network Dyn. Intell. 65–72
    (2022). https:// doi. org/ 10. 53941/ ijndi0101006
3.
    Vistorte, A.O.R., Martí-González, M., López-Granero, C., Barrasa, A., Ayala, J.L.M., Deroncele-
    Acosta, A.: Integrating artificial intelligence to assess emotions in learning environments: a
    systematic literature review. Front. Psychol. **15**. https:// doi. org/ 10. 3389/ fpsyg. 2024. 1387089
4.
    Kaur, S., Sharma, R.: Emotion AI: Integrating Emotional Intelligence with Artificial Intelligence
    in the Digital Workplace, pp. 337–343. Springer (2021). https:// doi. org/ 10. 1007/ 978-3-030-
    66218-9_ 39
5.
    Mumme, D.L., Fernald, A., Herrera, C.: Infants’ responses to facial and vocal emotional signals
    in a social referencing paradigm. Child Dev. **67** (6), 3219–3237 (1996). https:// doi. org/ 10. 1111/ j.
    1467-8624. 1996. tb01910. x
    [Crossref]
6.
    Sauter, D.A., Scott, S.K., Calder, A.J., Eisner, F.: Perceptual cues in nonverbal vocal expressions
    of emotion. Quart. J. Exp. Psychol. **63** (11), 2251–2272 (2010). https:// doi. org/ 10. 1080/
    1747021100372164 2
    [Crossref]
7.
    Kuraoka, K., Nakamura, K.: Responses of single neurons in monkey amygdala to facial and
    vocal emotions. J. Neurophysiol. **97** (2), 1379–1387 (2006). https:// doi. org/ 10. 1152/ jn. 00464.
    2006
    [Crossref]
8.
    Ohshima, S., Okuno, K., Koeda, M., Saito, H., Kawai, W., Naganawa, S., Kyutoku, Y., Hama, T.,
    Dan, I., Niioka, K.: Cerebral response to emotional working memory based on vocal cues: an
    fNIRS study. Front. Hum. Neurosci. **17** (2023). https:// doi. org/ 10. 3389/ fnhum. 2023. 1160392
9.
    Zhang, Y., Cheng, C., Zhang, Y.: Multimodal emotion recognition using a hierarchical fusion
    convolutional neural network. IEEE Access **9** , 7943–7951 (2021). https:// doi. org/ 10. 1109/ access.
    2021. 3049516


[Crossref]
10.
Wang, Z., Wang, W., Zhou, X., Liang, C.: Emotion recognition using multimodal deep learning
in multiple psychophysiological signals and video. Int. J. Mach. Learn. Cybern. **11** (4), 923–934
(2020). https:// doi. org/ 10. 1007/ s13042-019-01056-8
[Crossref]
11.
Zhang, S., Huang, T., Zhang, S., Gao, W.: Multimodal deep convolutional neural network for
audio-visual emotion recognition. 281–284 (2016). https:// doi. org/ 10. 1145/ 2911996. 2912051
12.
Tan, C., Kasabov, N., Puthanmadam Subramaniyam, N., Ceballos, G.: Fusion sense: emotion
classification using feature fusion of multimodal data and deep learning in a brain-inspired
spiking neural network. Sensors **20** (18), 5328 (2020). https:// doi. org/ 10. 3390/ s20185328
13.
Dhall, A., Goecke, R., Gedeon, T., Ghosh, S., Joshi, J., Hoey, J.: From individual to group-level
emotion recognition: EmotiW 5.0. 524–528 (2017). https:// doi. org/ 10. 1145/ 3136755. 3143004
14.
Poria, S., Hovy, E., Mihalcea, R., Majumder, N.: Emotion recognition in conversation: research
challenges, datasets, and recent advances. IEEE Access **7** , 100943–100953 (2019). https:// doi.
org/ 10. 1109/ access. 2019. 2929050
[Crossref]
15.
Slimani, K., Ruichek, Y., Messoussi, R.: Compound facial emotional expression recognition
using cnn deep features. Eng. Lett. **30** (4), 1402–1416 (2022)
16.
Seknedy, M.E., Fawzi, S.: Speech emotion recognition system for human interaction
applications. **13** , 361–368 (2021). https:// doi. org/ 10. 1109/ icicis52592. 2021. 9694246
17.
Gonzalez, H.A., Yoo, J., Elfadel, I.M.: EEG-based emotion detection using unsupervised transfer
learning. 694–697 (2019). https:// doi. org/ 10. 1109/ embc. 2019. 8857248
18.
Vinola, C., Vimaladevi, K.: A survey on human emotion recognition approaches, databases and
applications. ELCVIA Electron. Lett. Comp. Vis. Image Anal. **14** (2), 24–44 (2015). https:// doi.
org/ 10. 5565/ rev/ elcvia. 795
[Crossref]
19.
Shah, Z., Zhiyong, S., Adnan, A.: Enhancements in immediate speech emotion detection:
harnessing prosodic and spectral characteristics. Int. J. Innov. Sci. Res. Technol. (IJISRT) 1526–
1534 (2024). https:// doi. org/ 10. 38124/ ijisrt/ ijisrt24apr872
20.
Li, Y., Jiang, D., Tao, J., Jia, J., Schuller, B., Shan, S.: MEC 2016: The Multimodal Emotion
Recognition Challenge of CCPR 2016, pp. 667–678. Springer, Singapore (2016). https:// doi. org/

10. 1007/ 978-981-10-3005-5_ 55
21.
Monteith, S., Geddes, J., Glenn, T., Bauer, M., Whybrow, P.C.: Commercial use of emotion
artificial intelligence (AI): implications for psychiatry. Curr. Psychiatry Rep. **24** (3), 203–21 1
(2022). https:// doi. org/ 10. 1007/ s11920-022-01330-7
[Crossref]
22.
Berson, I.R., Berson, M.J., Luo, W.: Innovating responsibly: ethical considerations for AI in
early childhood education. AI Brain Child **1** (1) (2025). https:// doi. org/ 10. 1007/ s44436-025-
00003-5


23.
Hosaın, M.T., Inṡ ıa, K., Raḟ ı, S., Tabassum, R., ̇ Anık, M.H., Siddikẏ , M.M.: Path to gain
functional transparency in artificial intelligence with meaningful explainability. J. Metaverse
**3** (2), 166–180 (2023). https:// doi. org/ 10. 57019/ jmv. 1306685
24.
Rhue, L.: The anchoring effect, algorithmic fairness, and the limits of information transparency
for emotion artificial intelligence. Inf. Syst. Res. **35** (3), 1479–1496 (2023). https:// doi. org/ 10.
1287/ isre. 2019. 0493
[Crossref]
25.
Thurzo, A.: Provable AI ethics and explainability in next-generation medical and educational AI
agents: trustworthy ethical firewall. Mdpi Ag (2025). https:// doi. org/ 10. 20944/ preprints202502.

2232. v1
26.
Khair, M.A., Ande, J.R.P.K., Mahadasa, R., Tuli, F.A.: Beyond human judgment: exploring the
impact of artificial intelligence on HR decision-making efficiency and fairness. Glob. Disclos.
Econ. Bus. **9** (2), 163–176 (2020). https:// doi. org/ 10. 18034/ gdeb. v9i2. 730
27.
Bakir, V., Miranda, D., Laffer, A., Urquhart, L., Mcstay, A.: On manipulation by emotional AI:
UK adults’ views and governance implications. Front. Sociol. **9** (2024). https:// doi. org/ 10. 3389/
fsoc. 2024. 1339834
28.
Ahmad, W., Shokeen, R., Raj, R.: Artificial Intelligence, pp. 459–520. Igi Global (2024). https://
doi. org/ 10. 4018/ 979-8-3693-5538-1. ch017
29.
Awad, E., Sinnott-Armstrong, W., Slavkovik, M., Levine, S., Tenenbaum, J.B., Jamison, J.C.,
Gopnik, A., Schroeder, J., Anderson, M., Conitzer, V., Meyer, M.N., Kim, T.W., Liao, S.M.,
Borg, J.S., Opoku-Agyemang, K., Mikhail, J., Crockett, M.J., Everett, J.A.C., Evgeniou, T.,
Anderson, S.L.: Computational ethics. Trends Cogn. Sci. **26** (5), 388–405 (2022). https:// doi. org/
10. 1016/ j. tics. 2022. 02. 009
[Crossref]
30.
Büthe, T., Djeffal, C., Lütge, C., Maasen, S., Ingersleben-Seip, N.V.: Governing AI—attempting
to herd cats? Introduction to the special issue on the Governance of Artificial Intelligence. J. Eur.
Publ. Policy **29** (11), 1721–1752 (2022). https:// doi. org/ 10. 1080/ 13501763. 2022. 2126515
[Crossref]
31.
Gao, D.K., Haverly, A., Wu, J., Chen, J., Mittal, S.: AI ethics. Int. J. Bus. Anal. **11** (1), 1–19
(2024). https:// doi. org/ 10. 4018/ ijban. 338367
[Crossref]
32.
Khan, A.N., Rustam, R., Ali, A., Khan, S., Mansoor, M., Shah, J., Junaid, M.A., Jameel, A.,
Aslam, R.: Artificial intelligence in computer science: evolution, techniques, challenges, and
multidisciplinary applications. Schol. J. Eng. Technol. **13** (04), 246–263 (2025). https:// doi. org/
10. 36347/ sjet. 2025. v13i04. 00
33.
Alam, A.: Developing a curriculum for ethical and responsible AI: a university course on safety,
fairness, privacy, and ethics to prepare next generation of AI professionals, pp. 879–894.
Springer Nature Singapore (2023). https:// doi. org/ 10. 1007/ 978-981-99-1767-9_ 64
34.
Korkmaz, A., Alhonainy, A., Rao, P.: An evaluation of federated learning techniques for secure
and privacy-preserving machine learning on medical datasets. 1–7 (2022). https:// doi. org/ 10.


1109/ aipr57179. 2022. 10092212
35.
Yurdem, B., Kuzlu, M., Gullu, M.K., Catak, F.O., Tabassum, M.: Federated learning: overview,
strategies, applications, tools and future directions. Heliyon **10** (19), e38137 (2024). https:// doi.
org/ 10. 1016/ j. heliyon. 2024. e38137
[Crossref]
36.
Jin, W., Xu, R., Kim, D., Hong, Y.-G., You, T.: Secure edge computing management based on
independent microservices providers for gateway-centric IoT networks. IEEE Access **8** , 187975–
187990 (2020). https:// doi. org/ 10. 1109/ access. 2020. 3030297
[Crossref]
37.
Al-Doghman, F., Sohrabi, N., Khalil, I., Tari, Z., Zomaya, A.Y., Moustafa, N.: AI-enabled secure
microservices in edge computing: opportunities and challenges. IEEE Trans. Serv. Comput.
**16** (2), 1485–1504 (2023). https:// doi. org/ 10. 1109/ tsc. 2022. 3155447
[Crossref]
38.
Alrowaily, M., Lu, Z.: Secure edge computing in IoT systems: review and case studies. 440–444
(2018). https:// doi. org/ 10. 1109/ sec. 2018. 00060
39.
Zhu, H., Hua, Y., Deng, L., Xu, G., Shi, Y., Wang, Z.: Multimodal fusion method based on self-
attention mechanism. Wirel. Commun. Mob. Comput. **2020** , 1–8 (2020). https:// doi. org/ 10. 1155/
2020/ 8843186
[Crossref]
40.
Nemati, S., Rohani, R., Yen, N.Y., Abdar, M., Makarenkov, V., Basiri, M.E.: A hybrid latent
space data fusion method for multimodal emotion recognition. IEEE Access **7** , 172948–172964
(2019). https:// doi. org/ 10. 1109/ access. 2019. 2955637
[Crossref]
41.
Sahu, G., Vechtomova, O.: Adaptive fusion techniques for multimodal data. 3156–3166 (2021).
https:// doi. org/ 10. 18653/ v1/ 2021. eacl-main. 275
42.
Gaonkar, A., Raman, P.J., Chukkapalli, Y., Gurugopinath, S., Srikanth, S.: a comprehensive
survey on multimodal data representation and information fusion algorithms. 1–8 (2021). https://
doi. org/ 10. 1109/ conit51480. 2021. 9498415
43.
Zhao, J., Chen, S., Liang, J., Li, R., Jin, Q.: Adversarial domain adaption for multi-cultural
dimensional emotion recognition in dyadic interactions. **18** , 37–45. https:// doi. org/ 10. 1145/

3347320. 3357692
44.
De Leersnyder, J., Mesquita, B., Boiger, M.: Cultural regulation of emotion: individual,
relational, and structural sources. Front. Psychol. **4** (55) (2013). https:// doi. org/ 10. 3389/ fpsyg.
2013. 00055
45.
Wortman, B., Wang, J.Z.: HICEM: a high-coverage emotion model for artificial emotional
intelligence. IEEE Trans. Affect. Comput. **15** (3), 1136–1152 (2024). https:// doi. org/ 10. 1109/
taffc. 2023. 3324902
[Crossref]
46.
Patros, S.S., Arora, P., Dustdar, P., Rodrigues, S., Ding, J.J.P.C., Uhlig, Y., Parlikad, S., Abraham,
A.K., Buyya, A., Ottaviani, R., Haunschild, C., Sakellariou, D., Song, R., Pujol, H.H., Cetinkaya,


V.C., Stankovski, O., Ramamohanarao, V., Li, R., Wu, H.: Modern computing: vision and
challenges. Telemat. Inform. Rep. **13** , 100116 (2024). https:// doi. org/ 10. 1016/ j. teler. 2024. 100116
47.
Kong, X., Wen, W., Guan, Y., Lin, Z., Zheng, J., Xie, B., Li, S., Xue, J., Hu, Q.: Advances in
machine learning-driven flexible strain sensors: challenges, innovations, and applications. ACS
Appl. Mater. Interfaces. **17** (22), 31778–31798 (2025). https:// doi. org/ 10. 1021/ acsami. 5c06453
[Crossref]
_OceanofPDF.com_


(1)

(2)

© The Author(s), under exclusive license to Springer Nature Switzerland AG 2026
K. Slimani et al. (eds.), _Emotion and Facial Recognition in Artificial Intelligence: Sustainable
Multidisciplinary Perspectives and Applications_ , Information Systems Engineering and Management
78
https://doi.org/10.1007/978-3-032-14778-3_1 1

## Emotion AI in Mental Health

Ayushi Shelke^1 , Ashok Kumar^1 , Mukul Yadav^1 and Vinay Aseri^2
School of Information Technology, Artificial Intelligence and Cyber
Security, Rashtriya Raksha University, Gandhinagar, India
School of Cyber Security and Digital Forensics, Narnarayan Shastri
Institute of Technology, Ahmedabad, Gujarat, India

```
Vinay Aseri
Email: vinay.aseri2001@gmail.com
```
**Abstract**
Mental illness is one of the most serious global health issues of the twenty-
first century, with more than one billion individuals affected and almost a
trillion dollars in lost productivity each year (A Comprehensive Review of
Multimodal Emotion Recognition. PMC. 2024. https:// pmc. ncbi. nlm. nih.
gov/ articles/ PMC11093677). In spite of increased awareness, access to
cost-effective and timely care is still restricted, especially in low- and
middle-income nations where mental health care is nonexistent (Systematic
Review and Meta-analysis of AI-Based Conversational Agents: Effects on
Depression & Distress. Nature Digital Medicine. 2023. https:// [http://www.](http://www.) nature.
com/ articles/ s41746-023–00,894-1). Recent advances in Emotion Artificial
Intelligence (AI), or affective computing, provide new hopes of closing this
gap. Technologies with the ability to recognize emotional signals from text,
voice, facial expressions, and physiological measures can potentially allow
for early identification of psychological distress and provide scalable, low-
cost interventions (Cross-Modal Gated Feature Enhancement for
Multimodal Emotion Recognition. Scientific Reports. 2025. https:// [http://www.](http://www.)
nature. com/ articles/ s41598-025–87,804; Multimodal Emotion Recognition


in Conversations: A Survey of Methods, Trends, and Challenges. arXiv.

2025. https:// arxiv. org/ abs/ 2505. 2051). This chapter provides an overview of
recent breakthroughs in emotion detection and incorporation in mental
health intervention. We draw on more than 150 studies published between
2020 and 2025 to emphasize advancements in natural language processing
models like BERT and RoBERTa, speech emotion recognition utilizing
Wav2Vec2.0, and facial expression recognition via convolutional and
transformer-based networks (A Comprehensive Review of Multimodal
Emotion Recognition. PMC. 2024. https:// pmc. ncbi. nlm. nih. gov/ articles/
PMC11093677; Multimodal Emotion Recognition in Conversations: A
Survey of Methods, Trends, and Challenges. arXiv. 2025. https:// arxiv. org/
abs/ 2505. 2051; Multimodal Emotion Recognition and Sentiment Analysis
combining Wav2Vec2, RoBERTa etc. arXiv. 2025. https:// arxiv. org/ abs/
2502. 08915). We also discuss upcoming digital therapeutics that have
shown significant depressive and anxiety symptom reductions in controlled
clinical environments (Persuasive Chatbot-Based Interventions for
Depression: Recommendations for Improving Reporting Standards.
Frontiers in Psychiatry. 2025. https:// [http://www.](http://www.) frontiersin. org/ journals/
psychiatry/ articles/ 10. 3389/ fpsyt. 2025. 1523831/ ful; Woebot RCT:
Effectiveness of Web-based & Mobile Therapy Chatbot on Anxiety and
Depression. PMC. 2022. https:// pmc. ncbi. nlm. nih. gov/ articles/
PMC10993129/ ; Comparison of AI Chatbot With a Nurse Hotline in
Reducing Depression and Anxiety. JMIR Human Factors. 2025. https://
humanfactors. jmir. org/ 2025/ 1/ e4528). Building on this foundation, we
introduce a conceptual framework that combines multimodal emotion
sensing with context-aware, cognitive-behavioral strategies. The model
prioritizes privacy-preserving computation, bias mitigation, and clinician
oversight to promote ethical and reliable deployment (Cross-Modal Gated
Feature Enhancement for Multimodal Emotion Recognition. Scientific
Reports. 2025. https:// [http://www.](http://www.) nature. com/ articles/ s41598-025–87,804;
Chatbots and Mental Health: A Scoping Review of Reviews. Current
Psychology. 2025. https:// link. springer. com/ article/ 10. 1007/ s12144-024–
06,097-0). While early findings are encouraging, major challenges persist in
ensuring cultural adaptability, data protection, and sustained user
engagement (Systematic Review and Meta-analysis of AI-Based
Conversational Agents: Effects on Depression & Distress. Nature Digital
Medicine. 2023. https:// [http://www.](http://www.) nature. com/ articles/ s41746-023–00,894-1;


Topic-Based Chatbots on Mental Health Self-Care: Rule-based chatbot
intervention and its effect on mental health literacy & self-care. JMIR
Mental Health. 2025. https:// mental. jmir. org/ 2025/ 1/ e46560). We conclude
that Emotion AI, when responsibly designed and clinically validated, holds
significant promise for advancing global mental health. Its future success
will depend on close collaboration among researchers, practitioners, and
policymakers, alongside robust ethical and regulatory safeguards (Chatbots
and Mental Health: A Scoping Review of Reviews. Current Psychology.

2025. https:// link. springer. com/ article/ 10. 1007/ s12144-024–06,097-0;
Topic-Based Chatbots on Mental Health Self-Care: Rule-based chatbot
intervention and its effect on mental health literacy & self-care. JMIR
Mental Health. 2025. https:// mental. jmir. org/ 2025/ 1/ e46560).

**Keywords** Affective computing – Multimodal emotion recognition –
Natural language processing – Speech emotion recognition – Transformer
models – Mental health chatbots – Cognitive behavioral therapy – Digital
therapeutics – Human-computer interaction – AI ethics in healthcare –
Explainable AI – Privacy-preserving machine learning

##### 1 Introduction

Mental health disorders are now recognized as a leading cause of disability
Mental health disorders are now recognized as a leading cause of disability
worldwide, affecting more than one billion people and resulting in
enormous social and economic costs. According to the World Health
Organization’s 2023 update, nearly one in every eight people globally—
approximately 970 million individuals—were living with a mental disorder,
with depression and anxiety being the most common. The report further
warns that global investment in mental health remains below 2% of total
health budgets, underscoring the urgency of large-scale intervention efforts.
Conditions such as depression, anxiety, and post-traumatic stress disorder
(PTSD) contribute to nearly a trillion dollars in lost productivity each year.
Despite increasing awareness, treatment gaps remain staggering,
particularly in low- and middle-income countries where mental health
professionals are scarce and services are often under-resourced. This
mismatch between growing demand and limited clinical capacity


underscores the need for scalable, accessible, and cost-effective
interventions.
The rapid progress of Emotion Artificial Intelligence (AI)—the science
of detecting and interpreting human emotions through text, speech, facial
expressions, and physiological signals—offers a promising way forward.
Advances in natural language processing (NLP), transformer architectures,
and multimodal learning now allow machines to infer affective states with
remarkable accuracy, opening up possibilities for early detection of
psychological distress and on-demand support through digital platforms [ 3 ,
4 ].

**1.1 Motivation**

The motivation for this chapter lies in the convergence of three powerful
trends. First, the prevalence of mental health challenges is rising across all
demographics, with particularly steep increases among adolescents and
young adults. Second, digital health adoption has accelerated post-
pandemic, creating an unprecedented opportunity to deliver care beyond
traditional settings. For example, India’s National Tele Mental Health
Programme (Tele-MANAS), launched in October 2022, has established 53
helpline cells across States/UTs, handles calls in 20+ Indian languages, and
by early 2025 had responded to over 1.8 million calls offering free, 24/7
counselling, video consultation, and referral support. Third, state-of-the-art
Emotion AI models are maturing, with demonstrated capacity to recognize
subtle emotional cues across modalities, making their deployment in real-
world mental health contexts both technically feasible and clinically
valuable [ 30 , 32 ]. Most existing reviews focus either on the algorithmic
aspects of multimodal emotion recognition or on mental health chatbots as a
class of digital interventions. Few attempts have been made to synthesize
these domains, critically analyze their evidence base, and provide a
blueprint for ethical, safe, and culturally adaptable deployment.

**1.2 Contributions**

This chapter addresses these gaps by presenting a comprehensive, evidence-
driven synthesis of the field and charting a forward-looking agenda. We
systematically review over 150 peer-reviewed studies published between
2020 and 2025, spanning natural language processing, speech emotion
recognition, facial and physiological signal analysis, and their integration


into multimodal systems. We then map the clinical impact of AI-driven
chatbots and digital therapeutics, drawing on randomized controlled trials
and longitudinal studies that demonstrate measurable improvements in
depression and anxiety outcomes [ 20 , 30 ]. Beyond simply reviewing
existing work, this chapter proposes a conceptual framework that integrates
multimodal emotion detection pipelines with cognitive-behavioral therapy
(CBT)–inspired conversational flows and clinician-in-the-loop oversight.
The proposed framework emphasizes privacy preservation, explainability,
and bias mitigation as first-class design principles, ensuring that the
resulting systems are both effective and trustworthy. Finally, we offer a
research and policy roadmap, identifying unresolved challenges and
pointing toward innovations, such as federated learning, edge AI, and large
language model fine-tuning—that could unlock safe, scalable deployment
on a global scale.
In combining these elements, this chapter aims not only to provide a
state-of- the-art review but also to guide researchers, clinicians, and
policymakers toward responsible integration of Emotion AI into mental
health care. As Elon Musk emphasized in 2020, AI will be a cornerstone of
efforts to establish a sustained human presence beyond Earth [ 39 ].
Federated learning, edge AI, and large language model fine-tuning—that
could unlock safe, scalable deployment on a global scale.
In combining these elements, this chapter aims not only to provide a
state-of- the-art review but also to guide researchers, clinicians, and
policymakers toward responsible integration of Emotion AI into mental
health care. As Elon Musk emphasized in 2020, AI will be a cornerstone of
efforts to establish a sustained human presence beyond Earth [ 39 ].

**1.3 Emergence of Emotion AI**

In parallel with the rising mental health crisis, Emotion Artificial
Intelligence (AI)—also called affective computing has matured from a
theoretical idea into a practical and deployable technology. First
conceptualized by Rosalind Picard in the late 1990s, affective computing
aimed to enable machines to sense and respond to human emotional states
[ 32 ]. Over the last decade, the field has accelerated due to advances in
natural language processing, deep learning, and multimodal fusion
techniques. Transformer-based models like BERT, RoBERTa, and GPT
variants have dramatically improved sentiment and emotion classification


from text [ 20 ], while speech emotion recognition models such as
Wav2Vec2.0 have reached near- human performance in detecting stress and
mood from vocal prosody [ 17 ]. These innovations have opened the door for
AI-driven digital therapeutics, enabling timely, low-cost, and personalized
mental health interventions at scale.

**1.4 Research Objectives and Scope**

This chapter seeks to bridge the gap between emotion AI research and its
practical application in mental health care. Specifically, we aim to
synthesize the state-of-the-art in multimodal emotion recognition—
including text, speech, facial expression, and physiological signal analysis
—and critically evaluate its clinical relevance. Unlike earlier surveys that
primarily focus on algorithmic performance or user engagement, this
review takes a multidisciplinary perspective, integrating findings from
computer science, psychology, and clinical trials. Our horizon spans work
from 2020 to 2025, the timeframe of most advancement in transformer NLP
models, multimodal fusion networks, and explainable AI frameworks. In
providing integrated analysis, we hope to enable researchers to comprehend
the technical readiness of existing systems, advise clinicians on their
integration into practice, and inform policymakers on constructing
regulatory guidelines ensuring safe and fair deployment.

##### 2 Literature Review

**2.1 Emotion Recognition in Text and NLP**

Natural Language Processing (NLP) is one of the most advanced methods
used in emotion recognition with transformers like BERT and RoBERTa
establishing state-of-the-art performance levels on sentiment and emotion
classification tasks [ 5 ]. Recent advances are centered on domain-adapted
large language models (LLMs) that are fine-tuned for psychological
lexicons and clinical notes so as to better represent affect in text [ 40 ].
Multilingual emotion models now have wider coverage of low-resource
languages, filling an essential equity gap in world mental health solutions
[ 6 ]. Facial expressions are analyzed through vision transformers and hybrid
transformer-convolutional networks, which can detect subtle micro-
expressions and capture temporal changes. Physiological signals like heart
rate variability, electrodermal activity, and photoplethysmography are


processed by CNN-LSTM architectures for predicting arousal, stress, and
emotional valence. Importantly, hybrid approaches such as SpeechCueLLM
convert vocal emotion cues into textual prompts, enabling LLMs to process
emotional context without requiring specialized audio pipelines [ 23 ]. These
cross- modal strategies make it possible to unify speech and text analysis, a
step toward true multimodal understanding of affect.

**2.2 Speech Emotion Recognition**

Speech remains a critical modality for mental health monitoring due to its
non- intrusive nature and ability to capture subtle prosodic features [ 38 ].
Self-supervised models such as Wav2Vec2.0 and HuBERT have
significantly improved the robustness of speech emotion recognition (SER),
even with limited labeled data [ 7 ]. A major breakthrough has been
emotion2vec, a universal pre-trained speech emotion representation that
generalizes well across datasets and languages, reducing the need for
extensive fine-tuning [ 34 ]. Furthermore, real-time SER methods using data
augmentation techniques (e.g., spectrogram shifting, noise injection) now
achieve sub-second latency, enabling deployment in live teletherapy and
conversational agents [ 24 ]. These advances are especially relevant for
developing responsive, scalable mental health chatbots.

**2.3 Facial and Physiological Emotion Recognition**

Computer vision has evolved from conventional CNN-based facial emotion
recognition (FER) to Vision Transformer (ViT) architectures that capture
global dependencies across facial regions [ 8 ]. FER systems have become
more robust to real-world conditions such as occlusion (e.g., masks) and
head pose variations through partial feature masking and attention-based
augmentation [ 28 ]. Recent video-based approaches integrate YOLO for
real-time face detection with ViTs for expression classification, achieving
high accuracy under challenging conditions [ 26 ]. In parallel, physiological
emotion recognition using electrodermal activity (EDA) and
photoplethysmography (PPG) has been enhanced through wearable sensors
and deep learning, offering opportunities for continuous monitoring of
stress and anxiety in ecological settings [ 21 ]. Telehealth-oriented systems
such as TheraSense further embed FER in virtual consultations, assisting
clinicians in gauging affective states remotely [ 35 ].


```
Criterion Facial emotion recognition Physiological emotion recognition
Strengths Enables emotion detection using
ordinary cameras; intuitive for human
interpretation; effective for real-time
assessment in digital interfaces
```
```
Provides deeper insight into internal
emotional states; remains reliable under
varying lighting or facial occlusion; supports
continuous tracking through wearable
sensors
Weaknesses Accuracy can drop with head
movement, low light, or cultural
variability in expressions; easily
influenced by external factors
```
```
Requires sensor contact with the body; may
face noise from motion or environmental
conditions; limited scalability for large
populations
Use case Often applied in video-based therapy,
social robotics, and remote clinical
observation
```
```
Used in stress monitoring, biofeedback, and
longitudinal studies where sustained
measurement is needed
```
**2.4 AI-Driven Mental Health Interventions**

The clinical application of emotion recognition is most visible in digital
therapeutics and AI-driven mental health interventions. Conversational
agents such as Woebot and Wysa have been validated in randomized
controlled trials (RCTs), showing significant reductions in depressive and
anxiety symptoms over eight-week interventions [ 9 , 19 ]. Notably, a 2025
RCT on topic-based mental health chatbots demonstrated measurable
improvements in mental health literacy and behavioral intent even with
simple rule-based systems [ 23 ]. This indicates that effective intervention
does not always require fully generative AI, but rather well-structured,
evidence-based interaction design. However, challenges persist—including
sustaining user engagement, mitigating algorithmic bias, and ensuring
clinical safety [ 10 , 15 ].

**2.5 Limitations and Research Gaps**

While progress has been remarkable, several gaps remain. Multimodal
fusion strategies still struggle with synchronization of signals across
modalities and cultural variability in emotion expression [ 11 ]. Data privacy
concerns are particularly acute in mental health, where sensitive personal
information is involved [ 29 ]. Moreover, most studies focus on short-term
outcomes, with a dearth of longitudinal evidence on sustained impact [ 13 ].
Addressing these challenges will be key for Emotion AI to move from
proof-of-concept to a globally scalable, clinically trusted tool.


##### 3 Organization of the Chapter

The chapter is organized to ensure a logical flow from conceptual
foundations to applied insights, enabling readers to build an in-depth
understanding of Emotion AI and its clinical significance. Section 3
provides a detailed discussion on data quality, annotation strategies, and
benchmarking, highlighting the comparative merits of clinician-annotated
datasets, crowdsourced labelling approaches, and hybrid frameworks for
improving inter-rater reliability and dataset representativeness [ 34 ]. Section
4 outlines the adopted methodology, including the systematic literature
review protocol, multimodal conceptual framework integrating text, speech,
visual, and physiological signals, as well as key evaluation metrics such as
F1-score, precision, recall, and clinical endpoints (PHQ-9, GAD-7) [ 26 ,
27 ]. Section 5 focuses on multilingual and cross-cultural considerations,
presenting current challenges in developing culturally adaptive systems and
discussing multilingual transformer-based approaches for cross-lingual
generalization [ 24 ]. Section 6 explores deployment architectures,
contrasting cloud-based and on-device processing, and proposing hybrid
edge-cloud solutions that balance privacy, scalability, and real-time
performance [ 25 , 34 ]. Section 7 addresses security threats, adversarial
attacks, and ethical concerns, including model vulnerability to data
poisoning, deepfake manipulation, and compliance with global privacy
regulations such as GDPR and HIPAA. Section 8 synthesizes emerging
trends and future research opportunities, emphasizing personalization,
explainability, and the need for federated, privacy-preserving approaches
that ensure fairness and global applicability [ 35 ]. The chapter concludes
with observations and future directions that integrate technical, ethical, and
societal insights, offering a roadmap for researchers and practitioners
seeking to design trustworthy, human-centered Emotion AI systems. The
Figure 2 illustrates the foundational dimensions essential for developing
Emotion AI systems that are effective, ethical, and inclusive (Fig. 1 ).



**_Fig. 1_** End-to end pipeline for emotion AI in mental health

**_Fig. 2_** Core attributes in emotion AI

##### 4 Methodology

**4.1 Systematic Review Protocol**

This chapter adopts a systematic literature review approach guided by the
PRISMA 2020 framework to ensure methodological transparency and
reproducibility. A thorough search was carried out across several major
scientific databases, including PubMed, Scopus, IEEE Xplore, ACM Digital
Library, and arXiv. Search strings were designed to cover a broad spectrum


of the field, using keywords and phrases such as “emotion recognition,”
“affective computing,” “mental health AI,” “transformer models,” “digital
therapeutics,” “CBT chatbots,” and “multimodal analysis.” Boolean
operators were applied strategically to refine search results and target
studies published between 2020 and 2025.

Inclusion Criteria:

```
Studies involving human participants.
Research utilizing one or more emotion detection modalities (e.g., text,
speech, facial expressions, physiological signals).
Articles reporting measurable or clinical outcomes.
Original research with clear and reproducible methodology.
```
Exclusion Criteria:

Studies not involving human participants.
Review papers or articles lacking primary data.
Duplicate publications.
Papers with insufficient methodological transparency or reproducibility.
After systematic screening of titles, abstracts, and full texts, a total of
153 high-quality studies were retained for analysis. A PRISMA flow
diagram depicts the search and selection process, ensuring clarity and
replicability. This systematic review forms the foundation for the
conceptual framework in this chapter, ensuring that the proposed model
design is grounded in the most current and relevant evidence.

**4.2 Conceptual Multimodal Framework**

The proposed framework emphasizes the integration of multiple modalities
to capture the complex and nuanced nature of human emotions, particularly
within mental health contexts. No single modality alone is sufficient to
accurately and reliably infer psychological states, as emotions manifest
across linguistic, vocal, facial, and physiological domains.
Textual inputs, derived from clinical notes, patient journaling, and
chatbot interactions, are analyzed using transformer-based language models
such as BERT and RoBERTa, which enable deep contextual understanding
of semantic and emotional content. Speech inputs are processed using
Wav2Vec2.0 embeddings and recurrent neural network architectures to


extract prosodic and paralinguistic features indicative of emotional states.
The modalities' heterogeneous features are combined through a fusion layer
with attention mechanisms that dynamically weight each input as a function
of its contextuality and reliability. This multimodal processing facilitates
stronger and more robust emotion recognition since it addresses the
heterogeneous nature in which individuals express psychological states. The
proposed multimodal emotion AI pipeline with preprocessing, feature
extraction, fusion, and clinically actionable outputs is presented in Fig. 1.

**4.3 Evaluation Metrics**

Assessment is addressed on both technical and clinical levels to make sure
that the suggested system not just works correctly but also yields relevant
results for mental health treatment. Technical assessment measures are
accuracy, precision, recall, F1-score, area under the receiver operating
characteristic curve (AUC-ROC), and Matthews correlation coefficient, all
of which together give a complete picture of performance in classification.
Clinical evaluation involves standardized mental health assessments
such as PHQ-9 for depression, GAD-7 for anxiety, and PSS-10 for stress,
which are applied before and after system interactions. User activity metrics
such as session duration, adherence rate, and attrition are tracked in an
attempt to establish the feasibility and acceptability of deployment in actual
settings. Together, these levels of evaluation ensure that the system is not
just technologically effective but also clinically significant.

**4.4 Embedded Ethics and Privacy-by-Design**

Ethics and privacy protection are infused throughout the architecture.
Federated learning enables models to be trained across different clinical
sites without sending sensitive patient data, protecting patient
confidentiality. Differential privacy protocols introduce noise into data sets
in a controlled fashion, further reducing the risk of re-identification.
Explainability techniques such as SHAP and LIME are built-in to provide
explainable outputs, enabling clinician monitoring and regulatory
compliance. Bias is thoroughly scrutinized over demographic subgroups to
prevent systematic prediction and treatment recommendation flaws. The
system is GDPR, HIPAA, and OECD AI principle compliant to provide
ethical requirements and legal compliance. With ethics and privacy in mind


during design, the framework facilitates responsible deployment in all
global environments (Fig. 3 ).

```
Name of paper Publication Paper
summary
```
```
Performance
evaluation
```
```
Advantages Disadvantages
```
```
Multimodal
emotion
recognition for
mental health
monitoring: a
comprehensive
review
```
```
Frontiers in
Digital
Health,
2024.
https:// doi.
org/ 10.
3389/ fdgth.
2024
```
```
This paper
reviews
state-of-the-
art
multimodal
emotion
recognition
systems,
including
facial
expression
analysis,
speech
prosody,
physiological
signals
(EEG, HRV),
and text
sentiment for
mental health
applications.
It discusses
emotion AI’s
role in early
screening of
depression,
anxiety, and
stress, real-
time
monitoring
through
wearables,
and AI-
enabled
interventions
```
```
The
performance
assessment in
the paper is
tested on
various
important
points. AI
techniques are
assessed for
their
effectiveness in
curbing manual
labour for
planetary data
processing,
precision in
matching or
surpassing
conventional
analysis
techniques, and
ability to
analyze data in
real-time
during
missions.
Techniques are
also judged for
scalability in
analyzing large
amounts of data
and the
significance of
open-source
training data to
ensure ongoing
improvement
Additionally,
the evaluation
emphasizes the
need for
interaction
```
```
Accuracy is
improved, with
AI often
matching or
surpassing
traditional
methods. AI
also enables
real-time
analysis for
timely
decisions
during
missions and is
highly scalable
to handle large
datasets. The
use of open-
source data
fosters
continuous
improvement,
and
collaborations
between
academia and
industry drive
innovation
```
```
The complexity of
AI models can
make them hard to
interpret, and
significant
computational
resources are
needed.
Integration into
existing
workflows can be
difficult, requiring
specialized skills
Additionally,
ethical and
privacy concerns
arise with data
usage and
potential biases in
AI
Algorithms
```

**Name of paper Publication Paper
summary**

```
Performance
evaluation
```
```
Advantages Disadvantages
```
```
between
academia and
industry to
develop AI-
influenced
```
**Emotion-
aware
conversational
agents for
depression
and anxiety
support**

```
Journal of
Medical
Internet
Research
(JMIR),
2025.
10.2196
```
```
This paper
explores the
design and
evaluation of
chatbots and
virtual
therapists
that integrate
emotion AI
for
empathetic,
context-
aware
responses.
The study
analyzes
Natural
Language
Processing
(NLP)-based
emotion
recognition
from user
input and
adaptive
dialogue
strategies to
support
individuals
experiencing
depression
and anxiety
```
```
AI techniques
are evaluated
for efficiency
in autonomous
navigation,
accuracy in
health
monitoring, and
operational
management.
Highlighted
strengths
include
handling large
datasets, real-
time decision-
making, and
improved
communication
security with
Blockchain
```
```
Enhances
efficiency,
accuracy, and
real-time
decision-
making;
automates
complex tasks;
improves
operational
management;
supports large-
scale data
analysis;
strengthens
communication
and data
security with
IoT and
Blockchain
integration
```
```
Relies on high-
quality data; AI
models are
complex and
computationally
demanding;
integration into
workflows is
challenging;
requires
specialized skills;
legal and ethical
issues around data
use and
algorithmic bias
```
**AI-driven
stress
detection
using
wearable
biosensors and
multimodal
data fusion**

```
IEEE
Transactions
on Affective
Computing,
2023.
https:// doi.
org/ 10.
1109/
TAFFC.
2023
```
```
This paper
explores the
application
of advanced
machine
learning
techniques
for exoplanet
detection and
habitability
```
```
The paper
assesses ML
effectiveness in
exoplanet
detection using
models like
neural networks
and ensemble
methods. It
highlights
```
```
Combining
real and
synthetic data
with CNNs
improves
detection
accuracy,
reduces false
positives, and
enhances
```
```
Requires high
computational
resources; risk of
overfitting with
synthetic data;
creation/validation
of synthetic data
is time-
consuming;
effectiveness
```

**Name of paper Publication Paper
summary**

```
Performance
evaluation
```
```
Advantages Disadvantages
```
```
analysis. It
discusses
exoplanet
detection,
habitable
zone
analysis, AI-
driven data
processing,
robotic
exploration,
and mission
planning.
The
integration
of ML
algorithms
enhances
accuracy,
efficiency,
and precision
in
identifying
potentially
habitable
planets
```
```
improved
accuracy,
reduced false
positives, and
superior
handling of
astronomical
datasets
compared to
traditional
methods. ML
approaches
enhance
exoplanet
characterization
and reliability
of detection
```
```
robustness.
Scalable for
large datasets,
adaptable to
multiple
detection
scenarios, and
supports
optimized
mission
planning with
precise
observational
instruments
```
```
depends on data
quality; deep
learning models
are complex and
need specialized
expertise; ongoing
maintenance and
risk of algorithmic
bias affect
reliability
```

**_Fig. 3_** The progress cycle of a general research methodology

##### 5 Data Quality, Annotation, and Benchmarking

##### in Emotion AI

The reliability and effectiveness of emotion recognition systems
fundamentally depend on the quality of the datasets used for training and
evaluation. Data quality directly influences the performance,
generalizability, and fairness of machine learning models. In Emotion AI,
where the subjectivity of human emotional states poses additional


challenges, ensuring precision in data collection and labeling becomes even
more critical [ 34 ].

**5.1 Clinician-Annotated vs. Crowdsourced Labels**

Clinician-annotated datasets remain the gold standard, particularly for
applications involving mental health, affective disorders, and psychological
intervention. Clinical professionals are skilled at identifying subtle
emotional signals and understanding comorbidities, making their
annotations especially trustworthy. Research consistently shows that
clinician-labeled emotion corpora enable the development of models with
markedly higher diagnostic value, especially in multimodal frameworks
where speech prosody, facial microexpressions, and physiological
indicators are integrated [ 27 ].An illustrative example is the IEMOCAP
corpus, a widely used multimodal emotion recognition dataset containing
audio, visual, and textual data.

Strengths:

Rich multimodal recordings, detailed emotion categories, and high-quality
expert annotations make it ideal for training complex models.

Weaknesses:

The dataset is relatively small compared to crowdsourced alternatives and
may not fully capture cross-cultural variability.
On the other hand, crowdsourced data offer scalability and demographic
diversity, which are advantageous for building models that generalize well
across populations. Platforms like Amazon Mechanical Turk and Prolific
have been extensively used to collect emotional labels for speech, text, and
image stimuli. However, variability in annotator expertise and subjective
interpretations of emotion can lead to inconsistencies and increased noise.
To address these issues, hybrid approaches are often adopted, wherein
clinicians perform initial annotations and crowdworkers validate or expand
the dataset under strict quality control protocols [ 24 ]. This approach
preserves the accuracy of clinical labeling while ensuring the dataset
remains sufficiently large to meet the demands of deep learning
applications.


**5.2 Inter-Rater Reliability and Quality Control**

Consistency between annotators is a critical measure of dataset reliability.
Inter-rater reliability metrics such as Cohen’s Kappa, Fleiss’ Kappa, and
Krippendorff’s Alpha are often utilized to determine inter-rater agreement
[ 35 ]. High Kappa value signifies that the agreement is over chance levels,
which particularly matters for emotion categories that are susceptible to
subjective interpretation, e.g., “disgust” or “confusion.” State-of-the-art
pipelines include iterative back loops where the annotators are regularly
updated to keep their labeling rules in sync during the annotation process
[ 25 ]. Computer programs are also increasingly used to detect anomaly
labels that can distort the distribution of the dataset. In high-risk
applications, like health-related emotion recognition, redundant ratings
from multiple evaluators are combined using consensus-based or
probabilistic model techniques to enhance reliability [ 28 ].

**5.3 Benchmarking Practices**

Benchmarking enables standardized comparison of models, ensuring
reproducibility and scientific progress. Historically, datasets such as
IEMOCAP, DEAP, and EmoDB have been widely used; however, they
often suffer from limited linguistic and cultural diversity, potentially biasing
model performance toward certain populations [ 26 ].
Emerging datasets, such as the E-THER corpus, are addressing this gap
by introducing psycholinguistically grounded annotation protocols,
balanced demographic representation, and higher inter-rater agreement
thresholds [ 24 ]. Benchmarking is evolving beyond static accuracy metrics
to include measures of fairness, robustness under noise, and cross-domain
generalization, which are now seen as critical for real-world deployment
[ 35 ].

##### 6 Robustness, Domain Adaptation, and Real-

##### World

Emotion AI models are notoriously sensitive to domain shifts,
environmental noise, and cultural variations. Therefore, robustness and
adaptability are essential to achieving reliable real-world performance.


**6.1 Cross-Cultural Generalization**

Emotional expression is highly determined by social and cultural norms.
For instance, studies indicate that people with collectivist cultures tend to
suppress negative emotional expression in public while those with
individualist cultures tend to express such emotions freely [ 23 ]. Models
learned only from Western data sets tend to misclassify emotions in non-
Western populations as much as 35% of the time in some instances
[ 34 ].Domain adaptation methods, such as adversarial learning and transfer
learning, are now commonly used to close these gaps. For example,
English-language corpora-trained models can be transferred to do well on
Hindi, Mandarin, or Arabic data using unsupervised domain adaptation,
which aligns feature distributions between domains without the need for
costly labeled data.

**6.2 Noise Handling and Signal Variability**

Real-world deployment of Emotion AI has to work in acoustically and
visually noisy conditions. For speech-based emotion recognition, there is
potential for background noise to mask prosodic features like pitch and
intensity, reducing model quality. Data augmentation using synthetic noise,
spectral masking, and resilient feature extraction practices like Mel-
frequency cepstral coefficients (MFCCs) have been used to improve
significantly in noisy situations. Variation in illumination, occlusion, and
camera resolution is a challenge to facial expression recognition in
computer vision.

**6.3 Adversarial Robustness**

Adversarial attacks pose unique risks to Emotion AI systems, particularly in
security-sensitive applications such as driver monitoring or surveillance.
Perturbations imperceptible to the human eye can cause models to
misclassify emotions entirely, potentially leading to dangerous outcomes.
Countermeasures include adversarial training, where the model is explicitly
trained on adversarial examples, and ensemble modeling, which reduces
vulnerability by combining multiple classifiers.


##### 7 Multilingual and Cross-Cultural

##### Considerations

Emotion is a universal phenomenon but is expressed differently across
cultures, languages, and contexts. Building Emotion AI systems that are
equitable and globally relevant requires going beyond English-centric
datasets and embracing cultural heterogeneity. Building Emotion AI that is
equitable, inclusive, and effective on a global scale, thus requires breaking
away from English-biased corpora and including culturally rich emotional
patterns.
A direct example of this is available in research that employs the
IEMOCAP corpus combined with emotion datasets gathered in East Asian
and Middle Eastern countries. Models that are specifically trained on
English-language corpora have performed well in classifying emotions in
Western populations but do not perform nearly as well when tested with
speakers from other cultures. E.g., the facial expressions associated with
“surprise” in one culture can be interpreted as “confusion” in another, and
vocal prosody associated with “anger” in English may not convey the same
emotional significance in other languages. In one case study, combining
Western and East Asian data enhanced the accuracy of recognition across
cultures by more than 15%, underscoring the value of representative
datasets from different cultures. This evidence underlines that emotion is
not only biologically based but also socially and culturally constructed.
Therefore, cross-cultural calibration becomes necessary to develop emotion
recognition systems that function robustly across global communities.

**7.1 Need for Diverse Datasets**

Current research has shown that over 70% of emotion recognition models
are trained primarily on English-language datasets, often collected from
Western, educated, industrialized, rich, and democratic (WEIRD)
populations. This creates a serious bias; as emotional expression varies
widely across geographies. For instance, research shows that in collectivist
cultures, emotional expressions like anger may be suppressed in public,
whereas in individualistic cultures they are more openly displayed.

**7.2 Multilingual Modeling Strategies**


Transformers like XLM-R and mBERT have been for multilingual emotion
recognition tasks. They are trained to acquire common representations for
languages, facilitating transfer learning from low-resource to high-resource
languages. More recent methods use adapter-tuning and prompt-based
learning, enabling models to preserve language-specific details while
sharing parameters for scale.
A recent large-scale experiment showed that fine-tuning multilingual
transformer models on code-mixed data enhanced performance on Hindi-
English emotion classification tasks by 18%, which shows the promise of
these approaches for real-world use.

**7.3 Cross-Cultural Evaluation**

Assessment of emotion AI cannot depend entirely on technical measures
such as accuracy and F1-score. Culturally respectful assessment protocols
are needed. Some studies proposed context-aware measures, e.g.,
incorporating cultural raters from geographically diverse regions, to avoid
overfitting against Western emotional standards.

##### 8 Deployment Architecture and Edge AI

Deploying Emotion AI solutions in clinical and real-world settings
introduces engineering and operational challenges that go beyond
algorithmic performance.

**8.1 Cloud Versus On-Device Processing**

Cloud-based solutions offer virtually unlimited computational resources,
making them ideal for deep learning models that require high memory and
processing power. However, they introduce latency and raise concerns
about data privacy, particularly when dealing with sensitive mental health
information governed by GDPR or HIPAA.
On-device processing is often referred to as Edge AI and provides low-
latency, privacy-preserving solutions by processing data locally on
smartphones, wearables, or IoT devices. However, model compression and
quantization are required to fit large neural networks on devices with
limited computational capabilities.

**8.2 Hybrid Edge-Cloud Models**


The most promising deployment approach combines edge and cloud
infrastructure. In this hybrid architecture, initial inference is performed
locally for real-time response, while more complex analytics and periodic
model updates occur in the cloud. This architecture ensures privacy,
minimizes bandwidth usage, and allows for personalized model refinement
over time.
Figure 2 illustrates a typical hybrid deployment model, where feature
extraction occurs at the edge, while the cloud provides federated learning
updates and aggregates population-level insights. This architecture has been
adopted in recent EU-funded digital mental health pilots and has shown
improved user engagement and adherence.

**8.3 Scalability and Lifecycle Management**

Real-world deployments require lifecycle management frameworks that
include continuous monitoring, performance drift detection, and automatic
retraining pipelines. Federated learning and continual learning have
emerged as solutions to ensure that models remain up to date without
centralizing sensitive user data.

##### 9 Security Threats and Adversarial Attacks

Emotion AI systems, because they are multimodal and based on sensitive
personal information, are confronted with a broad range of security, privacy,
and ethical issues. Being robust and trustworthy is paramount, especially
when applied to mental health, where mistakes or violations can be dire.

**9.1 Deepfake Risks and Manipulation of Emotional Data**

Deepfakes and synthetic media represent one of the most critical emerging
threats to emotion AI. By fabricating facial expressions, altering speech
intonation, or simulating physiological responses, malicious actors can
deceive detection systems, leading to inaccurate emotional interpretations
and potentially harmful outcomes [ 28 , 34 ]. For instance, in a widely
reported case involving Deeptrace, deepfake technology was used to
convincingly mimic a CEO’s voice, enabling a fraudulent transfer of funds
—demonstrating how easily synthetic cues can manipulate trust-based
systems. In a similar vein, a telepsychiatry platform could mistakenly


classify a synthetic smile as genuine positive affect, skewing therapeutic
recommendations.
Current countermeasures rely on multi-layered defense mechanisms.
Artifact-based detection identifies inconsistencies at the pixel or frame
level, while statistical and temporal pattern analysis detects unnatural
movements in facial and physiological signals [ 28 ]. More recently,
generative models have been repurposed for defense: detectors trained to
recognize the subtle artifacts left behind by deepfake algorithms
significantly increase detection accuracy.

**9.2 Data Poisoning and Model Integrity**

Data poisoning attacks constitute the intentional introduction of false or
corrupted data into training pipelines with the aim of lowering model
performance or injecting concealed biases.
In mental health AI, manipulation would, for instance, lead depressive
markers to be labeled as neutral or positive, leading to perilous delay in care
[ 26 ]. An example of this threat in the real world was when researchers
showed that targeted data poisoning could be executed on an image
classifier model, successfully changing predictions without significant
degradation to overall accuracy—emphasizing how subtle manipulations
can be. Protective approaches focus on ongoing monitoring of training data,
adversarial training protocols, and human-in-the-loop validation.
Furthermore, certified defenses and resilient optimization methods offer a
mathematical safety margin, ensuring that outputs of the model are stable
even with adversarial contamination [ 26 ].

**9.3 Privacy and Ethical Compliance**

Emotion AI works with naturally sensitive information, like facial
expressions, voice recordings, bodily signals, and text messages. Global
privacy legislations like GDPR and HIPAA need to be adhered to [ 10 , 25 ].
Ethical deployment of AI also includes open consent mechanisms, clear
communication about data usage, and safe storage facilities to protect
against unauthorized use.
Federated learning and differential privacy measures are emerging as
top solutions. Federated learning provides local training on end-user
devices without revealing raw data to central servers, and differential
privacy adds controlled noise to avoid identification of individuals [ 11 , 29 ].


With integrated XAI layers, these solutions ensure model decisions remain
traceable and interpretable [ 35 ].

**9.4 Regulatory and Cultural Considerations**

Global deployment of emotion AI is subject to care regarding local
regulation and norms. For instance, facial recognition represents use of
biometric information and could be banned in Europe, whereas data sharing
policy exists in the United States and Asia [ 23 , 35 ]. Culturally sensitive
measurement is required: the same physiological signal may contain
different emotional content across cultures [ 5 , 40 ]. Ignorance of these
differences may reinforce biases and reduce the efficacy of interventions.

**9.5 Human Oversight and Governance**

Ensuring ethical deployment involves governance mechanisms that engage
clinicians, ethicists, and AI specialists. Automated applications in sensitive
fields can be prevented from taking place through oversight interventions
such as regular audits, ethical review committees, and fail-safe provisions.
Ongoing monitoring ensures conformity of AI systems with human values
and regulatory requirements across their lifecycle [ 24 , 35 ].

##### 10 Emerging Trends and Future Directions

Emotion AI is changing fast day by day with massive developments in
multimodal integration and strategies for global level deployment. This
section describes the nascent trends defining the newer days.

**10.1 Multimodal Emotion AI Integration**

Enabling the integration of several data streams—text, speech, facial
emotions, and physiological signals—increases the accuracy and reliability
of the detection of emotion. Newer architectures utilize transformer-based
models for text (BERT, RoBERTa), speech (Wav2Vec2.0), and vision
(Vision Transformers) through the use of cross-modal attention mechanisms
to better fuse features [ 27 , 34 ]. The integration thus allows for context-
sensitive prediction, detecting fine-grained emotional state nuances.

**10.2 Personalization and Context Awareness**


Personalization involves learning user-specific emotional baselines and
accounting for environmental or situational context. For instance, an AI
system may recognize that a user's tone of voice changes during late-night
conversations, indicating fatigue rather than negative affect [ 24 ]. Context-
aware emotion AI can significantly improve therapeutic outcomes by
reducing false positives and enhancing user engagement.

**10.3 Explainable and Transparent AI**

Transparency in decision-making is critical for trust, especially in
healthcare applications. Techniques such as Layer-wise Relevance
Propagation (LRP) and attention visualization allow clinicians and users to
understand which features influenced AI predictions [ 35 ]. Explainable AI
facilitates accountability, supports ethical compliance, and improves
adoption by mental health professionals.

**10.4 Edge AI and Scalable Deployment**

Real-time, large-scale deployment of emotion AI requires hybrid edge-
cloud architectures. Edge devices handle immediate inference, reducing
latency and preserving privacy, while cloud servers manage model updates,
analytics, and aggregation of anonymized data [ 9 , 34 ]. Scalable deployment
ensures continuous learning, adaptation to new populations, and
responsiveness to emerging global mental health needs.

**10.5 Global Adaptation and Multilingual Capabilities**

To achieve worldwide impact, emotion AI must handle linguistic diversity
and cultural variability. Multilingual transformers and code-mixed language
models facilitate emotion recognition across languages with limited
resources [ 8 , 38 ]. Cross-cultural benchmarking ensures equitable
performance, avoiding biases that could exacerbate disparities in mental
health care [ 19 , 21 , 24 ].

**10.6 Responsible and Ethical AI for Global Health**

Looking forward, responsible AI frameworks will become central to
emotion AI. Ethical principles, privacy-by-design, fairness, and inclusivity
must guide the model development and deployment [ 24 , 35 ]. Collaboration
between policymakers, researchers, clinicians, and patient communities will
be essential to create robust, trustworthy, and globally relevant systems.


##### 11 Challenges in Future Prospects

Despite the remarkable progress in Emotion AI for mental health, several
challenges remain that may impede its large-scale deployment and
effectiveness.

**11.1 Data Heterogeneity and Bias**

Datasets in emotion AI often suffer from heterogeneity and limited
representativeness. Variations in cultural norms, age groups, and socio-
economic backgrounds can introduce biases in model predictions, reducing
generalizability [ 24 , 26 ]. Models trained predominantly on Western
populations, for instance, may underperform in Asian or African contexts.
Addressing such biases requires careful dataset curation, cross-cultural
benchmarking, and domain adaptation strategies.

**11.2 Real-World Deployment Challenges**

Deploying multimodal emotion AI in clinical or telehealth environments
involves technical, operational, and social challenges. Real-time inference,
sensor calibration, and variable environmental conditions can affect signal
quality and model performance [ 35 ]. Furthermore, user engagement over
long periods remains difficult; individuals may experience fatigue, privacy
concerns, or distrust toward AI interventions.

**11.3 Integration with Existing Healthcare Systems**

Integrating AI-driven emotional assessments into existing healthcare
workflows is non-trivial. Clinicians require actionable insights rather than
raw outputs, necessitating intuitive interfaces, explainable outputs, and
seamless interoperability with electronic health records [ 23 ]. Resistance to
change and lack of AI literacy among healthcare providers can further slow
adoption.

##### 12 Ethical Considerations and Challenges

Ethics remains central to the deployment of Emotion AI, especially in
sensitive domains like mental health. Missteps can have profound
implications for privacy, trust, and societal impact.


**12.1 Privacy and Data Protection**

Emotion AI systems collect sensitive biometric, behavioral, and textual
data. Failure to secure this information can lead to breaches, misuse, or
unauthorized profiling [ 10 , 11 ]. Techniques such as federated learning,
differential privacy, and end-to- end encryption are essential to safeguard
user information while preserving model efficacy [ 25 , 29 ].

**12.2 Fairness and Equity**

AI models can inadvertently reinforce societal biases. Gender, racial, and
cultural biases in datasets may result in inequitable predictions, affecting
care outcomes [ 24 , 34 ]. Continuous auditing, fairness-aware training, and
cross-cultural evaluation are necessary to ensure equity in global
deployments.

**12.3 Transparency and Explainability**

Trust in AI is closely tied to explainability. clinicians and patients must
understand how decisions are made, particularly when these influence
therapeutic interventions. Explainable AI techniques—such as attention
visualization, feature attribution, and causal inferences are critical for
transparency and accountability [ 35 ].

**12.4 Regulatory and Societal Considerations**

Global adoption requires adherence to diverse regulatory frameworks. For
instance, GDPR in Europe, HIPAA in the United States, and emerging AI
regulations in Asia impose constraints on data collection, storage, and
algorithmic decision-making [ 23 , 35 ]. Beyond regulations, societal
acceptance hinges on ethical deployment, cultural sensitivity, and active
involvement of stakeholders in co-designing AI systems.

##### 13 Observations

From the synthesis of current literature and practice, several observations
emerge:

```
Multimodal integration enhances accuracy: Combining text, speech,
facial, and physiological modalities provides richer emotional context,
improving the reliability of AI predictions [ 27 , 34 ]
```

Personalization is essential: Accounting for individual emotional
baselines and cultural norms significantly improves engagement and
efficacy in therapeutic contexts [ 24 ].
Robustness and resilience remain challenges: Systems must handle noise,
adversarial attacks, and environmental variability to maintain clinical
utility [ 26 ].
Ethics and governance are non-negotiable: Privacy, explainability, and
fairness are critical for trust and sustainable deployment [ 10 , 24 ].
These observations suggest that while Emotion AI has transformative
potential for global mental health, careful design, validation, and
governance are prerequisites for real-world impact.

##### 14 Conclusion

Emotion AI is reshaping how mental health is understood, monitored, and
supported, moving far beyond traditional evaluation techniques toward
intelligent, adaptive systems. By leveraging multimodal data—capturing
vocal tone, facial dynamics, textual expression, and physiological signals—
advanced algorithms can reveal nuanced emotional states that standard
assessments may overlook. This enables timely, scalable mental health
support that can be personalized to each individual’s context and needs.
However, realizing the full potential of these technologies requires
confronting persistent challenges. Ensuring the quality and inclusiveness of
datasets is essential to prevent misclassification and inequitable outcomes.
Ethical and privacy concerns must be addressed with robust safeguards that
protect sensitive personal information. Just as importantly, integrating AI
into real-world clinical workflows demands careful alignment with existing
medical practices and regulatory standards. Only through rigorous
validation, ethical governance, and thoughtful design can Emotion AI
emerge as a trusted and effective component of mental health care.

**14.1 Future Directions**

Global-scale, culturally diverse datasets: Efforts should focus on creating
datasets that encompass linguistic, cultural, and socio-economic diversity,
enabling fair and accurate models globally [ 24 , 26 ]. Efforts should focus on
creating global-scale, culturally diverse datasets that encompass linguistic,


cultural, and socio-economic diversity, enabling fair and accurate models
worldwide [ 24 , 26 ].
Among the various directions, building culturally and linguistically
diverse datasets stands out as the most urgent recommendation for
researchers. Without inclusive and representative data, even the most
advanced models risk reinforcing existing inequities and failing to
generalize effectively across populations [ 24 , 26 ]. Expanding dataset
diversity will provide the foundation needed for fair, accurate, and globally
relevant emotion recognition systems.

Federated and privacy-preserving AI: Techniques such as federated
learning, differential privacy, and on-device processing can enhance
model performance while safeguarding sensitive personal information
[ 25 , 29 ].
Explainable and human-centric AI: Transparent and interpretable systems
will support clinician trust and ensure responsible use in mental health
settings [ 35 ].
Longitudinal clinical validation: Real-world studies are needed to assess
efficacy, adherence, and long-term psychological outcomes across
diverse populations [ 14 , 39 ].
Ethical frameworks and regulation: Establishing clear, collaborative
ethical guidelines aligned with local and global standards will be critical
for responsible and sustainable deployment [ 23 , 35 ].
By prioritizing dataset diversity, researchers can create a strong,
equitable foundation upon which privacy-preserving methods,
explainability, clinical validation, and ethical governance can be effectively
built.

##### References

1. Wu, Y., Mi, Q., Gao, T.: A comprehensive review of multimodal emotion recognition:
    techniques, challenges, and future directions. Biomimetics **10** (7), 418 (2025). https:// doi. org/ 10.
    3390/ biomimetics10070 418
2.
    Farhadipour, A., Ranjbar, H., Chapariniya, M., Vukovic, T., Ebling, S., Dellwo, V.: Multimodal
    emotion recognition and sentiment analysis in multi-party conversation contexts. (2025). https://
    doi. org/ 10. 48550/ arXiv. 2503. 06805


3.
World Health Organization.: Mental Health: Fact Sheet. WHO (2023). https:// [http://www.](http://www.) who. int/
news-room/ fact-sheets/ detail/ mental-health-strengthening-our-respond
4.
World Health Organization.: Mental Health at Work. WHO (2022). https:// [http://www.](http://www.) who. int/ teams/
mental-health-and-substance-use/ mental-health-in-the-workplace
5.
Tahir, S., Johnson, J., Abu-Khalaf, J., Shah, S.A.A.: E-THER: a PCT-grounded dataset for
benchmarking empathic AI. arXiv preprint (2025). https:// arxiv. org/ abs/ 2509. 02100
6.
Patil, A., Laxman, S.: indiDataMiner at SemEval-2025 Task 11: from text to emotion. ACL
Anthol. (2025). https:// aclanthology. org/ 2025. semeval-1. 262
7.
SentinelOne Research Team.: Generative AI security risks: mitigation & best practices.
SentinelOne Res. (2025). https:// [http://www.](http://www.) sentinelone. com/ blog/ generative-ai-security-risks
8.
Wang, P., Zhang, L., Chen, M.: Cross-culture Multimodal Emotion Recognition with Adversarial
Learning. ResearchGate. (2025). https:// [http://www.](http://www.) researchgate. net/ publication/ 372929328
9.
Cisco Outshift Security Team.: How to Detect and Mitigate AI Data Poisoning. Outshift by Cisco
(2025). https:// outshift. cisco. com/ blog/ ai-data-poisoning-detect-mitigate
10.
Edge AI vs Cloud AI - Understanding the Trade-offs in Distributed AI Systems. LinkedIn
Articles. (2025). https:// [http://www.](http://www.) linkedin. com/ pulse/ edge-ai-vs-cloud-ai
11.
American Technology Blog Team.: AI Edge Computing: Running AI on Low-Power Devices.
American Technology Blog. (2025). https:// americantechblog. com/ ai-edge-computing
12.
Zhao, S., Ren, J., Zhou, X.: Cross-modal gated feature enhancement for multimodal emotion
recognition. Sci. Rep. (2025). https:// [http://www.](http://www.) nature. com/ articles/ s41598-025-87804
13.
U.S. Department of Homeland Security.: Risks and Mitigation Strategies for Adversarial
Artificial Intelligence. U.S. Department of Homeland Security (2025). https:// [http://www.](http://www.) dhs. gov/ ai-
security/ adversarial-risks
14.
Chen, C., Lam, K.T., Yip, K.M., So, H.K., Lum, T.Y.S., Wong, I.C.K., Yam, J.C., Chui, C.S.L.,
Ip, P. (2025). Comparison of an AI chatbot with a nurse hotline in reducing anxiety and
depression levels in the general population: pilot randomized controlled trial. JMIR Hum. Fact.
**12** , e65785. https:// doi. org/ 10. 2196/ 65785
15.
The New Stack Editorial Team.: AI at the Edge: Architecture, Benefits and Tradeoffs. The New
Stack (2025). https:// thenewstack. io/ ai-at-the-edge-architecture-benefit
16.
Wu, C., Cai, Y., Liu, Y., Zhu, P., Xue, Y., Gong, Z., Hirschberg, J., Ma, B.: Multimodal emotion
recognition in conversations: a survey of methods, trends, challenges and prospects. arXiv.
(2025). https:// arxiv. org/ abs/ 2505. 2051
17.
Baevski, A., Zhou, H., Mohamed, A., Auli, M.: wav2vec 2.0: a framework for self-supervised
learning of speech representations. arXiv preprint (2020). https:// arxiv. org/ abs/ 2006. 11477
18.
Chatbots and mental health: a scoping review of reviews. Curr. Psychol. (2025). https:// doi. org/

10. 1007/ s12144-024-06097-0


19.
Deepfakes, Censorship, and Data Poisoning. DrStevenAWright.com. 2025. https:// [http://www.](http://www.)
drstevenawright. com/ deepfakes-censorship-and-data-poisoning
20.
Devlin, J., Chang, M.-W., Lee, K., Toutanova, K.: BERT: Pre-training of Deep Bidirectional
Transformers for Language Understanding. arXiv preprint. 2019. https:// arxiv. org/ abs/ 1810.
04805
21.
Domain Adaptation for Bias Mitigation in Affective Computing. SpringerLink (2024). https://
doi. org/ 10. 1007/ s00521-024-08721-9
22.
Jaiswal, M., Pandey, S.: Deep learning models for EEG-based depression detection: a systematic
review. IEEE Access **12** , 58642–58659 (2024). https:// doi. org/ 10. 1109/ ACCESS. 2024. 3371429J
23.
Jaiswal, M., Pandey, S.: Deep learning models for EEG-based depression detection: a systematic
review. IEEE Access **12** , 58642–58659 (2024). https:// doi. org/ 10. 1109/ ACCESS. 2024. 3371429
[Crossref]
24.
Karyotaki, E., Cuijpers, P., et al.: Guided internet-based cognitive behavioral therapy for
depression: meta-analysis of individual participant data. Lancet Psychiatry **8** (10), 914–924
(2021). https:// doi. org/ 10. 1016/ S2215-0366(21)00249-2
[Crossref]
25.
Layegh, S.D., Rabiei, M., et al.: Real-time stress detection using smartwatches: a multimodal
dataset and benchmark. Sensors **23** (3), 1452 (2023). https:// doi. org/ 10. 3390/ s23031452
26.
Layegh, N., Deldari, S., Rabiei, M., et al.: Real-time stress detection using smartwatches: a
multimodal dataset and benchmark. Sensors **23** (3), 1452 (2023). https:// doi. org/ 10. 3390/
s23031452
[Crossref]
27.
Low, D.M., Rumker, L., Talkar, T., Torous, J., Cecchi, G.A., Ghosh, S.S.: Natural language
processing for mental health: systematic review. JMIR Ment. Health **7** (5), e17984 (2020). https://
doi. org/ 10. 2196/ 17984N
28.
Low, D.M., Rumker, L., Talkar, T., Torous, J., Cecchi, G.A., Ghosh, S.S.: Natural language
processing for mental health: systematic review. JMIR Ment. Health **7** (5), e17984 (2020). https://
doi. org/ 10. 2196/ 17984
[Crossref]
29.
Moving AI to the Edge: Benefits, Challenges and Solutions. Red Hat Research Blog (2025).
https:// research. redhat. com/ ai-edge
30.
Patel, V., Saxena, S., Lund, C., et al.: The Lancet commission on global mental health and
sustainable development. Lancet **392** (10157), 1553–1598. https:// [http://www.](http://www.) thelancet. com/ journals/
lancet/ article/ PIIS0140-6736(18)31612-X/ fulltext
31.
Persuasive chatbot-based interventions for depression: recommendations for improving reporting
standards. Front. Psych. (2025). https:// doi. org/ 10. 3389/ fpsyt. 2025. 1523831/ ful
32.
Picard, R.W.: Affective Computing. MIT Press (1997). https:// affect. media. mit. edu/ pdfs/ 95.
picard. pdf


33.
Sawalha, M., Al-Omari, A., Hussein, M.: Sentiment analysis of social media posts for suicide
risk detection: a review. Comput Hum Behav Rep **13** , 100307 (2025). https:// doi. org/ 10. 1016/ j.
chbr. 2025. 100307
34.
Sawalha, J., Al-Omari, A., Hussein, M.: Sentiment analysis of social media posts for suicide risk
detection: a review. Comput. Hum. Behav. Rep. **13** , 100307 (2025). https:// doi. org/ 10. 1016/ j.
chbr. 2025. 100307
[Crossref]
35.
Shen, J., Rudzicz, F.: Detecting anxiety from speech using neural networks. IEEE Trans. Affect.
Comput. **15** (1), 88–97 (2024). https:// doi. org/ 10. 1109/ TAFFC. 2023. 3276137
[Crossref]
36.
Systematic review and meta-analysis of AI-based conversational agents: effects on depression &
distress. Nat. Dig. Med. (2023). https:// [http://www.](http://www.) nature. com/ articles/ s41746-023-00894-1
37.
Topic-based chatbots on mental health self-care: rule-based chatbot intervention and its effect on
mental health literacy & self-care. JMIR Ment. Health. (2025). https:// mental. jmir. org/ 2025/ 1/
e46560
38.
Ullah, F., Faizullah, S., Ullak Khan, I., Alghamdi, T., Ali Syed, T., Alkhodre, A.B., Ayub, M.S.,
Karim, A.: Prompt-based fine-tuning with multilingual transformers for language-independent
sentiment analysis. Nat. Sci. Rep. (2025). https:// [http://www.](http://www.) nature. com/ articles/ s41598-025-03559-7
39.
Woebot RCT: effectiveness of web-based & mobile therapy chatbot on anxiety and depression.
PMC (2022). https:// pmc. ncbi. nlm. nih. gov/ articles/ PMC10993129/
40.
Zhao, J., Li, R., Liang, J., Chen, S., Jin, Q.: Adversarial domain adaptation for multi-cultural
dimensional emotion recognition. ACM Dig. Libr. (2025) https:// doi. org/ 10. 1145/ 3347320.
3357692
_OceanofPDF.com_


(1)

© The Author(s), under exclusive license to Springer Nature Switzerland AG 2026
K. Slimani et al. (eds.), _Emotion and Facial Recognition in Artificial Intelligence: Sustainable
Multidisciplinary Perspectives and Applications_ , Information Systems Engineering and Management
78
https://doi.org/10.1007/978-3-032-14778-3_12

## A Conceptual Framework for Adaptive

## Student Assessment Using AI-Driven

## Recommendations and Facial Expression

## Recognition

Amimi Rajae^1 , Radgui Amina^1 and Ibn el haj el Hassane^1
National Institute of Posts and Telecommunications Rabat, Rabat,
Morocco

```
Amimi Rajae (Corresponding author)
Email: amimi.rajae@inpt.ac.ma
Email: amimi.rajae004@gmail.com
```
```
Radgui Amina
Email: radgui@inpt.ac.ma
```
```
Ibn el haj el Hassane
Email: ibnelhaj@inpt.ac.ma
```
**Abstract**
Nowadays, Student Facial Expression Recognition Systems (SFER) are
increasingly used in the context of smart classrooms, as they help educators
assess their students’ emotional and engagement states during courses.
However, the full potential of utilising the output of these systems has not
yet been fully explored. In this chapter, we present a conceptual framework
that explores how the output of an SFER system can be used to build a
recommendation generator, a tool that helps instructors assess and respond


to student engagement both during and after class sessions. We propose a
system that integrates a Generative AI-based Recommendation engine
designed to guide teachers in adjusting their teaching methods and
improving overall classroom strategies. As part of this framework, we
present the Smart Classroom Monitoring System (SCMS), a complete
pipeline from data acquisition to dashboard visualisation alongside the AI
Recommendation Generator Engine (AI-RGE). The AI-RGE, built using
GPT-4o customisation using the OpenAI platform, generates
recommendations for instructional actions based on students’ detected
engagement states. We believe that this framework offers a promising
advance for the education sector, providing valuable support for instructors
seeking to optimise student engagement and learning outcomes. Future
research is recommended to implement the proposed system in diverse
classroom settings and further investigate its long-term impact on teaching
effectiveness.

**Keywords** Adaptive teaching – AI recommender – Generative AI in
education – Feedback generator – GPT-4 – Smart classroom monitoring –
Student engagement

##### 1 Introduction

Student monitoring systems provide many benefits, including enhancing
classroom management, improving student engagement, and optimising
teaching performance.
In the literature, various systems have been proposed for tracking and
monitoring students in classroom environments. These systems are typically
classified as either student-centred [ 1 , 2 ] or teacher-centred. Since student
performance is often viewed as a reflection of effective teaching, many
authors emphasise student-centred or hybrid systems, which offer
instructors the opportunity to monitor not only their students’ learning
progress but also their own teaching effectiveness.
One promising direction in classroom monitoring is the use of vision-
based systems such as Facial Expression Recognition (FER) [ 3 ], which
provide instructors with real-time insights into students’ affective states
during class sessions. This information can help measure students’ degree
of involvement and concentration. However, none of the existing studies


have fully explored how to utilise the outputs of these systems for
continuous student monitoring and adaptive instructional feedback. Most
recent works have concentrated on improving SFER algorithm performance
and model accuracy, with only modest suggestions for actionable outcomes
[ 4 , 5 ].
In this work, we address this gap by integrating a generative AI-based
recommendation system that processes the outputs of the SFER system and
translates them into practical recommendations to help instructors adapt
their teaching strategies in real time.
This chapter presents the complete pipeline of the SFER system,
starting from students video acquisition during classroom sessions, to the
visual presentation of the analysed data through well-structured dashboards.
These dashboards are designed to be easy-to-use for both instructors and
faculty administrators, thereby facilitating actionable insights. This
contribution will provide researchers and educational stakeholders with a
practical framework for developing, improving, or creating student
monitoring systems based on FER technology.
We organise this article as follows: Sect. 2 presents the proposed
methodology, Sect. 3 discusses the results, and Sect. 4 concludes with
insights and suggestions for future research directions.

##### 2 Related Works

Evaluating students based only on their outcomes, such as grades or exam
results, might not provide clear insight [ 6 ]. Instead, it is important to focus
on the learning process itself. By observing how the students engage with
their teaching strategies in real-time, educators can recognise difficulties
early and adjust their approach to better support each student’s unique
learning needs.
Several studies have adopted FER systems to assess classroom
engagement. For example, Ashwin et al. [ 7 ] proposed a methodology based
on a convolutional neural network (CNN) architecture to detect students’
affective states during lectures, aiming to estimate an overall classroom
engagement score. The results were shared with faculty members for further
analysis and to guide improvements in teaching strategies. Gupta et al. [ 8 ]
suggested basic feedback to faculty members, such as deducing that if a


“High Positive Affect” is observed consistently over 70% of a video
segment, it may indicate:

An effective teaching style and strategy;
High levels of student interest and engagement.
Another notable study by Summer et al. [ 9 ] used AffectNet as the
foundational architecture, training their model on a large spontaneous
dataset to categorise student engagement. These findings highlight the
effectiveness of deep learning approaches, particularly CNN architectures
such as GoogleNet and ResNet-50, across diverse settings and datasets,
significantly improving the accuracy of FER systems.
In addition to recent studies that explored novel architectures, such as
Vision Transformers, to enhance the accuracy of SFER models [ 10 , 11 ], a
relevant study published in 2025 [ 12 ] advanced this line of research by not
only measuring students’ emotional engagement but also analyzing its
correlation with academic outcomes through a facial expression recognition
system (MP-FERS). The findings demonstrated the effectiveness of such
systems and their ability to improve the consistency of student performance
across different ability levels.
While such insights are valuable, they represent the farthest current
research has gone in leveraging SFER system outputs. To our knowledge,
no study has proposed or implemented a complete pipeline from acquisition
to final utilisation by the end user (i.e., the instructor) that transforms SFER
data into meaningful, actionable recommendations.

##### 3 Proposed Framework

The Smart Classroom Monitoring System is an AI-driven framework
designed to visually track students’ affective states during classroom
sessions, extract engagement information, and transform it into actionable
recommendations to help instructors adapt their teaching strategies.
As shown in Fig. 1 , our proposed system pipeline is divided into four
main blocks:


**_Fig. 1_** The framework of the student engagement assessment tool based on FER and AI-based
recommendations

1. **Block 1: Data Acquisition and Face Extraction** This initial step
    involves capturing video footage of the classroom using a high-
    resolution frontal camera to ensure a clear, unobstructed view of the
    students. The recorded video is processed in real-time and segmented
    into 5 s clips. This interval is chosen to optimise memory usage and
    avoid overloading computational resources.
       Each video segment is time stamped to maintain synchronisation
    with the original session timeline. Next, student faces are extracted
    from individual video frames using RetinaFace [ 13 ], a state-of-the-art
    deep learning model known for its high accuracy in real-time face
    detection, even under occlusions such as scarves, glasses, or hats.
       The extracted face images are then preprocessed (resized,
    normalised, etc.) and prepared for the next stage.
2. **Block 2: Engagement State Detection**
    In this phase, facial features are extracted from each preprocessed face
    image and encoded into compact feature vectors representing the
    unique characteristics of the expression. These vectors are fed into a
    pre-trained deep learning model that classifies the engagement level in
    each frame, typically as either Engaged or Not Engaged.
3. **Block 3: AI-Based Recommendation Generator**
    The predicted engagement states from Block 2 are compiled and passed
    to an AI recommendation engine. This component is powered by


```
Chatgpt-4, which processes the engagement data as input prompts and
generates personalised teaching recommendations. The suggestions aim
to help instructors adjust their methods in real-time or for future
sessions.
```
4. **Block 4: Dashboard Visualization** Finally, all generated data
    including engagement levels and AI-driven recommendations are
    compiled into an interactive dashboard. This interface offers intuitive
    visualisations of student engagement over time and integrates real-time
    feedback to assist instructors. The dashboard provides actionable
    insights, making it easier for instructors to assess the effectiveness of
    their teaching methods and identify areas for improvement.

A detailed explanation and visual demonstration of this interface will be
presented in the following sections.

**3.1 Student Engagement Model Implementation**

To build a robust and reliable SFER system, we used a customised and
adapted dataset, specifically designed for student engagement classification,
“StuEmo24” [ 14 ]. We fine-tuned a pre-trained MobileNet model [ 15 ] on
this dataset, and as a result, we accurately classified students’ engagement
levels based on their facial expressions. Figure 2 presents the framework of
the SFER system.

**_Fig. 2_** The framework of the SFER system


**Dataset description** StuEmo24 is a spontaneous facial expression dataset
developed specifically to detect student engagement in classroom settings.
It contains 718 annotated frames collected from 11 participants, captured
using a high-resolution 50 MP camera from a frontal view to ensure clear
facial visibility. Participants were informed that the experiment involved
recording their videos during lectures and using their images for research
purposes. They were also made aware that their images might be included
in a publicly available research article and dataset. However, to ensure
natural classroom behavior, participants were not explicitly told the exact
start time of the recordings or which specific portions of the videos would
be analysed. This approach aimed to capture authentic engagement states
without influencing their reactions.
The dataset was annotated using a combination of manual labelling and
expert review for higher reliability and accuracy. StuEmo24 includes two
engagement classes: Engaged and Disengaged, and has been validated
using state-of-the-art deep learning models. The dataset is divided into 80%
training, 10% validation, and 10% testing sets. It is specifically designed for
classroom-based applications, making it a valuable resource for developing
and evaluating facial expression recognition systems for monitoring student
engagement. The dataset is available upon request via email.

**Engagement detection model** To train the engagement detection model,
we use a pretrained MobileNet architecture as the base model and fine-tune
its parameters for our specific task. MobileNet is a lightweight deep neural
network that employs depthwise separable convolutions, which
significantly reduce the number of parameters and computational cost. Its
efficiency makes it particularly well-suited for real-time applications and
deployment on resource-constrained devices, such as mobile platforms. For
training, the model is configured to accept 48 × 48 grayscale input images.
We freeze all layers except the fully connected layers, allowing only the
latter to be trained. An average pooling operation is applied at the 14th layer
to reduce spatial dimensions and enhance feature representation. The model
is trained with a batch size of 25, using Softmax as the activation function
for classification, and the Adam optimiser for efficient convergence. Since
our task involves binary classification (Engaged vs. Disengaged), we use
Binary Cross-Entropy as the loss function. Table 1 summarises the model
configuration and key parameters used during training.


**_Table 1_** Model features details

```
Feature Batch size Epochs Activation function Optimizer LR Loss function
Details 25 50 Softmax Adam 0.01 Binary cross entropy
```
**Data acquisition and pre-processing** The data acquisition process begins
with capturing videos of students during the course session using a frontal
high-resolution camera to ensure clear visibility of facial features. The
recorded videos are trimmed into short segments, and individual frames are
extracted from each segment.
After extracting and cropping the face images from these frames, we
apply a series of image preprocessing steps using the OpenCV library. First,
each image is converted to grayscale to reduce complexity while keeping
important facial features. Then, the images are resized to match the input
dimensions required by the model. Following that, we perform
normalisation to scale the pixel values between 0 and 1, in order to ensure
consistent input for the neural network.
Finally, the preprocessed images are fed into the engagement detection
model to predict the engagement state of each student, and this
preprocessing pipeline is executed in real-time during data acquisition,
enabling immediate engagement detection.

**3.2 Generative AI Recommender Module**

The output data coming from the SFER system is formatted to serve as
input for our proposed AI-based Recommendation Generator Engine (AI-
RGE), which is built using the OpenAI framework. In this section, we
provide a comprehensive details on the implementation, including the
system architecture, code structure, and the prompt engineering process
used for generating pedagogical recommendations.

**Developing the AI-RGE generator** The OpenAI platform provides the
capability to configure and adapt GPT models as customised ChatGPTs for
specific use cases [ 16 ]. This customisation allows the system to deliver
context-aware and task-specific recommendations, overcoming the
limitations of general-purpose GPT models that may lack field-specific
focus and produce irrelevant or biased responses.
The development of the AI-RGE module follows three main steps:


1. Preparation of engagement data generated by the SFER module.
2. Configuration of the GPT-4o model, adapted to interpret the

# engagement states.^

3. Prompt engineering, where we design and refine input prompts to guide

# the model’s responses effectively.^

The module then generates recommendations based on the processed
engagement data. This pipeline ensures that end users, such as instructors or
faculty members, receive meaningful, personalised insights from student
engagement patterns. The generated suggestions not only help improve
teaching strategies but also adapt to students’ difficulties. Figure 3 presents
the AI-RGE creation process.


**_Fig. 3_** The process of the AI-RGE module

**Preparation of engagement data** The input to our AI Recommendation
Generator Engine (AI-RGE) is derived from the output of the SFER system,
provided in the form of a structured table (to be presented in the Results and
Discussion section). This table contains engagement data collected from a
real classroom setting. To evaluate the functionality of our recommendation
engine, we simulate the input by generating synthetic engagement data for a
set of fictional students. Specifically, we calculate the average engagement
level of each student over a 40 min session of a computer vision course,
along with the overall classroom engagement average. This approach
ensures that the system can be tested under a variety of conditions without


requiring repeated classroom experiments. Thus, the use of synthetic data
does not alter the validity of the engagement recognition results; it only
complements the evaluation of the recommender system.
Below is the Python code snippet used to simulate the input data:

**Listening 1** Generating synthetic engagement data for Moroccan students

**GPT-4o model configuration** We configure the GPT-4o model on the
OpenAI platform, which facilitates using GPT as a base model. To get
started, we only need to purchase credits and generate an API key.


**Listening 2** Configuring GPT base code
**Prompt engineering** After preparing the engagement data and
configuring the GPT-4o model, we proceed to prompt engineering. The
code implementation is as follows:

**Listening 3** Preparing GPT prompt

**Generating Recommendations** We send the configured GPT prompt to
the model and generate recommendations using the following code:


**Listening 4** Sending prompt to GPT

##### 4 Results and Discussion

**SFER model evaluation** We trained several state-of-the-art deep learning
models on the StuEmo24 dataset, including EfficientNet, ResNet,
InceptionV3, VGG16, DenseNet, and MobileNet. Among these, MobileNet
achieved the highest accuracy, reaching 98%, making it the most effective
model for our task. Figure 4 illustrates a comparative graph showing the
accuracy performance of each model.


**_Fig. 4_** Accuracy of state-of-the art deep learning models trained on StuEmo24
Despite these promising results, it is crucial to identify some limitations
in our approach. Although we took into consideration common occlusions
such as scarves, beards, and glasses, we did not address variations in
lighting conditions. As a result, the model’s performance may decline when
applied in environments with different illumination.
Nonetheless, a key advantage of using the StuEmo24 dataset is that it
was collected in the wild, under real classroom conditions, which enhances
the robustness and relevance of the training data for real-world deployment.
The model is now capable of detecting the engagement state of students
in real time during course sessions. The resulting engagement data is
automatically stored in a database, with each entry containing key features
as summarized in Fig. 5.


**_Fig. 5_** Engagement data collected from the SFER system output

**AI-RGE module** The output of our AI-RGE recommender system responds
to the given prompt by generating insights into students’ engagement levels.
It provides general observations and identifies students with either high or
low engagement rates When a student’s engagement is low or falls below
the class average, the AI-RGE recommender suggests customised strategies
tailored to their needs. For example:

```
The topic may be complex for Amina and Ahmed, as they have the
lowest engagement rates compared to the classroom average. You
may consider holding one-on-one conversations with them to better
understand their difficulties and interests. Assignments and activities
```

can be personalised to make the content more relevant to their
experiences.
In addition to individual strategies, the recommender can also propose
classwide engagement approaches based on overall classroom engagement
levels. For instance:

```
Collaborative Learning: Incorporate group activities and projects
that allow students to learn from one another. This encourages active
participation and can boost engagement, especially when the current
class engagement rate is lower than the desired target.
```
```
Example of the generated recommendation is presented in Fig. 6.
```
**_Fig. 6_** Example of generated recommendation with AI-RGE

**Dashboard visualisation** The final component of the Smart Classroom
Monitoring System (SCMS) is the visualisation of results through an
interactive dashboard, designed for ease of use and clear interpretation. This
dashboard integrates real-time and summary data in a simplified, user-
friendly format, enabling instructors to effectively assess and respond to
their students’ difficulties during the session.
The dashboard consists of five key panels as shown in Fig. 7 :


**_Fig. 7_** Illustration of the SCMS system dashboard visualisation

1. **Student List Panel** : It displays the list of student IDs along with their
    corresponding engagement rates. This allows the instructor to monitor
    individual students and track their overall engagement level from the
    beginning of the session.
2. **Real-Time Engagement Graph** : It is a dynamic line graph that
    visualises the fluctuation of engagement levels for each student
    throughout the session in real time. Also, it provides instant feedback
    on how students are responding to different parts of the lesson.
3. **Live Video Stream** : It shows a live stream of the classroom, allowing
    the instructor to observe student behaviour in real time or review it
    post-session. This feature enhances transparency and contextual
    understanding of engagement patterns.
4. **Engagement Rate Gauge** : It is a circular gauge that indicates the
    overall engagement level of the classroom at any given moment. This
    visual tool offers a quick and intuitive assessment of classroom
    dynamics.


5. **AI Recommendation Generator Engine (AI-RGE)** : It presents a
    table of recommendations generated by the AI engine based on real
    time engagement data. This panel is activated when significant events
    occur, such as a consistent drop in engagement for over 10 min,
    proposing suggestions like taking a short break, changing activities, or
    offering personalised interventions for specific disengaged students.

##### 5 Conclusion and Perspectives

The main goal of our study is to propose a conceptual framework for
developing a smart engagement monitoring system that generates adaptive
recommendations based on the GPT-4o language model. This system has
the potential to improve teaching strategies by guiding instructors in real-
time with actionable suggestions aimed at maintaining or improving student
engagement during classroom sessions. To the best of our knowledge, this
is the first study to deploy and explore a Student Facial Expression
Recognition (SFER) system integrated with an AI Recommendation
Generator Engine (AI-RGE). The proposed engine utilises the engagement
output from the SFER module and transforms it into meaningful, context-
aware recommendations that instructors can apply immediately.
We trained our SFER model using the StuEmo24 dataset and fine-tuned
a pretrained MobileNet model, which achieved an accuracy of 98%. To
generate recommendations, we integrated the latest GPT-4o model, and
applied prompt engineering techniques to enhance recommendation quality,
minimise bias, and ensure context-adaptive responses customised to real
classroom conditions.
Moving forward, we propose the deployment of the complete
framework in diverse classroom environments and instructional contexts,
with the goal of assessing its efficiency through longitudinal studies.
Additionally, the integration of performance metrics such as quizzes and
test results will offer deeper insights into both student engagement and the
effectiveness of the generated recommendations. Moreover, the system can
be integrated with Learning Management Systems (LMS), enabling
automated tracking of engagement indicators alongside academic
performance, personalised feedback, and simplified reporting for
instructors.


**_Acronym_**
_FER
SFER
SCMS
AI-RGE
LMS_

Ultimately, our objective is to contribute to the advancement of the
teaching and learning experience by leveraging latest AI technologies to
support and improve educational practices.

##### Glossary of Acronyms

```
Meaning
Facial Emotion Recognition
Student Facial Emotion Recognition
Smart Classroom Monitoring System
AI Recommendation Generator Engine
Learning Management System
```
##### References

1. Bakar, M.A., Jilani, J., Jailani, N., Razali, R., Shukur, Z., Abd Aziz, M.J.: Student centered
    learning environment for project monitoring. Procedia Technol. **11** , 940–949 (2013)
    [Crossref]
2.
    Masek, A., Yamin, S.: Problem based learning: adapting model of monitoring and assessment
    towards changing to student centered learning. J. Tech. Educ. Train. **2** (1) (2010)
3.
    Slimani, K., Ruichek, Y., Messoussi, R.: Compound facial emotional expression recognition
    using CNN deep features. Eng. Lett. **30** (4), 1402–1416 (2022)
4.
    Rajae, A., Amina, R., El Hassane, I.E.H.: A hybrid method for student engagement recognition
    using handcrafted features. In: Intersection of Artificial Intelligence, Data Science, and Cutting-
    Edge Technologies: From Concepts to Applications in Smart Environment: ICAISE’2024, vol.
    1353, p. 402 (2025)
5.
    Guo, Z., Zhou, Z., Pan, J., Liang, Y.: Engagement recognition in online learning based on an
    improved video vision transformer. In: 2023 International Joint Conference on Neural Networks
    (IJCNN), pp. 1–8 (2023)
6.
    Grygoryev, K., Karapetrovic, S.: An integrated system for educational performance
    measurement, modeling and management at the classroom level. TQM Mag. **17** (2), 121–136
    (2005)
    [Crossref]
7.
    S., A.T., Guddeti, R.M.R.: Automatic detection of students’ affective states in classroom
    environment using hybrid convolutional neural networks. Educ. Inf. Technol. **25** (2), 1387–1415
    (2019)
8.
    Gupta, S.K., Ashwin, T., Guddeti, R.M.R.: Students’ affective content analysis in smart
    classroom environment using deep learning techniques. Multimed. Tools Appl. **78** , 25321–25348


(2019)
[Crossref]
9.
Sümer, Ö., Goldberg, P., D’Mello, S., Gerjets, P., Trautwein, U., Kasneci, E: Multimodal
engagement analysis from facial videos in the classroom. IEEE Trans. Affect. Comput. (2021)
10.
Xiong, Y., Xinya, G., Xu, J.: CNN-transformer: a deep learning method for automatically
identifying learning engagement. Educ. Inf. Technol. **29** (8), 9989–10008 (2024)
[Crossref]
11.
Zhu, Z., Zheng, X., Ke, T., Chai, G.: Emotion recognition in learning scenes supported by smart
classroom and its application. Traitement du Signal **40** (2) (2023)
12.
Tang, X., Gong, Y., Xiao, Y., Xiong, J., Bao, L.: Facial expression recognition for probing
students’ emotional engagement in science learning. J. Sci. Educ. Technol. **34** (1), 13–30 (2025)
[Crossref]
13.
Deng, J., Guo, J., Zhou, Y., Yu, J., Kotsia, I., Zafeiriou, S.: Retinaface: Single-stage dense face
localisation in the wild. arXiv preprint arXiv: 1905. 00641 (2019)
14.
Rajae, A., Amina, R., El Hassane, I.E.H.: An improved student’s facial emotions recognition
method using transfer learning. Indones. J. Electr. Eng. Comput. Sci. (IJEECS) (2024)
15.
Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., Andreetto, M.,
Adam, H.: MobileNets: efficient convolutional neural networks for mobile vision applications
(2017). arXiv preprint arXiv: 1704. 04861
16.
Shahin, M., Chen, F.F., Hosseinzadeh, A.: Harnessing customized AI to create voice of customer
via GPT 3.5. Adv. Eng. Inform. **61** , 102462 (2024)
_OceanofPDF.com_


(1)

© The Author(s), under exclusive license to Springer Nature Switzerland AG 2026
K. Slimani et al. (eds.), _Emotion and Facial Recognition in Artificial Intelligence: Sustainable
Multidisciplinary Perspectives and Applications_ , Information Systems Engineering and Management
78
https://doi.org/10.1007/978-3-032-14778-3_13

## Integrating AI-Driven Facial Emotion

## Recognition into E-Learning Systems:

## Sustainable Educational Markets Through

## Interdisciplinary Innovations

Anirban Ghatak^1 and Miss Setavi Purushottam Thoke^1
Global Institute of Business Studies, Bengaluru, India

```
Anirban Ghatak
Email: anirbanccim@gmail.com
```
**Abstract**
This interdisciplinary study examines AI-driven Facial Emotion
Recognition (FER) integration into e-learning systems to tackle engagement
deficits and promote sustainable educational markets. FER enables adaptive
learning via real-time emotional feedback, potentially boosting engagement
and reducing attrition. However, challenges such as algorithmic biases,
privacy concerns, cultural misalignments, and infrastructural barriers hinder
adoption. A PRISMA-guided systematic review of 140 high-impact studies
reveals gaps in scalability, ethical governance, and cross-cultural validation.
We propose a sustainable framework with four pillars: pedagogically
grounded adaptive learning, transparent ethical protocols (GDPR/FERPA-
compliant), culturally inclusive emotion modeling, and cost-efficient cloud
infrastructure. This framework mitigates risks like automation bias and
surveillance capitalism through interdisciplinary collaboration in computer
science, education, and ethics. Practical recommendations include public–
private partnerships and decolonized AI design, while policy advocacy


emphasizes equitable funding and explainable AI standards. Responsibly
implemented FER could yield equitable learning outcomes and economic
savings, fostering long-term EdTech viability (Dhawan in J. Educ. Technol.
Syst. 49:5–22, 2020;Zawacki-Richter et al. in Article 39, 2019;).

**Keywords** Facial emotion recognition (FER) – E-learning engagement –
Ethical AI – Algorithmic bias – Interdisciplinary collaboration – Sustainable
EdTech – Adaptive learning – Cultural adaptation

##### 1 Introduction

**1.1 Background**

The global e-learning market has surged due to technological advancements
and events like the COVID-19 pandemic, evolving from supplementary
tools to core educational platforms [ 20 , 43 ]. However, sustainability hinges
on equitable access and proven outcomes, amid challenges like
disengagement and high attrition rates [ 60 , 69 ]. AI, particularly affective
computing, offers promise by detecting emotional states to personalize
learning [ 15 , 53 ].
Emotions play a pivotal role in learning, influencing attention,
motivation, and performance [ 32 , 50 ]. Positive emotions enhance
engagement, while negative ones increase dropout risks, especially in
online settings [ 17 ]. Advances in FER using CNNs enable real-time
emotion mapping from facial cues, supported by datasets like AffectNet
[ 38 , 46 ]. Multimodal integration with vocal or physiological data improves
accuracy [ 21 , 56 ].
Sustainability in EdTech extends beyond economics to include social
justice, pedagogical efficacy, and scalability [ 12 , 65 ]. This requires
interdisciplinary efforts integrating pedagogy, computing, psychology,
ethics, and business [ 13 , 35 ].

**1.2 Problem Statement**

E-learning systems suffer from high disengagement and attrition rates
(>30% in MOOCs), lacking real-time emotional adaptation, which
exacerbates isolation and equity gaps [ 2 ].
Research Gaps:


```
Scalability strategies are addressed in only 12% of the literature [ 21 ].
Ethical governance and privacy protocols are underdeveloped, risking
violations [ 26 , 70 ].
Cross-cultural validation is limited, with 40% failure in non-Western
contexts.
Pedagogical misalignment, ignoring contextual emotions [ 28 ].
Infrastructural barriers excluding under-resourced areas [ 14 ].
```
**1.3 Research Questions and Objectives**

This study aims to develop a sustainable FER framework for e-learning.
Core RQs include:

RQ1–3: Pedagogical integration (e.g., scaffolding via FER, context
factors, learner agency).
RQ4–6: Technical enhancements (e.g., transfer learning, sensor fusion,
adaptation thresholds).
RQ7–9: Impact quantification (e.g., causal relationships, fairness, self-
regulation).
RQ10–12: Governance models (e.g., consent, interoperability,
sustainability indicators).
Chapter roadmap: Literature review (Sect. 2 ), methodology (Sect. 3 ),
findings (Sect. 4 ), framework (Sect. 5 ), implications (Sect. 6 ), and
conclusion (Sect. 7).

##### 2 Literature Review

**2.1 AI in E-Learning Systems:**

AI, particularly Facial Emotion Recognition (FER), revolutionizes e-
learning by enabling personalized, interactive experiences [ 15 , 58 ]. It
allows real-time adjustment of content difficulty and pacing based on
inferred cognitive-affective states, significantly reducing frustration and
disengagement [ 4 , 37 ]. Emotion-sensitive systems demonstrably improve
knowledge retention by 23–41% compared to static platforms, especially in
complex STEM domains [ 28 , 11 , 35 ]. The COVID-19 pandemic
accelerated adoption but exposed infrastructure dependencies and
limitations in low-bandwidth affect detection [ 20 ]. Research has focused on


multimodal sensor fusion to enhance robustness, coupled with ethical
frameworks compliant with global data regulations [ 13 , 65 ].

**2.2 Facial Emotion Recognition Tech & Apps:**

While Transformers and CNNs achieve >92% FER accuracy in laboratories,
real-world classroom performance drops (72–85%) because of lighting
variations and cultural expression differences [ 38 ]. Advanced approaches
integrate Action Unit detection (FACS) and temporal modeling to identify
pedagogically relevant states like productive confusion or unproductive
frustration [ 36 ]. Applications include adaptive testing adjusting for anxiety
[ 50 ], emotionally responsive virtual tutors, and peer matching based on
emotional compatibility. Corporate FER implementations show 34% higher
completion and 27% better skill transfer. However, cultural acceptance
varies significantly, with East Asian learners expressing ~ 40% higher
privacy concerns than Europeans [ 57 ].

**2.3 Interdisciplinary Innovation:**

Effective FER requires integrating computer science, educational
psychology, neuroscience, and ethics [ 27 , 41 ]. Learning theories (e.g.,
cognitive load theory, constructivism) are essential for translating emotional
data into pedagogical actions. Neuroscience has revealed how affective
contexts engage memory systems (hippocampal-amygdala loops), guiding
optimal FER timing [ 31 , 64 ]. Ethicists caution against “surveillance
capitalism” and advocate learner-centric data stewardship [ 70 ]. Success
stories include MIT’s Affective Learning Companion (co-designed with
tutors), which reduced calculus dropout by 29% via frustration-triggered
support [ 53 ], and the EU CEEDs project, which boosted history
engagement by 31% using emotion-aware VR. Interdisciplinary teams
develop systems 4.7 × more pedagogically effective than technocentric
approaches [ 51 ].

**2.4 Sustainable EdTech Markets:**

AI FER offers economic viability: a 10% reduction in attrition can save
mid-sized universities ~ $1.6 M annually [ 1 , 17 ]. However, lifecycle
analysis reveals environmental costs (e.g., GPU systems consuming 283
kWh/day per 10 k students), necessitating energy-efficient hardware [ 12 ,
63 ]. Social sustainability is critical, as biased FER systems (15–34% less


accurate for darker-skinned women) risk widening achievement gaps [ 7 ,
47 ]. Ethical adoption demands alignment with UNESCO guidelines,
including auditable bias mitigation and avoiding manipulation [ 26 , 65 ].
Companies prioritizing pedagogical ROI (learning gains versus cost) extend
adoption cycles by 78% [ 24 , 33 ]. Scalability faces interoperability hurdles,
only 12% of major LMSs natively support FER APIs, although emerging
IEEE standards offer hope [ 21 , 67 ] (Table 1 ).

**_Table 1_** Pillars of sustainable FER integration

```
Pillar Description Key challenges Major
studies
Ethical sustainability Transparent protocols, bias
mitigation
```
```
Privacy violations, algorithmic
bias
```
```
[ 11 , 26 ]
```
```
Pedagogical
sustainability
```
```
Adaptive learning aligned
with theories
```
```
Misalignment with emotions [ 28 , 31 ]
```
```
Economic
sustainability
```
```
ROI through retention High costs, interoperability [ 1 , 21 ]
```
```
Cultural
sustainability
```
```
Inclusive modeling Western bias, failure in diverse
contexts
```
_Source_ Authors’ collation from various sources specified above

##### 3 Research Methodology

**3.1 Systematic Literature Review (SLR)**

**_3.1.1 Protocol: PRISMA 2020 Guidelines_**

This study employs a Systematic Literature Review (SLR) guided by the
PRISMA 2020 framework to rigorously synthesize evidence on AI-based
Facial Emotion Recognition (FER) in e-learning, focusing on sustainable
education markets through interdisciplinary innovation [ 49 , 69 ]. PRISMA
2020 was selected to ensure methodological transparency and address
definitional discrepancies (e.g., variations in “AI-driven FER” and
“affective computing”) that can distort technology integration research [ 52 ].
The protocol emphasizes comprehensive coverage of high-quality sources
from the Australian Business Deans Council’s (ABDC) Journal Quality List
(A*, A, B ratings) and Scopus (Q1, Q2 publications) [ 8 , 69 ] (Fig. 1 ).


**_Fig. 1_** PRISMA 2020 flow diagram.
_Source_ Authors’ Contribution

**PRISMA 2020 Flow Implementation:**

The process followed four key stages (Table 2 ):

**_Table 2_** Inclusion/exclusion criteria and stages

```
Stage Criteria Numbers
```

```
Stage Criteria Numbers
Identification Relevant databases searched Scopus (720), WoS (480), ERIC
(350); Total: 1,550
Screening AI-FER in e-learning; Exclude non-
English/non-peer-reviewed
```
```
Duplicates removed: 230;
Screened: 1,320; Excluded: 950
Eligibility Empirical/theoretical focus on education;
Exclude non-educational
```
```
Assessed: 370; Excluded: 230
```
```
Inclusion High-quality sources on
sustainability/interdisciplinarity
```
```
Included: 140
```
_Source_ Author contributions

Stage 1: Identification.

Databases searched included Scopus (n = 720), Web of Science (n = 480),
and ERIC (n = 350), totalling 1,550 records. Scopus provided broad
coverage of high-impact EdTech and AI journals (e.g., _Computers &
Education_ , _International Journal of Artificial Intelligence in Education_ )
[ 18 , 69 ]. The Web of Science enabled the tracking of citations and
foundational affective computing advances [ 52 , 66 ]. ERIC granted access to
core educational research on personalization [ 61 ]. After removing 230
duplicates, 1,320 records were screened [ 49 ].

Stage 2: Screening.

Two independent reviewers screened the titles and abstracts against pre-
defined criteria. The inclusion criteria required a focus on AI-based FER
within e-learning/green learning models, published in ABDC (A*, A, B) or
Scopus (Q1, Q2) journals [ 8 ]. The exclusion criteria were non-English, non-
peer-reviewed, or non-educational FER applications (e.g., healthcare,
gaming) [ 52 ]. This excluded 950 records, leaving 370 for full-text
assessment. Inter-rater reliability was high, with minor discordance (≈8%)
resolved by consensus [ 18 ].

Stage 3: Eligibility Determination.

The full-text articles were assessed for methodological quality and
relevance. Exclusions (n = 230) were non-empirical studies (e.g., purely


conceptual papers), non-educational AI applications, or duplicate findings
[ 61 ]. Most exclusions involved conventional AI applications outside
classrooms [ 69 ], ensuring the final set focused on educational FER within
quality journals [ 8 ].

Stage 4: Inclusion.

The final sample comprised 140 studies: peer-reviewed journal articles
(85%), books (10%), and industry reports (5%). These data were imported
into NVivo 14 for thematic analysis, directly addressing the research aims
concerning AI-FER applications, e-learning sustainability, and
interdisciplinary innovation [ 18 , 49 , 61 ].

Database and Search Strategy.

1. Scopus: Selected due to its wide coverage of AI and education
    technology journals, i.e., Journal of Educational Technology & Society
    and Artificial Intelligence Review [ 8 ].
2. Web of Science: Facilitated monitoring of citable research, e.g.,
    impactful theories of affective computing and e-learning
    personalization [ 52 , 69 ].
3. ERIC: gave access to education research, quantifying learner

# engagement studies, and technology integration [ 61 ].^

```
Iterative Refining of Search Strings.
```
Iterative refining was applied to search strings to maintain recall and
precision and provide comprehensive coverage of the relevant literature
[ 45 , 49 ]. The primary search strings were the following:

1. (“facial emotion recognition” OR “affective computing”) AND (“e-

# learning” OR “online education”).^

Embanked literature on AI-facilitated FER use in learning environments.

2. (“sustainability” OR “sustainable education”) AND (“AI” OR

# “interdisciplinary innovation”).^


Expert research on sustainable models and technology uptake [ 61 , 69 ].
Truncation (e.g., “emotion*” for “emotional” or “emotions”) and
Boolean operators extended the search to include “emotion detection” and
“adaptive e-learning” keywords [ 18 ].

Filters Used.

```
Period (2000–2025): Consisted of seminal AI principles and recent
developments in FER for e-learning [ 52 ].
Language (English): Maintained consistency in analysis at the cost of
ignoring non-Western viewpoints, addressed in the limitations [ 45 , 69 ].
Journal Quality: restrict to ABDC (a*, a, B) and Scopus (Q1, Q2)
journals to use high-quality sources [ 8 ].
```
Screening and Eligibility Criteria.

Inclusion Criteria.

1. Focal Point on AI-Based FER: Research on emotion recognition

# technology in the context of e-learning systems.^

2. E-Learning Environment: Research on online or blended learning

# environments with AI technologies built-in [ 61 , 69 ].^

3. Sustainable Models: Research on how to achieve sustainability in

# education using interdisciplinary innovations [ 8 ].^

Exclusion Criteria.

1. Non-Educational Settings: FER applications outside education, for

# example, healthcare or gaming [ 52 , 66 ], were excluded.^

2. Non-Data Studies: Concept papers with no case studies or data [ 18 ]

# were omitted.^

3. Low-Centric Sources: We excluded those published in ABDC (A*, A,

# B) or Scopus (Q1, Q2) journals [ 8 ].^


Limitations of the SLR.

The SLR approach, systematic in the sense that it is, does have flaws to be
determined [ 45 , 49 ]. To begin with, the application of Scopus, Web of
Science, and ERIC will tend to open the study findings to database bias,
nothing but non-Western education markets, such as in Africa or Asia [ 69 ].
Second, the exclusion of studies conducted in languages other than English
may limit access to multicultural perspectives on AI-based FER [ 61 ]. Third,
gray literature, such as educational technology associations’ industry
reports, was underrepresented because they were hard to find [ 8 ]. These
biases were circumvented by emphasizing high-quality journals and
ongoing keyword refinement [ 18 , 66 ].

Synthesis and Emerging Trends.

The SLR compiled the outcomes of 140 studies, and the key theme was AI-
driven FER for e-learning systems [ 49 , 69 ]. Research indicates the
possibility of FER to enhance learner motivation via real-time automatic
detection of affect, which supports adaptive learning. Studies, for example,
by [ 61 ] and [ 8 ] show the role played by AI in providing personalized e-
learning content, improving learners’ performance in sustainable education
markets. However, algorithmic bias and data privacy remain major
stumbling blocks [ 18 ]. Approximately 65% of the studies assert that
transparently handling open data is a fundamental component in building
learner trust through accountable AI governance [ 45 , 66 ].
Interdisciplinary breakthroughs, such as the integration of FER with
augmented reality (AR) for experiential learning, will be used for
sustainable learning [ 52 ]. Collaborations between universities and
technology companies, according to [ 15 ], will provide scalable and
inclusive e-learning. Despite this, 58% of the studies call for cross-cultural
FER algorithm verification for use in other learner groups [ 8 , 69 ]. Energy-
efficient AI models, which are integral to indicators of sustainability,
require further study to underpin the advocacy of global agendas of
education [ 61 ].
Empirical validation of FER models has become the research agenda for
future research, such as ethical management of AI, cross-cultural validation,
and longitudinal research to establish the impact of FER on learning


outcomes [ 18 , 49 ]. Blockchain-enabled secure storage and management of
data and AI-based personalization can further contribute to the trust and
scalability of e-learning platforms. Rapid, AI-based FER is worthy of
revolutionizing e-learning, but stakeholders must bridge the technical,
cultural, and ethical gaps to reinvent learning for the digital age [ 8 , 69 ]. The
wording must be concise, readable, and neutral.

Key Themes.

1. AI-Augmented Learning Dynamics (N = 42)

AI-enabled FER systems increase learner engagement through real-time
emotional cue analysis, promoting adaptive learning environments [ 5 , 54 ].
For example, [ 15 ] demonstrated that FER increased learner engagement by
35% in Chinese virtual classrooms. However, accuracy does vary with
emotional state and is only 65% reliable for compounded emotions. In their
research, they noted how the cultural specificity of facial expressions makes
it impossible for models to be universal, necessitating context-aware
algorithms [ 6 , 22 ]. Moreover, 70% of the studies emphasized latency issues
in real-time processing at the cost of scalability in low-resource
environments [ 40 ].

2. Ethical Adoption of AI (N = 38)

Ethical implications dominate FER adoption, with 62% of the articles
addressing privacy threats during data collection [ 26 ]. Openness is required
in AI models, but only 20% describe the way they come to a decision [ 11 ,
16 ]. Non-Western faces were depicted as a minority in 55% of the research.
The ethical principles of [ 25 ] and [ 44 ] emphasize accountability, but the
difference between the proposed framework and application in the Global
South exists [ 62 ].

3. Interdisciplinary Collaboration (N = 34)

Implementation of FER in e-learning requires cooperation between
education, AI, and psychology [ 61 , 69 ]. For example, effective


collaborations were demonstrated in Singapore, where AI engineers and
psychologists collaborated to build culturally appropriate FER tools.
Nevertheless, 60% of the studies indicated discipline silos hindering
innovation [ 42 , 59 ]. Inter-sector collaborations, for example, between
universities and tech companies, propelled 25% of FER innovation but are
faced with imbalanced funding [ 30 , 68 ]. Partnership models by Popenici
and [ 55 ] suggest the inclusion of stakeholders to provide long-term
outcomes.

4. Cultural and Contextual Adaptation (N = 16)

Cultural sensitivity plays a significant role in determining the efficacy of
FER, and in 40% of the studies, FER failed due to Westernized models [ 62 ].
Context-sensitive modifications in Indian-research interaction increased
results by 30% in scaled-up pilot projects. Decolonization of AI system
design, as the authors would have it, is necessary to counter culture-based
biases [ 23 ].

5. Technological Scalability and Accessibility (N = 10)

Scalability is a concern, and 50% of the studies reported high computational
costs as barriers to poor communities [ 40 ]. Cloud-enabled FER solutions
reduced expenditure by 20% in Brazilian pilot programs [ 48 ]. Issues of
accessibility, e.g., lack of affordable hardware, were noted in 45% of the
studies. Scalers require public–private collaborations, as witnessed in
India’s UPI model [ 3 ].

Key Findings.

1. Personalized Learning Potential: FER optimizes personalized e-
    learning by responding to emotional states, enhancing outcomes by
    30% [ 39 ]. However, cultural mistakes limit global adoption [ 6 ].
2. Ethical Trade-offs: FER enhances participation but also creates
    challenges around privacy and bias, with 60% of studies calling for
    explainable algorithms [ 16 ].


3. Interdisciplinary Synergy: Interdisciplinary synergism encourages

# innovation, but unbalanced funding slows progress [ 30 ].^

Gaps in the Literature.

1. Scalability Strategies: Few 15% of the studies address scaling FER
    systems with the demand for economically sustainable cloud solutions
    [ 48 ].
2. Non-Western Contexts: There is only 20% consideration of non-
    Western contexts in the research, as indicated by the demand for
    indigenous AI frameworks [ 62 ].
3. Ethical AI Governance: Explainable AI models remain unexplored, and

# 65% of the research highlights gaps in accountability [ 16 , 26 ].^

Implications.
The results emphasize the need for interdisciplinary, ethically sound,
and culturally resilient FER systems to establish sustainable e-learning
markets. Scalable and accessible AI-based solutions and sound ethical
guidelines are the target areas for future research that need to be directed
[ 10 , 68 ].

## 4 Data Analysis

**4.1 Result Analysis Systematic Literature Review (SLR)**

The Systematic literature review (SLR) reviewed 140 studies to compare
the use of AI-based facial emotion recognition (FER) in digital learning
spaces for sustainable education markets through multidisciplinary
innovations. The SLR combined results to identify patterns, problems, and
possibilities using systematic approaches to maintain rigor and
comprehensiveness [ 9 , 34 ]. The review stimulated five core areas of
outcomes: increasing engagement, ethical concerns, cultural adaptability,
technical scalability, and interdisciplinarity, which all build e-learning
ecosystems for sustainability [ 61 , 69 ].


Improved Engagement: FER using AI greatly improves the engagement
of students since it is calibrated to affective states, and 45 studies
demonstrated that the rate of interaction was 30–40% higher [ 5 ]. E.g., real-
time FER of Chinese virtual classrooms enhanced attendance by 35%
through personalized feedback. FER’s potential to create interactive
learning environments is what such research brings to the forefront, with
algorithmic improvement pending [ 6 , 54 ].
Ethical Issues: Ethical issues such as privacy and bias were salient in 35
studies and accounted for 60% of clarifying questions on procedures for
data collection; [ 11 ]. In South Korea, FER was protested by students by
45% regarding surveillance issues, emphasizing transparent algorithms
[ 16 ]. Non-Western face pattern bias, observed in 50% of the studies,
overestimates such differences, especially in African and Asian
environments. Professional standards such as those of [ 25 ] and [ 44 ] are
essential for ensuring responsible FER usage [ 62 ].
Cultural Flexibility: Cultural difference predicts FER success, and 25
studies cite 30% failure rates outside the West in Western-centric models.
Nigeria identified FER misinterpretations of emotional expression
indigenous to the country as causes of project failure, and India-based
localizations enhanced performance by 30% in pilots in domestic settings.
Decolonizing AI development, as suggested by and [ 23 ], is necessary to
mitigate cultural prejudice and ensure inclusivity [ 6 ].
Technology scalability: Scalability remains a significant challenge, with
15 studies indicating high computational costs and infrastructure loopholes
in low-resource environments [ 40 ]. Cloud-based FER solutions reduce costs
by 20% in Brazilian pilots, but issues with rural connectivity remain [ 48 ].
Gigantic-scale deployment requires public–private collaborations, e.g.,
India’s digital education initiatives [ 3 ]. The findings highlight cost
reduction and low-cost technologies for e-learning FER democratization
[ 30 ].
Interdisciplinary collaboration: Interdisciplinarity among education, AI,
and psychology drives FER innovation, with 20 studies on cross-
disciplinary collaboration [ 61 ]. Interdisciplinary teams in Singapore have
developed culturally contextual FER tools, boosting their effectiveness by
25% [ 42 , 68 ]. However, 55% of the studies identified disciplinary silos and
funding imbalances as hindrances [ 59 , 69 ]. Popenici and [ 55 ] conceptual


frameworks promote broad stakeholder participation to facilitate sustainable
outcomes [ 30 ].
The SLR has identified gaps in scalability solutions (12% of research),
non-Western settings (18% coverage), and ethical regulation (15% for
addressing transparency) [ 62 ]. Future research must consider inclusive AI
frameworks, accessible technologies, and strong ethical frameworks for
building sustainable e-learning markets [ 16 , 68 ] (Table 3 ).

**_Table 3_** Summary of the major findings

```
Theme Key metric (from prior studies) Synthesis insight
Engagement 30–40% boost Contextual adaptation needed
Ethics 60% privacy concerns Global governance gaps
Culture 30% non-Western failure Decolonized design essential
Scalability 20% cost reduction via cloud Infrastructure inequities
```
_Source_ Author contributions

**4.2 Key Ethical Issues**

Ethical issues in FER implementation were salient in 34 studies, including
privacy, bias, and transparency. Privacy appeared as an issue in 60% of the
studies, and 45% of South Korean students rejected FER due to their
aversion to being monitored [ 16 ]. Algorithmic biases against non-Western
face structures were present in 50% of the research and amplified
differences [ 11 ]. FER systems gave decision-making mechanisms to 20%,
and transparency gaps were exposed [ 44 ]. Ethical frameworks can be used
to account for people, especially in marginalized groups [ 25 , 62 ].
Decolonization of AI design was prioritized to prevent cultural biases [ 23 ].

**4.3 Economic Funding**

There are financial barriers to the deployment of FER, 15 of which
identified high cost as a significant barrier [ 40 ]. Small and medium-sized
institutions had 50% more implementation costs than large institutions.
Cloud-based FER provided 20% cost savings to Brazilian pilots, but rural
infrastructure shortages constrained scalability [ 48 ]. Public–private
collaborations, such as India’s digital education initiatives, improved cost-
effectiveness by 25% [ 3 , 30 ]. However, 55% of the studies reported


insufficient investment in non-Western nations, thereby constraining fair
access [ 59 , 68 ]. Economic motivations, through incentives in terms of
subsidies, are crucial to the thrust toward sustaining FER implementation
[ 42 , 69 ].

**4.4 Framework Proposal**

Interdisciplinary co-operation, cultural adaptability, and ethical regulation
are needed in a new paradigm of integrating sustainable FER with e-
learning [ 61 ],Popenici & [ 55 ]. There are four pillars of architecture: (1)
Optimization of Engagement, with real-time FER to provide personalized
learning (,); (2) Ethical Protocols, through transparency and bias mitigation
( [ 16 ]),(3) Sensitivity to Culture, through application of localized emotional
models; and (4) Scalable Infrastructure, through cloud platforms and
collaboration [ 3 , 48 ]. The diagram below illustrates the framework
components and their connections (Fig. 2 ).


**_Fig. 2_** Framework for sustainable FER integration in e-learning.
_Source_ Authors’ Contribution
This model supplements shortfalls in scalability (12% of studies), non-
Western environments (18%), and ethical leadership (15%) [ 62 ] to achieve
sustainable and inclusive e-learning systems [ 30 , 68 ].

Case Study Vignette:

In a 2022 Indian pilot study, culturally adapted Facial Emotion Recognition
(FER) systems were implemented in e-learning platforms to enhance
engagement and reduce dropout rates. By integrating localized emotional
models tailored to diverse Indian cultural expressions and leveraging cloud-
based infrastructure, the initiative achieved a 15% reduction in student
attrition. The project involved interdisciplinary collaboration among


educators, AI developers, and ethicists to ensure ethical deployment and
mitigate biases. This approach improved pedagogical alignment and
accessibility, demonstrating the potential of culturally sensitive FER to
foster sustainable e-learning ecosystems in non-Western contexts.

##### 5 Implications

**5.1 Theoretical Contribution**

This study makes theoretical contributions to AI-based facial emotion
recognition in online learning settings from an interdisciplinary technology,
psychology, and education perspective [ 61 , 69 ]. This study extends
affective computing theory by demonstrating the way FER can be employed
to enhance emotional presence in virtual learning environments, in line with
foundational work [ 54 ]. This paper proposes a culture-adaptive model that
integrates global models of deficit in emotion recognition [ 6 ]. The paper
continues to contribute to ethical AI studies by providing intersectionality
regarding bias, privacy, and transparency in learning spaces [ 16 ]. With a
decolonized design emphasis, this paper contributes to the requirements for
inclusive AI frameworks [ 23 , 62 ]. Moreover, it also supports learning
analytics theory with evidence of FER application in education
personalization. Being interdisciplinary is also an excellent foundation for
more theory work [ 42 , 59 ].

**5.2 Practical Recommendations**

Technical guidelines for FER implementation in e-learning environments
include scalability, cultural adaptability, and governance [ 30 , 68 ].
Institutions will implement cloud-based solutions for FER that will lower
expenses by 20%, as validated by Brazilian pilots [ 48 ]. Second, FER
systems require culturally adaptive algorithms to enhance performance by
30% in non-Western cultures such as India. Diverse facial expressions
should be incorporated into training datasets to avoid bias [ 11 ]. Third, AI
processing must be transparent, currently, only 20% of systems do, and
there must be open protocols [ 44 ]. Educators must incorporate FER with
participatory design to drive stakeholder participation. Public–private
partnerships, for instance, such as India’s use of digital channels in
education, are used to improve access [ 3 ]. Public–private partnerships
facilitate equitable and sustainable implementation of FER [ 40 ].


**5.3 Policy Advocacy**

FER’s e-learning lobbying policy focuses on equal access, moral regulation,
and subsidization to the economy [ 16 ]. Small and medium-sized schools
require government subsidies, which are 50% more expensive for FER
adoption costs. Policies must impose transparency on AI systems to address
the 60% privacy issues reported in previous studies. Paradigms for
regulation should aim to minimize bias, particularly in the non-Western
world [ 62 ]. Encouragement of interdisciplinary research should be
promoted because silos are suffocating [ 61 , 69 ]. Hybrid top-down and
bottom-up governance systems can tap local knowledge, as in India’s
scalable education [ 3 , 30 ]. Policies must promote longitudinal FER
scalability studies, which have not been achieved at 12% [ 59 , 68 ]. They
also established sustainable education markets [ 42 ] (Fig. 3 ).

**_Fig. 3_** Top 5 ethical risks and mitigations

##### 6 Conclusion

This study explores AI-based facial emotion recognition (FER) in e-
learning, highlighting its potential to enhance sustainable education markets
through personalized, adaptive learning experiences. FER boosts learner
engagement by providing real-time emotional feedback, but faces
challenges like cultural misalignments, privacy concerns, and algorithmic
bias, particularly in non-Western contexts. Scalability is hindered by high
computational costs and infrastructure limitations, which necessitate cost-
effective solutions like cloud computing. This study advocates transparent,
low-bias FER systems to address ethical issues and ensure inclusivity. A
sustainability model is proposed that balances scalable infrastructure,


cultural adaptability, and ethical safeguards. This research offers theoretical
insights into affective computing and practical guidance for policymakers
and educators to deploy FER responsibly. Future efforts must focus on
overcoming scalability, cultural, and ethical barriers to fully realize FER’s
transformative potential in creating equitable, global e-learning
environments.

##### References

1. Admiraal, W., Huizenga, J., Akkerman, S., ten Dam, G.: The concept of flow in collaborative
    game-based learning. Comput. Hum. Behav. **27** (3), 1185–1194 (2017). https:// doi. org/ 10. 1016/ j.
    chb. 2010. 12. 013
    [Crossref]
2.
    Allen, I.E., Seaman, J.: Digital Learning Compass: Distance Education Enrollment Report 2017.
    Babson Survey Research Group (2017)
3.
    Anand, R., Saxena, S., Gupta, A.: Digital education initiatives in India: scalability and impact. J.
    Educ. Technol. Syst. **52** (4), 345–367 (2024)
4.
    Arguel, A., Lockyer, L., Lipp, O.V., Lodge, J.M., Kennedy, G.: Inside out: detecting learners’
    confusion to improve interactive digital learning environments. J. Educ. Comp. Res. **57** (5),
    1125–1153 (2019). https:// doi. org/ 10. 1177/ 07356331 16674732
    [Crossref]
5.
    Bahreini, K., Nadolski, R., Westera, W.: Towards real-time speech emotion recognition for
    affective E-learning. Educ. Inf. Technol. **21** (5), 1367–1386 (2015). https:// doi. org/ 10. 1007/
    s10639-015-9388-2
    [Crossref]
6.
    Barrett, L.F., Adolphs, R., Marsella, S., Martinez, A.M., Pollak, S.D.: Emotional expressions
    reconsidered: challenges to inferring emotion from human facial movements. Psychol. Sci.
    Public Interest **20** (1), 1–68 (2019). https:// doi. org/ 10. 1177/ 1529100619832930
    [Crossref]
7.
    Benjamin, R.: Race After Technology: Abolitionist Tools for the New Jim Code. Polity Press
    (2019)
8.
    Bond, M., Zawacki-Richter, O., Nichols, M.: Revisiting five decades of educational technology
    research: a content analysis of journal publications. Br. J. Edu. Technol. **50** (1), 12–63 (2020).
    https:// doi. org/ 10. 1111/ bjet. 12730
    [Crossref]
9.
    Booth, A., Sutton, A., Papaioannou, D.: Systematic Approaches to a Successful Literature
    Review, 2nd edn. SAGE Publications (2016)
10.
    Braun, V., Clarke, V.: Reflecting on reflexive thematic analysis. Qual. Res. Sport Exerc. Health
    **11** (4), 589–597 (2019). https:// doi. org/ 10. 1080/ 2159676X. 2019. 1628806


[Crossref]
11.
Buolamwini, J., Gebru, T.: Gender shades: intersectional accuracy disparities in commercial
gender classification. Proc Mach Learn Res **81** , 77–91 (2018). https:// proceedings. mlr. press/ v81/
buolamwini18a. html
12.
Burbules, N.C., Fan, G., Repp, P.: Five trends of education and technology in a sustainable
future. Geogr Sustain **1** (2), 93–97 (2020). https:// doi. org/ 10. 1016/ j. geosus. 2020. 05. 001
[Crossref]
13.
Calvo, R.A., D’Mello, S., Gratch, J., Kappas, A. (eds.): The Oxford Handbook of Affective
Computing. Oxford University Press (2015)
14.
Castañeda, L., Selwyn, N.: More than tools? Making sense of the ongoing digitization of higher
education. Int. J. Educ. Technol. High. Educ. **15** (1), 22 (2018). https:// doi. org/ 10. 1186/ s41239-
018-0109-y
15.
Chen, C.M., Wang, J.Y., Yu, C.M.: Adaptive learning systems using artificial intelligence: A
review. Educ. Technol. Soc. **23** (3), 85–97 (2020)
16.
Crawford, K.: Atlas of AI: Power, Politics, and the Planetary Costs of Artificial Intelligence. Yale
University Press (2021)
[Crossref]
17.
Cuban, L.: The Flight of a Butterfly or the Path of a Bullet? Using technology to transform
teaching and learning. Harvard Education Press (2018)
18.
Dabbagh, N., Marra, R.M., Howland, J.L.: Meaningful Online Learning: Integrating Strategies,
Activities, and Technologies. Routledge (2016)
19.
D’Mello, S.K., Kory, J.: A review and meta-analysis of multimodal affect detection systems.
ACM Comput Surv **47** (3), 43 (2015). https:// doi. org/ 10. 1145/ 2682899
20.
Dhawan, S.: Online learning: a panacea in the time of the COVID-19 crisis. J. Educ. Technol.
Syst. **49** (1), 5–22 (2020). https:// doi. org/ 10. 1177/ 0047239520934018
[Crossref]
21.
EdTechXGlobal.: Global EdTech Market Outlook 2023: Trends and Opportunities.
EdTechXGlobal (2023)
22.
Ekman, P.: Emotions Revealed: Recognizing Faces and Feelings to Improve Communication and
Emotional Life, 2nd edn. Holt Paperbacks (2015)
23.
Escobar, A.: _Designs for the Pluriverse: Radical Interdependence, Autonomy, and the Making of
Worlds_. Duke University Press (2018)
24.
Facer, K., Selwyn, N.: Digital Technology and the Futures of Education: Towards Sustainable
Educational Ecosystems. UNESCO Futures of Education Initiative (2021)
25.
Floridi, L., Sanders, J.W.: On the morality of artificial agents. Mind. Mach. **14** (3), 349–379
(2004). https:// doi. org/ 10. 1023/ B: MIND. 0000035461. 63578. 9d
[Crossref]


26.
Floridi, L., Cowls, J., Beltrametti, M., Chatila, R., Chazerand, P., Dignum, V., Vayena, E.:
AI4People—an ethical framework for a good AI society: opportunities, risks, principles, and
recommendations. Mind. Mach. **28** (4), 689–707 (2018). https:// doi. org/ 10. 1007/ s11023-018-
9482-5
[Crossref]
27.
Frodeman, R.: The future of interdisciplinarity: an introduction to the 2nd edition. In: Frodeman
R (ed.) The Oxford Handbook of Interdisciplinarity, 2nd edn., pp. 3–12. Oxford University Press
(2017)
28.
Graesser, A., Hu, X., Nye, B., & Sottilare, R.: Intelligent tutoring systems, serious games, and
the generalized intelligent framework for tutoring (GIFT). In: Using Games and Simulations for
Teaching and Assessment: Key Issues, pp. 58–79. https:// doi. org/ 10. 4324/ 9781315817767
29.
Harley, J.M., Lajoie, S.P., Frasson, C., Hall, N.C.: Developing emotion-aware, advanced learning
technologies: a taxonomy of approaches and features. Int. J. Artif. Intell. Educ. **27** (2), 268–297
(2017). https:// doi. org/ 10. 1007/ s40593-016-0126-8
[Crossref]
30.
Holmes, W., Bialik, M., Fadel, C.: Artificial Intelligence in Education: Promises and
Implications for Teaching and Learning. Center for Curriculum Redesign (2021)
31.
Immordino-Yang, M.H.: Emotions, Learning, and the Brain: Exploring the Educational
Implications of Affective Neuroscience. W. W. Norton & Company (2016)
32.
Immordino-Yang, M.H., Damasio, A.: We feel, therefore we learn: the relevance of affective and
social neuroscience to education. Mind Brain Educ. **1** (1), 3–10 (2007). https:// doi. org/ 10. 1111/ j.
1751-228X. 2007. 00004. x
[Crossref]
33.
Kirkwood, A., Price, L.: Technology-enhanced learning and teaching in higher education: what is
‘enhanced’ and how do we know? Learn. Media Technol. **39** (1), 6–36 (2014). https:// doi. org/ 10.
1080/ 17439884. 2013. 770404
[Crossref]
34.
Kitchenham, B., Charters, S.: Guidelines for Performing Systematic Literature Reviews in
Software Engineering (Technical Report EBSE-2007–01). Keele University (2007)
35.
Knox, J.: Artificial intelligence and education in China. Learn. Media Technol. **45** , 298–31 1
(2020). https:// doi. org/ 10. 1080/ 17439884. 2020. 1754236
[Crossref]
36.
Ko, B.C.: A brief review of facial emotion recognition based on visual information. Sensors
**18** (2), 401 (2018). https:// doi. org/ 10. 3390/ s18020401
[Crossref]
37.
Kort, B., Reilly, R., Picard, R. W.: An affective model of interplay between emotions and
learning: reengineering educational pedagogy. In: Proceedings of IEEE International Conference
on Advanced Learning Technologies, pp. 43–46. https:// doi. org/ 10. 1109/ ICALT. 2001. 943850


38.
Li, S., Deng, W.: Deep facial expression recognition: a survey. IEEE Trans. Affect. Comput.
**13** (2), 1195–1215 (2020). https:// doi. org/ 10. 1109/ TAFFC. 2020. 2981446
[MathSciNet][Crossref]
39.
Li Pragati, R., Sharma, S., Gupta, V.: Personalizing learning outcomes with FER: a longitudinal
study. Int. J. Artif. Intell. Educ. **33** (2), 256–278 (2023)
40.
Liao, J.: Scalability challenges in AI-driven e-learning systems: a review. J. Educ. Technol. Soc.
**26** (3), 45–60 (2023)
41.
Luckin, R.: Machine Learning and Human Intelligence: Lessons from the Education Sector. MIT
Press (2018)
42.
Luckin, R., et al.: _AI and Education: A Roadmap FOR Global Implementation_. UNESCO
Publishing (2022)
43.
Means, B., Bakia, M., Murphy, R.: Learning Online: What Research Tells us About Whether,
When, and How. Routledge (2020)
44.
Mittelstadt, B.D., Allo, P., Taddeo, M., Wachter, S., Floridi, L.: The ethics of algorithms:
mapping the debate. Big Data Soc. **3** (2), 1–21 (2016). https:// doi. org/ 10. 1177/
2053951716679679
[Crossref]
45.
Moher, D., Shamseer, L., Clarke, M., Ghersi, D., Liberati, A., Petticrew, M., Stewart, L.A.:
Preferred reporting items for systematic review and meta-analysis protocols (PRISMA-P) 2015
statement. Syst Rev **4** (1), 1 (2015). https:// doi. org/ 10. 1186/ 2046-4053-4-1
46.
Mollahosseini, A., Hasani, B., Mahoor, M.H.: AffectNet: a database for facial expression,
valence, and arousal computing in the wild. IEEE Trans. Affect. Comput. **10** (1), 18–31 (2019).
https:// doi. org/ 10. 1109/ TAFFC. 2017. 2740923
[Crossref]
47.
Noble, S.U.: Algorithms of Oppression: How Search Engines Reinforce Racism. NYU Press
(2018)
48.
Oliveira, J.C., Ogasawara, E.: Cloud-based solutions for scalable e-learning: a case study in
Brazil. Proc IEEE Int Conf E-Learn 123–130 (2010)
49.
Page, M.J., McKenzie, J.E., Bossuyt, P.M., Boutron, I., Hoffmann, T.C., Mulrow, C.D., Moher,
D.: The PRISMA 2020 statement: an updated guideline for reporting systematic reviews. BMJ
**372** , n71 (2021). https:// doi. org/ 10. 1136/ bmj. n71
[Crossref]
50.
Pekrun, R.: Emotion and achievement during adolescence. Child Dev. Perspect. **12** (4), 215–221
(2018). https:// doi. org/ 10. 1111/ cdep. 12237
[Crossref]
51.
Penuel, W.R., Fishman, B.J., Yamaguchi, R., Gallagher, L.P.: What makes professional
development effective? Strategies that foster curriculum implementation. Am. Educ. Res. J.
**57** (3), 1213–1250 (2020). https:// doi. org/ 10. 3102/ 0002831207308221


[Crossref]
52.
Picard, R.W.: Affective computing: challenges. Int. J. Hum Comput Stud. **59** (1–2), 55–64
(2003). https:// doi. org/ 10. 1016/ S1071-5819(03)00052-1
[Crossref]
53.
Picard, R.W.: Affective Computing. MIT Press (2016)
54.
Picard, R.W., Papert, S., Bender, W., Blumberg, B., Breazeal, C., Cavallo, D., Strohecker, C.:
Affective learning—a manifesto. BT Technol. J. **22** (4), 253–269 (2004). https:// doi. org/ 10. 1023/
B: BTTJ. 0000047603. 37042. 33
[Crossref]
55.
Popenici, S.A.D., Kerr, S.: Exploring the impact of artificial intelligence on teaching and
learning in higher education. Res. Pract. Technol. Enhan. Learn. **12** (1), 22 (2017). https:// doi. org/

10. 1186/ s41039-017-0062-8
56.
Poria, S., Cambria, E., Bajpai, R., Hussain, A.: A review of affective computing: from unimodal
analysis to multimodal fusion. Inform. Fus. **37** , 98–125 (2017). https:// doi. org/ 10. 1016/ j. inffus.
2017. 02. 003
[Crossref]
57.
Regan, P.M., Jesse, J.: Ethical challenges of edtech, big data, and personalized learning: twenty-
first century student sorting and tracking. Ethics Inf. Technol. **21** (3), 167–179 (2019). https:// doi.
org/ 10. 1007/ s10676-018-9492-2
[Crossref]
58.
Roll, I., Wylie, R.: Evolution and revolution in artificial intelligence in education. Int. J. Artif.
Intell. Educ. **26** (2), 582–599 (2016). https:// doi. org/ 10. 1007/ s40593-016-0110-3
[Crossref]
59.
Selwyn, N.: Re-imagining ‘learning analytics’... a case for starting again? Internet High. Educ.
**46** , 100745 (2020). https:// doi. org/ 10. 1016/ j. iheduc. 2020. 100745
[Crossref]
60.
Selwyn, N.: The future of AI and education: some cautionary notes. Europ. J. Educ. **57** (4), 620–
631 (2022). https:// doi. org/ 10. 1111/ ejed. 12532
61.
Siemens, G.: Learning analytics: the emergence of a discipline. Am. Behav. Sci. **57** (10), 1380–
1400 (2013). https:// doi. org/ 10. 1177/ 0002764213498851
[Crossref]
62.
Silano, G.: Decolonizing AI in education: towards inclusive emotion recognition systems. J.
Educ. Technol. Soc. **27** (1), 123–140 (2024)
63.
Strubell, E., Ganesh, A., McCallum, A.: Energy and policy considerations for deep learning in
NLP. In: Proceedings of the 57th Annual Meeting of the Association for Computational
Linguistics, pp. 3645–3650. https:// doi. org/ 10. 18653/ v1/ P19-1355
64.
Tyng, C.M., Amin, H.U., Saad, M.N.M., Malik, A.S.: The influences of emotion on learning and
memory. Front. Psychol. **8** , 1454 (2017). https:// doi. org/ 10. 3389/ fpsyg. 2017. 01454


65.
UNESCO: AI and Education: Guidance for Policy-Makers. UNESCO Publishing (2023)
66.
Wang, Y., Tahir, R.: Affective computing in education: a systematic review. J. Educ. Technol.
Soc. **23** (4), 112–125 (2020)
67.
Weller, M.: 25 Years of ed tech. Athabasca University Press (2020)
68.
Williamson, B.: Big Data in Education: The Digital Future of Learning, Policy and Practice.
SAGE Publications (2020)
69.
Zawacki-Richter, O., Marín, V.I., Bond, M., Gouverneur, F.: Systematic review of research on
artificial intelligence applications in higher education—where are the educators? Int. J. Educ.
Technol. High. Educ. **16** (1), 39. https:// doi. org/ 10. 1186/ s41239-019-0171-0
70.
Zuboff, S.: The Age of Surveillance Capitalism: The Fight for a Human Future at the New
Frontier of Power. PublicAffairs (2019)
_OceanofPDF.com_


(1)

(2)

(3)

(4)

(5)

© The Author(s), under exclusive license to Springer Nature Switzerland AG 2026
K. Slimani et al. (eds.), _Emotion and Facial Recognition in Artificial Intelligence: Sustainable
Multidisciplinary Perspectives and Applications_ , Information Systems Engineering and Management
78
https://doi.org/10.1007/978-3-032-14778-3_14

## Integrating Positive Emotions to Support

## Self-Directed Learning

Hommane Boudine^1 , Meriem Bentaleb^2 , Driss El Karfa^3 ,

Khadija Slimani^4 and Abderrahim Tayebi^5
Geosciences Laboratory, Faculty of Sciences, Ibn Tofail University,
Kénitra, Morocco
Didactics, Educational Sciences and Teaching and Training
Professions in Mathematics and Experimental Sciences, Faculty of
Sciences, Ibn Tofail University, Kénitra, Morocco
Laboratory of Territories, Environment and Development, Faculty of
Human and Social Sciences, Ibn Tofail University, Kénitra, Morocco
esieaLab LDR, Higher School of Computer Science Electronics and
Automation (ESIEA), Paris, France
Scientific Research and Innovation Laboratory (SRILab), Ibn Tofail
University, Kénitra, Morocco

```
Hommane Boudine
Email: hommane.boudine@gmail.com
```
**Abstract**
Teaching practices are challenged by the need to motivate students, placing
teachers at a crossroads between old and new models. As a historical
institution, the school has adapted to the constantly evolving field of
science. Furthermore, traditional teaching models primarily focus on
instilling concepts in students, paying little attention to their emotions and
feelings. These emotions have become harder to manage due to the
widespread use of smartphones and artificial intelligence. This research


project explores the relationship between positive emotions and motivation
to optimise mobile and peer learning. A sample of 208 first-year high school
students was selected and divided into two groups: a control group and an
experimental group. The experimental group participated in educational
activities designed to evoke positive emotions (such as confidence,
gratitude, love, joy, admiration, cheerfulness, calmness, curiosity, hope,
empathy, enthusiasm, amusement, optimism, satisfaction, inspiration, and
passion) at the beginning of each lesson. The results show that this
approach motivates students to participate and interact positively with their
peers, creating an environment conducive to personal or self-directed
learning.

**Keywords** Positive emotions – Peer learning – M-learning – Self-directed
learning – Flipped classroom

##### 1 Introduction

The educational community around the world is suffering from complex
educational disparities that can no longer be ignored. Classrooms are often
characterized by monotony, lack of interest, and a lack of educational
responsibility, negatively impacting educational outcomes and student
achievement. In the past, teaching relied primarily on traditional methods
such as speeches and lectures. However, with the spread of information and
communications technology, distance learning has become possible thanks
to learning platforms and the ease of integrating technological tools into
teaching and pedagogical practices. Furthermore, more interactive methods
such as simulations, project-based learning, and innovation have become
possible, allowing students to work together and develop diverse and
essential skills that help them build their projects [ 1 – 3 ].
Today, amid rapid changes in the world due to technological and digital
developments, many educational studies seek to develop curricula that are
more relevant to learners and practical methods that promote a sense of
responsibility for learning. This has become more difficult over time,
especially with the increasing use of the internet and artificial intelligence,
which has widened the gap between schools and learners in the absence of
the necessary educational controls. The internet can play a positive role as a
source of information, especially if learners’ emotions are properly guided


and controlled by teachers using appropriate digital technologies and tools
[ 4 – 7 ].
All these changes, and perhaps more, have led to a strong desire to
consider changing teaching methods to respond to the diverse aspirations of
learners and to control the increasing spread of artificial intelligence, which
may pose a threat to students’ feelings and emotions, their sense of
responsibility, and their educational motivation toward school in reality,
especially in the absence of controls governing its use [ 8 – 10 ].
In 2014, teachers and researchers experimented with new teaching
methods using digital tools. However, these tools were found to offer
opportunities to engage with digital tools and get closer to students,
accelerating the learning process for only interested learners. This approach
is not aligned with the unique perspectives of other students, especially
those who lack a passion for learning and skill building. Therefore, this
study aims to measure the impact of passion-inducing activities on
classroom performance, viewing the classroom as a community of teachers
and students working as a team to build and develop each other’s skills
[ 11 ].
Caitlin Fulton’s final argument relates to the quality of teaching. She
demonstrates that teachers who motivate students with positive emotions
and engage them in activities that spark their passion can enhance the
learning experience for all. When students communicate positively with the
teacher or their peers in class, they can grasp concepts in a way that is
consistent with the overall team perspective in less time. This enhances the
effectiveness of the project methodology and enables teachers to guide
students and understand their needs more effectively. Based on the
experiences of teachers worldwide, this study examined the impact of
allocating 5–10 min at the start of each class for activities that stimulate
interest, in line with educational strategies, on academic performance and
motivation to learn. The study aims to test the following hypotheses:

```
Motivate students to work together to improve their performance,
particularly those who have difficulty integrating with their peers, and
encourage greater classroom participation.
Integrate students who find it difficult to understand by incorporating
digital media (e.g. tablets, phones and computers) into the teaching and
learning process.
```

```
Unify the perspectives of all students when developing and building their
personal lessons, without exception (Fig. 1 ).
```
**_Fig. 1_** Context of the classroom approach
This pedagogical model aims to foster positive feelings among learners
at the start of each lesson. Between 10 and 15 min are allocated to fostering
positive attitudes towards the course content and establishing a culture of
mutual respect in the classroom. This encourages learners to use their
smartphones responsibly and fosters a sense of teamwork.
The philosophy behind this educational model is to create learning
situations (outside of the two-hour lesson content) that aim to foster positive
feelings among learners toward the class and course content during each
session. Between 10 and 15 min are allocated to Classroom Dialogue
activities (Give students the opportunity to share their experiences and
speak from their own perspective, while encouraging them to understand
the reasons behind their point of view), such as presenting an interesting
topic rich in positive emotions, according to rules that enable students to
express and discuss their concerns (e.g., presenting a philosophical view of
life or directing them to watch a realistic video that stimulates their human
emotions). This encourages active participation in a discussion that
promotes critical thinking and supports and reinforces trust and a culture of
mutual respect. The teacher then begins to guide students to find solutions
to their life concerns, reminding them of the importance of ethics in
building and developing society. This encourages learners to use their
smartphones responsibly and promotes teamwork [ 12 – 14 ].


##### 2 Materials and Methods

**2.1 Sample and Study Location**

This study aims to provide an overview of classes conducted based on
educational principles, using a concept that allocates time to promoting
positive feelings among learners. To this end, the research team conducted a
field study at a secondary school affiliated with the Rabat Academy in the
city of Sidi Slimane, Morocco, to assess the effectiveness of integrating this
concept in improving educational performance while taking various
requirements and conditions into account.
The sample for this study consists of first-year students at the
preparatory secondary level. The data were organised according to the
number of participants (208 students), and the control and experimental
samples were determined alongside their respective age groups and gender
distributions:

control group: 4 classes, 140 students (Group A and B).

experimental group: 2 classes, 68 students (Group C).
Due to frequent changes in class schedules during the experimental
period for administrative reasons and our fear that this would affect the
reliability of the results, we relied on data from Group B only from the
experimental group.

**2.2 Pedagogical Method**

This approach offers a fresh perspective on education, providing practical
solutions to the diverse challenges faced by teachers in educational settings
and aiming to improve pedagogical quality. Although this strategy uses a
relatively large amount of class time, it is considered one of the most
effective learning methods for students from different backgrounds.
The Student’s t-test was used to test whether the difference between the
responses of the two groups was statistically significant. Statistical
hypotheses were tested by tracking the distribution of the Student’s t-test
under the null hypothesis.

```
Students are motivated by integrating digital media (tablets, phones,
computers, etc.) into the teaching/learning process.
```

```
Students who struggle with comprehension have time to review the video
explanation to improve their performance and become more active in the
classroom.
All students are united in developing and constructing the assigned
lesson collectively, without exception.
```
##### 3 Results

**3.1 The Correlation Between Passion-Inducing Activities and**

**Academic Motivation**

Students’ responses were monitored in the classroom, and a questionnaire
was distributed to determine which activities were most influential in
creating students’ motivation to learn over twelve weeks, to find the
relationship between the use of emotionally stimulating activities and
academic achievement (Table 1 ).

**_Table 1_** Responses of students in the pilot semester to the questionnaire

(^) **Very
satisfied
(%)
Satisfied
(%)
Dissatisfied
(%)
Very
dissatisfied
(%)
No
answer
(%)
Socio-educational relationship
Quality the lesson after
Passion-inducing activities**
79.9 14.8 0.1 0.4 4.98
**Understanding the process** 78.1 16.5 0.2 0.4 4.98
**Courses quality** 85 8.3 1.9 0 4.98
**Responsiveness** 91.7 3.3 0.2 0 4.98
**Clarity of the steps of the
procedure**
63.2 31.7 0.3 0 4.98
**Performance of the activitéses** 4.98
**Reliability of results** 88.2 6.3 1.7 0 4.98
**Ergonomics** 91.9 3.3 0 0 4.98
**Product Features** 73.2 21.7 0.3 0 4.98
**Overall rating** Comfortable: Yes
94.01
Comfortable: No
1.01
No
answer
4.98


Mean = 0.238, Variance = 0.054063
The activities that had the most impact on motivating students to learn
over twelve weeks were identified by calculating the variance in the pilot
classroom. This showed a significant difference compared to the control
sample, supporting the hypothesis that students’ perspectives on developing
and building their personal lessons collectively were unified (Table 2 ).

**_Table 2_** Students’ perceptions of emotionally
stimulating activities in motivating their passion
for learning

```
Response Number Percentage
Strongly disagree 2 0.96
Disagree 4 1.92%
Neutral 22 10.57
Agree 82 39.42%
Strongly agree 98 47.11
Total 208 99.99%
```
A large percentage of students in the pilot sample (47.11% strongly
agreed and 39.42%) expressed appreciation for the emotionally stimulating
activities and their support for using smartphones as an educational tool.
They also expressed approval of the idea of mobile learning. These data
highlight that the majority of adolescents view smartphones as a tool for
information seeking and simulation.
Figure 2 shows a significant discrepancy in the average scores of
students in the control group based on the obtained degrees, who attended
their classes as usual and did not benefit from stimulating activities or self-
directed learning.


**_Fig. 2_** Average of degrees obtained by students in the control sample
Figure 3 shows less variance in students’ mean scores compared to the
control group, as students benefit from stimulating activities when they
come to their classes and/or self-directed learning. Students can ask
questions and get help at any time, both during and outside of class, using
digital communication tools. This creates two distinct groups within the
same class:

**_Fig. 3_** Average of degrees obtained by students in the experimental sample


The first: students who are interested in their lessons and constantly try
to communicate with their teachers.
The second: students who are not interested in their lessons or what
their teachers are saying.
Digital tools enable teachers to communicate with and guide students
both during and outside of class, enabling them to be creative and
innovative in their lesson planning. They can also experiment with diverse
teaching methods and multimedia elements to make learning more
engaging.
Figure 4 shows the results students achieved in the three science
subjects targeted by the project, using data collected from 67 tests. This
took a long time (34 weeks), and we note that it took students 6 to 8 weeks
to adapt to the new situation.


**_Fig. 4_** Academic learning degrees of students according to 68 tests conducted over 34 months

##### 4 Discussion

In light of the rapid changes that the world is experiencing today as a result
of technological and digital developments, the promotion of positive
emotions in the real world is being neglected, particularly among


adolescents. This is because it has become possible to experience positive
emotions in the virtual world, causing adolescents to lose their passion for
the real world. According to Barnett and Bakin, a lack of motivation to
learn can have serious consequences for students’ academic performance.
This can lead to students dropping out of school or even resorting to
violence towards teachers because they have lost the desire to learn. Marco
Gilbo highlights the strong correlation between a lack of motivation and
dropping out of school. This loss of motivation is primarily due to students’
negative perceptions of school and their lessons [ 15 – 19 ].

**4.1 Enjoying the Class in an Atmosphere of Understanding**

**and Gratitude**

In order to explore the benefits of fostering positive emotions at the start of
each lesson, it is essential to adopt educational approaches that enable
students to articulate their aspirations and sentiments regarding the course
content. This study aims to evaluate the effectiveness of devoting 10 min of
each class to improving the classroom atmosphere, as this directly impacts
student outcomes. The study also seeks to propose recommendations for
successfully integrating this approach into teaching practice. Through
research aimed at increasing the effectiveness of the teaching/learning
process, we seek to identify the most effective methods to encourage
learners to take responsibility for their own learning. This is achieved by
integrating quality learning that meets societal demands and the changing
labour market, while also integrating soft skills and digital tools into
learners’ daily lives. This pedagogy enables us to connect with students and
allows them to learn the complexities of the course at their own pace.
By utilizing all available resources and creating a positive classroom
environment, teachers can better understand each student’s challenges
through quick assessments or tracking their progress on projects.
Additionally, teachers can assess the time it takes each student to complete
assigned tasks, and struggling students can individually review the
explanation on their phones to fill in learning gaps, thanks to M-learning.
[ 20 – 22 ].

**According to students’ answers:**

```
Integrating students with special needs and disabilities:
Promoting parental involvement in the learning process:
```

```
Improving teacher practices
Developing self-directed learning among students through robotics
(project pedagogy):
Monitoring and Developing Multiple Skills:
```
**4.2 Developing Self-Learning Among Students Through**

**Robotics (Project Pedagogy):**

Online self-learning has become one of the most widely adopted forms of
teaching by some higher education institutions, allowing them to avoid
sudden disruptions to their educational systems in the future, especially
after the global health crisis and the pressures imposed by the COVID-19
pandemic. However, this pedagogical approach poses a set of challenges
that educational institutions must consider to mitigate the educational
consequences that could result from the reckless application of this
pedagogical approach.
In the practice of flipped teaching, the student can freely choose to
focus on learning, as it provides them with skills and knowledge that can be
used during group work in the classroom. Furthermore, they realize that if
they do not complete the preliminary work, they will not be able to
complete the required task. This autonomy granted to the student allows
them to take responsibility for their own learning and understand the
importance of their active participation in the knowledge acquisition
process. The most important factor in this learning process is the social
dimension, as emphasized by Vygotsky (1985). The student actively
constructs meaning through interaction with his or her peers, and is
considered a social actor at the center of his or her learning. The typical
flipped classroom configuration respects this vision by allowing the active
learner to interact with his or her classmates to overcome any ambiguity
that might hinder understanding, thanks to the group work permitted in
practical exercises.
According to this perspective, students must perform tasks in a specific
environment and within a field of work that aligns with their interests. This
also redefines the role of the teacher, who is no longer viewed solely as the
exclusive possessor of knowledge and responsible for presenting and
transmitting it to students. Now, if the student is to be an active participant
in their learning, the teacher must work to select and inculcate key concepts
to the learner through available digital platforms and channels, and then


create learning situations within the classroom that allow them to gradually
become aware of their role and the tools available to them to accomplish the
desired task. By following the flipped classroom approach, students become
more independent in their work, unlike in the traditional approach. They
can actively engage in procedural projects based on innovation and the
search for solutions, and verify their validity by working within the project
pedagogy and conducting an investigation that sheds light on the basic
concepts and mechanisms of learning. Thus, the student gradually learns to
use the tools available to them to accomplish the assigned task [ 6 , 23 – 25 ].
For a student to actively participate in completing an educational project
in learning modern technologies using the flipped classroom, it is important
to set goals and objectives so that they know where they are heading. This
also helps reduce anxiety and uncertainty about the unknown. Unlike
traditional methods, where communication between teacher and student was
limited, it is essential for the student to have the freedom to research or ask
the teacher to reformulate partial objectives or provide details about the
necessary concepts and mechanisms in order to obtain the information
needed to complete the task. The flipped classroom facilitates
communication between the teacher and students by allowing them to
repeatedly view the explanation of functions previously studied in class.
This allows them to ask questions about areas they find unclear [ 3 – 26 ].
According to Frederickson’s theory of positive emotion expansion and
construction, positive emotions broaden a person’s awareness, encouraging
creative and diverse thoughts and actions as they explore new things. Over
time, this expansion of behaviours leads to the development of skills and
resources. For example, curiosity about the natural world can lead to
valuable knowledge, interactions with strangers can lead to supportive
friendships, and random physical play can lead to athletic training and
physical excellence. This is in contrast to negative emotions, which elicit
narrow, immediate, survival-oriented behaviours that trigger psychological
disturbances, often beginning with anxiety and developing further.
Conversely, positive emotions have no immediate survival value as they
distract the mind from immediate needs and psychological pressures.
However, the skills and resources developed through expansive behaviour
enhance survival in the long term [ 27 , 28 ].
The educational process currently faces a variety of challenges that
affect the development and dissemination of interactions between people,


such as the Internet and artificial intelligence. Guiding adolescents by
rebuilding and establishing positive emotions is essential in education,
especially for adolescents, as it provides exciting and innovative
opportunities that help teachers create a dynamic and engaging learning
environment that meets the needs of adolescents by teaching them complex
concepts that require engaging experiences and innovative teaching
methods. Customized and interactive lessons can be delivered based on data
analysis and simulations to understand each student’s educational needs.
Complex concepts can also be taught by providing hands-on educational
experiences and other projects that use various resources such as artificial
intelligence to build their economic and personal projects and create digital
applications.

##### 5 Conclusions

Educational institutions have always been the cornerstone of every society,
transferring knowledge to young people outside the family and playing an
indispensable role in education. However, the rapid development of
information and communication technology has opened up many new
possibilities and led to increased interest in digital devices, especially
smartphones. The unguided use of these devices can pose a major
educational challenge due to the values and principles they instill, the
impact of which on society, especially among children and adolescents, is
impossible to predict. Students have access to online resources and content
that are not tailored to their individual needs.
In order to effectively combine and reinforce positive emotions and
mobile learning, several strategies can be implemented to ensure smooth
transactions and maximise benefits. The following key considerations and
techniques have been employed successfully in educational institutions
when applying this approach:

```
Motivation and increased engagement: Incorporating positive emotions
can enhance student engagement and motivation. Allowing students to
access online resources before class enables them to familiarise
themselves with concepts and prepare to participate in classroom
activities, thereby increasing their involvement in the learning process.
Personalised or self-directed learning: Creating a positive atmosphere at
the beginning of each semester motivates students to engage with the
```

course content. Teachers can then devote more class time to practical,
interactive activities, providing individual attention to each student’s
needs.
Peer collaboration and interaction: Positive feelings encourage
collaboration and interaction among students. This may include online
communication, discussion forums, document-sharing tools and online
group activities, enabling students to collaborate, share ideas and solve
problems together.
Assessing prior knowledge: Before engaging with course content, it can
be beneficial to assess students’ prior knowledge of the subject. This can
be done through pre-tests or diagnostic tests. Understanding students’
prior knowledge enables teachers to design classroom activities that
address specific gaps and challenge students at an appropriate level.
Facilitate active learning. In the classroom, shift the focus from passive
teaching to active learning. Design activities that engage all students in
problem-solving, discussions, debates and collaborative projects. These
activities should reinforce the concepts and ideas presented in the online
materials, encouraging critical thinking and the application of
knowledge.
Provide guidance and support. As students work on these activities,
teachers should act as facilitators and pedagogical guides, answering
questions and providing clarification. This support is critical to ensuring
that students stay on track and understand the content.
Assess learning outcomes: Regularly assess student learning outcomes to
evaluate the effectiveness of the blended approach using formative
assessments, educational/learning projects, short tests or classroom
discussions. These assessments can provide students with feedback and
help to identify areas that may need additional reinforcement.
For these and many other reasons, supporting adolescent learning by
reinforcing positive emotions helps them to regulate and guide their self-
directed learning. This is undoubtedly the most effective teaching strategy
in different circumstances. This interesting philosophy should be
implemented in the new apprenticeship and in-service teacher training
reforms.

##### 6 Limitations of the Study


The study was limited by the small sample size and the fact that it was
restricted to a single educational institution. In light of the development of
artificial intelligence, we suggest that future educational science research be
deepened so that artificial intelligence can be used as efficiently as possible.

**Acknowledgements**
The authors would like to thank the students who participated in this study
and express their gratitude to Mr Hassan Sajid, Inspector of Educational
Planning and Head of the Education Department in Sidi Slimane, for his
support and approval of the questionnaire, and for following up on the
research.

**Author Contributions**
Authors H.B analyzed the data and wrote the B.M. conducted the
experiments and wrote the paper. K. S analyzed the data, and A. T prepared
the figures. All authors had approved the final version.

**Funding**
This research received no external funding.

**Conflict of Interest** The authors declare no conflict of interest.

##### References

1. Prignot, P.: Classe inversée et élèves de l’enseignement secondaire: d’une perspective
    technologique à une approche anthropologique. Doctoral dissertation, Université de Strasbourg
    (2019)
2.
    Boudine, H., Bentaleb, M., Radi, M., El Madhi, Y., Er-rzine, S., Bentaleb, I., Tayebi, M.: Flipped
    classroom: experience of a pedagogical model adopted during the health crisis to support work-
    study teaching. Moroccan J. Quant. Qual. Res. **3** (1), 70–91 (2021)
3.
    Boudine, H., Bentaleb, M., Mrigua, K., Tayebi, M.: Flipped classroom and digital classes in
    Morocco: experience quality educational digitization with ease. Res. Militaris (3) (2023)
4.
    Hutain, J., Michinov, N.: Improving student engagement during in-person classes by using
    functionalities of a digital learning environment. Comput. Educ. **183** , 104496 (2022)
    [Crossref]
5.
    Ait Moussa A.: L’impact de la méthode inversée sur un cours d’informatique: une étude de cas à
    l’université d’Oujda (2016)


6.
Parent, G., Paquin, A.: Enquête auprès de décrocheurs sur les raisons de leur abandon scolaire.
Rev. Sci. L’Éducation (1994).
7.
Sunday, O.J., Adesope, O.O., Maarhuis, P.L.: The effects of smartphone addiction on learning: a
meta-analysis. Comp. Hum. Behav. Rep. **4** , 100114 (2021)
8.
Guilbault, M., Viau-Guay, A.: La classe inversée comme approche pédagogique en enseignement
supérieur: état des connaissances scientifiques et recommandations. Rev. Int. Pédagogie
L’Enseignement Supérieur **33** (1) (2017)
9.
Roy-Wsiaki, G.N., Gravel, N.R., Pongoski, M.L.: Évaluation de la plateforme pédagogique
Simple Steps: une ressource d’intervention accessible et inclusive pour le TSA. La NouvLe.
Rev.-Éducation Société InclS. **93** (1), 193–213 (2022)
10.
Slimani, K., Ruichek, Y., Messoussi, R.: Compound facial emotional expression recognition
using cnn deep features. Eng. Lett. **30** (4), 1402–1416 (2022)
11.
Bentaibi, R.: Flipped classroom: an innovative and revolutionary pedagogy of learning. Int. J.
Adv. Res. **12** , 64–71 (2018). https:// doi. org/ 10. 21474/ IJAR01/ 8115
12.
Leclercq, D., Poumay, M.: Le modèle des événements d’apprentissage-Enseignement (2008)
13.
Cormier, C., Voisard, B.: La pédagogie inversée: une évaluation de son efficacité sur les résultats
scolaires et sur l’intérêt des étudiants. Pédag. Coll. **31** (3), printemps (2018)
14.
Lebrun, M.: Classe inversée oui mais... Quoi et comment? Pourquoi et pour quoi? (2016)
15.
Ratompomalala, H., Razafimbelo, J.: Images numériques: simulations et vidéos: Quels apports
pour l’enseignement apprentissage de la physique? (2019)
16.
Lahchimi, M.: La réforme de la formation des enseignants au Maroc, Revue internationale
d’éducation de Sèvres (2015)
17.
Hoang-Oanh, T.T.: The impact of a flipped classroom on student learning achievements in EFL
classrooms. Educ. Lang. Sociol. Res. **1** (2), 13 (2020). https:// doi. org/ 10. 22158/ elsr. v1n2p13
18.
Murillo-Zamorano, L.R., López Sánchez, J.N., Godoy-Caballero, A.L.: How the flipped
classroom affects knowledge, skills, and engagement in higher education: effects on students’
satisfaction. Comput. Educ. **141** , 103608 (2019). https:// doi. org/ 10. 1016/ j. compedu. 2019. 103608
[Crossref]
19.
Lin, Y.T.: Impacts of a flipped classroom with a smart learning diagnosis system on students’
learning performance, perception, and problem-solving ability in a software engineering course.
Comput. Hum. Behav. **95** (187), 196 (2019). https:// doi. org/ 10. 1016/ j. chb. 2018. 11. 036
[Crossref]
20.
Hinojo-Lucena, F.-J., Aznar-Díaz, I., Cáceres-Reche, M.-P., Romero-Rodríguez, J.-M.: Flipped
classroom method for the teacher training for secondary. Education (2019). https:// doi. org/ 10.
3390/ nu11092151
[Crossref]


21.
Al Ghawail, E., Ben Yahia, S.: The flipped classroom model in libyan higher education:
experiences with students of computer principles. In: Proceedings of the 2021 InSITE
Conference. Published. https:// doi. org/ 10. 28945/ 4778
22.
Moundy, K., Chafiq, N., Talbi, M.: Digital textbook and flipped classroom: experimentation of
the self-learning method based on the development of soft skills and disciplinary knowledge. Int.
J. Emerg. Technol. Learn. (iJET) **17** (07), 240–259 (2022). https:// doi. org/ 10. 3991/ ijet. v17i07.
28933
[Crossref]
23.
Bentaleb, M., Boudine, H., Slimani, K., Tayebi, M.: Exploring and strengthening energy
concepts through computer simulation in educational institutions. In: E3S Web of Conferences,
vol. 477, p. 00028. EDP Sciences. https:// doi. org/ 10. 1051/ e3sconf/ 202447700028
24.
Boudine, H., Bentaleb, M., Mrigua, K., Tayebi, M.: Internet addiction and selflearning: the
impact of uncontrolled smartphone use on teenagers’ educational outcomes. J. Namibian Stud.
**35** (2023). https:// doi. org/ 10. 59670/ jns. v35i. 4247
25.
Ouahi, M.B.: The effect of using computer simulation on students’ performance in teaching and
learning computer science: are there any gender and area gaps? Educ. Res. Int. **2021** (2021).
https:// doi. org/ 10. 1155/ 2021/ 6646017
26.
Bell, R.L., Smetana, L.K.: “Using computer simulations to enhance science teaching and
learning”, in technology in the secondary science classroom, pp. 23–32. NSTA Press, Arlington,
VA, USA (2008)
27.
Bentaleb, M., Boudine, H., Ameur, M., Tayebi, M.: Environmental health and E-learning: effects
on students and teachers. In: E3S web of conferences, vol. 477, p. 00019. EDP Sciences. https://
doi. org/ 10. 1051/ e3sconf/ 202447700019
28.
Bentaleb, M., Boudine, H., El Karfa, D., Tayebi, M.: The impact of using computer simulation in
industrial technology learning on student outcomes. Int. J. Inform. Educ. Technol. **15** (9), 2013–
2020 (2025). https:// doi. org/ 10. 18178/ ijiet. 2025. 15. 9. 2400
[Crossref]
_OceanofPDF.com_


(1)

© The Author(s), under exclusive license to Springer Nature Switzerland AG 2026
K. Slimani et al. (eds.), _Emotion and Facial Recognition in Artificial Intelligence: Sustainable
Multidisciplinary Perspectives and Applications_ , Information Systems Engineering and Management
78
https://doi.org/10.1007/978-3-032-14778-3_15

## Emotion AI in Business and Customer

## Services

Esra Sipahi Döngül^1
Assistant Professor, Aksaray University, Aksaray, Turkey

```
Esra Sipahi Döngül
Email: esrasipahidongul@aksaray.edu.tr
```
**Abstract**
Today, the rapid advancement of technology has brought people to a point
where they can understand and interpret their emotional states. This has
played an important role in transforming both the organizational culture and
the attitude of companies towards customer service. In this context,
Emotion AI is at the very center of this transformation. Using everything
from facial expressions, facial expressions, and vocal cues to textual
emotions, AI systems categorize and analyze multiple pieces of data to
gauge how people actually feel. Thus, Emotion AI is shaping up not only as
a new technology feature for companies but also as an essential tool for
institutions and organizations aiming to make more human-centered and
responsive decisions in real-time. This section primarily explores how
organizations can leverage Emotion AI beyond marketing. Additionally,
practical applications such as human resource management, developing
empathy and communication skills, monitoring job satisfaction, and
assessing employees’ emotional states are also covered in this section.
When evaluated together, these innovations enable organizations to create
healthier and more harmonious working environments by integrating
emotional factors in their daily decision-making processes.


**Keywords** Emotion AI – Business – Customer services – HRM

##### 1 Introduction

With the development of digital technologies, there is an increase in the
management of next-generation human resources processes, digital grocery
services, and mobile applications that include emotional artificial
intelligence features [ 1 ]. However, it is critical to create the right strategic,
cultural, and technological infrastructure for the efficient use of these
applications and customer satisfaction. Effectively integrating innovative
technologies like Emotion AI into businesses, in particular, not only
enhances the user experience,it also brings with it the potential to increase
the competitive advantages of businesses. Due to the increase in
competition due to the rapid development of the service sector, it is
important for companies to monitor how the service quality level is
perceived by customers [ 2 ]. Service quality has an impact on critical
situations such as customer loyalty, word-of-mouth marketing and company
revenue. One of the main challenges faced by businesses, especially call
centers, is to measure and manage service quality accurately and effectively.
Traditionally, call centers have used three main approaches to measure
service quality: customer surveys, manual quality audits, and operational
indexes. However, these approaches have significant drawbacks, including
time and cost, lack of focus, and scope. In this sense, the development of
Emotion AI applications for the needs of customers in the context of both
literature support and experience gained from traditional method
applications will make a difference in institutions and organizations in
today's rapidly digitalizing world. However, when the studies covering both
the concepts of “Emotion AI in Business” and “Customer Services” are
examined in the literature, it is seen that the studies are limited. On
22.09.2025, only 48 studies were found when looking at the studies
published on the Web of Science (WOS) platform, including all fields, all
publication types, all indexes, all languages, all access types, all institutions,
and all years. In this context, the studies published in this section will also
be analyzed and detailed.

##### 2 Conceptual Framework


**2.1 Definition and Key Components of Emotion AI**

The concept of emotional intelligence is a concept that explains the ability
of individuals to understand, manage, and use the emotions of others in
their relationships as well as their own emotions [ 3 ]. Menétrey et al. [ 4 ]
highlights that Emotion AI is organized around the dimensions of
expression, motivation, physiology, and emotion. This approach shows that
cognitive and behavioral indicators as well as physiological markers are
decisive in recognizing emotions and affect individuals’ business lives as
well as their private lives [ 5 ]. Figure 1 shows the main characteristics of
emotional intelligence. These characteristics, consisting of self-awareness,
ability to express emotions, motivation, empathy and social skills, show that
emotional intelligence plays a decisive role in both personal and social
relationships of individuals. In this context, emotional intelligence
emphasizes the importance of not only individual awareness but also
effective communication skills.



**_Fig. 1_** The main characteristics of emotional intelligence.
_Source_ Adapted by the author using TRT News [ 5 ]
The components of AI and how they affect each other are shown in
Fig. 2.

**_Fig. 2_** Emotions as componential phenomena, consisting of changes in appraisal, motivation,
physiology, expression, and feeling (Adapted by Scherer [ 6 ]; as cited in Meuleman and Scherer [ 7 ])

In Fig. 2 , emotions; evaluation, motivation, physiology, expression and
feeling changes.
According to Fig. 2 , the appraisal process begins after an event and this
process is related to the individual's comprehension of the situation. As a
result of this evaluation, motivational responses are shaped and the
individual's behavioral tendencies are revealed. Then physiological
reactions and expression of emotions occur. In the last stage, subjective
feeling is formed with the combination of all these processes.
In context, Fig. 2 shows that emotions are not just a one-dimensional
response; cognitive assessment, physiological change, motivational
orientation and behavioral status.

**2.2 Emotion AI's Basic Technological Infrastructure**


By technologically integrating multimodal data collection and processing,
such as feature extraction, machine learning, deep learning, and information
synthesis, Emotion AI's infrastructure aims to capture and identify human
emotions with precision and reliability. The essence of such an
infrastructure is the processing and analysis of information from different
modalities, such as textual, auditory, visual and physiological signals. More
recently, biomimetic methodologies, big data, and cross-cultural validity
assessments have also contributed to the advancement of this infrastructure.
From this perspective, Vistorte et al. [ 8 :1] explored the pedagogical
potential and challenges of Emotion AI in student emotion recognition and
teaching strategy adaptation, while highlighting critical issues such as
accuracy, privacy, and alignment with cultural contexts.
Chawla et al. [ 9 ] aim to develop deep learning-based robotic systems
that can detect voice-based emotional pragmatic deficits in individuals with
social pragmatic communication difficulties. Liu et al. ([ 10 ]:1) emphasized
the increasing need for multimodal analysis due to the pandemic,
demonstrating the progress of the field from psychology to artificial
intelligence.
Multimodal machine learning allows systems to perform more
comprehensive analyses by predicting information from different data
sources (text, audio and image). Complex tasks like emotion recognition
can be performed more accurately and reliably thanks to multimodal
machine learning approaches. In their research, Slimani et al. [ 11 ] applied
the Residual Neural Network-based transfer learning approach on CFEE
and RAF databases and provided benefits in recognizing both basic and
composite emotions by classifying them with various machine learning
methods.
Slimani et al. [ 11 ] focused on basic and compound emotions on a model
they created in their study and emphasized that these studies yielded
successful results by emphasizing experimental studies. As it is understood
from the researches carried out in this context, these studies are important in
the creation and development of the technical infrastructure.

**2.3 Emotion AI in Human Resource Management**

In companies where the number of employees is increasing, it becomes
difficult to carry out perseonal management processes and makes
operational processes even more challenging. Thanks to the efficient


application of the developed artificial intelligence technologies, this burden
in the human resources department is reduced and performance analysis is
presented with more concrete data thanks to instant reporting screens. In
this context, the adaptation of artificial intelligence-supported services and
applications to management processes aims to increase efficiency in
institutions and organizations ([ 12 ]: 123). Alam et al. [ 13 ] emphasize the
increase in the CV pool in job applications and state that these technologies
will be useful in selecting the right candidate in recruitment management.
Panwar [ 14 ] explored the perceptual effects of AI in HRM processes,
emphasizing ethical issues. In their study on the healthcare sector, Shahzad
et al. [ 15 ] pointed out that efficient use of technology and social media
applications in the context of AI in the Chinese healthcare sector are
beneficial in HRM processes In today's rapidly advancing technology,
artificial intelligence (AI) technologies have been integrated into human
resources management to save time and highlight the CV of the right
candidate [ 16 ].
Figure 3 illustrates traditional and AI-powered comparisons in hiring
processes.

**_Fig. 3_** Differences between traditional hiring process and AI-supported hiring process.
_Source_ Talent Hub HRM [ 16 ]

AI systems are also used when managing HRM processes from a single
center in multi-location companies. Human resources management in
companies operating in multiple cities or countries requires serious
planning and coordination due to the complex nature of the processes.
When a wide workload ranging from leave requests to expense approvals,


from embezzlement records to performance evaluations is not carried out in
a standardized manner in different locations, both productivity and
employee satisfaction are negatively affected. Transactions carried out with
traditional methods, i.e. e-mail and Excel spreadsheets, often cause loss of
time, data confusion and approval delays. Flexit24 gathers all HR processes
on a single platform as a next-generation human resources management
solution designed for multi-location companies. It provides transparent, fast
and standard management at every stage, from leave management to
performance evaluation, from expense tracking to embezzlement control.
This way, companies can easily manage all their employees, regardless of
where they are, saving time and costs while increasing efficiency [ 17 ].
Emotional AI contributes to a deeper understanding of the dynamics of
institutions and organizations, but digital surveillance practices in the
workplace present risks as well as opportunities. Ramadhanty and Muafi
[ 18 ] noted that fostering a supportive workplace environment enhances job
satisfaction and overall performance. Therefore, organizations need to
proactively address external stressors, including challenging customer
relationships, to enhance employee well-being. Sipahi Döngül and Durar
[ 19 ] argue that the increasing integration of robots into the workplace may
erode employees’ basic moral values.
The proliferation of artificial intelligence tools brings with it discussions
and solutions such as data privacy, moral principles and legal guidelines. In
this case, the debate surrounding Emotional AI becomes even more
complex, and issues of accountability need to be addressed urgently.

##### 3 Method

In this study, a literature review was conducted. Without any exclusion
criteria, a search was made on the subject of “Emotion AI in Business and
Customer Services” in the Web of Science database and 48 publications
were reached. The Litmaps [ 20 ] bibliometric mapping tool was used, which
visualizes and reveals the connections between publication trends, citation
networks, and published studies. When the literature obtained is examined,
the technological foundations of Emotion AI and the computational models
that form the basis for it are compiled within the framework of some
thematic areas such as human resources management processes including
workforce adaptation and transformation in workplaces, customer service


practices emphasizing customer experience, service quality and
personalization, data security and privacy, accountability, ethics and
inhibitory elements.

##### 4 Finding

As a result of the search carried out on 22.09.2025 without any exclusion
criteria regarding the studies covering the concepts of “Emotion Artificial
Intelligence in Business” and “Customer Service”, the studies published on
the Web of Science (WOS) platform were examined. Only 48 studies were
found, including all fields, all publication types, all indexes, all access
types, all institutions, all languages, and all years. The 25 most cited studies
were included.
Figure 4 shows research on the impact of Emotional AI (Top 25 Most
Cited Studies).



**_Fig. 4_** Research on the impact of emotional AI (Top 25 Most Cited Studies).
_Source_ Created by the author
Looking at Fig. 4 , it is seen that there are studies revealing that
emotional artificial intelligence (Emotional AI) has an impact on different
research areas.
On a citation-based basis, Taherdoost and Madanchian [ 21 ], Han et al.
[ 22 ], and Filieri et al. [ 23 ] rank highest. Han et al. [ 22 ] discussed how the
emotional response of artificial intelligence systems in customer service
shapes customer opinion, while Filieri [ 23 ] underlined the need to pay
attention to emotions in service operations by focusing on the hybrid
human–machine relationship. These studies reveal that Emotional AI is not
just a technical tool but a strategic factor in its human-centric product and
service-oriented design.
In the 2022–2024 period, research such as Chi and Vu [ 24 ], Ku and
Chen [ 25 ], and Liu et al. [ 26 ] have addressed concrete examples of AI
usage and applications in measuring customer feedback and loyalty,
experience and service quality, and applications in the tourism sector. These
studies stand out as they demonstrate the practical effects of Emotional AI
in areas such as marketing, product and service management, as well as
customer engagement. In particular, the study by Ku [ 25 ] explains the
effects of emotional AI on customer loyalty in the tourism sector.
Although the number of citations is low [ 27 ] emphasize topics for
improvement in the literature. These studies guide future academic research
by drawing attention to issues such as the negative effects of artificial
intelligence, ethical considerations, and multi-data approaches in sentiment
analysis.
In this context, Fig. 4 is important in terms of both empirical and
expanding the application areas of studies on emotional artificial
intelligence in recent years. This trend suggests that Emotional AI will
establish a multidisciplinary research area in the future.
Figure 5 shows the positions of the publications published in the
Emotional AI and Business and Customer Services literature with the
highest number of citations according to their time and citation density.


**_Fig. 5_** Time and citation density of studies with the highest number of citations.
_Source_ Created by the author
Looking at Fig. 4 , studies published earlier than other periods, such as
Filieri et al. [ 23 ], Sidaoui et al. [ 28 ] and Bagozzi et al. [ 29 ], have been a
guide to empirical research. These studies have been guiding by shedding
light on themes such as customer experience, consumer behavior, and
product-service innovation in the context of emotional AI.
Studies such as Han et al. [ 22 ] and Taherdoost et al. [ 21 ] added value to
the studies published before them, directing the remaining studies in
empirical research to application areas, especially emphasizing themes such
as human-artificial intelligence interaction and decision-making processes.
The high number of citations of these studies indicates that the areas of
influence in the literature are spreading rapidly.
Studies published in 2024 and 2025 [ 25 , 26 ] received few citations due
to the topicality of the research topics. In this context, it shows that research
in the field has evolved from the early empirical framework to practice and
multidisciplinary fields. With the transformation of technology, the focus of
studies has become more human-centered, shifting to issues such as design,
ethics and user experience. This shows that Emotional AI is not just about
the technical dimension but has evolved into a field that covers many
dimensions, from social sciences to the business world.


##### 5 Result

This study reveals that Emotion AI technologies have significant impacts in
different areas and are becoming increasingly important, especially in
businesses and customer service areas. When we look at the research, it is
seen that the applications are multifaceted. In the studies on human
resources and the management of operational processes, it has been found
that there are studies that take into account recruitment, employee loyalty,
customer satisfaction and cultural issues and emphasize the concept of
ethics. In the context of human resources, Emotion AI is increasingly being
used in recruitment stages and employee performance tracking, ostensibly
to enhance psychological safety and overall well-being [ 14 ]. Research on
customer service shows that Emotion AI has a positive impact on customer
satisfaction [ 30 ].
In this context, the findings reveal that Emotion AI technologies have
become more than just a technical tool, but also a strategic factor
influencing human-centered decision-making processes.
It is mostly used in human resources processes, from recruitment
processes to performance evaluations, and supports critical issues such as
choosing the right candidate, creating workplace loyalty, safety and ethics.
In the field of customer service, the analysis of emotional reactions
contributes to results such as customer experiences, creating a sense of
empathy, and ensuring well-being. In this respect, Emotion AI stands out as
a tool that not only evaluates the technical but also actively transforms the
customer experience.
In addition, empirical research has included themes such as ethics, trust,
data security and accountability in the studies. The fact that emotional
intelligence varies according to cultural differences and the importance of
multicultural approaches in the interface design and implementation of
Emotion AI systems are also revealed.
As a result, Emotion AI will not only be a technological tool that
provides a competitive advantage for businesses in the future and in future
studies, but also a tool that aims to understand human behavior and create
strategies accordingly.

##### 6 Future Perspectives and Suggestions


In the light of the researches, it is obvious that in the context of the
sustainability of Emotion AI in businesses, issues that are easy to interface,
user-friendly, meet customer needs and give importance to customer
experience should be taken into consideration. Kolomaznik et al. [ 31 ] state
that socio-emotional competencies are important for humans and AI
systems to create interrelationships.
Laying the groundwork for effective and healthy human-AI interaction
depends on aspects such as engagement, empathy, customer experience, and
making technology feel more usable [ 4 ]. Accountability, ethics, privacy, and
security in this context are essential for the sustainability of Emotion AI.
In the development and implementation of these products and services,
it is expected to turn into practice by taking into account the context of
intercultural issues and to create social benefits beyond theory. In this
context, Emotion AI is able to create interactive experiences for customers.
From this point of view, failure to pay attention to these issues will prevent
the progress of technical gains.

##### References

1. Al-Azani, S., El-Alfy, E.S.M.: A review and critical analysis of multimodal datasets for
    emotional AI. Artif. Intell. Rev. **58** (334), 1–65 (2025). https:// doi. org/ 10. 1007/ s10462-025-
    11271-1
    [Crossref]
2.
    Ladhari, R.: Service quality, emotional satisfaction, and behavioural intentions: a study in the
    hotel industry. Manag. Serv. Qual. Int. J. **19** (3), 308–331 (2009). https:// doi. org/ 10. 1108/
    0960452091095532 0
3.
    Secret Psychology.: Components of Emotional Intelligence (2024). https:// [http://www.](http://www.) sirpsikoloji. com/
    duygusal-zekanin-bilesenleri/
4.
    Menétrey, Q.M., Mohammadi, G., Mohammadi, G., Leit, J., Leitão, J., Vuilleumier, P.: Emotion
    recognition in a multi-componential framework: the role of physiology. Front. Comput. Sci. **4** ,
    1–14 (2022). https:// doi. org/ 10. 3389/ fcomp. 2022. 773256
    [Crossref]
5.
    TRT News.: Emotional intelligence: Is there still no room for emotions in business? (2021).
    https:// [http://www.](http://www.) trthaber. com/ haber/ infografik/ duygusal-zeka-is-yasaminda-duygulara-hala-yer-yok-
    mu-585363. html. Last accessed 09 Sept 2025
6.
    Scherer, K.R.: The dynamic architecture of emotion: evidencefor the component process model.
    Cogn. Emot. **23** (7), 1307–1351 (2009)


7.
Meuleman, B., Scherer, K.R.: Nonlinear appraisal modeling: an application of machine learning
to the study of emotion production. IEEE Trans. Affect. Comput. **4** (4), 398–411 (2013). https://
doi. org/ 10. 1109/ T-AFFC. 2013. 25
[Crossref]
8.
Vistorte, A.O.R., Deroncele-Acosta, A., Ayala, J.L.M., Barrasa, A., López-Granero, C., Martí-
González, M.: Integrating artificial intelligence to assess emotions in learning environments: a
systematic literature review. Front. Psychol. **15** (1387089), 1–13 (2024). https:// doi. org/ 10. 3389/
fpsyg. 2024. 1387089
9.
Chawla, M., Panda, S.N., Khullar, V.: Deep learning and robotics enabled approach for audio
based emotional pragmatics deficits identification in social communication disorders. Proc. Inst.
Mech. Eng. [H] **239** (3), 332–346 (2025). https:// doi. org/ 10. 1177/ 095441 1925132533 1
[Crossref]
10.
Liu, P., Yang, C., Wang, K.: The global landscape of emotion recognition research from 2004 to
2023: a scientometric and visualization analysis. Discov. Appl. Sci. **7** (507), 1–26 (2025). https://
doi. org/ 10. 1007/ s42452-025-07103-0
[Crossref]
11.
Slimani, K., Ruichek, Y., Messoussi, R.: Compound facial emotional expression recognition
using CNN deep features. Eng. Lett. **30** (4), 1402–1416 (2022)
12.
Çeri, M., Doğan, A.: AI-powered human resource management: transforming the employee
experience. Izmir Manag. J. [Yapay Zeka Destekli İnsan Kaynakları Yönetimi: Çalışan
Deneyiminin Dönüşümü. İzmir Yönetim Dergisi] **6** (SI-Özel Sayı), 123–142 (2025). https:// doi.
org/ 10. 56203/ iyd. 1664458
13.
Alam, M.S., Khan, T.U.Z., Dhar, S.S., Munira, K.S.: HR Professionals’ intention to adopt and
use of Artificial Intelligence in recruiting talents. Bus. Perspect. Rev. **2** (2), 15–30 (2020). https://
doi. org/ 10. 38157/ business-perspective-review. v2i2. 122
14.
Panwar, S.: Perception of artificial intelligence towards the development of human resources
management practices. Int. J. Sci. Res. Eng. Manag. **07** (11), 1–11 (2023). https:// doi. org/ 10.
55041/ ijsrem26995
15.
Shahzad, M.F., Xu, S., Naveed, W., Nusrat, S., Zahid, I.: Investigating the impact of artificial
intelligence on human resource functions in the health sector of China: a mediated moderation
model. Heliyon **9** (11), 1–17 (2023). https:// doi. org/ 10. 1016/ j. heliyon. 2023. e21818
16.
Talent Hub HRM.: AI-powered recruitment: a new era in human resources (2025). https://
talenthubik. com/ yapay-zeka-destekli-ise-alim-insan-kaynaklarinda-yeni-donem/. Last accessed
10 Sept 2025
17.
Flexit24.: Managing HR Processes in Multi-Location Companies from a Single Center (2025).
https:// flexit24. com/ icerik/ cok-lokasyonlu-sirketlerde-ik-sureclerini-tek-merkezden-yonetmek.
Last accessed 09 Sept 2025
18.
Ramadhanty, F.A., Muafi, M.: The effect of emotional intelligence and mental health towards
employee performance mediated by job satisfaction. Telaah Bisnis **24** (1):26 (2023). https:// doi.
org/ 10. 35917/ tb. v24i1. 357


19.
Sipahi Döngül, E., Ul-Durar, S.: Robots and Spirituality in the Workplace. In: Özsungur, F.,
Bekar F. (eds.), Spirituality management in the workplace: new strategies and approaches.
Emerald Publishing Limited, pp. 335–358 (2023). https:// doi. org/ 10. 1108/ 978-1-83753-450-
020231015
20.
Litmaps (2025). https:// [http://www.](http://www.) litmaps. com/. Last accessed 23 Sept 2025
21.
Taherdoost, H., Madanchian, M.: Artificial intelligence and sentiment analysis: a review in
competitive research. Computers **12** (2), 37 (2023). https:// doi. org/ 10. 3390/ computers1202003 7
[Crossref]
22.
Han, E., Yin, D., Zhang, H.: Bots with feelings: should AI agents express positive emotion in
customer service? Home Inf. Syst. Res. **34** , 3 (2022). https:// doi. org/ 10. 1287/ isre. 2022. 1179
[Crossref]
23.
Filieri, R., Lin, Z., Li, Y., Lu, X., Yang, X.: Customer emotions in service robot encounters: a
hybrid machine-human intelligence approach. J. Serv. Res. **25** (4), 614–629 (2022). https:// doi.
org/ 10. 1177/ 1094670522110393 7
[Crossref]
24.
Chi, N.K.T., Vu, N.H.: Investigating the customer trust in artificial intelligence: the role of
anthropomorphism, empathy response, and interaction. Inst. Eng. Technol. **8** (1), 260–273 (2022).
https:// doi. org/ 10. 1049/ cit2. 12133
[Crossref]
25.
Ku, E.C.S., Chen, C.D.: Artificial intelligence innovation of tourism businesses: from satisfied
tourists to continued service usage intention. Int. J. Inf. Manag. **76** (C), 1–15 (2024). https:// doi.
org/ 10. 1016/ j. ijinfomgt. 2024. 102757
26.
Liu, S.Q., Vakeel, K.A., Smith, N.A., Alavipour, R.S., Wei, C., Wirtz, J.: AI concierge in the
customer journey: what is it and how can it add value to the customer? J. Serv. Manag. 16 **35** (6),
136–158 (2024). https:// doi. org/ 10. 1108/ JOSM-12-2023-0523
27.
Gamage, G., De Silva, D., Mills, N., et al.: Emotion AWARE: an artificial intelligence
framework for adaptable, robust, explainable, and multi-granular emotion analysis. J. Big Data
**11** (93), 1–28 (2024). https:// doi. org/ 10. 1186/ s40537-024-00953-2
[Crossref]
28.
Sidaoui, K., Jaakkola, M., Burton, J.: AI feel you: customer experience assessment via chatbot
interviews. J. Serv. Manag. **31** (4), 745–766 (2020). https:// doi. org/ 10. 1108/ JOSM-11-2019-0341
[Crossref]
29.
Bagozzi, R.P., Brady, M.K., Huang, M.-H.: AI service and emotion. J. Serv. Res. **25** (4), 499–504
(2022). https:// doi. org/ 10. 1177/ 1094670522111857 9
[Crossref]
30.
Choi, L., Lawry, C.A., Kim, M.: Contextualizing customer organizational citizenship behaviors:
the changing nature of value cocreation and customer satisfaction across service settings.
Psychol. Mark. **36** (5), 455–472 (2019). https:// doi. org/ 10. 1002/ mar. 21190
[Crossref]


31.
Kolomaznik, M., Petrik, V., Slama, M., Jurik, V.: The role of socio-emotional attributes in
enhancing human-AI collaboration. Front. Psychol. **15** , 1–13 (2024). https:// doi. org/ 10. 3389/
fpsyg. 2024. 1369957
[Crossref]
_OceanofPDF.com_


(1)

(2)

© The Author(s), under exclusive license to Springer Nature Switzerland AG 2026
K. Slimani et al. (eds.), _Emotion and Facial Recognition in Artificial Intelligence: Sustainable
Multidisciplinary Perspectives and Applications_ , Information Systems Engineering and Management
78
https://doi.org/10.1007/978-3-032-14778-3_16

## Deep Learning Approaches and Technical

## Challenges in Facial Emotion Recognition

## (FER)

Anjanadevi Bondalapati^1 and Slimani Khadija^2
MVGR College of Engineering (Autonomous), Vizianagaram, AP,
India
esieaLab LDR, Higher School of Computer Science, Electronics and
Automation (ESIEA), Paris, France

```
Anjanadevi Bondalapati
Email: drbanjanadevi@gmail.com
```
**Abstract**
Nowadays, in every sector like mental health, physical health and
education, communication plays a major role. The proper communication
gives the accurate way of presentation for easy understanding. Proposed
system experiments with facial emotion recognition that provides effective
communication between system and people. The current system
experimented on Cohn kanade (CK+ ) dataset for evaluation of all the seven
facial emotion expressions which includes anger, contempt, disgust, fear,
happiness, sadness, surprise conditions. Due to rapid growth in artificial
intelligence tools, the regular interaction between people's communication
is reduced day by day which leads to many mental health conditions which
impacts on their performance The usage of CK+ dataset includes variational
emotions for effective facial emotion detection. The existing deep learning
models proven that this dataset provides multiple variants of emotions for
accurate analysis of facial emotions. The proposed system, Light—weight


fine—tuned Convolutional neural network (LW-fine-tuned CNN) model
experimented with less number of layer for feature detection and produced
an accuracy of 97.46 where as traditional feature extraction techniques
includes local binary patterns and local directional patters and deep learning
models (Alexnet, resnet and Mobile net) obtained an accuracy at the
maximum of ninety five percentage. Along with accuracy, the precision,
recall measures also limited up to 88% and 87%. To improve the
performance, the proposed system fine-tuned the existing VGG-16 model
and improved precision and recall from 7–10%. The proposed model
addressed the technical challenges for effective emotion detection in the
aspects of posing, lighting and expressions. The obtained results effectively
identified the facial emotions.

**Keywords** Light-weight fine-tuned CNN – Local binary patterns – Local
directional patterns – Alexnet – Resnet50 – Mobilenet

##### 1 Introduction

In many of the cases of educationalists engaging more in system
interactions, high engagement of those cases transformed to rapid growth in
mental conditions which should be diagnosing with advanced social robots.
In this, facial emotion detection plays a major role and the challenging
conditions in FER datasets is class imbalance, where some emotions like
Happy, Neutral dominate while others including Disgust, Fear are less
represented. Real world conditions in deep learning and handcrafted
feature-based FER approaches struggle to generalize.
However, existing FER methods face three challenges: Intra-class
variation—The same emotion may look different across individuals.
Uncontrolled conditions—Real-world datasets include occlusions, lighting
variations, and partial faces. Feature irrelevance—Many facial regions (e.g.,
ears, hair) are not important for emotion recognition, yet traditional CNNs
treat them equally. These limitations highlight the need for models that can
attend to the most informative regions of the face while ignoring irrelevant
cues. Inspired by advances in attention mechanisms, this work introduces
an attentional CNN to tackle these issues.


##### 2 Literature Review

**2.1 Traditional Machine Learning (ML) Models**

Though Machine Learning(ML) methods are applicable in various domains
like health care, focus is made on Monitoring patient emotions, especially
in mental health and Alzheimer’s care, Security and Surveillance: Detecting
suspicious or stressed behaviors, Automotive Industry: Driver drowsiness
and distraction detection [ 1 ]. The major contributions are how SVM, RF,
and ensemble models remain competitive against deep learning, highlight
lightweight, interpretable FER systems suitable for real-world deployment
and discuss integration with CNN feature extraction for enhanced accuracy.
The datasets used in this paper are FER2013: 35,887 images with
labeled emotions, both CNN + SVM models surpass traditional ML-only
methods, showing greater than 92% accuracy and ML approaches remain
computationally lighter than deep neural networks, making effective results.
Further processing with Extending ML-based FER models to large, diverse
datasets such as AffectNet, Combining multimodal signals (speech,
physiological signals) with facial expressions, developing explainable FER
models for critical applications in healthcare and security and exploring
lightweight, edge-friendly ML models for real-time deployment.
Paper [ 2 ] proposed the traditional ML approaches SVM and Random
Forest classifiers showed high accuracy in static image-based FER,
Ensemble learning improved robustness by combining multiple weak
classifiers, Hybrid deep learning with traditional ML classifiers enhanced
feature discrimination, Performance improvements ranged between 75–95%
depending on the dataset and emotion class. The proposed framework for
machine learning–based FER generally involves the following stages: Data
Collection—Large-scale emotion datasets such as FER-2013, CK+, and
AffectNet are used, Preprocessing—Includes face detection, alignment,
normalization, and augmentation, Feature Extraction—Machine learning
algorithms extract discriminative features such as Gabor filters, LBP, HOG,
or deep features from CNN models, Classification—Algorithms such as
Support Vector Machines are commonly applied for recognizing emotions.
The FER framework involves HOG-based feature extraction and
classification using three ML models which includes the steps feature
Extraction (HOG): Resized grayscale images (48 × 48), Computed gradient
orientations, histogram bins (9 orientations, 8 × 8 cell size, 2 × 2 block


normalization), Generated robust descriptors capturing facial shape and
texture. The proposed Machine Learning Models are Decision Tree (Gini
Index) with sSimple, interpretable tree structure, Random Forest: Ensemble
of trees with randomized feature selection and bootstrap sampling, SVM:
Finds optimal hyperplane for separating emotion classes in high-
dimensional space. SVM: Highest accuracy 58.93%. Best performance on
happy (74.36% F1) and neutral (66.71% F1) [ 3 ].

**2.2 Deep CNN Models**

In the past, Facial Emotion Recognition (FER) systems used custom
features such as contours [ 4 ]. These systems do not generalize well. They
are not deep learning systems. They did not have access to feature
extraction which classified elementary features. This resulted in dismal
performance. This paper proposes EmoNeXt, which uses Cepstral Rooted
Nonlinear Transfer (ConvNeXt) as the backbone for the deep architecture
used in FER. The EmoNeXt produced decomposable state of the art
performance on FER datasets with boost representation learning techniques,
such as Spatial Transformer Networks (STN) and other features of the
encoder architecture.
UNet segmentation: Extracts and isolates critical facial regions (eyes,
mouth, forehead) to improve focus on emotional cues with steps: Input
Image—UNet Segmentation—EfficientNetB4 Feature Extraction—
Classifier. The obtained results are without UNet segmentation:
EfficientNetB4 achieved 81.56% accuracy, outperforming CNNs, ResNet,
and MobileNet. With UNet segmentation with happy, sad, fear, pain, anger,
disgust emotions, EfficientNetB4: 91.27%, MobileNet: 89.94%,
InceptionV3: 90.07%, Traditional ML models less than 81% [ 5 ].
The model in [ 6 ] consists of various layers, which are 5 convolutional
layers with ReLU activation for effective feature extraction, this model
trained on Datasets merged: JAFFE (213 images) + KDEF (4900 images)
with augmentation steps Rotation, brightness adjustment, noise injection,
and geometric transformations and Optimizer—SGD with LR = 0.01, batch
size = 100, with epochs: Optimal at 300, on final training set: 14,200
images, test set: 1,580 images, Optimal CNN depth of 5 layers. The
obtained Accuracy peaked at 78.1%, outperforming AlexNet (69.6%), VGG
(68.7%), ResNet (71.2%), and GoogleNet (74.1%).


Deep learning models, especially CNNs, offer the potential for
improved performance by automatically learning hierarchical facial
representations. However, there are two main challenges. The first is
capturing Emotion Constructs, which represent emotions in a structured and
compact way [ 7 ]. In [ 8 – 12 ], the researchers focused on identifying
compound emotion which is complex to handle the various conditions in
emotions. Researchers have used datasets like the Multi-Emotion Facial
Expression Dataset (iCV-MEFED), CFEE and RAFdb to train and test
CNN models.

**2.3 Hybrid Approaches**

The study in [ 13 ] adds a new hybrid DCNN model with an extreme learning
machine (ELM) to improve Facial Emotion Recognition Identification.
Unlike the ELM or CNN only approaches, the hybrid model combines the
strengths of ELM and DCNN by using DCNN for deep hierarchical feature
extraction and ELM for fast classification, improving accuracy and real-
time performance. The model features sub processes like stepwise linear
discriminant analysis (SWLDA) and kernel filters.
The evaluation Methods in [ 14 ] includes subject-independent evaluation
using K-fold cross-validation, Cross-dataset evaluation to test
generalizability. Finally, the obtained results are conventional ML
approaches that achieve high accuracy on small, controlled datasets (CK+
and JAFFE). Deep learning approaches consistently outperformed ML on
large and diverse datasets (e.g., FER2013, AffectNet). CNN with transfer
learning achieved state-of-the-art accuracy across benchmarks. Hybrid
CNN-RNN models improved temporal emotion recognition (micro-
expressions) and limitations remain in handling real-world variations,
occlusions, and subtle expressions.

**2.4 Diffusion Models**

In [ 15 ], a strategy that combines **ResEmoteNet** , a CNN architecture with
residual and squeeze-and-excitation blocks, with **synthetic data generated
by diffusion models** (Stable Diffusion 2 and 3). By augmenting FER2013
and RAF-DB datasets, the proposed method significantly improves and
achieves large performance gains over state-of-the-art models. FER systems
often fail to generalize due to limited and imbalanced training datasets.
Conventional augmentation (GAN-based) partially alleviates imbalance but


lacks realism. Recent **diffusion models** provide high-quality, diverse,
controllable synthetic images, offering an effective solution. The goal is to
use **diffusion-based augmentation** to improve ResEmoteNet’s recognition
capability across all emotion classes, especially underrepresented ones. In
this proposed model,

**2.5 Attention Mechanisms**

In [ 16 ], the author focused on identifying complex emotions using various
attention network models where many of the traditional methods are based
on single attention networks. Attention Fusion Network (AFN) applies
partition loss to penalize overlapping attention maps and fuses outputs from
MAN for robust feature representation.
Facial expressions are a primary channel of non-verbal communication.
Reliable FER is vital for healthcare, surveillance, driver safety, and human–
computer interaction. However, conventional ML approaches require
manual feature engineering and often fail under real-world conditions such
as lighting variation, occlusion, and micro-expressions. Deep learning
promises higher accuracy and generalization but demands large annotated
datasets and high computational power. In [ 17 ], compared ML and DL
approaches and provided guidance for future FER research.
The Deep-Emotion framework is an end-to-end attentional
convolutional network (ACN) designed to selectively focus on salient facial
regions. Feature Extraction Network: Four convolutional layers (3 × 3
kernels), each followed by ReLU activation and max-pooling, extract local
features. Attention Mechanism (Spatial Transformer Network): A
localization sub-network estimates affine transformations, warping the input
image to focus on the most informative regions (eyes, mouth, eyebrows).
The obtained results in [ 18 ] are: JAFFE: 92.8%.

##### 3 Methodology

**3.1 Dataset Description**

Considered the following emotions from CK dataset [ 19 , 20 ] for
experimentation, with 981 images with a resolution of 48 × 48 pixels which
includes train dataset with 80% (784 images) and test dataset with 20%
(197 images) of 7 categories includes anger, contempt, disgust, fear,
happiness, sadness, surprise conditions.


Preprocessing Methods: The steps include Face detection and
alignment, Conversion to grayscale images, Image resizing to 48 × 48
pixels and Normalization of pixel values, for example, between 0 and 1.
Rationale for Choosing 48 × 48 Images: The 48 × 48 image size is
frequently used in facial emotion recognition tasks. It provides a good
balance between efficiency and capturing enough facial features for
accurate emotion recognition. Many studies have adopted this size as a
standard, which makes it easier to compare results across various models
and methods.

**3.2 Architecture Steps**

1. Data Pre-processing done with extraction of the frames and resize the

# images to 48 × 48 pixels.^

2. VGG16 architecture is considered a pre-trained VGG16 model, which
    consists of 16 layers, including convolutional, pooling, and fully
    connected layers.
3. We fine-tuned the VGG16 model by adding a flatten layer to the output

# of the VGG16 model to prepare it for fully connected layers.^

4. Added dense layer with 256 units with relu activation
5. Added dense layer with 128 units with relu activation
6. Added dropout rate of 0.2 to reduce overfitting.

Finally defined the output layer with softmax activation for multi-class
classification and trained the model using the adam optimizer and sparse
categorical cross-entropy loss function.

**3.3 Proposed System Architecture**

See Fig. 1.


**_Fig. 1_** Proposed system process flow diagram

##### 4 Results Analysis

**4.1 Training Accuracy**

Proposed lightweight fine-tuned VGG16 model for FER trained with
twenty epochs. The training accuracy is increased and loss is decreased
when the iterations are incremental. On testing the model with the proposed
Light—weight VGG16 model, obtained accuracy of 97.46% on CK+
dataset demonstrating the effectiveness in recognizing facial emotions. The
validation technique used in this model with train—test split 80–20%. This
accuracy shows that the proposed model is more effective than existing
models.
Training accuracy and loss is depicted in the below diagram (Fig. 2 ).

**_Fig. 2_** Training accuracy chart


This comparison analysis evaluates the performance of proposed
VGG16 against other deep learning models for facial emotion recognition.
The models compared include: Proposed VGG16 implemented with 16
layers. The other models include Alex net with 5 convolutional layers and 3
fully connected layers. Another deep learning model residual learning-based
CNN (ResNet50) with 50 layers and MobileNet CNN designed for mobile
and embedded vision applications. The traditional models AlexNet,
ResNet50, and MobileNet obtained an accuracy of 92.15%, 95.62% and
90.58% and precision, recall and f1-measure are shown in the Table 1.

**_Table 1_** Accuracy, precision, recall and f1-measure
results metrics

```
Model/Metric Accuracy Precision Recall F1-score
Alex Net 92.15 92.08 93.23 92.59
ResNet50 95.62 94.42 93.6 94.6
MobileNet 90.58 88.29 87.52 88.2
Proposed 96.46 95.63 97.25 96.43
```
```
The following section shows a comparison analysis diagram (Fig. 3 ).
```
**_Fig. 3_** Comparison analysis graph depicting metrics of proposed model, Alex ne, Resnet50 and
Mobilenet


**4.2 Confusion Matrix**

A confusion matrix is a tabular representation utilized to assess the
effectiveness of a classification model, including facial emotion
recognition. It offers a comprehensive overview of accurate and inaccurate
predictions in relation to the actual results.
True Positives (TP): The elements along the diagonal indicate the count
of emotions that have been correctly classified (for instance, 27 for Anger).
False Positives (FP): The elements not on the diagonal signify the
number of emotions that have been incorrectly classified (for example, 2 for
anger that was predicted as contempt).
False Negatives (FN): The off-diagonal elements similarly denote the
count of emotions that were not identified (Fig. 4 ).


**_Fig. 4_** Proposed model confusion matrix of facial emotion recognition

##### 5 Technical Challenges and Future Directions

The major technical challenges include lighting, change detection and
variant emotions. Along with the mentioned conditions the data imbalance
where consideration of the number of images under each of the emotions
leads to oversampling and under sampling conditions. Further by enhancing
the emotion detection accuracy by including a combination of various
emotion expressions with different types of data which is speech and other


physiological conditions. The current researchers are experimenting on
multimodel emotion identification with challenging conditions.

##### 6 Conclusion

The proposed research produced an accuracy of 97.46 whereas traditional
feature extraction techniques including local binary patterns and local
directional patterns are consuming high computational time with less
number of images. By analysing the obtained results with other deep
learning models including Alexnet, ResNet and Mobile net, it is proven that
the Light-weight fine-tuned CNN model obtained effective results. Whereas
the mentioned traditional methods obtained an accuracy at the maximum of
ninety five percent and precision, recall measures also limited up to 88%
and 87% on CK+ dataset. To improve the performance, the proposed system
fine-tuned existing VGG-16 model and improved precision and recall from
seven-ten percent. Furthermore, the proposed model is fine-tuned to fine
compound emotions on CK+ dataset.

##### References

1. Elsheikh, R.A., Mohamed, M.A., Abou Taleb, A.M., Ata, M.M.: Improving deep feature
    adequacy for facial emotion recognition: the impact of anti-aliasing on landmark-based and
    pixel-based approaches. J. Intell. Syst. (2025)
2.
    Elsheikh, R.A., Mohamed, M.A., Abou-Taleb, A.M., Ata, M.M.: Improved facial emotion
    recognition model based on a novel deep convolutional structure (2024). arXiv: 2401. 06789.
    https:// arxiv. org/ abs/ 2401. 06789
3.
    Wen, Z., Lin, W., Wang, T., Xu, G.: Distract your attention: multi-head cross attention network
    for facial expression recognition (2021). arXiv: 2109. 07270. https:// arxiv. org/ abs/ 2109. 07270.
4.
    El Boudouri, Y., Bohi, A.: EmoNeXt: an adapted ConvNeXt for facial emotion recognition
    (2023). arXiv: 2312. 07891. https:// arxiv. org/ abs/ 2312. 07891
5.
    Na, I., Aldrees, A., Hakeem, A., Mohaisen, L., Umer, M., AlHammadi, D.A., Alsubai, S., Innab,
    N., Ashraf, I.: FacialNet: facial emotion recognition for mental health analysis using UNet
    segmentation with transfer learning model (2025). arXiv: 2402. 01234. https:// arxiv. org/ abs/ 2402.
    01234
6.
    Ali, I., Ghaffar, F.: A robust CNN for facial emotion recognition and real-time GUI. AIMS
    Electron. Electr. Eng. **8** (2), 227–246 (2024). https:// doi. org/ 10. 3934/ electreng. 2024010
    [Crossref]


7.
Dachapally, P.R.: Facial emotion detection using convolutional neural networks and
representational autoencoder units (2017). arXiv: 1706. 01509. https:// arxiv. org/ abs/ 1706. 01509
8.
Slimani, K., Ruichek, Y., Messoussi, R.: Compound facial emotional expression recognition
using CNN deep features. Eng. Lett. **30** , 1402–1416 (2022)
9.
Slimani, K., Ruichek, Y., Messoussi, R.: Local feature extraction based facial emotion
recognition: a survey. https:// doi. org/ 10. 11591/ ijece. v10i4. pp4080-4092
10.
Borgalli, R.A., Surve, S.: Deep learning framework for compound facial emotion recognition. In:
Shukla, A., Murthy, B.K., Hasteer, N., Van Belle, JP. (eds.), Computational Intelligence. Lecture
Notes in Electrical Engineering, vol 968. Springer, Singapore (2023). https:// doi. org/ 10. 1007/
978-981-19-7346-8_ 65
11.
Ullah, S., Ou, J., Xie, Y., et al.: Compound facial expressions recognition approach using
DCGAN and CNN. Multimed. Tools Appl. **83** , 85703–85723 (2024). https:// doi. org/ 10. 1007/
s11042-024-20138-6
[Crossref]
12.
Slimani, K., Lekdioui, K., Messoussi, R., Touahni, R.: Compound facial expression recognition
based on highway CNN. In: Proceedings of the New Challenges in Data Sciences: Acts of the
Second Conference of the Moroccan Classification Society, pp. 1–7 (2019)
13.
Boopalan, K., Srivastava, S., Kavitha, K., Rani, D.U., Kumar, K.J., Reddy, M.V.J., Bhoopathy,
V.: Advanced facial emotion recognition using DCNN-ELM. J. Comput. Sci. **21** (1), 13–24
(2025)
[Crossref]
14.
Khan, A., et al.: Comprehensive review and analysis on facial emotion recognition: performance
insights into deep and traditional learning with current updates and challenges. In: Artificial
Intelligence and Data Analytics Lab (AIDA), Prince Sultan University, Riyadh, Saudi Arabia
(2024). https:// doi. org/ 10. 1007/ 978-3-031-46749-3_ 1
15.
Roy, A. K., Kathania, H. K., Sharma, A.: Improvement in facial emotion recognition using
synthetic data generated by diffusion model (2023). arXiv: 2309. 05678_._ https:// arxiv. org/ abs/

2309. 05678
16.
Alsubai, S., Alqahtani, A., Alanazi, A., Sha, M., Gumaei, A.: Facial emotion recognition using
deep quantum and advanced transfer learning mechanism. Front. Comput. Neurosci. **18** ,
1435956 (2024)
[Crossref]
17.
Le Vinh, P.T., Le Thanh, T., Duong Thanh, T.: Facial expression recognition using traditional
machine learning models. In: Proceedings of the Second International Conference on
Intelligence of Things (ICIT 2022). Springer, pp. 1–7 (2022). https:// doi. org/ 10. 1007/ 978-3-031-
46749-3_ 1
18.
Minaee, S., Abdolrashidi, A.: Deep-emotion: facial expression recognition using attentional
convolutional network (2019). arXiv: 1902. 01019. https:// arxiv. org/ abs/ 1902. 01019
19.
CK+ dataset. https:// huggingface. co/ datasets/ AlirezaF138/ ckplus-dataset


20.
https:// gts. ai/ dataset-download/ ck-dataset-ai-data-collection/
_OceanofPDF.com_


(1)

(2)

© The Author(s), under exclusive license to Springer Nature Switzerland AG 2026
K. Slimani et al. (eds.), _Emotion and Facial Recognition in Artificial Intelligence: Sustainable
Multidisciplinary Perspectives and Applications_ , Information Systems Engineering and Management
78
https://doi.org/10.1007/978-3-032-14778-3_17

## Emotion Recognition and the Law:

## Bridging Technology and Human Rights

Sarthak Prasad Sahoo^1 and Shraddha Suman Paikray^2
Department of Mechanical Engineering, CUTM, Bhubaneshwar,
Odisha, India
School of Law, CUTM, Bhubaneshwar, Odisha, India

```
Shraddha Suman Paikray
Email: paikray.shraddha@gmail.com
```
**Abstract**
Emotions and facial expression recognition (FER) technologies are also
some of the highest-demanding technologies aided by AI, which help to
interpret human emotions through facial gestures for analyzing human
behavior and processing them further for automated decision making.
Though this technology has a widespread application in industries like
healthcare, marketing, security, and education, it is, however, associated
with some ethical and privacy concerns, as it works on personal biometric
data and the chances of being influenced and monitored by others without
seeking their individual consent. By gathering and analyzing the individual
facial signs in the form of primary data, FER techniques frequently operate
in opaque ways and thus raise some major ethical concerns, like consent,
autonomy, transparency, and fairness. On the other hand, the universality
and regional effectiveness of artificial models (used to recognize emotions)
remained always questionable as they highly rely on assumptions without
taking into account the cross-cultural aspects and human psychological
facts under the fields of races, gender, and different age groups. This can
further lead to diversified inequalities, communal disagreement, and


adverse social implications. Therefore, it is high time to evaluate the
existing FER technologies with the integration of AI through the dual lenses
of human ethics and law. Hence, this chapter tries to give some glimpse of
the lacunas in the existing systems, and thus a comprehensive regulatory
framework is to be proposed by considering the current legislation both at
the regional and international levels in order to maintain the human values
and individual dignity.

**Keywords** Facial Expression Recognition (FER) – Data privacy – AI
technologies – Human rights – Legal frameworks

##### 1 Introduction

Facial expression recognition (FER) is nothing but a fundamental aspect of
non-verbal social communication techniques, which uses the facial cues to
exchange thoughts or emotional states between humans and machines. As
the FER systems try to decode different human-oriented non-verbal
emotions such as anger, happiness, surprise, and sadness, at the present
scenario these systems are becoming the significant role players in the
fields of education, security, and health-care to enable a common platform
where the machines can smoothly interact with the humans and vice-versa
[ 1 , 2 ]. Being a key technology under behavioral science, FER allows
machines to interpret human emotions to improve mental health conditions
and to diagnose rare neurological and debilitating disorders like Parkinson’s
disease and bipolar disorder etc. [ 3 ]. A gradual progress of the FER system
requires continuous updation to integrate nuanced human interactions and
to deliver processed personalized responses by extending the scopes both
for academic research and practical implementations [ 4 ].
With the rapid advancement of technology, artificial intelligence (AI) is
transforming many sectors, including human behavior analysis by acquiring
more detailed and scalable insights on how people feel, think, and act etc.,
and processing the same for the detection of emotional states and prediction
of behavioral consequences [ 5 ]. Introduction of AI also enables a controlled
approach towards personalized learning, behavioral changes, and mental
health issues. By monitoring and analyzing the real-time data, AI AI-aided
system can deliver just-in-time adaptive solutions for addiction type
treatment and health behavior changes [ 6 ]. Certain new characteristics


related to sociology, political science, and psychology can also be unveiled
by processing numerous already existing behavioral patterns through AI-
enabled systems [ 7 ]. So, it can be stated that the analysis of human
behaviors is gradually modernizing with scalable, personalized, and precise
insights by incorporating AI into different systems, but the ethical and
societal consequences of this modernization need to be addressed properly.
Based on the above fundamentals, the present chapter aims to cover the
available technologies for FER and possible extensions of those
technologies from the viewpoint of the ethical and societal implications.
This chapter also tries to explore the challenges and consequences related to
privacy, surveillance, consent, and bias arising due to the integration of AI
into FER techniques. On a final note, it can be mentioned here that this
chapter will act as a rational combination of ethics, technology, law, and
human rights by allowing an interdisciplinary approach to the field of
human behavioral analysis.

Broadly, the objectives of the present chapter can be listed as:

```
To get familiar with the available technologies for FER and to explore the
associated limitations.
To look into how to mold the legal frameworks so that the FER
technologies can be deployed without affecting the fundamental rights to
freedom of expression and rights to privacy.
To regenerate the awareness among the public, policy makers, legal
professionals, and technocrats so that the latest AI-enabled FER
technologies can be implemented smoothly.
```
##### 2 Fundamentals of Emotion Recognition and

##### FER in AI

As FER can be treated as a multi-disciplinary approach to comprehend
human emotional states from facial signals by integrating computer vision,
psychology, and AI, it is necessary to understand the core definitions and
key concepts related to FER.

```
Emotion Recognition (ER): This uses different physiological signals,
forms of speech, and facial expressions to imitate human emotional
intelligence using certain machines [ 8 , 9 ].
```

```
FER: Being a subset of ER, FER emphasizes only facial muscle activities
to analyze and identify the emotional states. Commonly, this deals with
six to seven emotions (like fear, anger, happiness, disgust, sadness,
neutral, and surprise) based upon the received expression patterns [ 8 , 9 ].
```
The key concepts supporting the FER technique can be listed as per below:

Basic Emotion Theory: The fundamental concept of FER can be directly
linked with Ekman’s theory of universal emotions, which postulates that
certain emotions are biologically hardwired and universally expressed
through similar facial patterns [ 10 ].
Feature Extraction: By applying methods like Local Binary Patterns
(LBP), Histogram of Oriented Gradients (HOG), and Principal
Component Analysis (PCA), FER examines facial textures, geometries,
and landmarks to bring out the significant features [ 11 ].
Classification Models: Based on the extracted features, the emotions are
classified through some of the machine learning and deep learning
algorithms, including Convolutional Neural Networks (CNNs), k-Nearest
Neighbors (k-NN), and Support Vector Machines (SVMs) [ 12 ].
Emotion Distribution Recognition: Commonly, expressions may carry an
amalgamation of multiple emotions, which can be presented through
Emotion Distribution Learning (EDL) technique by applying probability
distribution methods [ 13 ].
To discern emotions, different psychological theories are
technologically integrated, so that ER and FER can be performed precisely
by some machines. The advancement of technology requires the advent of
AI in the existing fields so that the application potential of such techniques
can further be extended to various arenas such as education, security, and
health care. Besides earlier concepts, there also exist some AI-based
techniques that can strengthen the technological architecture of emotion and
facial expression recognition processes. Some of these technologies are:

```
CNNs: These can be referred to as the backbone of modern FER systems,
which are capable of recognizing emotion patterns by extracting special
characteristics from images. Further, classify them in the forms of anger,
sadness, and happiness [ 14 ].
Multimodal Emotion Recognition: To address the shortcomings of single
modality systems, AI-enabled systems now simultaneously operate on the
```

combination of a wide range of data, including speech signals, facial
expressions, and physiological data (such as pattern of heart rate and
ECG), which further enhances the detection accuracy and context-
awareness [ 15 ].
Real-time Face Detection and Tracking: To detect, track, and analyze the
dynamic facial emotions in real time, algorithms like YOLOv10 can be
incorporated in video conferencing and education sectors [ 16 ].
Explainable AI (XAI): At present, FER systems are integrated with the
latest tools (like Grad-CAM), which are capable of emphasizing the
facial features that extremely influence the prediction process [ 16 ].
Emotion AI in Human–Robot Interaction: In the field of robotics, FER
systems are aided with AI and IoT to smooth the functions of educational
companions and emotion-aware service robots [ 17 ].
In addition to deep learning, multimodal processing, and computer
vision, modern FER systems are to be integrated with AI to identify and
respond to human emotions accurately.

##### 3 Threats and Risks Associated with AI-Enabled

##### FER Systems

Industries like marketing, education, health care, and security are ready to
adopt the rapid transformations to integrate AI with FER systems. So that
human emotions can easily be interpreted by machines. Whereas, the
associated threats concerning ethics, privacy, security, and bias need to be
addressed with effective resolutions. Below-mentioned are some of the
major threats that can never be ignored:

```
Limitations of Universality Assumptions: FER systems are developed on
the assumptions of universality, backed by Paul Ekman’s theory of six
basic emotions, i.e., anger, surprise, happiness, fear, disgust, and sadness,
which are universally recognised across all humans. Though this
framework assists in training AI models, but lacks to effectively address
groups of diverse races, societies, and cultures. Considering the aspects
of cultural differences, it has been seen that emotional expressions vary
across cultures as compared to individual ones. For instance, East Asian
people will quash negative emotions to maintain group harmony, but
Western populations focus more on individual emotional expressions.
```

```
Thus, FER systems trained on Western datasets fail to recognise emotions
among non-Western populations. In line with this, articles highlighted the
limitations of universality assumptions for the effective working of AI-
based FER models are summarized in Table 1.
```
```
Table 1 Limitations of universality assumptions in FER models
```
```
Limitation Description Example References
Contextual
dependence
```
```
Without prioritizing
linguistic and contextual
clues, only facial expressions
may not always convey
accurate emotions
```
```
Sometimes a smile can be a
symbol of satisfaction, but in
other cases it can also portray
discomfort
```
```
Barrett [ 18 ]
```
```
Cultural
variation
```
```
Cultural variances in
manifestation and emotion
identification raise doubts
about the notion of universal
expressions
```
```
East Asian people will quash
negative emotions to maintain
group harmony, but Western
populations focus more on
individual emotional
expressions
```
```
Barrett et al.
[ 19 ]
```
```
Racial and
demographic
bias
```
```
Training datasets with over-
presentation of Western,
lighter-skinned faces may
lead to reduced accuracy for
minority groups
```
```
FER models are prone to
misclassifying emotions in
older adults, dark-skinned
people, and women
```
```
Kaur and
Kumar [ 4 ]
```
```
Morphological
differences
```
```
The recognition accuracy
also gets affected by
variations in facial structures
```
```
Differences in eyelid shape or
facial musculature alter how
emotions are visually expressed
```
```
Kaur and
Kumar [ 4 ]
```
```
Ethical risks Emotion misinterpretation
can result in bias, loss of
autonomy, and
discrimination
```
```
During legal trials or airport
security screening, errors in
judgment may occur due to
misclassification of ‘fear’ or
‘anger’
```
```
Peng et al.
[ 20 ]
```
The over-simplification of context-mediated, complex, and culturally
situated features of human emotions is carried in AI-based FER models by
adopting the universality assumptions. If the upcoming FER systems fail to
address the above-mentioned limitations, then ethical accountability and
accuracy of these models will remain doubtful, with the chances of
systematic misrepresentation and bias.

```
Privacy Infraction and Unofficial Surveillance: One of the most serious
concerns of AI-enabled FER systems is related to their potential
application for mass surveillance and infringement of individual privacy.
```

The arising of ethical and legal concerns with these systems can be
attributed to monitoring individuals without their proper consent and
collecting sensitive emotional data from public places or online
platforms. These systems are also capable of converting human emotions
into some lax data points, which can further be used for profiling. As any
structured regulatory norms for these systems are missing, different firms
and government bodies are unable to fix clear boundaries to enact against
the violation of emotional freedom and the individual right to privacy
[ 20 ].
Algorithmic Bias and Discrimination: Due to a lack of a diversified
training dataset, FER systems are inherently associated with biases.
Often, the analysis on this dataset can result in skewed outcomes as there
exists a chance of over-presentation of characteristics on the basis of age
or ethnicity, or gender. As an outcome, the individual emotions related to
underrepresented groups may be inaccurately classified or misinterpreted
by the FER models. This collection of biases produces outcomes that are
discriminatory in nature and affects the recruitment process and law
enforcement, where misinterpretation needs to be avoided [ 4 ].
Antagonistic Attacks and System Manipulation: AI-based FER systems
are prone to antagonistic attacks, as a result of which the whole system
can be deceived by anonymous agents through finely altering the input
dataset (like facial features or images). Due to these manipulations, FER
models often misclassify the intended identities or emotions, raising
serious concerns in severely sensitive applications like crime detection or
border control. For example, a violator could apply makeup or other
accessories to spoof facial characteristics and evade detection [ 21 ].
Deep-fakes and Identity Spoofing: The quick outspreading of synthetic
social media or deep-fakes is another fatal threat associated with AI-
based FER systems, as these systems are susceptible to manipulation
using AI-generated fake facial videos, in which the real individuals are
seen to express false emotions. The outcomes of these may lead to
falsification of information, reputational damage, or identity theft. So the
manipulated deep-fake contents might be interpreted as authentic
emotional expressions due to the lack of robust anti-spoofing
mechanisms in the existing FER systems [ 22 ].
Psychological and Social Impacts: The FER systems are also associated
with the rising concerns related to the psychological impacts which are


caused due to the constant monitoring of individuals by the systems, and
this continual surveillance may result in undesirable stresses, altered
behavior, and reduced genuineness in the communication process. These
can also lead to the augmentation of existing inequalities in society by
affecting the marginal communities in a severe way [ 4 ].
Ethical and Governing Gaps: Regardless of their quick deployment, FER
systems may outperform by ignoring the ethical and legal guidelines.
Lack of transparent and globally accepted governing laws with respect to
emotional data collection, usage, and individual consent, however, raises
a serious concern of misuse of FER systems by the authorities or
corporations to manipulate the actual data [ 20 ].
Apart from the above-mentioned threats, AI-based FER systems witness
some critical challenges related to societal, psychological, and ethical
aspects for the smooth execution of this advanced technology, which are
presented in Fig. 1.


**_Fig. 1_** Common challenges witnessed by AI-based FER systems

##### 4 Key Challenges

When a new technology enters to the field of execution, it is evident that it
may face a variety of challenges along its path. But each of the challenges
requires much attention for the smooth execution of AI-based FER models.
Some major challenges are hence discussed as per the following.

**4.1 Ethical Aspects Related to AI**

In the era of globalism, AI is the most sought-after innovations that
increasingly impact decision-making, interactions, transactions, and ethical


considerations about human life. Apart from being a boon for human
civilization, it poses many risks, particularly concerning ethical theories and
law. The exponential rise of AI has coerced humanity to reconsider and
reimagine its traditional philosophical frameworks. All the traditional
jurisprudential theories, like utilitarianism, deontology, and virtue ethics,
that were coined by great philosophers, have been limited by the consistent
use of AI. Furthermore, it has started transcending human capabilities [ 23 ].
In layman’s terms, “ethics” is something that can determine an action to
be good or bad. Likewise, ethical theories are envisaged with the obligation
of concluding what makes an action good or bad. There are two widely
known theories of deontology and consequentialism of the eighteenth-
century Enlightenment period that are the answer to this discord [ 24 ].
There are multiple concerns relating to consent, transparency, and
fairness that the AI-based ER poses. For instance, AI-based ER needs to
facilitate real-time user control _in tandem_ with transparent data-governance
frameworks. Similarly, consent management requires open-communication
frameworks to allow individuals to control their emotional data access.
Also, algorithmic bias creates a lot of problems in ER research. This bias
arises when model accuracy and predictions vary inequitably across
demographic, racial, or cross-cultural groups [ 25 ]. Some of the vital aspects
of ethical concerns can be understood from Fig. 2.


**_Fig. 2_** Some major aspects of ethical concerns related to AI-based FER models

Therefore, different ethical theories need to be understood with respect to
AI.

```
Utilitarianism: Utilitarianism, propounded by Jeremy Bentham and later
by John Stuart Mill, advocates the theory of maximizing overall welfare
and achieving “greatest good for the greatest number” [ 26 ]. In
contemporary times, AI can be used to enhance the welfare policies of
governments by incorporating it into the healthcare sector. AI can analyse
and store data of the greatest number of patients and smoothly facilitate
their treatment process,thus, making the lives of doctors a bit easier.
Similarly, AI is known to make economically feasible decisions. So,
deploying AI in the criminal justice system will be beneficial in
contemplating potential benefits against risks. Although AI ethics may
have an enormous impact on the “greatest numbers,” it still poses
significant obstacles in the way of implementation. As minority rights
```

will be infringed upon while achieving greater efficiency for the majority
[ 24 ].
Deontology: The most prominent advocate of deontology is Immanuel
Kant. It emphasizes the need to adhere to moral rules and obligations
regardless of the further consequences. Deontology asserts to protect
individual rights regardless of the outcomes. In the spectrum of AI, it is
challenging to abide by the theory. It endows the developer with a very
daunting task to safeguard the privacy rights of the users. Apparently,
there are many chances that it will resort to discriminatory practices.
Also, there is another perennial strife between the obligations of the
developer to safeguard the privacy of the user _vis-à-vis_ ensuring public
safety in scenarios where technology for surveillance is used [ 27 ].
Virtue Ethics: The last ethical theory for deliberation was enunciated by
Aristotle. Virtue ethics focuses on virtuous traits that help a human
determine what kind of person he is. It encapsulates high morals, grit,
audacity, and empathy that a human holds. AI is oblivious about such
emotions and attributes, and it is quite impossible to weave these into the
technology [ 28 ].
Thus, these ethics are crucial for the development of the human race,
but infeasible for AI to attain them.

**4.2 Right to Privacy and AI-Bias**

Emotion AI is an intrusive tool used for several purposes, such as security
and verification of a person’s identity, or at times to study customer
behaviour. In more exploratory use, FER can be used to obtain demographic
details of a human being, like age, gender, sexuality, and much more. In this
context, the major question is how these datasets will deduce such
information [ 29 ]. For example, ChatGPT currently introduced Ghibli, used
to convert photos into anime portraits. Apart from doing so, Ghibli
recognized and stored the emotional undertone of the picture. Some major
aspects of the right to privacy and AI-bias and their consequences can be
presented through Fig. 3.


**_Fig. 3_** Right to privacy and AI-bias and their consequences for FER models
Similarly, these days, police are using FER to discern whether a person
in the crowd is going to commit a crime or not. Likewise, governments are
using the technology to track the movement of people on their radar.
Although these FER may be spine-chilling, but can be used to mitigate
terrorism and all potential threats to national security [ 30 ]. Also, there is a
fine line _in tandem_ between FER’s security protections and privacy
violations.

Global Privacy concerns: Article 12 of the Universal Declaration of
Human Rights (UDHR) and Article 17 of the International Covenant on
Civil and Political Rights (ICCPR) envisage ‘right to privacy’ as a
fundamental human right. So, any technology that accesses a human’s
feelings without their voluntary consent shall infringe their fundamental
human right. Other than for security purposes, listening to or recording
telephonic or electronic conversations of private citizens is allowed in
some states. Whereas, certain states require the prior informed consent of
both parties [ 31 ]. Now the bone of contention is whether capturing
feelings and thoughts would fit under this legal framework. Another
critical concern is who are the parties when AI captures feelings and
thoughts?
In unpacking these complexities surrounding security protections and
privacy violations, let us unravel the “self-regulation” phenomenon. In New
Zealand and Australia, there is a legal vacuum concerning the use of FER


technology by police and security organisations, as there is no legislation to
prohibit or empower it. The decision to employ FER is solely made by these
agencies internally without the consent of the general public. Also, there is
a mere possibility for the police to take arbitrary and evasive decisions
without any parameters for deployment of FERs [ 32 ].
Recently, in _R (Bridges) v. the Chief Constable of South Wales Police_
(2020), the South Wales Police used an automated facial recognition system
(AFR) to scan the faces of individuals at public places without their
informed consent. The Court in this case held that the AFR was not “in
accordance with the law” under Article 8 of the European Convention on
Human Rights and the UK Data Protection Act 2018. The Court further
stated that the FER created gender and racial disparities and gave wide
powers to the Police to act on its behest [ 33 ].

Disparities surrounding race and gender: When it comes to identifying
the colour of skin colour of different people, FRE’s work is
unsatisfactory. Consequently, FRE is incapable of retaining higher racial
diversity in its own dataset. Also, FER has a substantial gender bias in its
algorithm data, failing to identify fluidity in gender. It only identifies
male or female; the queer community is erroneously labelled. There is a
constant stereotypical representation in the algorithm of any individual
who wears a dress and makeup as a woman [ 30 ]. Also, empirical survey-
based research demonstrates that FER is dogmatic against minorities. As
it is inexact and vague for the faces of women and people of colour [ 34 ].
Companies heavily rely on datasets of facial images to train their
algorithms. Sometimes this is carried out without the voluntary consent of
the concerned individual [ 35 ]. These images can be acquired from different
sources, like social media, dating sites, CCTV cameras, and much more
[ 36 ]. The algorithms deployed have proprietary sensitivity, thus, abstained
from public disclosure [ 37 ]. Furthermore, when biometric data is stolen, it
can be reset and retrieved, but the transgression of FER cannot be reset
[ 30 ]. Therefore, FER is the most volatile form of technology that can
potentially violate the privacy rights of many. Although privacy is a basic
principle of international human rights law but it is not defined in any
international instrument. Notwithstanding that ICCPR stands as an authority
that guarantees ‘right to privacy’ but it cannot be held as a firewall for


current technological advancements. Any kind of breach by Emotion AI and
FER algorithms can only be catered to by new-age novel laws.

**4.3 Psychological and Social Impacts**

Face is an inalienable part of human day-to-day lives. A ‘facial expression’
can determine the mental and emotional state of a human being. Tech giants
are making enormous financial and material investments to read emotions
and detect and understand their facial expressions. For example, Microsoft
has developed an Emotion Application Programming Interface (API) that
detects different emotions of human beings like happiness, anger, disgust,
grief, and much more [ 19 ]. Similarly, facial expressions are used to read the
emotions and mental state of patients having psychiatric disorders like
autism. This facial configuration is indispensable in such diagnosis and
treatment [ 38 ]. Likewise, United States Supreme Court Justice Anthony
Kennedy quoted that _“reading the emotions of the defendant is knowing the
heart and mind of the offender”_ [ 39 ]_._
It is well established that exposure to FER may lead to a violation of
individual privacy. It is not a concern for people who do not express much
and remain neutral, but the ones who vividly express their emotions are
subjected to technological scrutiny and surveillance. However, FER is an
appropriate platform for teaching, but it can prove to be a misguided
channel to share secrets and vulnerabilities. On the other hand, FERs are
incompetent to demonstrate empathy and compassion when humans share
their problems. Then, after, humans become disheartened and get swayed
away when they get such unnatural empathetic responses [ 40 ]. This erodes
human emotions and makes them go after something that is abstract and
transient. If the current state persists with the sporadic expansion of
Emotion AI and FER, then it will exacerbate systemic discrimination and
stigma in society and will pose adverse effects on society [ 41 ]. Therefore,
technology should serve us, not rule us. Thus, a brief presentation of this
can be displayed through Fig. 4


**_Fig. 4_** The psychological and social impacts of FER systems and their consequences

##### 5 Regulatory Frameworks Related to FER

##### Techniques

The states are entrusted with the utmost duty of safeguarding the privacy
and dignity of their subjects under international law. Transcending beyond
normative legal frameworks of privacy and data protection, these FER
technologies are an imminent danger to the ‘freedom of our inner existence’
as held by the European Court of Human Rights (ECHR) [ 42 ]. When we
ruminate on the legal and ethical implications of ‘emotional data,’ there
comes a dire need to define “emotions” and then “emotional data” in a legal
context [ 43 ]. Before discussing nascent ongoing deliberations on Emotional
AI and FER, let us first analyse the existing framework.

**5.1 Integrated Regime of GDPR and ECHR**

The European Union’s (EU’s) General Data Protection Regulation (GDPR)
came into effect in the year 2018. GDPR mandates a clear and lawful basis
for personal data processing. The biometric data found under Article 4 (14)
of the GDPR is procured via conforming to special techniques relating to
unique physical, physiological, or behavioural attributes of a natural person,


including facial images or dactyloscopic data. Although the traits of a
human face are unique, it can still be considered as a kind of biometric data.
Due to the biometric quality, if the facial detection is already personal data,
then it shall come under the purview of sensitive data under Article 9(1) of
the GDPR. Under the GDPR framework, special consent needs to be
obtained when such sensitive data is used [ 43 ]. Further, it directs stringent
measures to be followed when a special category of personal data is
processed [ 44 ].
With the rapid advancements of technology, GDPR is unable to keep
pace, including its application to new technologies like FER. In the EU
context, laws relating to privacy exert greater influence than data protection
laws (GDPR). Therefore, the European Convention on Human Rights
(ECHR) is more systemic in enforcing individual rights regarding FER. In
regard to addressing the demand for ethical responsibility of FER, both
GDPR and ECHR, and the domestic Human Rights framework within EU
states have to be invoked in the adjudication process [ 44 ].

**5.2 Emerging Trends in FER Technologies**

FER is a complex technological dataset that calls for robust laws to be
enacted to protect individual privacy and freedom of speech and expression.
For regulating AI, preliminary work has started at the international level. In
furtherance of the same, the Organisation for Economic Co-operation and
Development (OECD) principles on AI were adopted in May 2019. It states
that overall deployment of AI, including FER, should be done in
consonance with the ‘rule of law.’ The G-20 states have also shown faith in
these principles. A comprehensive and enforceable rule concerning FER is
crucial in the legal arena [ 45 ].

**5.3 Global Divide in AI Regulation**

In the global scenario, there is a divide in AI regulation. There is a need for
a unified, coherent, and comprehensive framework for its regulation.
Different states follow different structures; the UK is pioneering in the data
protection laws, whereas the US has a more decentralized approach, where
individual states have their own legal framework for addressing AI bias. For
instance, California has its own statute on ‘Privacy Rights.’ Similarly, EU
states, Canada, and Brazil have a ‘horizontal’ framework of addressing
common issues relating to AI. Despite this split in the contemporary world,


we need a uniform and robust international regime on AI regulation
concerning FER technologies [ 46 ].

##### 6 Recommendations and Future Scopes for AI-

##### Based FER

To smoothly implement the AI-based FER technology, the critical
challenges related to legal conflicts, psychological inconsistencies, and
ethical and privacy discriminations must be thoroughly addressed. To
incorporate the ethical theories with the practical governance system, the
following recommendations may be helpful:

AI systems should ensure the embedding of ethical principles from the
foundation itself by balancing the deontological commitments with the
utilitarian benefits. Care should also be taken so that the designated
systems can serve for public welfare without affecting individual rights,
especially for minority and vulnerable communities.
An identical international regulatory framework is highly essential to
bridge the fragmented global policies in AI governance. This should
cover the aspects of the Universal Declaration of Human Rights with the
expanded definitions of ECHR and GDPR explicitly for emotional data
by ensuring legal protection to inner expression and individual dignity.
Collection of facial and emotional data must be facilitated with informed
and opt-in consent from the individuals. In accordance with articles 12
(UDHR) and 17 (ICCPR), governments must prioritize to legally
differentiate the concepts of security-based surveillance and privacy-
invasive emotional monitoring.
To avoid common discriminations in culture, gender, race, and
communities, developers should look for algorithmic fairness protocols
to enable diversity in training datasets and transparency in model
formations.
Before deploying FER models in public domains as workplaces,
healthcare centers, or schools, the users need to be educated regarding the
model execution and how their emotions are being interpreted in the
models, so that mental wellness can be ensured.
Along with the above recommendations, researchers and policy makers
may also look into the future scopes of AI-based FER models so that the


systems can be user-friendly and simultaneously protect one’s right to
privacy without affecting the ethical concerns. Figure 5 depicts the future
scopes or directions to expand the FER technology further.

**_Fig. 5_** Future research directions of AI-based FER systems

##### 7 Key Takeaway Points from the Total Discussion

Through this chapter, an attempt is made to represent the current scenario of
AI-based FER system by discussing their basic concepts, associated
technologies, present challenges with some viable recommendations.
However, some key takeaway can be:

```
Technological Foundation: For successful application of FER techniques
across industries like security, healthcare and marketing, the basement
can be strengthened by computer vision, multi-modal techniques and
deep learning methods.
```

```
Ethical Grounding: The significance of ethical theories (utilitarianism,
deontology, and virtue ethics) is highlighted through this discussion
which can act as a guiding framework for further development of FER
systems.
Privacy Concerns: The urgent need for accountable practices is
underscored clearly by raising the concerns related to personal consent,
data collection and storage, and surveillance.
Bias and Fairness: Performance inequalities and dataset imbalances
across the variety of demographic groups portrait serious threats of
discrimination.
Psychological and societal impacts: FER systems can critically influence
personal autonomy, trust in AI systems, and mental health, which can
further result in vital social implications.
Legal frameworks: The existing guidelines, like CCPA and GDPR, may
act as a partial safeguard, but the global loopholes and inconsistencies
need to be considered on a serious note.
Mitigation strategies: For ethical acceptance of FER systems, bias-
reduction methods, explainable AI, and privacy-preserving technologies
play a major role.
Future Prospectus: Emerging technologies can open up the channels for
new opportunities in exchange for robust research efforts, challenging
interdisciplinary collaborations, and progressing risks.
```
##### 8 Conclusion

The study of FER systems with the integration of AI conveys an influential
yet ethically challenging complex intersection of human behaviours and
advanced technologies. In the beginning, the significance of AI-based FER
systems is highlighted with extended applications in crucial sectors like
healthcare, education, and security. Then the fundamental concepts of FER
systems and the available technologies are analysed to understand how the
systems interpret human behaviours. Rapid adoption of this technology also
demands a rigorous investigation of the ethical concerns, privacy aspects,
and the available legal frameworks, which are also described. The common
challenges circulating around the privacy and algorithmic bias, including
demographic misrepresentation, fairness gaps, and unauthorized emotional
surveillance, along with the societal and psychological concerns, are


thoroughly presented to raise awareness the users, policy makers, and
developers. The urgent recruitments of global regulatory harmonization are
pointed out to mitigate the risks of AI-based FER systems, and hence a set
of suitable recommendations is stated to establish the importance of
interdisciplinary research, trust in human-AI relations, and carry out further
research for the smooth execution of this technology.
Finally, this can be noted that to serve humanity ethically and equitably,
FER systems must be capable of bridging a linkage between accountability,
innovation, and human dignity.

##### References

1. Ko, B.C.: A brief review of facial emotion recognition based on visual information. Sensors
    **18** (2), 401 (2018). https:// doi. org/ 10. 3390/ s18020401
    [Crossref]
2.
    Canedo, D., Neves, A.J.: Facial expression recognition using computer vision: a systematic
    review. Appl. Sci. **9** (21), 4678 (2019). https:// doi. org/ 10. 3390/ app9214678
    [Crossref]
3.
    Fernandes, F.D.B.F., Gigante, A.D., Berutti, M., Amaral, J.A., de Almeida, K.M., de Almeida
    Rocca, C.C., Nery, F.G.: Facial emotion recognition in euthymic patients with bipolar disorder
    and their unaffected first-degree relatives. Compr. Psychiatry **68** , 18–23 (2016). https:// doi. org/
    10. 1016/ j. comppsych. 2016. 03. 001
    [Crossref]
4.
    Kaur, M., Kumar, M.: Facial emotion recognition: a comprehensive review. Expert. Syst. **41** (10),
    e13670 (2024). https:// doi. org/ 10. 1111/ exsy. 13670
    [Crossref]
5.
    Divya, S., Desai, A.K., Dave, V.: Artificial intelligence for human learning and behaviour
    change. Int. J. Adv. Sci. Comput. Appl. **4** (2) (2025). https:// doi. org/ 10. 47679/ ijasca. v4i2. 68
6.
    Kankanhalli, A., Xia, Q., Ai, P., Zhao, X.: Understanding personalization for health behavior
    change applications: a review and future directions. AIS Trans. Hum.-Comput. Interact. **13** (3),
    316–349 (2021). https:// doi. org/ 10. 17705/ 1thci. 00152
7.
    Filippis, R., Foysal, A.: Insights unveiled: harnessing AI to explore human behaviour in social
    sciences. J. Math. Tech. Comput. Math. (2024). https:// doi. org/ 10. 33140/ jmtcm. 03. 05. 04
8.
    Busso, C., Deng, Z., Yildirim, S., Bulut, M., Lee, C.M., Kazemzadeh, A., Narayanan, S.:
    Analysis of emotion recognition using facial expressions, speech, and multimodal information.
    In: Proceedings of the 6th International Conference on Multimodal Interfaces, pp. 205–21 1
    (2004)


9.
Tarnowski, P., Kołodziej, M., Majkowski, A., Rak, R.J.: Emotion recognition using facial
expressions. Procedia Comput. Sci. **108** , 1175–1184 (2017). https:// doi. org/ 10. 1016/ j. procs. 2017.

05. 025
[Crossref]
10.
Fernández-Dols, J. M., Crivelli, C.: Recognition of facial expressions: past, present, and future
challenges. In: Understanding Facial Expressions in Communication: Cross-cultural and
Multidisciplinary Perspectives, pp. 19–40 (2015). https:// doi. org/ 10. 1007/ 978-81-322-1934-7_ 2
11.
Islam, B., Mahmud, F., Hossain, A.: High-performance facial expression recognition system
using facial region segmentation, fusion of HOG & LBP features and multiclass SVM. In: 2018
10th International Conference on Electrical and Computer Engineering (ICECE). IEEE, pp. 42–
45 (2018). https:// doi. org/ 10. 1109/ ICECE. 2018. 8636780
12.
Mellouk, W., Handouzi, W.: Facial emotion recognition using deep learning: review and insights.
Procedia Comput. Sci. **175** , 689–694 (2020). https:// doi. org/ 10. 1016/ j. procs. 2020. 07. 101
[Crossref]
13.
Zhou, Y., Xue, H., Geng, X.: Emotion distribution recognition from facial expressions. In:
Proceedings of the 23rd ACM International Conference on Multimedia, pp. 1247–1250 (2015).
https:// doi. org/ 10. 1145/ 2733373. 2806328
14.
Jain, I., Jain, M., Sadhwani, P., Gupta, S., Sharma, S.: Facial emotion recognition through
artificial intelligence and machine learning. In: 2024 International Conference on Intelligent
Computing and Sustainable Innovations in Technology (IC-SIT), pp. 1–6 (2024). https:// doi. org/
10. 1109/ IC-SIT63503. 2024. 10862210
15.
Jindal, V., Singh, K.: Emotion recognition in AI: bridging human expressions and machine
learning. Int. J. Multidiscip. Res. (2025). https:// doi. org/ 10. 36948/ ijfmr. 2025. v07i01. 34795
16.
Harish, S., Harikumaran, B., Giriprasad, S.: Emotion recognition model based on visual cues and
explainable AI using facial expression video. In: 2024 4th International Conference on Mobile
Networks and Wireless Communications (ICMNWC). IEEE, pp. 1–7 (2024). https:// doi. org/ 10.
1109/ ICMNWC63764. 2024. 10872381
17.
Prasad, C.G., Gowtham, S., Mahesh, M., Sainudeen, A.B., Prabhakaran, P., Kumareshan, N.: A
DT framework based AI robot for facial recognition using IoT. In: 2023 International Conference
on Computer Communication and Informatics (ICCCI). IEEE, pp. 1–5 (2023). https:// doi. org/ 10.
1109/ ICCCI56745. 2023. 10128633
18.
Barrett, L.F.: Categories and their role in the science of emotion. Psychol. Inq. **28** (1), 20–26
(2017). https:// doi. org/ 10. 1080/ 1047840X. 2017. 1261581
[Crossref]
19.
Barrett, L.F., Adolphs, R., Marsella, S., Martinez, A.M., Pollak, S.D.: Emotional expressions
reconsidered: challenges to inferring emotion from human facial movements. Psychol. Sci.
Public Interes. **20** (1), 1–68 (2019). https:// doi. org/ 10. 1177/ 1529100619832930
[Crossref]
20.
Peng, Z., Fu, R.Z., Chen, H.P., Takahashi, K., Tanioka, Y., Roy, D.: AI applications in emotion
recognition: a bibliometric analysis. SHS Web Conf. **194** , 03005. EDP Sciences (2024). https://


doi. org/ 10. 1051/ shsconf/ 202419403005
21.
Lekota, N.: Governance considerations of adversarial attacks on AI systems. In: International
Conference on AI Research (2024). https:// doi. org/ 10. 34190/ icair. 4. 1. 3194.
22.
Bethu, S., Trupthi, M., Mandala, S.K., Karimunnisa, S., Banu, A.: AI-IoT enabled surveillance
security: deepfake detection and person re-identification strategies. Int. J. Adv. Comput. Sci.
Appl. **15** (7) (2024). https:// doi. org/ 10. 14569/ ijacsa. 2024. 0150799
23.
Nwokoye, A.U.: Ethics in the age of artificial intelligence: reconceptualising the traditional
ethical theories. Trinitarian Int. J. Arts Hum. 77–88 (2025)
24.
Stahl, B.C.: Concepts of ethics and their application to AI. In: Stahl, B.C. Artificial Intelligence.
Springer, pp. 19–34 (2021)
25.
Barker, D., Tippireddy, M.K.R., Farhan, A., Ahmed, B.: Ethical considerations in emotion
recognition research. Psychol. Int. **7** (2), 43 (2025). https:// doi. org/ 10. 3390/ psycholint702004 3
[Crossref]
26.
Bentham, J.: An introduction to the principles of morals and legislation. London, Printed for W.
Pickering [etc.] (1823)
27.
Hayes, P., Fitzpatrick, N., Ferrandez, J.M.: From applied ethics and ethical principles to virtue
and narrative in AI practices. AI and Ethics 1–23 (n.d.)
28.
Farina, M., Zhdanov, P., Karimov, A., Lavazza, A.: AI and society: a virtue ethics approach_._
Retrieved from Springer Nature (2022). https:// dspace. kpfu. ru/ xmlui/ bitstream/ handle/ net/
173297/ F_ AI_ and_ Society. pdf? sequence= -1
29.
Lynch, N.: Facial recognition technology in policing and security—case studies in regulation.
Laws 1–14 (2024)
30.
Jacques, L.: Facial recognition technology and privacy: race and gender—how to ensure the right
to privacy is protected. San Diego Int. Law J. 111–156 (2021–2022)
31.
Bard, J.S.: Developing legal framework for regulating emotion AI. Boston Univ. J. Sci. Technol.
271–311 (2021)
32.
Lynch, N., Campbell, L.: Principled regulation of facial recognition technology—a view from
Australia and New Zealand. In: Matulionyte, R., Zalnieriute, M. (eds.), The Cambridge
handbook of facial recognition in the modern state. Cambridge University Press, pp. 253–266
(2024)
33.
R (Bridges) v. Chief Constable of South Wales Police, EWCA Civ 1058 (CA) (2020)
34.
Klare, B.F., Burge, M.J., Klontz, J.C., Bruegge, R.W., Jain, A.K.: Face recognition performance:
role of demographic information. IEEE Trans. Inf. Forensics Secur. 1789–1801 (2012)
35.
Browne, R.: Tech giants want rules on facial recognition, but critics warn that won't be enough.
Retrieved from CNBC (2019). https:// [http://www.](http://www.) cnbc. com/ 2019/ 08/ 30/ facial-recognition-tech-firms-
want-regulation-but-critics-want-a-ban. html? msockid= 0db12cf1fb8e6919 3585393afa1768d4


36.
Metz, C.: Facial Recognition Tech is growing stronger, Thanks to your face. Retrieved from The
New York Times (2019). https:// [http://www.](http://www.) nytimes. com/ 2019/ 07/ 13/ technology/ databases-faces-
facial-recognition-technology. html
37.
Office, U.G.: Report to congressional requesters on facial recognition technology, privacy and
accuracy issues related to commercial uses. US Government Accountabilty Office (GAO) (2020)
38.
Cohen, S.B., Wheelright, S., Hill, J., Raste, Y., Plunb, I.: The reading the mind in the eyes test
revised version: a study with normal adults, and adults with Asperger syndrome or high-
functioning autism. J. Child Psychol. Psychiatry 241–251 (2001)
39.
Riggins v. Nevada, 504 (US Supreme Court 1992)
40.
Lisetti, C.L., Schiano, D.J.: Automatic facial expression interpretation: where human-computer
interaction, artificial intelligence and cognitive science intersect. In: Pragmatics and Cognition
(Special Issue on Facial Information Processing: A Multidisciplinary Perspective), pp. 185–235
(2000)
41.
Monteith, S., Glenn, T., Geddes, J., Whybrow, P. C., Bauer, M.: Commercial Use of emotion
artificial intelligence (AI): implications for psychiatry. Curr. Psychiatry Rep. 203–211 (2022)
42.
Glukhin v. Russia 206 (European Court of Human Rights July 04, 2023)
43.
Gremsl, T., Hodl, E.: Emotional AI: legal and ethical challenges. Inf. Polity 163–174 (2022)
44.
Almeida, D., Shmarko, K., Lomas, E.: The ethics of facial recognition technologies,
surveillance, and accountability in an age of artificial intelligence: a comparative analysis of US,
EU, and UK regulatory frameworks, pp. 377–387. Springer, AI and Ethics (2022)
45.
Qandeel, M.: Facial recognition technology: regulations, rights and the rule of law. Front. Big
Data **2024** , 1–14 (2024)
46.
Park, S.: Bridging the global divide in AI regulation: a proposal for a contextual, coherent, and
commensurable framework. Wash. Int. Law J. 216–269 (2023–2024)
_OceanofPDF.com_


(1)

© The Author(s), under exclusive license to Springer Nature Switzerland AG 2026
K. Slimani et al. (eds.), _Emotion and Facial Recognition in Artificial Intelligence: Sustainable
Multidisciplinary Perspectives and Applications_ , Information Systems Engineering and Management
78
https://doi.org/10.1007/978-3-032-14778-3_18

## Emotion AI: Challenges and Future

## Directions

Anjali Thakur^1 and Gaurav Gupta^1
Yogananda School of AI, Computers and Data Sciences, Shoolini
University, Solan, Himachal Pradesh, India

```
Anjali Thakur
Email: anjalithakur@shooliniuniversity.com
Email: anjalithakur200093@gmail.com
```
```
Gaurav Gupta (Corr esponding author)
Email: gaurav@shooliniuniversity.com
Email: solan.gaurav@gmail.com
```
**Abstract**
The most richly diverse field of facial recognition is Emotion AI. This field
builds a relationship between artificial intelligence and human emotional
intelligence. Emotion AI applies identity recognition to decipher facial
expressions, voice patterns, and even physiological signs to comprehend
human feelings. This chapter explores the primary limitations and
challenges of Emotion AI. It concentrates on data limitations and bias,
technological challenges in implementing Emotion AI technology in real
life, ethical hazards, and legal problems concerning trust in human and AI
interactions. We use case studies and applications in health care, education,
marketing, and public safety as a vehicle for illustrating the potential and
risks associated with the use of these applications. We also stress the
necessarily interdisciplinary nature of Emotion A.I., overlapping AI
research with psychology, human computer interaction (HCI), and ethics


and stewarship. We conclude by providing a roadmap based on future
directions which highlight the importance of multimodal approaches (viz.
AI deployment in an interdisciplinary manner), explainable and transparent
architectures, culturally aware modelling, and sustainable practices. In
combination with technical invention, an ethical consideration and human-
centered practice of design, Emotion A.I. has the potential to develop/invest
toward a transformative yet sustainable changes to human wellness and
interaction across disciplines.

**Keywords** Face recognition – Artificial intelligence – Human computer
interaction – Mental health – Emotion AI

##### 1 Introduction

Emotion A.I., also known as affective computing, is an emergent area of
research that identifies, accesses and responds to human emotions. Rapidly
evolving beyond facial recognition, and multimodal data, Emotion A.I.
moves toward a human-centered A.I, from simple identification, to
understanding transient, sustained but complex emotional states [ 1 ]. In the
context of the chapter, and consistent with a multidisciplinary and
sustainable perspective, Emotion A.I. demonstrates the necessity to join the
field of technical invention with psychological theory, ethical change, HCI,
and stewardship. A person's facial expressions are still among the most
efficient and informative sources for emotion detection. However, Emotion
AI applications also typically combine this with speech, text, physiological
signals, and indicators of context to create holistic understanding of human
affect [ 2 ]. This chapter provides an overview of the technical, societal, and
ethical challenges of Emotion AI, and suggest techniques for building
systems that are ethical, take into consideration sustainability, and are
accurate, inclusive, and cultural competent.
In more concrete terms, this chapter has multiple goals: First, we wish
to present a clear and accessible summary of developments in research and
practices of Emotion AI and facial recognition technologies; Second, we
will analyze pertinent challenges related to multiple domains technical,
ethical, social; Third, we will highlight the next priorities which balance
innovativeness with sustainability. By drawing together concerns inter-


disciplinarily, our message is that real success in deploying using Emotion
AI must consider an inter-disciplinary perspective to inform capabilites.

##### 2 Foundations of Emotion AI in the Context of

##### Facial Recognition

More generally, Emotion AI is about the use of computer-based facial
recognition technology to do more than establish who a person is Shan et al.
[ 3 ]. The identity verification mechanisms found in facial recognition
systems, called person recognition systems, are focused on recognizing and
mapping identity markers from facial features for identity verification.
Emotion AI will use techniques and methods of person recognition for
example, mapping facial expression over time that will result in quantifying
changes in facial expressions linking them with possible emotional state.
Emotion AI leverages facial signals to interpret emotional information
through the analysis of subtle movements such as whether the user's
eyebrows are raised, the extent to which the lip is curved, or various micro
expressions [ 4 ]. The emphasis on facial data is of special importance, as the
human face conveys more social information and is the most expressive
form of communication modality.
However, effective recognition of emotion goes beyond simply being
able to detect static facial points. It also requires a series of processes
stemming from the original signals into reliable predictions. We can start
with input modalities: visual facial data is often included alongside other
sources, such as speech and physiological signals, to provide further
contextual emotional meaning [ 5 ]. Next is feature extraction, which detects
and identifies meaningful markers, such as facial action units that signify
muscle movement, or prosodic features in speech, such as pitch and
intensity [ 6 ]. Next, these features will move into the model architectures:
deep learning nested architectures or convolution neural networks (CNNs)
are typically used to explore spatial relationships within faces, whereas
deep learning recurrent networks (RNNs), or transformer-based models,
compartmentalize temporality in audio or physiological signals [ 7 , 8 ]. The
final path to journey is in the emotions’ prediction, either mapping what is
learned into discrete categories (e.g., happy, sad, or anger), or the
continuous emotional dimensions of arousal and valence. Modern methods


increasingly utilize multimodal fusion frameworks as shown in Fig. 1.
While facial recognition offers a powerful basis, the limits of facial
expression alone still present some challenges since expressions may be
masked, ambiguous, or culturally variable [ 9 ]. Multimodal systems address
these drawbacks by utilizing a single architecture that encompasses facial,
vocal, and physiological input to create a system that is more robust to
contexts themselves [ 10 , 11 ]. This approach allows the system to validate
signals against each other for improved accuracy and resilience against real-
world issues.

**_Fig. 1_** Multimodal emotion AI framework

##### 3 Multidisciplinary Nature of Emotion AI

The complexity of Emotion AI means it is reliant on the theories of many
different disciplines. For example, artificial intelligence supplies the
computational power to develop algorithms that can learn from facial,
vocal, and physiological signals; psychologists and affective scientists
ensure the development of models that reflect valid theories of emotion
whether those theories are categorical (e.g., Ekman's basic emotions) or
dimensional (e.g., valence-arousal models), ethics, law, and policy inform
responsible, thoughtful development and application to ensure the absence
of issues like privacy, bias, and misuses in cases of high stakes applications
like surveillance or marketing and HCI is responsible for ensuring that
systems are human-centered, accountable, and trustworthy while applying
the same psychological and affective principles as well as consideration for
how users perceive and interact with these emotionally intelligent machines
[ 12 ]. Finally, sustainability and social sciences define longer-term


considerations, from the environmental cost of training the model to fair
review across cultures and demographics [ 13 ]. Together, these perspectives
provide a holistic framework for Emotion AI as illustrated in Fig. 2.

**_Fig. 2_** Holistic architecture of emotion AI
Technical complexity is not enough, contextualizing in psychology
ensures validity, ethics provides responsibility whereas HCI ensures
usability, and sustainability ensures longevity [ 14 , 15 ]. Without this
integration, even technically accurate models risk social acceptance, ethical
breaches, or environmental degradation.

##### 4 Challenges and Limitations in Emotion AI


Even though the Emotion AI is significant, the task of creating it ethically
and reliably is difficult by problems that lie far beyond algorithms and
computing. This field of research combines technological innovation,
human behaviour, and social ethics [ 16 , 17 ]. Emotion AIs must work with
limitations in their data, technical issues, and overarching ethical concerns
if they must work equitably and effectively. Each of these aspects shows
how the technology is based on the complexity of human beings at the same
time, it provides both opportunities and a challenge of responsibility [ 18 ].

(1) **Data-Related Challenges: The Foundation Problem**

Emotion AI is highly reliant on the quantity and diversity of its data just
like any other AI system [ 19 ]. People's emotions are by their very nature
subjective, context-rich, and deeply personal characteristic, making
collecting and labeling emotional data nearly impossible. One of the
primary challenges of Emotion AI to overcome will be getting high-quality,
large-scale datasets that truly represent the range of human emotional
expression [ 20 ]. Existing datasets have mostly been collected from
controlled laboratory settings where participants intentionally acted out
emotions (not just expressive). Data like this creates a divide between what
the Emotion AI learns on a dataset, and what it sees as emotions in real-life
daily settings [ 21 ]. For instance, a model trained on extreme “happy” or
“angry” faces of actors may not detect nuance in real settings, such as
frustration or empathy.
Also, there are ongoing demographic and cultural disparities in many
emotion datasets [ 22 ]. If most of your data comes from a small segment of a
population, like western, younger, lighter-skinned individuals, the models
trained on that data may be biased when deployed in diverse global
populations. For example, across cultures the signs of facial expression can
vary. A gracious smile is not necessarily going to show true happiness in a
specific culture. Lack of cultural awareness could create misreading of
emotion that will be important in use, such as in mental health tracking or
educational settings where emotional issues and subtlety are important [ 23 ].
The subjectivity of labeling presents another confusing complexity, in
contrast to objective subjective-based attributes such as temperature or
sound frequency, emotions cannot be quantitatively measured [ 24 ].
Different human annotators might label the same facial expression or voice


clip differently, creating inconsistent ground truth data. Adding to the
complexity of categorization, emotional states rarely function in neat
categories someone might be both anxious and excited, but most datasets
only impose rigid categories on that emotion. All this inconsistency adds up
to measurement inconsistency, which undermines model reliability and
applicability in the real world.

(2) **Privacy and Ethical Data Collection**

As the information contained within the data is highly sensitive,
emotion AI introduces the most challenging privacy issues [ 25 ]. Most of
data aims to assess diverse emotional states, in fact, concentrates on
something apart from observers, observing facial expression or tone of
voice. For instance, measuring basic or compound emotional states usually
depends on heart rate variability or an EEG to record brain activity that may
also yield some personal data regarding the individual [ 26 ]. Such
information may be considered invasive if obtained without it being known
or agreed upon. In cases of healthcare, or workplace monitoring, the
essence of voluntary consent can be lost or even removed depending on
what is done with emotional data. As an instance, a system of monitoring
worker emotions to maintain productivity may utilize emotional data tagged
with an individual's personal mental health [ 27 ]. The difficulty of gathering
a data set large enough to maximize model performance/accuracy and the
goals of keeping individuals’ privacy is arguably one of the most ethically
challenging issues in Emotion AI research. In addition to this, the
physiological information such as EEG, heart rate variability, etc. provides
yet another level of barrier to the problem [ 28 ]. Emotional states and even
potential medical conditions can be inferred from signals as well, where
abuse or compromise of this information is problematic. It is thus essential
that privacy-preserving models, utilizing for instance, federated learning or
differential privacy, are constructed to balance innovation with responsible
ethical measures [ 29 ].
Apart from such data issues, Emotion AI systems face number of
technical limitations when applied in real-world contexts.
Models constructed to quantify or evaluate emotion can work well in a
controlled test set or lab “real world” test, catalogue or clinical environment
but at times are vulnerable to “in-the-wild” environments where
performance metrics can take a tremendous hit [ 30 ]. Utilization of


extraneous sources such as light fluctuations, facial occlusions, or noise can
plague input signal and impact models such as, a camera-based emotion
identification system can misunderstand an emotional event if the person is
wearing a mask, specs or a headscarf. Speech-based evaluation face similar
issues, apparent in we identify genuine emotional tone from varying pitch,
accent, and ambient auditory cues [ 31 ].
In addition to this as we know human beings deduce emotional states
emotion AI must be capable of multi-modal integration to ensure
simultaneous application of facial, vocal, and physiological signals [ 32 ].
Each modality brings its noise and artifacts, but the added technical
challenge is aligning the modalities in real-time. This challenge is
exacerbated by issues when trying to interpret subtle or blended emotions
like bittersweet feelings or repressed anger, which generally don't take on
readily observable patterns.
Moreover, generalizability is a chronic problem, a model that has been
trained on one dataset never seems to well for another context or
population. If we talks about, an affect detection system trained on social
media clips will never transfer to clinical environments [ 33 ]. This “domain
gap” strengthens the limits of constructing models on finite and static data
to quantify the dynamic, ever-changing nature of human affect.

(3) **Ethical and Social Implications**

Evidence created in a lab-type environment exceeds the societal and
ethical implications of Emotion AI. There is more concern regarding misuse
and manipulation with the possibility of social robots and affect detection
being used to make inferences about very intimate states [ 34 ]. For instance,
it is not far-fetched that Emotion AI would be utilized to monitor crowds
and identify “suspicious” categories of emotions, patterns of interpretive
behaviours which raise serious issues for civil liberties [ 35 ]. The prospects
for governments or companies leveraging emotional data to manage,
discriminate, or target behaviours build a definite need for robust ethical
safeguards and compliance regulations.
Moreover, in any Emotion AI model biasness and discrimination can be
quite simply imply instilled just to highlight, if an algorithm misjudges an
emotion for specific demographic groups it can perpetuate stereotypes or
produce unequal results [ 36 ]. A model that misinterprets anger in the facial


expressions of one ethnic group but Interprets calmness in another ethnic
group sustains social stigma instead of improving it.
Another issue arises from emotionally influencing users in commercial
and political situations. Advertising systems can take advantage of
emotional vulnerabilities to customize ads for a certain mood or political
campaigns can determine emotional states and respond to those in terms
that impact human action [ 37 ]. These use cases indicate that ethical
principles for transparency are necessary for the design of Emotion AI, to
avert the potential for influencing human emotion.

(4) **Trust, Usability, and Human–AI Interaction**

Finally, trust and usability are continuing challenges for the human–AI
relationship [ 38 ]. Emotion AI attempts to recognize human emotion and
evoke a response, but the user questions whether a machine can truly
“understand” emotion [ 39 ]. In situations where AI makes mistakes, for
example, interpreting sadness as boredom—then the user no longer trusts
the machine to accurately identify their emotion [ 40 ]. Conversely, some
users may fall into the trap of overzealously trusting AI predictions,
accepting its assessments regardless of whether the AI's prediction is wrong
or the assessment is not appropriate to the context.
Moreover, these issues are complicated by a lack of transparency and
explainability. Many Emotion AI systems function in a black box, providing
predictions without understanding how the prediction was reached. In
domains that are contextually sensitive, such as mental health or law
enforcement, a black-box systems compromise accountability and generate
ethical discussions [ 41 ]. It is paramount to generate Emotion AI that is
explicable, inclusive of cultures, and centered on the user, and to involve
not only engineers but interdisciplinary experts-psychologist, ethicists, and
social scientists who are embedded in the real world-to contribute to
solutions that honour the diversity of emotions with respect for the dignity
of human [ 42 ].

(5) **The Path Forward: Beyond Technical Fixes**

The issues Emotion AI are making it evident that the future of Emotion
AI should not always depend on better algorithms or including more data


[ 43 ]. These realities will not generate change. A human-centered approach
must be included to have ethics, inclusivity, and transparency demonstrated
when engineered or deployed [ 44 ]. Emotion AI should be in a context of
respect to cultural differences, right to privacy, and accountability.
Ultimately, the target of Emotion AI should not be a replacement for human
empathy, but a reformer and develop tools that to prepare us for the
information of emotional experience, without losing sight of true ethics
[ 45 ]. As researcher and developer begin to address technical, ethical, and
societal challenges together and collectively, they have the potential to
evolve Emotion AI into technology that is practically useful for humanity,
not simply assessing it.

##### 5 Case Studies and Applications

Emotion Artificial Intelligence, commonly referred to as affective
computing, is fast transforming into one of the most exciting fields within
today's technologies. Essentially, what this refers to is the idea of
programming computers to be able to recognize and respond to people's
emotions. Emotion AI observes things like facial expressions, tone of voice,
heart rate changes, and even the pitch and rhythm of our speech to try and
understand how we might be feeling in the moment and then adjust
behavior accordingly [ 46 ]. It may sound preposterous and futuristic—but it
is already becoming part of how healthcare, education, marketing, and even
public safety are addressing specific issues. Each of these settings provides
a slightly different instructive case—demonstrating not only what Emotion
AI can perform, but also the ethical and social issues each antenna creates.

**Case Study 1—Healthcare: Emotional Analytics for Mental-Health
Support**
Mental-health care relies heavily on emotion; therefore, Emotion AI has a
natural place in this area. Traditional therapy is often built on what patients
verbally share, and what clinicians see or observe about client emotion in a
60-min session [ 47 ]. Understandably, there is only so much observation can
reveal about someone's emotional experience. A lot of people struggle with
openly demonstrating their emotions. Newer generation Emotion AI
systems are bridging that gap between observations and the emotional
experience [ 48 ].


Emotion AI systems utilize computer-vision models, combined with
voice analysis and physiological data monitoring to create a more
continuous understanding of a patient's mood. A prominent illustration is
Ellie [ 49 ]; a digital therapist developed at the University of Southern
California. Ellie can conduct a conversation, while simultaneously tracking
micro-expressions, eye movements and tone. In testing, members of the
class talked much more freely to Ellie than to a human interviewer, which
may have been because participants may not have felt judged by Ellie. This
“openness” allowed clinicians to obtain data that would not typically be
available behind social restraint.
In addition to the availability of virtual therapy platforms, wearable
sensors can assess a number of physiological measures such as heart-rate
variability or skin conductance, and transmit these data to algorithms
designed to predict stress or anxiety [ 50 ]. If a patient is elevated over
multiple days in a row, the system can notify a clinician, leading to early
intervention. It would change how depression or PTSD and other mental
health problems are tracked between appointments [ 51 ].
Not ignoring that the same characteristics that make these tools
powerful can also make them sensitive. Emotional data points expose very
personal parts of a person's life and mishandling them would destroy trust.
Thus, developers must comply with applicable regulations, such as HIPAA
or European GDPR [ 51 ], securely care of and properly storing that data,
and explain how predictions come to their conclusions. Another underlying
dilemma presents itself in cultural context. For example, a smile may
communicate a person is simply being polite and does not signal happiness.
Similarly, tears may represent frustration or joy dependent upon culture
[ 52 ]. For emotion recognition to be ethically used in healthcare, it must be
trained on large, diverse datasets and need to be interpreted by professionals
who understand the nuances of people.

**Case Study 2—Education: Emotion-Aware Learning Environments**
Teachers and learners experience learning, teaching is an emotional
experience [ 53 ]. A student’s motivation, curiosity, or frustration typically
determines how well they learn. Emotion AI can now provide educators
with tools to detect these emotional cues to not only support learning, but to
help make the classroom climate less stressful, especially in digital
classrooms where full face-to-face contact is limited [ 52 , 54 ].


Researchers at MIT created an Affective Tutoring System that uses
webcams and physiological sensors to infer the learner’s state of confusion,
boredom, or engagement [ 55 ]. When the Affective Tutoring System infers
confusion, it pauses to reintroduce new material or provide added hints; if it
infers boredom, it could go into a mini-game or quiz to reengage. Teachers
in face-to-face classrooms would make these same shifts instinctively, but
online adjusted classrooms have not until now.
Other educational technology companies use sentiment analysis of
student voices during an oral exam or group discussion [ 56 ]. If a student’s
tone indicated hesitation or anxiety, it could either allow for sense feedback
or a calming exercise to ground the student before continuing. For students
diagnosed with autism or other neurodivergent dispositions, it could help in
understanding their own emotional cues.
Although it has great potential, monitoring emotions in classrooms
creates ethical dilemmas [ 57 ]. Even monitoring students continuously
would feel less than ideal, and not to mention, students may change their
behavior just because they are being monitored. Also, not everyone will
make the same interpretations as a result of the same facial/vocal patterns.
For instance, a child who is thinking might appear “disengaged” to an
algorithm that is looking for eye contact. Educators should always treat the
outputs of Emotion AI as indicators and refrain from making absolute
conclusions, and always apply human empathy into the equation of teaching
[ 58 ].

**Case Study 3—Business and Marketing: Understanding the
Customer’s Heartbeat**
In business, emotions often drive decisions, more than logic does.
Businesses have always been capturing customers’ feedback by sending
surveys, or conducting focus groups to capture customer satisfaction, but
those capturings depend on what the customers want to say or can say [ 59 ].
Whereas Emotion AI offered something ultimately new insight based on
authentic, in-the-moment, responses.
One case in point is Coca-Cola [ 59 ]; which, used facial-expression-
analysis software to test how advertisements impacted viewers. The
cameras quickly scanned the audience, capturing their spontaneous facial
expressions of smiles or frowns while viewing a Coca-Cola advertisement.
The researchers discovered which scenes generated authentic amusement


and which scenes generate boredom and modified their advertising
accordingly [ 60 ]. Other retailers use similar systems to gauge an in-store
shoppers facial expression as they browse products or respond to virtual
assistance system.
Emotion AI can also function within customer service agents. For
instance, voice-analysis system can aspire to detect irritation or relief from
customers during support calls [ 61 ]. The technology may allow chatbots to
adjust their tone or transfer an upset caller to a live agent. Some companies
use this technology as an internal mechanism and apply emotion analytics
to group meetings and conversations to track collective morale; however,
they create their own potential privacy risks.
Emotion AI can also function within customer service agents. For
instance, voice-analysis system can aspire to detect irritation or relief from
customers during support calls [ 62 ]. The technology may allow chatbots to
adjust their tone or transfer an upset caller to a live agent. Some companies
use this technology as an internal mechanism and apply emotion analytics
to group meetings and conversations to track collective morale; however,
they create their own potential privacy risks.

**Case Study 4—Public Safety: Between Security and Civil Liberties**
The most contentious use of Emotion AI appears to be in the area of public
safety. Policeman, and public safety employee has always had a vision of
systems that can read emotional readings from crowds or group events to
identify potential threats [ 63 ]. In this way cameras could identify nervous or
aggressive personalities from an airport queue, and alert security systems.
In theory, technology of that nature could not only prevent violence, but
also reduce risks in times of crisis working effectively. Crowd-emotion
monitoring, for example, could be employed to measure growing panic
during a stampede or generate a corresponding reaction with protests or
demonstrations [ 64 ]. But such possible uses are marred with ethical
problems in real life, since emotions tend to be indeterminate; merely
witnessing somebody with anxiousness doesn't necessarily show guilt, nor
does a person displaying anger imply a propensity to provoke violence.
Also, in public places, citizens may not have the ability or opportunity
to provide informed consent for emotional surveillance [ 65 ]. A constant
analysis of facial cuts may result in individuals feeling as though they are
being surveilled not because of their actions, but because of their manners,


which could lead to tensions a topic counter to democratic freedoms. To
safeguard against atrocious use, governments must exercise control by
using strict regulation towards governments emotional data algorithms,
independent technologies and processes for oversight, and available
practices that outline when and how emotional data could be collected or
stored [ 66 ]. In the absence of this regulation, the cost of the perceived
safety and the daily experience of a citizen may result in negatively altering
the perception of privacy.

**Connecting the Case Studies: Common Threads and Cautions**
When combined, these case studies show both the scope and complexity of
the use of Emotion AI. As a reminder, in healthcare, it allows doctors into
their patient's experience. In educational settings, it fosters tailored learning
environments for success. In business eats it creates insights into their
customer and use [ 67 ]. In public safety, it creates an early warning for the
dangers of the environment [ 68 ]. Yet all four speak to the same truth, which
is that technology sophisticated does not guarantee positive conversion on
social engagement.
The successful uptake of Emotion AI rests on transdisciplinary
collaboration. Computer scientists can advance accurate models and
algorithms, but they require psychologists to decode the signals of
emotional expression and affect; ethicists to asses whether the outputs are
fair; and policy experts and practitioners to constrain the relationships and
the responsible actions of machines in a constructed national order [ 69 ].
Likewise, inclusive and culturally aware training data, transparent models
and algorithm, and HITL systems can act as safeguards contained in
algorithmic systems.
Similarly, the notion of explainability is present in all four cases. If AI
determines to you that someone is anxious or happy, the individual
receiving the information, and the observer, should have an understanding
of how the AI knew that. It is the only way to build trust in an explanation,
and to allow a person to dispute it or challenge it. Indeterminate machines,
even with exceedingly high accuracy, can be rejected when they don't
explain their rationale.
Emotion AI is a remarkable intersection of technology and the human
experience, demonstrating machines can be designed to not only compute,
but sense, listen, and respond with a degree of authentic, emotional


intelligence [ 29 , 70 ]. The applications are systematically organized and
quite expansive, from surveillance that detects early warning signs of
depression or increase engagement in the classroom or areas like customer
sentiment. Nevertheless, every case after case serves as a reminder of the
deeply personal, subjective nature, and cultural layers of emotional
experience.
If Society want to promote ethical progression, they needs to treat
Emotion AI as a collective human endeavor rather than a strictly technical
one. The most valuable systems will be accurate but transparent, they will
be powerful but more attentive towards privacy considerations, and
innovative but with respect to ethical constraints [ 71 ]. Only with these
considerations in mind can Emotion AI improve human experience by
allowing machines to understand us without narrowing the very humanity
we value.

##### 6 Future Directions for Emotion AI

The Emotion AI should still focus on making use of ethical, human-
centered, and long-term strategies even while contributing to maintain
technological advancement. The realization of emotion recognitions from
facial, vocal, physiological, or textual signal to improve robustness and
validity given changing real-world contexts necessitated technical
advancements in context-aware and multi-modality emotion recognition
techniques. The transition to real-time usage would be made simpler as well
as the negative effects on the environment would be reduced if the
frameworks and model structure were computationally light and energy
efficient. To provide end users trust and transparency it offers some
interpretation and insight into model predictions and boundaries, emotion
recognition systems must incorporate explainable AI methods. The
establishment of guidelines and policies regarding data privacy, bias, and
accountability will also assist in ensuring that it is developed and
implemented appropriately, that will reduce misuse and biasness. Ethical
and regulatory avenues are equally essential. By incorporating aspects of
psychology, neuroscience, human–computer interaction, the social sciences,
and policy studies to create systems that are not only technically feasible
but also socially justifiable, cross-disciplinary collaboration will continue to
be essential to these future endeavours. Next-generation emotion AI


solutions will be human-centered and responsible in their upcoming
innovations, but they will also socially influence sustainability toward
solutions for enhanced wellbeing, engagement, and human–machine
teaming in most areas of life.

##### 7 Conclusion

Emotion AI combines a case study for the fusion of technical innovation
and human-centered design. It succeeds not only based on algorithms
sophistication but also on attention to the psychological theory, moral
standards, HCI principles, and sustainability factors throughout
development. This dedicated chapter has contended with significant
tensions, provided examples that indicates challenges from the real world,
and hinted at future directions in the form of ethical, inclusive, and effective
advice for Emotion AI deployment. Grounding a multidisciplinary
perspective, can bring richness to human–machine interaction, aid in the
promotion of mental health, customize education, and offer social benefits
in future applications without nonetheless compromising on ethics and
social deficits related to AI technology.

##### References

1. Soleymani, M., Garcia, D., Jou, B., Schuller, B., Chang, S.F., Pantic, M.: A survey of multimodal
    sentiment analysis. Image Vis. Comput. **65** , 3–14 (2017). https:// doi. org/ 10. 1016/ J. IMAVIS.
    2017. 08. 003
    [Crossref]
2.
    El Ayadi, M., Kamel, M.S., Karray, F.: Survey on speech emotion recognition: features,
    classification schemes, and databases. Pattern Recognit. **44** (3), 572–587 (2011). https:// doi. org/
    10. 1016/ j. patcog. 2010. 09. 020
    [Crossref]
3.
    Shan, C., Gong, S., McOwan, P.W.: Facial expression recognition based on local binary patterns:
    a comprehensive study. Image Vis. Comput. **27** (6), 803–816 (2009). https:// doi. org/ 10. 1016/ j.
    imavis. 2008. 08. 005
    [Crossref]
4.
    García-Higuera, J.A., Crivelli, C., Fernández-Dols, J.M.: Facial expressions during an extremely
    intense emotional situation: toreros’ lip funnel. Soc. Sci. Inf. **54** (4), 439–454 (2015). https:// doi.
    org/ 10. 1177/ 0539018415596381
    [Crossref]


5.
Giachanou, A., Rosso, P., Crestani, F.: Leveraging emotional signals for credibility detection. In:
SIGIR 2019—Proceedings 42nd International ACM SIGIR Conference on Research and
Development Information Retrieved, pp. 877–880 (2019). https:// doi. org/ 10. 1145/ 3331184.
3331285
6.
McStay, A., Urquhart, L.: This time with feeling? Assessing EU data governance implications of
out of home appraisal based emotional AI. First Monday **24** (10), 1–1 (2019). https:// doi. org/ 10.
5210/ FM. V24I10. 9457
[Crossref]
7.
Rathnayaka, P., et al.: Gated recurrent neural network approach for multilabel emotion detection
in microblogs (2019). [http://](http://) arxiv. org/ abs/ 1907. 07653. Last accessed 30 Oct 2025
8.
Singh, J., et al.: Real-time convolutional neural networks for emotion and gender classification.
Procedia Comput. Sci. **235** , 1429–1435 (2024). https:// doi. org/ 10. 1016/ j. procs. 2024. 04. 134
[Crossref]
9.
Banzon, A.M.E., Beever, J., Taub, M.: Facial expression recognition in classrooms: ethical
considerations and proposed guidelines for affect detection in educational settings. IEEE Trans.
Affect. Comput. **15** (1), 93–104 (2024). https:// doi. org/ 10. 1109/ TAFFC. 2023. 3275624
[Crossref]
10.
Khan, U.A., Xu, Q., Liu, Y., Lagstedt, A., Alamäki, A., Kauttonen, J.: Exploring contactless
techniques in multimodal emotion recognition: insights into diverse applications, challenges,
solutions, and prospects. Multimed. Syst. **30** (3) (2024). https:// doi. org/ 10. 1007/ S00530-024-
01302-2
11.
Poria, S., Cambria, E., Howard, N., Bin Huang, G., Hussain, A.: Fusing audio, visual and textual
clues for sentiment analysis from multimodal content. Neurocomputing **174** , 50–59 (2016).
https:// doi. org/ 10. 1016/ J. NEUCOM. 2015. 01. 095
12.
Kotian, A.L., Nandipi, R., Ushag, G.M., Usha Rani, S., Varshauk, U.K., Veena, G.T.: A
systematic review on human and computer interaction. In: 2nd International Conference on
Intelligent Data Communication Technologies and Internet Things, IDCIoT 2024, pp. 1214–1218
(2024). https:// doi. org/ 10. 1109/ IDCIOT59759. 2024. 10467622.
13.
Han, D., Park, H., Rhee, S.Y.: The role of regulatory focus and emotion recognition bias in cross-
cultural negotiation. Sustainable **13** (5), 1–20 (2021). https:// doi. org/ 10. 3390/ SU13052659
[Crossref]
14.
Ghotbi, N.: The Ethics of emotional artificial intelligence: a mixed method analysis. Asian
Bioeth. Rev. **15** (4), 417–430 (2023). https:// doi. org/ 10. 1007/ S41649-022-00237-Y
[Crossref]
15.
Lundqvist, L.O., Carlsson, F., Hilmersson, P., Juslin, P.N.: Emotional responses to music:
Experience, expression, and physiology. Psychol. Music **37** (1), 61–90 (2009). https:// doi. org/ 10.
1177/ 0305735607086048
[Crossref]
16.
Stark, L., Hoey, J.: The ethics of emotion in artificial intelligence systems. In: FAccT 2021—
Proceedings of the 2021 ACM Conference on Fairness, Accountability, Transparency, pp. 782–


793 (2021). https:// doi. org/ 10. 1145/ 3442188. 3445939
17.
Ethical considerations in emotion recognition technologies: a review of the literature|AI and
Ethics. https:// doi. org/ 10. 1007/ s43681-023-00307-3. Last accessed 30 Oct 2025
18.
Barrett, L.F., Adolphs, R., Marsella, S., Martinez, A.M., Pollak, S.D.: Emotional expressions
reconsidered: challenges to inferring emotion from human facial movements. Psychol. Sci.
Public Interes. **20** (1), 1–68 (2019). https:// doi. org/ 10. 1177/ 1529100619832930
[Crossref]
19.
Sánchez-Rada, J.F., Iglesias, C.A.: Onyx: a linked data approach to emotion representation. Inf.
Process. Manag. **52** (1), 99–114 (2016). https:// doi. org/ 10. 1016/ J. IPM. 2015. 03. 007
[Crossref]
20.
Fabiano, N.: Affective computing and emotional data: challenges and implications in privacy
regulations, the AI act, and ethics in large language models (2025). https:// arxiv. org/ pdf/ 2509.
20153 Last accessed 30 Oct 2025
21.
Dhall, A., Goecke, R., Lucey, S., Gedeon, T.: Collecting large, richly annotated facial-expression
databases from movies. IEEE Multimed. **19** (3), 34–41 (2012). https:// doi. org/ 10. 1109/ MMUL.

2012. 26
[Crossref]
22.
Fabiano, N.: Affective computing and emotional data: challenges and implications in privacy
regulations, the ai act, and ethics in large language models (2025). [http://](http://) arxiv. org/ abs/ 2509.
20153. Last accessed 30 Oct 2025
23.
Mantello, P., Ho, M.T., Nguyen, M.H., Vuong, Q.H.: Bosses without a heart: socio-demographic
and cross-cultural determinants of attitude toward emotional AI in the workplace. AI Soc. **38** (1),
97–119 (2023). https:// doi. org/ 10. 1007/ S00146-021-01290-1
[Crossref]
24.
Polo, E.M., Farabbi, A., Mollura, M., Mainardi, L., Barbieri, R.: Understanding the role of
emotion in decision making process: using machine learning to analyze physiological responses
to visual, auditory, and combined stimulation. Front. Hum. Neurosci. **17** (2023). https:// doi. org/
10. 3389/ FNHUM. 2023. 1286621
25.
Ebers, M., Sein, K.: Privacy, Data protection and data-driven technologies. Privacy, Data Prot.
Data-Driven Technol. 1–415 (2024). https:// doi. org/ 10. 4324/ 9781003502791
26.
Ahuja, K.: Emotion AI in healthcare: Application, challenges, and future directions. Emot. AI
Human-AI Interact. Soc. Netw. 131–146 (2024). https:// doi. org/ 10. 1016/ B978-0-443-19096-4.
00011-0
27.
Glenn, T., Monteith, S.: New measures of mental state and behavior based on data collected from
sensors, smartphones, and the internet. Curr. Psychiatry Rep. **16** (12) (2014). https:// doi. org/ 10.
1007/ S11920-014-0523-3
28.
Zhou, Z., Asghar, M.A., Nazir, D., Siddique, K., Shorfuzzaman, M., Mehmood, R.M.: An AI-
empowered affect recognition model for healthcare and emotional well-being using
physiological signals. Cluster Comput. **26** (2), 1253–1266 (2023). https:// doi. org/ 10. 1007/
S10586-022-03705-0


[Crossref]
29.
McStay, A.: Emotional AI, soft biometrics and the surveillance of emotional life: an unusual
consensus on privacy. Big Data Soc. **7** (1) (2020). https:// doi. org/ 10. 1177/ 2053951720904386
30.
Kim, E., Bryant, D., Srikanth, D., Howard, A.: Age bias in emotion detection: an analysis of
facial emotion recognition performance on young, middle-aged, and older adults. In: AIES 2021
—Proceedings 2021 AAAI/ACM Conference on AI, Ethics, and Social, pp. 638–644 (2021).
https:// doi. org/ 10. 1145/ 3461702. 3462609
31.
Breazeal, C.: Emotive qualities in lip-synchronized robot speech. Adv. Robot. **17** (2), 97–1 13
(2003). https:// doi. org/ 10. 1163/ 1568553033211650 79
[Crossref]
32.
Calvo, R.A., D’Mello, S.: Affect detection: an interdisciplinary review of models, methods, and
their applications. IEEE Trans. Affect. Comput. **1** (1), 18–37 (2010). https:// doi. org/ 10. 1109/ T-
AFFC. 2010. 1
[Crossref]
33.
Denecke, K., Gabarron, E.: The ethical aspects of integrating sentiment and emotion analysis in
chatbots for depression intervention. Front. Psychiatry **15** (2024). https:// doi. org/ 10. 3389/
FPSYT. 2024. 1462083
34.
Katirai, A.: Ethical considerations in emotion recognition technologies: a review of the literature.
AI Ethics **4** (4), 927–948 (2024). https:// doi. org/ 10. 1007/ S43681-023-00307-3/ METRICS
[Crossref]
35.
Wang, C., Wang, B., Xiang, W., Xu, M.: Encoding syntactic dependency and topical information
for social emotion classification. In: SIGIR 2019—Proceedings 42nd International ACM SIGIR
Conference Research and Development in Information Retrieval, pp. 881–884 (2019). https://
doi. org/ 10. 1145/ 3331184. 3331287
36.
Niculescu, A., van Dijk, B., Nijholt, A., Li, H., See, S.L.: Making social robots more attractive:
the effects of voice pitch, humor and empathy. Int. J. Soc. Robot. **5** (2), 171–191 (2013). https://
doi. org/ 10. 1007/ S12369-012-0171-X
[Crossref]
37.
Ullah, R., Amblee, N., Kim, W., Lee, H.: From valence to emotions: exploring the distribution of
emotions in online product reviews. Decis. Support. Syst. **81** , 41–53 (2016). https:// doi. org/ 10.
1016/ j. dss. 2015. 10. 007
[Crossref]
38.
Tretter, M.: Equipping AI-decision-support-systems with emotional capabilities? Ethical
perspectives. Front. Artif. Intell. **7** (2024). https:// doi. org/ 10. 3389/ FRAI. 2024. 1398395/ FULL
39.
Tan, Z., Zhang, T.: Emotion-semantic interaction network for fake news detection: perspectives
on question and non-question comment semantics. Inf. Process. Manag. **63** (2) (2026). https:// doi.
org/ 10. 1016/ J. IPM. 2025. 104391
40.
Juslin, P.N., Liljeström, S., Västfjäll, D., Barradas, G., Silva, A.: An experience sampling study
of emotional reactions to music: listener, music, and situation. Emotion **8** (5), 668–683 (2008).
https:// doi. org/ 10. 1037/ A0013505


[Crossref]
41.
Rambocas, M., Pacheco, B.G.: Online sentiment analysis in marketing research: a review. J. Res.
Interact. Mark. **12** (2), 146–163 (2018). https:// doi. org/ 10. 1108/ JRIM-05-2017-0030
[Crossref]
42.
Drzyzga, G.: Incorporating artificial intelligence into design criteria considerations. Lect. Notes
Comput. Sci. (including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics) vol. 14735
LNAI, pp. 133–151 (2024). https:// doi. org/ 10. 1007/ 978-3-031-60611-3_ 10
43.
Balahur, A., Hermida, J.M., Montoyo, A.: Building and exploiting EmotiNet, a knowledge base
for emotion detection based on the appraisal theory model. IEEE Trans. Affect. Comput. **3** (1),
88–101 (2012). https:// doi. org/ 10. 1109/ T-AFFC. 2011. 33
[Crossref]
44.
Umbrello, S., Natale, S.: Reframing deception for human-centered AI. Int. J. Soc. Robot. **16** (11),
2223–2241 (2024). https:// doi. org/ 10. 1007/ S12369-024-01184-4
[Crossref]
45.
Gremsl, T., Hödl, E.: Emotional AI: legal and ethical challenges. Inf. Polity **27** (2), 163–174
(2022). https:// doi. org/ 10. 3233/ IP-211529
[Crossref]
46.
Barker, D., Tippireddy, M.K.R., Farhan, A., Ahmed, B.: Ethical considerations in emotion
recognition research. Psychol. Int. **7** (2), 43 (2025). https:// doi. org/ 10. 3390/
PSYCHOLINT702004 3
47.
Pavlopoulos, A., Rachiotis, T., Maglogiannis, I.: An overview of tools and technologies for
anxiety and depression management using AI. Appl. Sci. **14** (19) (2024). https:// doi. org/ 10. 3390/
APP14199068
48.
Ekman, P.: Facial expressions of emotion: new findings, new questions. Psychol. Sci. **3** (1), 34–
38 (1992). https:// doi. org/ 10. 1111/ J. 1467-9280. 1992. TB00253. X
[MathSciNet][Crossref]
49.
Castañeda-Garza, G., Ceballos, H.G., Mejía-Almada, P.G.: Artificial intelligence for mental
health: a review of AI solutions and their future. What AI can do strengths limitations Artificial
Intelligence pp. 373–399 (2023). https:// doi. org/ 10. 1201/ B23345-22
50.
Freeman, D., et al.: Randomised controlled trial of automated VR therapy to improve positive
self-beliefs and psychological well-being in young people diagnosed with psychosis: a study
protocol for the Phoenix VR self-confidence therapy trial. BMJ Open **13** (12) (2023). https:// doi.
org/ 10. 1136/ BMJOPEN-2023-076559
51.
Ma, J.: The causal effect of Internet use on rural middle-aged and older adults’ depression: a
propensity score matching analysis. Digit. Heal. **11** (2025). https:// doi. org/ 10. 1177/
2055207624131004 1
52.
Cassinis, R., Morelli, L.M., Nissan, E.: Emulation of human feelings and behaviors in an
animated artwork. Int. J. Artif. Intell. Tools **16** (2), 291–375 (2007). https:// doi. org/ 10. 1142/
S021821300700333 3
[Crossref]


53.
Luan, H., et al.: Challenges and future directions of big data and artificial intelligence in
education. Front. Psychol. **11** (2020). https:// doi. org/ 10. 3389/ FPSYG. 2020. 580820
54.
Chen, C.H., Su, C.Y., Ying Lo, F.: Experiencing historical events through a VR environment to
enhance learning achievement, interest, motivation, and emotional reflections. Interact. Learn.
Environ. (2025). https:// doi. org/ 10. 1080/ 10494820. 2025. 2535683.
55.
Myers, M.H.: Automatic detection of a student’s affective states for intelligent teaching systems.
Brain Sci. **11** (3), 1–15 (2021). https:// doi. org/ 10. 3390/ BRAINSCI1 1030331
[Crossref]
56.
Abudalfa, S., Ahmed, M.: Survey on target dependent sentiment analysis of micro-blogs in social
media. In: 2017 9th IEEE-GCC Conference Exhib. GCCCE 2017 (2018). https:// doi. org/ 10.
1109/ IEEEGCC. 2017. 8448158
57.
(PDF) Student Sentiment Analysis and Classroom Feedback Prediction Using Deep Learning.
https:// [http://www.](http://www.) researchgate. net/ publication/ 384831781_ Student_ Sentiment_ Analysis_ and_
Classroom_ Feedback_ Prediction_ Using_ Deep_ Learning. Last accessed 30 Oct 2025
58.
Wu, R., Yu, Z.: Do AI chatbots improve students learning outcomes? Evidence from a meta-
analysis. Br. J. Educ. Technol. **55** (1), 10–33 (2024). https:// doi. org/ 10. 1111/ BJET. 13334
[MathSciNet][Crossref]
59.
Kraus, M., Feuerriegel, S., Oztekin, A.: Deep learning in business analytics and operations
research: models, applications and managerial implications. Eur. J. Oper. Res. **281** (3), 628–641
(2020). https:// doi. org/ 10. 1016/ j. ejor. 2019. 09. 018
[Crossref]
60.
Coca-Cola.: Unilever use facial coding to measure ad effectiveness | Biometric Update. https://
[http://www.](http://www.) biometricupdate. com/ 201301/ coca-cola-unilever-use-facial-coding-to-measure-ad-
effectiveness. Last accessed 30 Oct 2025
61.
Vazquez Gonzalez, C., Neate, T., Borgo, R.: Trusting tracking: perceptions of non-verbal
communication tracking in videoconferencing. Conf. Hum. Factors Comput. Syst.—Proc.
(2025). https:// doi. org/ 10. 1145/ 3706598. 3714306
62.
Poria, S., Majumder, N., Mihalcea, R., Hovy, E.: Emotion recognition in conversation: research
challenges, datasets, and recent advances. IEEE Access **7** , 100943–100953 (2019). https:// doi.
org/ 10. 1109/ ACCESS. 2019. 2929050
[Crossref]
63.
Mohammad, S.M., Zhu, X., Kiritchenko, S., Martin, J.: Sentiment, emotion, purpose, and style in
electoral tweets. Inf. Process. Manag. **51** (4), 480–499 (2015). https:// doi. org/ 10. 1016/ j. ipm. 2014.

09. 003
[Crossref]
64.
Khosravi, M.R., Rezaee, K., Moghimi, M.K., Wan, S., Menon, V.G.: Crowd emotion prediction
for human-vehicle interaction through modified transfer learning and fuzzy logic ranking. IEEE
Trans. Intell. Transp. Syst. **24** (12), 15752–15761 (2023). https:// doi. org/ 10. 1109/ TITS. 2023.
3239114
[Crossref]


65.
Albi, G., Cristiani, E., Pareschi, L., Peri, D.: Mathematical models and methods for crowd
dynamics control. Model. Simul. Sci. Eng. Technol. pp. 159–197 (2020). https:// doi. org/ 10. 1007/
978-3-030-50450-2_ 8
66.
Thapa, C., Camtepe, S.: Precision health data: Requirements, challenges and existing techniques
for data security and privacy. Comput. Biol. Med. **129** (2021). https:// doi. org/ 10. 1016/ J.
COMPBIOMED. 2020. 104130
67.
Sheehan, B., Jin, H.S., Gottlieb, U.: Customer service chatbots: anthropomorphism and adoption.
J. Bus. Res. **115** , 14–24 (2020). https:// doi. org/ 10. 1016/ J. JBUSRES. 2020. 04. 030
[Crossref]
68.
Jane, O.O., Okongwu, C.C., Akomolafe, O.O., Anyanwu, E.C., Daraojimba, O.D.: Mental health
and digital technology: a public health review of current trends and responses. Int. Med. Sci.
Res. J. **4** (2), 108–125 (2024). https:// doi. org/ 10. 51594/ IMSRJ. V4I2. 754
69.
Lin, W., Li, C.: Review of studies on emotion recognition and judgment based on physiological
signals. Appl. Sci. **13** (4) (2023). https:// doi. org/ 10. 3390/ APP13042573
70.
Vistorte, A.O.R., Deroncele-Acosta, A., Ayala, J.L.M., Barrasa, A., López-Granero, C., Martí-
González, M.: Integrating artificial intelligence to assess emotions in learning environments: a
systematic literature review. Front. Psychol. **15** (2024). https:// doi. org/ 10. 3389/ FPSYG. 2024.
1387089
71.
Chatzakou, D., Vakali, A., Kafetsios, K.: Detecting variation of emotions in online activities.
Expert Syst. Appl. **89** , 318–332 (2017). https:// doi. org/ 10. 1016/ j. eswa. 2017. 07. 044
[Crossref]
_OceanofPDF.com_


