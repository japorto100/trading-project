Dynamic
Noncooperative
Game Theory
Second Edition
SIAM's Classics in Applied Mathematics series consists of books that were previously
allowed to go out of print. These books are republished by SIAM as a professional
service because they continue to be important resources for mathematical scientists.
Editor-in-Chief
Robert E. O'Malley, Jr., University of Washington
Editorial Board
Richard A. Brualdi, University of Wisconsin-Madison
Herbert B. Keller, California Institute of Technology
Andrzej Z. Manitius, George Mason University
Ingram Olkin, Stanford University
Stanley Richardson, University of Edinburgh
Ferdinand Verhulst, Mathematisch Instituut, University of Utrecht
Classics in Applied Mathematics
C. C. Lin and L. A. Segel, Mathematics Applied to Deterministic Problems in the
Natural Sciences
Johan G. F. Belinfante and Bernard Kolman, A Survey of Lie Groups and Lie Algebras
with Applications and Computational Methods
James M. Ortega, Numerical Analysis: A Second Course
Anthony V. Fiacco and Garth P. McCormick, Nonlinear Programming: Sequential
Unconstrained Minimisation Techniques
F. H. Clarke, Optimization and Nonsmooth Analysis
George F. Carrier and Carl E. Pearson, Ordinary Differential Equations
Leo Breiman, Probability
R. Bellman and G. M. Wing, An Introduction to Invariant Imbedding
Abraham Berman and Robert J. Plemmons, Nonnegative Matrices in the Mathemat-
ical Sciences
Olvi L. Mangasarian, Nonlinear Programming
*Carl Friedrich Gauss, Theory of the Combination of Observations Least Subject
to Errors: Part One, Part Two, Supplement. Translated by G. W. Stewart
Richard Bellman, Introduction to Matrix Analysis
U. M. Ascher, R. M. M. Mattheij, and R. D. Russell, Numerical Solution of Boundary
Value Problems for Ordinary Differential Equations
K. E. Brenan, S. L. Campbell, and L. R. Petzold, Numerical Solution of Initial-Value
Problems in Differential-Algebraic Equations
Charles L. Lawson and Richard J. Hanson, Solving Least Squares Problems
J. E. Dennis, Jr. and Robert B. Schnabel, Numerical Methods for Unconstrained
Optimization and Nonlinear Equations
Richard E. Barlow and Frank Proschan, Mathematical Theory of Reliability
Cornelius Lanczos, Linear Differential Operators
Richard Bellman, Introduction to Matrix Analysis, Second Edition
Beresford N. Parlett, The Symmetric Eigenvalue Problem
*First time in print.
ii
```
Classics in Applied Mathematics (continued)
```
Richard Haberman, Mathematical Models: Mechanical Vibrations, Population
Dynamics, and Traffic Flow
Peter W. M. John, Statistical Design and Analysis of Experiments
Tamer Ba§ar and Geert Jan Olsder, Dynamic Noncooperative Game Theory, Second
Edition
Emanuel Parzen, Stochastic Processes
Petar Kokotovic, Hassan K. Khalil, and John O'Reilly, Singular Perturbation Methods
in Control: Analysis and Design
Jean Dickinson Gibbons, Ingram Olkin, and Milton Sobel, Selecting and Ordering
```
Populations: A New Statistical Methodology
```
James A. Murdock, Perturbations: Theory and Methods
Ivar Ekeland and Roger Temam, Convex Analysis and Variational Problems
Ivar Stakgold, Boundary Value Problems of Mathematical Physics, Volumes I and II
J. M. Ortega and W. C. Rheinboldt, Iterative Solution of Nonlinear Equations in
Several Variables
David Kinderlehrer and Guido Stampacchia, An Introduction to Variational
Inequalities and Their Applications
F. Natterer, The Mathematics of Computerized Tomography
Avinash C. Kak and Malcolm Slaney, Principles of Computerized Tomographic Imaging
R. Wong, Asymptotic Approximations of Integrals
O. Axelsson and V. A. Barker, Finite Element Solution of Boundary Value Problems:
Theory and Computation
David R. Brillinger, Time Series: Data Analysis and Theory
Joel N. Franklin, Methods of Mathematical Economics: Linear and Nonlinear
Programming, Fixed-Point Theorems
Philip Hartman, Ordinary Differential Equations, Second Edition
Michael D. Intriligator, Mathematical Optimization and Economic Theory
Philippe G. Ciarlet, The Finite Element Method for Elliptic Problems
Jane K. Cullum and Ralph A. Willoughby, Lanczos Algorithms for Large Symmetric
Eigenvalue Computations, Vol. I: Theory
M. Vidyasagar, Nonlinear Systems Analysis, Second Edition
Robert Mattheij and Jaap Molenaar, Ordinary Differential Equations in Theory and
Practice
Shanti S. Gupta and S. Panchapakesan, Multiple Decision Procedures: Theory and
Methodology of Selecting and Ranking Populations
Eugene L. Allgower and Kurt Georg, Introduction to Numerical Continuation Methods
Heinz-Otto Kreiss and Jens Lorenz, Initial-Boundary Value Problems and the Navier-
Stokes Equations
J. L. Hodges, Jr. and E. L. Lehmann, Basic Concepts of Probability and Statistics,
Second Edition
iii
To
```
Tangul, Gozen, and Elif (T.B.)
```
and
```
Elke, Rena, Theda, and Kike (G.J.O.)
```
Tamer Basar
University of Illinois at Urbana-Champaign
Urbana, Illinois
Geert Jan Olsder
Delft University of Tecbnology
Delft, The Netherlands
Society for Industrial and Applied Mathematics
Philadelphia
Dynamic
Noncooperative
Game Theory
Second Edition
siam
Copyright ©1999 by the Society for Industrial and Applied Mathematics.
This SIAM edition is an unabridged, revised republication of the work first published by
Academic Press, New York, in 1982 and revised in 1995.
1 0 9 8 7 6 5 4 3
All rights reserved. Printed in the United States of America. No part of this book may be
reproduced, stored, or transmitted in any manner without the written permission of the
publisher. For information, write to the Society for Industrial and Applied Mathematics,
3600 University City Science Center, Philadelphia, PA 19104-2688.
Library of Congress Cataloging-in-Publication Data
Basar, Tamer.
Dynamic noncooperative game theory / Tamer Basar, Geert Jan
Olsder. •• 2nd ed.
p. cm. •• (Classics in applied mathematics ; 23)
Revised from the 2nd ed. published in 1995 by Academic Press, New
York.
Includes bibliographical references and index.
```
ISBN 0-89871429-X (softcover)
```
1. Differential games. 2. Noncooperative games (Mathematics)
I. Olsder, Geert Jan. II. Title. III. Series.
QA272.B37 1998
519.3–dc21 98-46719
siam is a registered trademark.
Contents
Preface to the Classics Edition xi
Preface to the Second Edition xiii
1 Introduction and Motivation 1
1.1 Preliminary Remarks 1
1.2 Preview on Noncooperative Games 3
1.3 Outline of the Book 12
1.4 Conventions, Notation and Terminology 13
Part I
2 Noncooperative Finite Games: Two-Person Zero-Sum 17
2.1 Introduction 17
2.2 Matrix Games 18
2.3 Computation of Mixed Equilibrium Strategies 29
2.4 Extensive Forms: Single-Act Games 36
2.5 Extensive Games: Multi-Act Games 45
2.6 Zero-Sum Games with Chance Moves 57
2.7 Two Extensions 60
2.7.1 Games with repeated decisions 61
2.7.2 Extensive forms with cycles 63
2.8 Action-Dependent Information Sets 65
2.8.1 Duels 66
2.8.2 A searchlight game 66
2.9 Problems 70
2.10 Notes 75
3 Noncooperative Finite Games: N-Person Nonzero-Sum 77
3.1 Introduction 77
3.2 Bimatrix Games 78
3.3 N-Person Games in Normal Form 88
3.4 Computation of Mixed-Strategy Nash Equilibria in Bimatrix Games 95
3.5 Nash Equilibria of N-Person Games in Extensive Form 97
vii
viii CONTENTS
3.5.1 Single-act games: Pure-strategy Nash equilibria 100
3.5.2 Single-act games: Nash equilibria in behavioral and mixed
strategies 116
3.5.3 Multi-act games: Pure-strategy Nash equilibria 118
3.5.4 Multi-act games: Behavioral and mixed equilibrium
strategies 126
3.5.5 Other refinements on Nash equilibria 128
3.6 The Stackelberg Equilibrium Solution 131
3.7 Nonzero-Sum Games with Chance Moves 148
3.8 Problems 153
3.9 Notes 159
4 Static Noncooperative Infinite Games 161
4.1 Introduction 161
4.2 e Equilibrium Solutions 162
4.3 Continuous-Kernel Games: Reaction Curves, and Existence and
Uniqueness of Nash and Saddle-Point Equilibria 168
4.4 Stackelberg Solution of Continuous-Kernel Games 179
4.5 Consistent Conjectural Variations Equilibrium 186
4.6 Quadratic Games with Applications in Microeconomics 190
4.7 Braess Paradox 203
4.8 Problems 205
4.9 Notes 210
Part II
5 General Formulation of Infinite Dynamic Games 215
5.1 Introduction 215
5.2 Discrete-Time Infinite Dynamic Games 216
5.3 Continuous-Time Infinite Dynamic Games 224
5.4 Mixed and Behavioral Strategies in Infinite Dynamic Games . . 230
5.5 Tools for One-Person Optimization 233
5.5.1 Dynamic programming for discrete-time systems 233
5.5.2 Dynamic programming for continuous-time systems . . . 236
5.5.3 The minimum principle 241
5.6 Representations of Strategies Along Trajectories, and Time Con-
sistency of Optimal Policies 247
5.7 Viscosity Solutions 255
5.8 Problems 260
5.9 Notes 262
6 Nash and Saddle-Point Equilibria of Infinite Dynamic Games 265
6.1 Introduction 265
6.2 Open-Loop and Feedback Nash and Saddle-Point Equilibria for
Dynamic Games in Discrete Time 266
CONTENTS ix
6.2.1 Open-loop Nash equilibria 267
6.2.2 Closed-loop no-memory and feedback Nash equilibria . . . 276
6.2.3 Linear-quadratic games with an infinite number of stages 288
6.3 Informational Properties of Nash Equilibria in Discrete-Time
Dynamic Games 292
6.3.1 A three-person dynamic game illustrating informational
nonuniqueness 292
6.3.2 General results on informationally nonunique equilibrium
solutions 296
6.4 Stochastic Nonzero-Sum Games with Deterministic Information
Patterns 303
6.5 Open-Loop and Feedback Nash and Saddle-Point Equilibria of
Differential Games 310
6.5.1 Open-loop Nash equilibria 310
6.5.2 Closed-loop no-memory and feedback Nash equilibria . . . 320
6.5.3 Linear-quadratic differential games on an infinite time
horizon 333
6.6 Applications in Robust Controller Designs: H°°-Optimal Control 342
6.7 Stochastic Differential Games with Deterministic Information Pat-
terns 350
6.8 Problems 355
6.9 Notes 361
7 Stackelberg Equilibria of Infinite Dynamic Games 365
7.1 Introduction 365
7.2 Open-Loop Stackelberg Solution of Two-Person Dynamic Games
in Discrete Time 366
7.3 Feedback Stackelberg Solution Under CLPS Information Pattern 373
```
7.4 (Global) Stackelberg Solution Under CLPS Information Pattern 376
```
```
7.4.1 An illustrative example (Example 7.1) 376
```
```
7.4.2 A second example (Example 7.2): Follower acts twice in
```
the game 382
7.4.3 Linear Stackelberg solution of linear-quadratic dynamic
games 385
```
7.4.4 Incentives (deterministic) 392
```
7.5 Stochastic Dynamic Games with Deterministic Information Pat-
terns 396
```
7.5.1 (Global) Stackelberg solution 396
```
7.5.2 Feedback Stackelberg solution 402
7.5.3 Stochastic incentive problems 403
7.6 Stackelberg Solution of Differential Games 407
7.6.1 The open-loop information structure 407
7.6.2 The CLPS information pattern 412
7.7 Problems 418
7.8 Notes 421
x CONTENTS
8 Pursuit-Evasion Games 423
8.1 Introduction 423
8.2 Necessary and Sufficient Conditions for Saddle-Point Equilibria . 424
8.2.1 The Isaacs equation 425
8.2.2 Upper and lower values, and viscosity solutions 432
8.3 Capturability 434
8.4 Singular Surfaces 442
8.5 Solution of a Pursuit-Evasion Game: The Lady in the Lake . . . 448
8.6 An Application in Maritime Collision Avoidance 451
8.7 Role Determination and an Application in Aeronautics 456
8.8 Problems 464
8.9 Notes 467
Appendix A Mathematical Review 471
A.1 Sets 471
```
A.2 Normed Linear (Vector) Spaces 472
```
A.3 Matrices 473
A.4 Convex Sets andFunctionals 473
A.5 Optimization of Functionals 474
Appendix B Some Notions of Probability Theory 477
B.1 Ingredients of Probability Theory 477
B.2 Random Vectors 478
B.3 Integrals and Expectation 480
B.4 Norms and the Cauchy-Schwarz Inequality 481
Appendix C Fixed Point Theorems 483
Bibliography 485
List of Corollaries, Definitions, Examples, Lemmas, Propositions,
Remarks and Theorems 507
Index 515
Preface to the Classics
Edition
This is the revised second edition of our 1982 book with the same title, which
presents a rather comprehensive treatment of static and dynamic noncooperative
```
game theory, with emphasis placed (as in the first and second editions) on the
```
interplay between dynamic information patterns and the structural properties
```
of several different types of equilibria. Whereas the second edition (1995) was
```
a major revision with respect to the original edition, this Classics edition only
contains some moderate changes with respect to the second one. There has been
a number of reasons for the preparation of this edition:
The second edition was sold out surprisingly fast.
After some fifty years from its creation, the field of game theory is still very
alive and active, as also reinforced by the selection of three game theorists
```
(John Harsanyi, John Nash and Reinhard Selten) to share the 1994 Nobel
```
prize in economics. Quite a few books on game theory have been published
```
during the last ten years or so (though most of them essentially deal with
```
```
static games only and are at the undergraduate level).
```
The recent interest in such fields as biological games, mathematical finance
and robust control gives a new impetus to noncooperative game theory.
The topic of dynamic games has found its way into the curricula of many
universities, sometimes as a natural supplement to a graduate level course
on optimal control theory, which is actively taught in many engineering,
applied mathematics and economics graduate programs.
At the level of coverage of this book, dynamic game theory is well estab-
lished by now and has reached a level of maturity, which makes the book
a timely addition to SIAM's prestigious Classics in Applied Mathematics
series.
For a brief description of and the level of the contents of the book, the
reader is referred to the Preface of the second edition, which follows. It suffices
to mention here the major changes made in this Classics edition with respect
to the second edition. They are:
XI
xii PREFACE TO THE CLASSICS EDITION
Inclusion of the "Braess paradox" in Chapter 4.
Inclusion of new material on the relationship between the existence of
solutions to Riccati equations on the one hand and the existence of Nash
equilibrium solutions to linear-quadratic differential games on the other,
in Chapter 6. In the same chapter some new results have been included
also on infinite-horizon differential games.
We hope that this revised edition may find its way as its predecessors did.
Tamer Ba§ar Geert Jan Olsder
Urbana, June 1998 Delft, June 1998
Preface to the Second
Edition
This is the second edition of our 1982 book with the same title, which presents an
extensive and updated treatment of static and dynamic noncooperative game
```
theory, with emphasis placed again (as in the first edition) on the interplay
```
between dynamic information patterns and the structural properties of several
different types of equilibria. There were essentially two reasons for producing
this revised edition. One was the favorable reception of the first edition and the
other one was the need to include new theoretical developments. Yet another
reason was that the topic of dynamic games has found its way into the curricula
of many universities. This new edition contains some substantial changes and
```
additions (and also a few deletions), but the flavor and the theme of the original
```
text remain intact.
The first part of the book, part I, which comprises Chapters 2 to 4, covers
the material that is generally taught in an advanced undergraduate or first-year
graduate course on noncooperative game theory. The coverage includes static
finite and infinite games of both the zero-sum and nonzero-sum type and in the
latter case both Nash and Stackelberg solution concepts are discussed. Further-
more, this part includes an extensive treatment of the class of dynamic games
in which the strategy spaces are finite—the so-called multi-act games. Through
an extensive tree formulation, the impact of information patterns on the exis-
tence, uniqueness and the nature of different types of noncooperative equilibria
of multi-act games is thoroughly investigated. Most of the important concepts of
static and dynamic game theory are introduced in these three chapters, and they
are supplemented by several illustrative examples. Exposition of the material
is quite novel, emphasizing concepts and techniques of multi-person decision
making rather than mathematical details. However, mathematical proofs of
most of the results are also provided, but without hindering the flow of main
ideas. The major changes in this part over the first edition are the inclusion of
additional material on: randomized strategies, finite games with repeated de-
```
cisions and action-dependent information sets (Chapter 2); various refinements
```
on the Nash equilibrium concept, such as trembling-hand, proper and perfect
```
equilibria (Chapter 3); and in the context of static infinite games, stability of
```
Nash equilibria and its relation with numerical schemes, consistent conjectural
xiii
xiv PREFACE TO THE SECOND EDITION
variations equilibrium, and some new theorems on existence of Nash equilibria
```
(Chapter 4). Some specific types of zero-sum games on the square (Chapter 4)
```
have been left out.
The second part of the book, Part II, which includes Chapters 5 to 8, ex-
tends the theory of the first part to infinite dynamic games in both discrete
```
and continuous time (the so-known differential games). Here the emphasis is
```
again on the close interrelation between information patterns and noncooper-
ative equilibria of such multi-person dynamic decision problems. We present
a unified treatment of the existing, but scattered, results in the literature on
infinite dynamic games as well as some new results on the topic. The treatment
is confined to deterministic games and to stochastic games under perfect state
information, mainly because inclusion of a complete investigation on stochastic
dynamic games under imperfect state information would require presentation
of some new techniques and thereby a volume much larger than the present
one. Again, some of the major changes in this part over the first edition are
```
the inclusion of new material on: time consistency (Chapters 5-7); viscosity
```
```
solutions of the Hamilton-Jacobi-Bellman-Isaacs equation (Chapters 5 and 8);
```
affine-quadratic dynamic games and results on infinite-horizon games in dis-
```
crete and continuous time (Chapters 5 and 6); applications in robust (H°°)
```
```
controller designs (Chapter 8); incentive theory and relationship with Stackel-
```
```
berg solutions (Chapter 7); and Stackelberg equilibrium in the continuous time
```
```
(Chapter 7). The material on the dolichobrachistochrone which was in Chap-
```
ter 8 of the first edition, has been left out. Furthermore, the three appendices
```
(A-C) are expanded versions of the earlier ones, and present the necessary back-
```
ground material on vector spaces, matrix algebra, optimization, probability and
stochastic processes and fixed point theorems.
```
Each chapter (with the exception of the first) is supplemented with a problem
```
section. Each problem section contains standard exercises on the contents of
the chapter, which a reader who has carefully followed the text should have no
difficulty in solving, as well as exercises which can be solved by making use of
the techniques developed in the text, but which require some elaborate thinking
```
on the part of the reader. Following the problem section in each chapter (except
```
```
the first) is a notes section, which is devoted to historical remarks and sources
```
for further reading on the topics covered in that particular chapter.
A one-semester course on noncooperative game theory, taught using this
book, would involve mainly Part I and also an appropriate blend of some of
the topics covered in Part II, this latter choice depending to a great extent on
the taste of the instructor and the background of the students. Such a course
would be suitable for advanced undergraduate or first-year graduate students
in engineering, economics, mathematics, operations research and business ad-
ministration. In order to follow the main flow of Chapters 2 and 3 the student
need not have a strong mathematical background—apart from some elemen-
tary analysis—but he should be able to think in mathematical terms. However,
proofs of some of the theorems in these two chapters, as well as the contents of
Chapter 4, require some basic knowledge of real analysis and probability, which
is summarized in the three appendices that are included towards the end of the
PREFACE TO THE SECOND EDITION xv
book.
Part II of the book, on the other hand, is intended more for the researcher in
the field, to provide him with the state of art in infinite dynamic game theory.
However, selected topics from this part could also be used as a part of a grad-
uate course on dynamic optimization, optimal control theory or mathematical
economics.
This edition has been prepared by means of LATEX. Toward that end the text
```
of the original edition was scanned; the formulas and figures were added sepa-
```
rately. Despite the flexibility of LATEX and an intensive use of electronic mail,
the preparation of this new edition has taken us much longer than anticipated,
and we are grateful to the publisher for being flexible with respect to dead-
lines. The preparation would have taken even longer if our secretaries, Francie
Bridges and Tatiana Tijanova, respectively, with their expertise of LATEX, had
not been around. In the first stage Francie was responsible for the scanning
and the retyping of the formulas, and Tatiana for the labeling and the figures,
but in later stages they helped wherever necessary, and their assistance in this
matter is gratefully acknowledged. The first author would also like to acknowl-
edge his association with the Center for Advanced Study at the University of
Illinois during the Fall 1993 semester, which provided him with released time
```
(from teaching) which he spent partly in the writing of the new material in this
```
second edition. The second author would like to thank INRIA Sophia-Antipolis
in the person of its director, Pierre Bernhard, for allowing him to work on this
revision while he spent a sabbatical there. We both would also like to take this
opportunity to thank many of our colleagues and students for their input and
suggestions for this second edition. Particular recognition goes to Niek Tholen,
of Delft University of Technology, who read through most of the manuscript,
caught many misprints and made penetrating comments and suggestions.
We hope that this revised edition may find its way as its predecessor did.
Tamer Ba§ar Geert Jan Olsder
Urbana, March 1994 Delft, March 1994
This page intentionally left blank
Chapter 1
Introduction and
Motivation
1.1 Preliminary Remarks
This book is concerned with dynamic noncooperative game theory. In a nutshell,
```
game theory involves multi-person decision making; it is dynamic if the order
```
in which the decisions are made is important, and it is noncooperative if each
person involved pursues his or her1 own interests which are partly conflicting
with others'.
A considerable part of everything that has been written down, whether it
is history, literature or a novel, has as its central theme a conflict situation—a
collision of interests. Even though the notion of "conflict" is as old as mankind,
the scientific approach has started relatively recently, in the years around 1930,
with, as a result, a still growing stream of scientific publications. We also
see that more and more scientific disciplines devote time and attention to the
```
analysis of conflicting situations. These disciplines include (applied) mathemat-
```
ics, economics, engineering, aeronautics, sociology, politics and mathematical
finance.
It is relatively easy to delineate the main ingredients of a conflict situation:
an individual has to make a decision and each possible decision leads to a dif-
ferent outcome or result, which are valued differently by that individual. This
```
individual may not be the only one who decides about a particular outcome; a
```
series of decisions by several individuals may be necessary. If all these individ-
uals value the possible outcomes differently, the germs for a conflict situation
are there.
The individuals involved, also called players or decision makers, or simply
persons, do not always have complete control over the outcome. Sometimes there
are uncertainties which influence the outcome in an unpredictable way. Under
1Without any preference to sexes, a decision maker, in this book, is most times referred to
as a "he". It could equally well be a "she".
1
T. BASAR AND G. J. OLSDER
Table 1.1: The place of dynamic game theory.
Static
Dynamic
One player
Mathematical
programming
Optimal
control theory
Many players
```
(Static)
```
game theory
```
Dynamic (and/or
```
```
differential) game theory
```
```
such circumstances, the outcome is (partly) based on data not yet known and
```
not determined by the other players' decisions. Sometimes it is said that such
data is under the control of "nature", or "God", and that every outcome is
caused by the joint or individual actions of human beings and nature.
```
The established names of "game theory" (developed from approximately
```
```
1930) and "theory of differential games" (developed from approximately 1950,
```
```
parallel to that of optimal control theory) are somewhat unfortunate. "Game
```
```
theory", especially, appears to be directly related to parlor games; of course it
```
is, but the notion that it is only related to such games is far too restrictive. The
term "differential game" became a generally accepted name for games where
differential equations play an important role. Nowadays the term "differential
game" is also being used for other classes of games for which the more general
term "dynamic game" would be more appropriate.
The applications of "game theory" and the "theory of differential games"
mainly deal with economic and political conflict situations, worst-case designs
and also modeling of war games. However, it is not only the applications in these
```
fields that are important; equally important is the development of suitable con-
```
cepts to describe and understand conflict situations. It turns out, for instance,
that the role of information—what one player knows relative to others—is very
crucial in such problems.
Scientifically, dynamic game theory can be viewed as a child of the par-
ents game theory and optimal control theory.2 Its character, however, is much
more versatile than that of its parents, since it involves a dynamic decision
```
process evolving in (discrete or continuous) time, with more than one decision
```
maker, each with his own cost function and possibly having access to different
information. This view is the starting point behind the formulation of "games
in extensive form", which started in the 1930s through the pioneering work
```
of Von Neumann, which culminated in his book with Morgenstern (Von Neu-
```
```
mann and Morgenstern, 1947), and then made mathematically precise by Kuhn
```
```
2In almost all analogies there is a deficiency; a deficiency in the present analogy is that
```
the child is as old as one of his parents—optimal control theory. For the relationship between
these theories and the theory of mathematical programming, see Table 1.1.
2INTRODUCTION AND MOTIVATION 3
```
(1953), all within the framework of "finite" games. The general idea in this
```
formulation is that a game evolves according to a road or tree structure, where
at every crossing or branching a decision has to be made as how to proceed.
In spite of this original set-up, the evolution of game theory has followed a
rather different path. Most research in this field has been, and is being, con
centrated on the normal or strategic form of a game. In this form all possible
sequences of decisions of each player are set out against each other. For a two-
player game this results in a matrix structure. In such a formulation dynamic
aspects of a game are completely suppressed, and this is the reason why game
theory is classified as basically "static" in Table 1.1. In this framework empha-
```
sis has been more on (mathematical) existence questions, rather than on the
```
development of algorithms to obtain solutions.
Independently, control theory gradually evolved from Second World War ser-
vomechanisms, where questions of solution techniques and stability were stud-
```
ied. Then followed Bellman's "dynamic programming" (Bellman, 1957) and
```
```
Pontryagin's "maximum principle" (Pontryagin et al., 1962), which spurred the
```
interest in a new field called optimal control theory. Here the concern has been
```
on obtaining optimal (i.e., minimizing or maximizing) solutions and developing
```
numerical algorithms for one-person single-objective dynamic decision problems.
The merging of the two fields, game theory and optimal control theory, which
leads to even more concepts and to actual computation schemes, has achieved
a level of maturity, which the reader will hopefully agree with after he/she goes
through this book.
At this point, at the very beginning of the book, where many concepts
have yet to be introduced, it is rather difficult to describe how dynamic game
theory evolved in time and what the contributions of relevant references are.
We therefore defer such a description until later, to the "notes" section of each
```
chapter (except the present), where relevant historical remarks are included.
```
1.2 Preview on Noncooperative Games
```
A clear distinction exists between two-player (or, equivalently, two-person) zero-
```
sum games and the others. In a zero-sum game, as the name implies, the sum
of the cost functions of the players is identically zero. Mathematically speaking,
if ul and I/ denote, respectively, the decision variable and the cost function
```
of the ith player (to be written Pi], then Yli=i Ll(ul, u1} = 0 in a zero-sum
```
```
game. If this sum is, instead, equal to a nonzero constant (independent of
```
```
the decision variables), then we talk about a "constant-sum" game which can
```
however, easily be transformed to a zero-sum game through a simple translation
without altering the essential features of the game. Therefore, constant-sum
games can be treated within the framework of zero-sum games, without any
loss of generality, which we shall choose to do in this book.
A salient feature of two-person zero-sum games that distinguishes them from
other types of games is that they do not allow for any cooperation between the
players, since, in a two-person zero-sum game, what one player gains incurs a
T. BA§AR AND G. J. OLSDER
loss to the other player. However, in other games, such as two-player nonzero-
```
sum games (wherein the quantity ^i=1 Ll(u1,u2) is not a constant) or three-
```
or more-player games, the cooperation between two or more players may lead
to their mutual advantage.
```
Example 1.1 A point object (with mass one) can move in a plane which is
```
```
endowed with the standard (xi,X2)-coordinate system. Initially, at t = 0, the
```
```
point mass is at rest at the origin. Two unit forces act on the point mass; one
```
is chosen by PI, the other by P2. The directions of these forces, measured
counter-clockwise with respect to the positive xi-axis, are determined by the
```
players and are denoted by u1 and w2, respectively; they may in general be
```
time-varying. At time t — 1, PI wants to have the point mass as far in the
```
negative Xi-direction as possible, i.e., he wants to minimize zi(l), whereas P2
```
wants it as far in the positive xi-direction as possible, i.e., he wants to maximize
```
o;i(l), or equivalently, to minimize — £i(l). (See Fig. 1.1.) The "solution" to
```
```
this zero-sum game follows immediately; each player pulls in his own favorite
```
direction, and the point mass remains at the origin—such a solution is known
as the saddle-point solution.
We now alter the formulation of the game slightly, so that, in the present
set-up, P2 wishes to move the point mass as far in the negative X2-direction as
```
possible, i.e., he wants to minimize £2(1). Pl's Soa^ '1S still to minimize £i(l).
```
This new game is clearly nonzero-sum. The equations of motion for the point
mass are
Figure 1.1: The rope-pulling game.
4
```
Let us now consider the pair of decisions {u1 = TT, u2 = —Tr/2} with the corre-
```
```
sponding values of the cost functions being Ll = #i(l) = —^ and L2 — £2(1) =
```
```
— \. If P2 sticks to u2 = — 7T/2, the best thing for PI to do is to choose u1 = TT;
```
```
any other choice of ul(i] will yield an outcome which is greater than — |. Analo-
```
gously, if PI sticks to u1 = TT, P2 does not have a better choice than u2 = —n/2.
```
Hence, the pair {u1 = TT, u2 = — Tr/2} exhibits an equilibrium behavior, and this
```
kind of a solution, where one player cannot improve his outcome by altering his
decision unilaterally, is called a Nash equilibrium solution, or shortly, a Nash
solution.
INTRODUCTION AND MOTIVATION 5
If both players choose u1 = w2 = 57T/4, however, then the cost values be-
come I/1 = L2 = — ?>\/2. which are obviously better, for both players, than the
costs incurred under the Nash solution. But, in this case, the players have to
cooperate. If, for instance, P2 would stick to his choice u2 = 57T/4, then Pi
can improve upon his outcome by playing u1 = c, where c is a constant with
TT < c < 5-7T/4. PI is better off, but P2 is worse off! Therefore, the pair of
```
strategies {u1 = 57r/4,u2 = 57T/4} cannot be in equilibrium in a noncooperative
```
```
mode of decision making, since it requires some kind of faith (or even negotia-
```
```
tion), and thereby cooperation, on part of the players. If this is allowable, then
```
the said pair of strategies—known as a Pareto-optimal solution—stands out as
```
a reasonable equilibrium solution for the game problem (which is called a coop-
```
```
erative game), since it features the property that no other joint decision of the
```
players can improve the performance of at least one of them, without degrading
the performance of the other. D
In this book we shall deal only with noncooperative games. The reasons
for such a seeming limitation are twofold. Firstly, cooperative games3 can, in
general, be reduced to optimal control problems by determining a single cost
```
function to be optimized by all players, which suppresses the "game" aspects of
```
the problem. Secondly, the size of this book would have increased considerably
by inclusion of a complete discussion on cooperative games.
Actions and strategies
Heretofore, we have safely talked about "decisions" made by the players, without
being very explicit about what a decision really is. This will be made more
precise now in terms of information available to each player. In particular, we
```
shall distinguish between actions (also called controls) on the one hand and
```
```
strategies (or, equivalently, decision rules) on the other.
```
If an individual has to decide about what to do the next day, and the options
are fishing and going to work, then a strategy is: "if the weather report early
tomorrow morning predicts dry weather, then I will go fishing, otherwise I will
go to my office". This is a strategy or decision rule: what actually will be done
```
depends on quantities not yet known and not controlled by the decision maker;
```
the decision maker cannot influence the course of the events further, once he
has fixed his strategy. Any consequence of such a strategy, after the unknown
```
quantities are realized, is called an action. In a sense, a constant strategy (such
```
```
as an irrevocable decision to go fishing without any restrictions or reservations)
```
coincides with the notion of action.
In the example above, the alternative actions are to go fishing and to go to
```
work, and the action to be implemented depends on information (the weather
```
```
report) which has to be known at the time it is carried out. In general, such
```
information can be of different types. It can, for instance, comprise the previous
3 What we have in mind here is the class of cooperative games without side payments. If side
```
payments are allowed, we enter the territory of (cooperative) games in characteristic function
```
```
form, which is an altogether different topic. (See Owen (1968, 1982) and Vorob'ev (1977).)
```
T. BA§AR AND G. J. OLSDER
Figure 1.2: One-period advertising game.
actions of all the other players. As an example, consider the following sequence
```
of actions: if he is nice to me, I will be kind to him; if he is cool, I will be cool,
```
etc. The information can also be of a stochastic nature, such as in the fishing
```
example. Then, the actual decision (action) is based on data not yet known and
```
not controlled by other players, but instead determined by "nature". The next
example will now elucidate the role of information in a game situation and show
```
how it affects the solution. The information here is deterministic; "nature" does
```
not play a role.
Example 1.2 This example aims at a game-theoretic analysis of the advertis-
ing campaigns of two competing firms during two periods of time. Initially,
```
each firm is allotted a certain number of customers; PI has 100 and P2 has 150
```
customers. Per customer, each firm makes a fixed profit per period, say one
dollar. Through advertisement, a firm can increase the number of its customers
```
(some are stolen away from the competitor and others come from outside) and
```
```
thereby its profit. However, advertising costs money; an advertising campaign
```
```
for one period costs thirty dollars (for each firm). The figures in this example
```
```
may not be very realistic; it is, however, only the ratio of the data that matters,
```
not the scale with respect to which it is expressed.
First, we consider the one-period version of this game and assume that the
game ends after the first period is over. Suppose that PI has to decide first
```
whether he should advertise (Yes) or not (No), and subsequently P2 makes his
```
choice. The four possible outcomes and the paths that lead to those outcomes
are depicted in Fig. 1.2, in the form of a tree diagram. At every branching, a
decision has to be made as how to proceed. The objective of each firm is to
```
maximize its profit (for this one-period game). The "best" decisions, in this
```
```
case, can be found almost by inspection; whatever PI does (Y or JV), P2 will
```
```
always advertise (Y), since that is more profitable to him. PI, realizing this, has
```
6INTRODUCTION AND MOTIVATION
Figure 1.3: P2 chooses independently of Pi's action.
```
to choose between Y (with profit 95) and N (with profit 75), and will therefore
```
choose Y. Note that, at the point when P2 makes his decision, he knows Pi's
```
choice (action) and therefore P2's choice depends, in principle, on what PI has
```
done. P2's best strategy can be described as: "if PI chooses AT, then I choose
Y] if PI chooses V, then I choose 1"', which, in this case is a constant strategy.
This one-period game will now be modified slightly so that PI and P2 make
their decisions simultaneously, or, equivalently, independently of each other.
```
The information structure associated with this new game is depicted in Fig. 1.3;
```
a dashed curve encircling the points D\ and DI has been drawn, which indicates
that P2 cannot distinguish between these two points. In other words, P2 has to
arrive at a decision without knowing what PI has actually done. Hence, in this
game, the strategy of P2 has a different domain of definition, and it can easily
```
be verified that the pair {V, Y} provides a Nash solution, in the sense described
```
before in Example 1.1, yielding the same profits as in the previous game.
We now extend the game to two periods, with the objective of the firms
```
being maximization of their respective cumulative profits (over both periods).
```
```
The complete game is depicted in Fig. 1.4 without the information sets; we shall,
```
in fact, consider three different information structures in the sequel. First, the
order of the actions will be taken as PI — P2 — PI — P2, which means that at
the time of his decision each player knows the actions previously taken. Under
the second information structure to be considered, the order of the actions is
```
(PI, P2)-(P1, P2), which means that during each period the decisions are made
```
independently of each other, but that for the decisions of the second period the
```
(joint) actions of the first period are known. Finally, as a third case, it will be
```
```
assumed that there is no order in the actions at all; PI has to decide on what to
```
do during both periods without any prior knowledge of what P2 will be doing,
and vice versa. We shall be looking for Nash solutions in all three games.
The P 1 - P 2 - P 1 - P 2 information structure
```
The solution is obtained by working backward in time (a la "dynamic program-
```
```
ming"). For the decision during the second period, P2 knows to which point
```
```
(£>7 — DU) the game has proceeded. From each of these points he chooses Y
```
7T. BA§AR AND G. J. OLSDER
or N, depending on which decision leads to higher profit. These decisions are
denoted by an asterisk in Fig. 1.4. At the time of his decision during the second
period, PI knows that the game has proceeded to one of the points D^—DQ. He
can also guess precisely what P2 will do after he himself has made his decision
```
(the underlying assumption being that P2 will behave rationally). Hence, PI
```
can determine what will be best for him. At point D%, for instance, decision N
```
leads to a profit of 144 (for PI) and Y leads to 142; therefore, PI will choose
```
N if the game would be at D^. The optimal decisions for PI at the second
```
period are indicated by an asterisk also. We continue this way (in retrograde
```
```
time) until the vertex of the tree is reached; all best actions are designated by
```
an asterisk in Fig. 1.4. As a result, the actual game will evolve along a path
passing through the points D2, DQ, and Di4, and the cumulative profits for PI
and P2 will be 170 and 261, respectively.
```
The (P1,P2)-(P1,P2) information structure
```
During the last period, both players know that the game has evolved to one of
the points D% — DQ, upon which information the players will have to base their
actions. This way, we have to solve four "one-period games", one for each of the
```
points DZ — DQ. The reader can easily verify that the optimal (Nash) solutions
```
associated with these games are:
Figure 1.4: The two-period advertising game, without the information sets.
information
```
(starting point)
```
D3
D4
D5
DQ
action
PI
N
N
Y
N
P2
TV
N
TV
Y
profit
PI
144
135
212
166
P2
229
303
216
270
8INTRODUCTION AND MOTIVATION 9
The profit pairs corresponding to these solutions can be attached to the respec-
tive points DS — DQ, after which a one-period game is left to be solved. The
game tree is:
It can be verified that both players should play Y during the first period,
since a unilateral deviation from this decision leads to a smaller profit for both
players. The realized cumulative profits of PI and P2 under this information
scheme are therefore 166 and 270, respectively.
The no-information case
Both players have four possible choices: NN, NY, YN and YY, where the first
```
(respectively, second) symbol refers to the first (respectively, second) period.
```
Altogether there are 4 x 4 = 16 possible pairs of realizable profits, which can be
written in a matrix form:
NN
NY
YN
YY
144
229
142
225
207
220
212
216
140
228
126
216
202
218
194
206
135
303
131
258
171
266
170
261
132
301
117
285
166
270
155
255
If PI chooses YN and P2 chooses YY, then the profits are 166 and 270, re-
spectively. If any one of the players deviates from this decision, he will be worse
```
off. (If, for example, PI deviates, his other profit options are 132, 117 and 155,
```
```
each one being worse than 166.) Therefore {(YN}, (YY}} is the Nash solution
```
under the "no-information"-scheme.
P2
chooses
PI
chooses
NN NY YN YY
10 T. BA§AR AND G. J. OLSDER
Game problems of this kind will be studied extensively in this book, specifi-
cally in Chapter 3 which is devoted to finite nonzero-sum games, i.e., games in
which each player has only a finite number of possible actions available to him.
Otherwise, a game is said to be infinite, such as the one treated in Example 1.1.
In Chapter 3, we shall also elucidate the reasons why the realized profits are, in
general, dependent on the information structure.
```
The information structure (PI, P2)-(P1, P2) in Example 1.2 is called a feed-
```
```
back information structure; during each period the players know exactly to which
```
```
point ("state") the game has evolved and that information is fed back into their
```
strategies, which then leads to certain actions. This structure is sometimes
referred to as closed-loop, though later on in this book a distinction between
feedback and closed-loop will be made. The no-information case is referred to
as an open-loop information structure.
Not every game is well-defined with respect to every possible information
```
structure; even though Example 1.2 was well-defined under both a feedback
```
and an open-loop information structure, this is not always true. To exemplify
this situation, we provide, in the sequel, two specific games. The game to
be formulated in Example 1.3 does not make much sense under an open-loop
information structure, and the one of Example 1.4 can only be played under the
open-loop structure.
Example 1.3 "The lady in the lake." A lady is swimming in a circular lake
with a maximum speed v\. A man who wishes to intercept the lady, and who has
not mastered swimming, is on the side of the lake and can run along the shore
with a maximum speed vm. The lady does not want to stay in the lake forever
and wants eventually to escape. If, at the moment she reaches the shore, the
man cannot intercept her, she "wins" the game since on land she can run faster
```
than the man. Note that open-loop solutions (i.e., solutions corresponding to
```
```
the open-loop information structure) do not make sense here (at least not for
```
```
the man). It is reasonable to assume that each player will react immediately to
```
what his/her opponent has done and hence the feedback information structure,
whereby the current position of man and lady are fed back, is more appropriate.
Example 1.4 "The princess and the monster." This game is played in com-
plete darkness. Just like the previous example, this one is also of the pursuit-
evasion type. Here, however, the players do not know where their opponent is,
```
unless they bump into each other (or come closer to each other than a given
```
```
distance e); that moment in fact defines the termination of the game. The veloc-
```
ities of both players are bounded and their positions are confined to a bounded
area. The monster wants to bump into the princess as soon as possible, whereas
the princess wants to push this moment as far into the future as possible. Since
the players cannot react to each other's actions or positions, the information
structure is open-loop.
```
The solution to Example 1.3 will be given in Chapter 8; for more infer-
```
INTRODUCTION AND MOTIVATION 11
```
mation on Example 1.4, see Foreman (1977). This latter example also leads,
```
```
rather naturally, to the concept of "mixed strategies"; the optimal strategies in
```
the "princess and monster" game cannot be deterministic, as were the strate-
gies in Examples 1.1 and 1.2, for instance. For, if the monster would have
a deterministic optimal strategy, then the princess would be able to calculate
```
this strategy; if, in addition, she would know the monster's initial position, this
```
would enable her to determine the monster's path and thereby to choose for
herself an appropriate strategy so that she can avoid him forever. Therefore,
```
an optimal strategy for the monster (if it exists) should dictate random actions,
```
so that his trajectory cannot be predicted by the princess. Such a strategy is
called mixed. Equilibria in mixed strategies will be discussed throughout this
book.
What is optimal?
```
In contrast to optimal control problems (one-player games) where optimality has
```
an unambiguous meaning, in multi-person decision making, optimality, in itself,
is not a well-defined concept. Heretofore we have considered the Nash equilib-
rium solution, which is a specific form of "optimality". We also briefly discussed
Pareto-optimality. There exist yet other kinds of optimality in nonzero-sum
games. Two of them will be introduced now by means of the matrix game en-
countered in the "no-information"-case of Example 1.2. The equilibrium strate-
gies in the "Nash" sense were YN for PI and YY for P2. Suppose now that
P2 is a careful and defensive player and wants to protect himself against any
irrational behavior on the part of the other player. If P2 sticks to YY, the
worst that can happen to him is that PI chooses YY, in which case P2's profit
becomes 255 instead of 270. Under such a defensive attitude P2 might play YN,
```
since then his profit is at least 258. This strategy (or, equivalently, action in
```
```
this case) provides P2 with a lower bound for his earnings and the correspond-
```
ing solution concept is called minimax. The player who adopts this solution
concept basically solves a zero-sum game, even though the original game might
be nonzero-sum.
Yet another solution concept is the one that involves a hierarchy in decision
```
making: one of the players, say PI, declares and announces his strategy before
```
```
the other player chooses his strategy and he (i.e., PI) is in a position to enforce
```
this strategy. In the matrix game of the previous paragraph, if PI says "I will
```
play yy" and irrevocably sticks to it (by the rules of the game we assume that
```
```
cheating is not possible), then the best P2 can do is to choose YN, in which case
```
Pi's profit becomes 170 instead of 166 which was obtainable under the Nash
```
solution concept. Such games in which one player (called the leader] declares
```
```
his strategy first and enforces it on the other player (called the follower) are
```
referred to as Stackelberg games. They will be discussed in Chapter 3 for finite
action sets, and in Chapters 4 and 7 for infinite action sets.
Static versus dynamic
As the last topic of this section, we shall attempt to answer the question: "When
is a game called dynamic and when is it static?" So far, we have talked rather
12 T. BA§AR AND G. J. OLSDER
```
loosely about these terms; there is, in fact, no uniformly accepted separating line
```
between static games on the one hand and dynamic games on the other. We shall
choose to call a game dynamic if at least one player is allowed to use a strategy
that depends on previous actions. Thus, the games treated in Example 1.2
```
with the information schemes PI - P2 - PI - P2 and (P1,P2)-(P1,P2) are
```
dynamic. The third game of Example 1.2, with the "no-information" scheme,
should be called static, but, by an abuse of language, such a game is often also
called dynamic. The reason is that the players act more than once and thus
time plays a role. A game in which the players act only once and independently
of each other is definitely called a static game. The game displayed in Fig. 1.3
is clearly static.
1.3 Outline of the Book
```
The book comprises eight chapters and three appendices. The present (first)
```
chapter serves the purpose of introducing the reader to the contents of the
book, and to the conventions and terminology adopted. The next three chapters
constitute Part I of the text, and deal with finite static and dynamic games and
infinite static games.
Chapter 2 discusses the existence, derivation and properties of saddle-point
equilibria in pure, mixed and behavioral strategies for two-person zero-sum finite
games. It is in this chapter that the notions of normal and extensive forms of a
dynamic game are introduced, and the differences between actions and strategies
are delineated. Also treated is the class of two-person zero-sum finite games in
```
which a third (chance) player with a fixed mixed strategy affects the outcome.
```
Chapter 3 extends the results of Chapter 2 to nonzero-sum finite games under
basically two different types of equilibrium solution concepts, viz. the Nash
```
solution and the Stackelberg (hierarchical) solution. The impact of dynamic
```
information on the structure of these equilibria is thoroughly investigated, and
in this context the notion of prior and delay commitment modes of play are
elucidated.
Chapter 4 deals with static infinite games of both the zero-sum and nonzero-
sum type. In this context it discusses the existence, uniqueness and derivation
```
of (pure and mixed) saddle-point, Nash and Stackelberg equilibria, as well as
```
the consistent conjectural variations equilibrium. It provides explicit solutions
for some types of games, with applications in microeconomics.
The remaining four chapters constitute Part II of the book, for which Chap-
ter 5 provides a general introduction. It introduces the class of infinite dynamic
games to be studied in the remaining chapters, and also gives some background
material on optimal control theory. Furthermore, it makes the notions of "rep-
resentations of strategies on given trajectories" and "time consistency" precise.
The major portion of Chapter 6 deals with the derivation and properties of
Nash equilibria with prescribed fixed duration under different types of determin-
istic information patterns, in both discrete and continuous time. It also presents
as a special case saddle-point equilibria in such dynamic games, with important
INTRODUCTION AND MOTIVATION 13
```
applications in worst-case controller designs (such as H°°-optimal control).
```
Chapter 7 discusses the derivation of global and feedback Stackelberg equilib-
ria for the class of dynamic games treated in Chapter 6, and also the relationship
with the theory of incentives.
Finally, Chapter 8 deals with the class of zero-sum differential games for
which the duration is not fixed a priori —the so-called pursuit-evasion games—
and under the feedback information pattern. It first presents some necessary
and sufficient conditions for the saddle-point solution of such differential games,
and then applies these to pursuit-evasion games with specific structures so as
to obtain some explicit results.
```
Each chapter (with the exception of Chapter 1) starts with an introduction
```
section which summarizes its contents, and therefore we have kept the descrip-
tions above rather brief. Following Chapter 8 are the three appendices, two of
```
which (Appendices A and B) provide some background material on sets, vec-
```
tor spaces, matrices, optimization theory and probability theory to the extent
to which these notions are utilized in the book. The third appendix, on the
other hand, presents some theorems which are used in Chapters 3 and 4. The
book ends with a list of references, a table that indicates the page numbers
of the Corollaries, Definitions, Examples, Lemmas, Propositions, Remarks and
Theorems appearing in the text and an index.
1.4 Conventions, Notation and Terminology
Each chapter of the book is divided into sections, and occasionally sections
are divided into subsections. Section 2.1 refers to Section 1 of Chapter 2, and
Section 8.5.2 refers to subsection 2 of Section 8.5. The following items appear
in this book and they are numbered per chapter, such as Prop. 3.7 referring to
the seventh proposition of the third chapter:
```
Theorem (abbreviated to Thm.) Corollary Lemma
```
```
Definition (abbreviated to Def.) Problem Equation
```
```
Figure (abbreviated to Fig.) Property Example
```
```
Proposition (abbreviated to Prop.) Remark
```
Unlike the numbering of other items, equation numbers appear within paren-
```
theses, such as equation (2.5) which refers to the fifth numbered equation in
```
Chapter 2.
```
References to bibliographical sources (listed alphabetically at the end of the
```
```
book) are made according to the Harvard system (i.e., by name(s) and date).
```
The following abbreviations and symbols are adopted in the book, unless
stated otherwise in specific contexts:
RHS right-hand side
LHS left-hand side
w.p. a with probability a
LP linear programming
= equality sign
14 T. BA§AR AND G. J. OLSDER
= defined by
D end of proof, remark, example, etc.
|| parallel to
undefined , if x = 0
-1, i f r r < 0
<9A boundary of the set A
Pi player i
```
P(E) pursuer (evader)
```
```
Ll cost function(al) of Pi for a game in extensive form
```
```
Jl cost function(al) of Pi for a game in normal form
```
N number of players
```
N = {1,..., N] players' set; i e N
```
```
ul action (decision, control) variable of Pi; ul G Ul
```
```
7* strategy (decision law) of Pz; 7* 6 P
```
rf information available to Pi
```
K number of stages (levels) in a discrete-time game
```
```
K ={!,...,#}; fceK
```
[0, T] time interval on which a differential game is defined: t e [0, T]
```
x state variable; x(t) in continuous time and Xk in discrete time
```
```
x(t) time-derivative of x(t]
```
```
V value of zero-sum game (in pure strategies)
```
V upper value
V_ lower value
Vm value of a zero-sum game in mixed strategies
R real line
Rn n-dimensional Euclidean space
```
\x\ Euclidean norm of a finite-dimensional vector x, i.e., {x'x}1/2
```
\\x\\ norm of a vector x in an infinite-dimensional space.
Other symbols or abbreviations used with regard to sets, unions, summation,
matrices, optimization, random variables, etc., are introduced in Appendices A
and B.
```
A convention that we have adopted throughout the text (unless stated oth-
```
```
erwise) is that in nonzero-sum games all players are cost-minimizers, and in
```
two-person zero-sum games PI is the minimizer and P2 is the maximizer. In
two-person matrix games PI is the row-selector and P2 is the column-selector.
The word "optimality" is used in the text rather freely for different equilibrium
```
solutions when there is no ambiguity in the context; optimum quantities (such
```
```
as strategies, controls, etc.) are generally identified by an asterisk (*).
```
```
sgn (x)+
```
+1, if x > 0
Part I
This page intentionally left blank
Chapter 2
Noncooperative Finite
```
Games:
```
Two-Person Zero-Sum
2.1 Introduction
This chapter deals with the class of two-person zero-sum games in which the
players have a finite number of alternatives to choose from. There exist two
```
different formulations for such games: the normal (matrix) form and the exten-
```
```
sive (tree) form. The former constitutes a suitable representation of a zero-sum
```
game when each player's information is static in nature, since it suppresses all
the dynamic aspects of the decision problem. The extensive form, on the other
hand, displays explicitly the evolution of the game and the existing information
exchanges between the players.
```
In the first part of the chapter (Sections 2.2 and 2.3), the normal form
```
is introduced together with several related concepts, and then existence and
computation of saddle-point equilibria are discussed for both pure and mixed
strategies.
```
In the second part of the chapter (Sections 2.4 and 2.5), extensive form
```
description for zero-sum finite games without chance moves is introduced, and
saddle-point equilibria for such games are discussed, also within the class of
behavioral strategies. This discussion is first confined to single-act games in
which each player is allowed to act only once, and then it is extended to multi-
act games.
```
The third part of the chapter (comprising the remaining three sections) dis-
```
cusses extensions in a number of directions. Section 2.6 is devoted to a brief
discussion on the nature of equilibria for zero-sum games which also incorpo-
rate chance moves. Section 2.7 deals with two extensions: games with nonstan-
dard information patterns and randomized strategies, and games in extensive
17
18 T. BA§AR AND G. J. OLSDER
form which exhibits cycles. Finally, Section 2.8 deals with games with action-
dependent information sets, where the information on which the players can
base future moves depends on previous moves.
2.2 Matrix Games
The most elementary type of two-person zero-sum games are matrix games.
```
There are two players, to be referred to as player 1 (PI) and player 2 (P2),
```
```
and an (ra x n)-dimensional (real-valued) matrix A = {a^}. Each entry of this
```
matrix is an outcome of the game, corresponding to a particular pair of decisions
made by the players. For PI, the alternatives are the m rows of the matrix,
while for P2 the possible choices are the n columns of the same matrix. These
alternatives are known as the strategies of the players. If PI chooses the ith row
and P2 the jth column, then a^- is the outcome of the game, and PI pays this
amount to P2. In case a^ is negative, this should be interpreted as P2 paying
```
PI the positive amount corresponding to this entry (i.e., — a^).
```
To regard the entries of the matrix as sums of money to be paid by one player
to the other is, of course, only a convention. In a more general framework, these
outcomes represent utility transfers from one player to the other. Thus, we can
```
view each element of the matrix A (i.e., a possible outcome of the game) as
```
the net change in the utility of P2 for a particular play of the game, which is
equal to minus the net change in the utility of PI. Then, regarded as a rational
decision maker, PI will seek to minimize the outcome of the game, while P2
will seek to maximize it, by independent decisions.
Assuming that this game is to be played only once, then a reasonable mode
```
of play for PI is to secure his losses against any (rational or irrational) behavior
```
```
of P2. Under such an incentive, PI is forced to pick that row (i*) of matrix A,
```
whose largest entry is no bigger than the largest entry of any other row. Hence,
if PI adopts the i*th row as his strategy, where i* satisfies the inequalities
then his losses will be no greater than V which we call the loss ceiling of PI, or
equivalently, the security level for his losses. The strategy "row i*" that yields
this security level will be called a security strategy for PI.
Adopting a similar mode of play, P2 will want to secure his gains against
```
any behavior of PI, and consequently, he will choose that column (j*} whose
```
smallest entry is no smaller than the smallest entry of any other column. In
mathematical terms, j* will be determined from the n-tuple of inequalities
By deciding on the j*ih column as his strategy, P2 secures his gains at the
level V_ which we call 'the gain-floor of P2, or equivalently, the security level for
his gains. Furthermore, any strategy for P2 that secures his gain-floor, such as
"column j>*", is called a security strategy for P2.
which holds true for all possible k and /. Now, letting k = i* and I — .7*, the
```
desired result, i.e., (2.3), follows.
```
The third part of Thm. 2.1 says that, if played by rational players, the
outcome of the game will always lie between V and V_. It is for this reason that
the security levels V and V_ are also called, respectively, the upper value and
lower value of the matrix game. Yet another interpretation that could be given
to these values is the following.
Consider the matrix game in which players do not make their decisions inde-
pendently, but there is a predetermined ordering according to which the players
act. First PI decides on what row he will choose, he transmits this information
to P2 who subsequently chooses a column. In this game, P2 definitely has an
advantage over PI, since he knows what his opponent's actual choice of strategy
is. Then, it is unquestionable that the best play for PI is to choose one of his
```
security strategies (say, row i*). P2's "optimal" response to this is the choice
```
of a "column j°", where j° is determined from
TWO-PERSON ZERO-SUM FINITE GAMES 19
```
Theorem 2.1 In every matrix game A — {dij},
```
```
(i) the security level of each player is unique,
```
```
(ii) there exists at least one security strategy for each player,
```
```
(Hi) the security level o/Pl (the minimizer) never falls below the security level
```
```
o/P2 (the maximizer), i.e.,
```
where i* and j* denote security strategies for PI and P2, respectively.
```
Proof. The first two parts follow readily from (2.1) and (2.2), since in every
```
matrix game there is only a finite number of alternatives for each player to
```
choose from. To prove part (iii), we first note the obvious set of inequalities
```
with the "min max" operation designating the order of play in this decision
process. Thus, we see that the upper value of the matrix game is actually
attained in this case. Now, if the roles of the players are interchanged, this time
PI having a definite advantage over P2, then P2's best choice will again be one
```
of his security strategies (say, column j*), and Pi's response to this will be a
```
"row i°", where i° satisfies
with the "max min" symbol implying that the minimizer acts after the max-
```
imizer. Relation (2.5) indicates that the outcome of the game is equal to the
```
lower value V, when PI observes P2's choice.
20 T. BA§AR AND G. J. OLSDER
To illustrate these facets of matrix games, let us now consider the following
```
(3 x 4) matrix.
```
```
Here P2 (the maximizer) has a unique security strategy, "column 3" (i.e., j* —
```
```
3), securing him a gain-floor of V_ = 0. PI (the minimizer), on the other hand,
```
```
has two security strategies, "row 2" and "row 3" (i.e., i* = 2, i% = 3), yielding
```
him a loss-ceiling of V = maxj a^j = rnax^ a^j — 2 which is above the security
level of P2. Now, if P2 plays first, then he chooses his security strategy "column
```
3", with Pi's unique response being "row 3" (i° = 3), resulting in an outcome
```
of 0 = V_. If PI plays first, he is actually indifferent between his two security
strategies. In case he chooses "row 2", then P2's unique response is "column
```
3" (j° = 3)?4 whereas if he chooses "row 3", his opponent's response is "column
```
```
2" 0° = 2), both pairs of strategies yielding an outcome of 2 = V.
```
The preceding discussion validates the argument that, when there is a defi-
nite order of play, security strategies of the player who acts first make complete
sense, and actually they can be considered to be in "equilibrium" with the cor-
```
responding response strategies of the other player. By two strategies (of PI and
```
```
P2) to be in equilibrium, we roughly mean here that, after the game is over
```
and its outcome is observed, the players should have no ground to regret their
past actions. In a matrix game with a fixed order of play, for example, there is
no justifiable reason for the player who acts first to regret his security strategy
after the game is over. But what about the class of matrix games in which the
players arrive at their decisions independently? Do security strategies have any
sort of an equilibrium property, in that case?
```
To shed some light on this question, let us consider the (3 x 3) matrix game
```
```
of (2.7) below, with the players acting independently, and the game to be played
```
only once.
Both players have unique security strategies, "row 3" for PI and "column 1"
for P2, with the upper and lower values of the game being V — 2 and V_ = 0,
respectively. If both players play their security strategies, then the outcome
of the game is 1, which is midway between the security levels of the players.
But after the game is over, PI might think: "Well I knew that P2 was going
to play his security strategy, it is a pity that I didn't choose row 2 and enjoy
an outcome of 0". Thinking along the same lines, P2 might also regret that
he did not play column 2 and achieve an outcome of 2. This, then, indicates
that, in this matrix game, the security strategies of the players cannot possibly
4Here, and also earlier, it is only coincidental that optimal response strategies are also
security strategies.
TWO-PERSON ZERO-SUM FINITE GAMES 21
possess any equilibrium property. On the other hand, if a player chooses a row
```
or column (whichever the case is) different from the one dictated by his security
```
strategy, then he will be taking chances, since there is always a possibility that
the outcome of the game might be worse for him than his security level.
For the class of matrix games with equal upper and lower values, however,
```
such a dilemma does not arise. Consider, for example, the (2 x 2) matrix game
```
in which "row 2" and "column 2" are the unique security strategies of PI and
P2, respectively, resulting in the same security level V = V_ = 1. These security
strategies are in equilibrium, since each one is "optimal" against the other one.
Furthermore, since the security levels of the players coincide, it does not make
any difference, as far as the outcome of the game is concerned, whether the
players arrive at their decisions independently or in a predetermined order.
```
The strategy pair {row 2, column 2}, possessing all these favorable features, is
```
clearly the only candidate that could be considered as the equilibrium solution
```
of the matrix game (2.8). Such equilibrium strategies are known as saddle-point
```
strategies, and the matrix game, in question, is then said to have a saddle point
in pure strategies. A more precise definition for these terms is given below.
```
Definition 2.1 For a given (mxn) matrix game A = {a^}, let {row i*, column
```
```
j*} be a pair of strategies adopted by the players. Then, if the pair of inequalities
```
```
is satisfied for all i = 1 , . . . , m and all j = 1 , . . . , n, the strategies {row i*,
```
```
column j*} are said to constitute a saddle-point equilibrium (or simply, they
```
```
are said to be saddle-point strategies,), and the matrix game is said to have a
```
saddle point in pure strategies. The corresponding outcome ai>j* of the game
is called the saddle-point value, or simply the value, of the matrix game, and is
```
denoted by V(A).
```
```
Theorem 2.2 Let A = {aij} denote an (m x n) matrix game with V(A) —
```
```
V(A}. Then,
```
```
(i) A has a saddle point in pure strategies,
```
```
(ii) an ordered pair of strategies provides a saddle point for A if, and only
```
if, the first of these is a security strategy for PI and the second one is a
security strategy for P2,
```
(Hi) V(A) is uniquely given by
```
thus proving that "row z*" is a security strategy for PI. Analogously, it can be
shown that "column j*" is a security strategy for P2. This completes the proof
```
of (ii). Part (iii) then follows readily from (ii).
```
We now immediately have the following property of saddle-point solutions,
```
which follows from Thm. 2.2 (ii).
```
```
Corollary 2.1 In a matrix game A, let {row i\, column j\} and {row 1%, col-
```
```
umn ^2} be two saddle-point strategy pairs. Then {row ii, column j^}, {row 1%,
```
```
column ji} are also in saddle-point equilibrium.
```
This feature of the saddle-point strategies, given above in Corollary 2.1,
is known as their ordered interchangeability property. Hence, in the case of
```
nonunique (multiple) saddle points, each player does not have to know (or guess)
```
the particular saddle-point strategy his opponent will use in the game, since all
such strategies are in equilibrium and they yield the same value—indeed a very
desirable property that an equilibrium solution is expected to possess.
For the case when the security levels of the players do not coincide, however,
```
no such equilibrium solution can be found within the class of (pure) strategies
```
that we have considered so far. One way of resolving this predicament is, as
we have discussed earlier, to assume that one player acts after observing the
decision of the other one, in which case the security level of the player who acts
first is attained by an equilibrium "strategy pair". Here, of course, the strategy
of the player who acts last explicitly depends on the action of the first acting
player and hence the game has a "dynamic character". The precise meaning of
this will be made clear in Section 2.4, where more details can be found on these
aspects of zero-sum games.
22 T. BA§AR AND G. J. OLSDER
Proof. Let i* denote a security strategy for PI and j* denote a security
```
strategy for P2, which always exist by Thm. 2.1 (ii). Now, since V = V_, we
```
have
```
for alH = 1,..., m and j = 1,..., n; and letting i = z*, j = j* in this inequality,
```
we obtain V = V_ = a^j-. Using this result in the same inequality yields
```
which is (2.9). This completes the proof of (i) and the sufficiency part of (ii). We
```
now show that the class of saddle-point strategy pairs is no larger than the class
```
of ordered security strategy pairs. Let {row i*, column j*} be a saddle-point
```
```
strategy pair. Then it follows from (2.9) that
```
TWO-PERSON ZERO-SUM FINITE GAMES 23
Mixed strategies
Yet another approach to obtain an equilibrium solution in matrix games that
do not possess a saddle point, and in which players act independently, is to
enlarge the strategy spaces, so as to allow the players to base their decisions on
the outcome of random events—thus leading to the so-called mixed strategies.
This is an especially convincing approach when the same matrix game is played
over and over again, and the final outcome, sought to be minimized by PI and
maximized by P2, is determined by averaging the outcomes of individual plays.
To introduce the concept of a mixed strategy, let us again consider the matrix
```
game A — {a^} with ra rows and n columns. In this game, the "strategy space"
```
of PI comprises m elements, since he is allowed to choose one of the m rows of A.
```
If F1 denotes this space of (pure) strategies, then F1 = {row 1,... , row m}. If we
```
allow mixed strategies for PI, however, he will pick probability distributions on
F1. That is, an allowable strategy for PI is to choose "row 1" with probability
```
(w.p.) yi, "row 2" w.p. 3/2, • • • , "row m" w.p. ym, where yi + y2 H 1- ym = 1.
```
The mixed strategy space of PI, which we denote by Y, is now comprised of all
such probability distributions. Since the probability distributions are discrete,
Y simply becomes the space of all nonnegative numbers yi that add up to 1,
which is a simplex. Note that, the m pure strategies of PI can also be considered
```
as elements of Y, obtained by setting (i) y\ = 1, y^ = 0, Vi ^ 1; (ii) 1/2 = 1,
```
```
yi = 0, Vi 7^ 2; The mixed strategy space of P2, which we denote by Z,
```
can likewise be defined as an n-dimensional simplex. A precise definition for a
mixed strategy now follows.5
Definition 2.2 A mixed strategy for a player is a probability distribution on
the space of his pure strategies. Equivalently, it is a random variable whose
values are the player's pure strategies.
Thus, in matrix games, a mixed strategy for each player can be considered
either as an element of a simplex, or as a random variable with a discrete
probability distribution. Typical mixed strategies for the players, under the
latter convention, would be independent random variables u and v, defined,
respectively, by
5This definition of a mixed strategy is valid not only for matrix games but also for other
types of finite games to be discussed in this and the following chapter.
Now, if the players adopt these random variables as their strategies to be im-
plemented during the course of a repeated game, they will have to base their
24 T. BA§AR AND G. J. OLSDER
```
actual decisions (as to what specific row or column to choose during each play
```
```
of the game) on the outcome of a chance mechanism, unless the probability
```
distributions involved happen to be concentrated at one point—the case of pure
strategies. If the adopted strategy of PI is to pick "row 1" w.p. 1/2 and "row
ra" w.p. 1/2, for example, then the player in question could actually implement
this strategy by tossing a fair coin before each play of the game, playing "row
1" if the actual outcome is "head", and "row ra" otherwise. It should be noted,
```
here, that the actual play (action) dictated by Pi's strategy becomes known
```
```
(even to him) only after the outcome of the chance mechanism (tossing of a fair
```
```
coin, in this case) is observed. Hence, we have a sharp distinction between a
```
```
player's (proper mixed) strategy6 and its implemented "value" for a particular
```
play, where the latter explicitly depends on the outcome of a chance experiment
designed so as to generate the odds dictated by the mixed strategy. Such a
dichotomy between strategy and its implemented value does not exist for the
class of pure strategies that we have discussed earlier in this section, since if a
player adopts the strategy "to play row i", for example, then its implementation
is known for sure—he will play "row z".
```
Given an (m x n] matrix game A, which is to be played repeatedly and
```
the final outcome to be determined by averaging the scores of the players on
individual plays, let us now investigate as to how this final outcome will be
```
related to the (mixed) strategies adopted by the players. Let the independent
```
```
random variables u and v, defined by (2.10a) and (2.10b), be the strategies
```
adopted by PI and P2, respectively. Then, as the number of times this matrix
game is played gets arbitrarily large, the frequencies with which different rows
and columns of the matrix are chosen by PI and P2, respectively, will converge
to their respective probability distributions that characterize the strategies u
and v. Hence, the average value of the outcome of the game, corresponding to
```
the strategy pair {u,v}, will be equal to
```
6Since, by definition, the concept of mixed strategy also covers pure strategy, we shall some-
```
times use the term "proper mixed" to emphasize (whenever the case is) that the underlying
```
probability distribution is not one point.
where y and z are the probability distribution vectors defined by
```
PI wishes to minimize this quantity, J(y, z}, by an appropriate choice of a proba-
```
bility distribution vector y £Y, while P2 wishes to maximize the same quantity
by choosing an appropriate z 6 Z, where the sets Y and Z are, respectively, the
ra- and n-dimensional simplices introduced earlier, i.e.,
```
The quantity Vm(A) = y*'Az* is known as the saddle-point value, or simply the
```
value, of the game, in mixed strategies.
```
Remark 2.1 The assumptions of existence of a maximum in (2.14) and a mini-
```
```
mum in (2.15) are justifiable since, in each case, the objective functional is linear
```
```
(thereby continuous) and the simplex over which optimization is performed is
```
```
closed and bounded (thereby compact). (See Appendix A.)
```
As a direct extension of Thm. 2.1, we can now verify the following properties
of the security levels and security strategies in matrix games when the players
are allowed to use mixed strategies.
Theorem 2.3 In every matrix game A in which the players are allowed to use
mixed strategies, the following properties hold:
```
(i) The average security level of each player is unique,
```
```
(ii) There exists at least one mixed security strategy for each player.
```
TWO-PERSON ZERO-SUM FINITE GAMES 25
The following definitions now follow as obvious extensions to mixed strategies
of some of the concepts introduced earlier for matrix games with strategy spaces
comprised of only pure strategies.
Definition 2.3 A vector y* £ Y is called a mixed security strategy for PI, in
the matrix game A, if the following inequality holds for all y 6 Y:
```
Here, the quantity Vm is known as the average security level of PI (or equiv-
```
```
alently the average upper value of the game,). Analogously, a vector z* € Z is
```
called a mixed security strategy for P2, in the matrix game A, if the following
inequality holds for all z G Z:
```
Here, Vm is known as the average security level of P2 (or equivalently, the
```
average lower value of the game/
```
Definition 2.4 A pair of strategies {y*, z*} is said to constitute a saddle point
```
for a matrix game A, in mixed strategies, if
```
Proof, (i) Uniqueness of Vm follows directly from its definition, Vm =
```
infy maxz y'Az, in view of Remark 2.1. Analogously, the quantity V_m =
supz miny y'Az is also unique.
```
(ii) The quantity, maxz€z y'Az, on the right-hand side (RHS) of (2.14) is
```
a continuous function of y G Y, a result which can be proven by employing
standard methods of analysis. But since y is a compact set, the minimum of
this function is attained on Y, thus proving the desired result for the security
strategy of PI. Proof of existence of a security strategy for P2 follows along
similar lines.
```
(iii) The middle inequality follows from the simple reasoning that the maxi-
```
mizer's security level cannot be higher than the minimizer's security level. The
other two inequalities follow, since the pure security strategies of the players are
included in their mixed strategy spaces. D
```
As an immediate consequence of part (ii) of Thm. 2.3 above, we obtain the
```
following.
Corollary 2.2 In a matrix game A, the average upper and lower values in
mixed strategies are given, respectively, by
and
Now, one of the important results of zero-sum game theory is that these
```
upper and lower values are equal, that is, Vm(A) = V_m(A). This result is given
```
in Thm. 2.4, below, which is known as the "Minimax Theorem". In its proof,
we shall need the following lemma.
```
Lemma 2.1 Let A be an arbitrary (m x n) -dimensional matrix. Then, either
```
```
(i) there exists a nonzero vector y € Rm, y > 0 such that A'y < 0,
```
or
```
(ii) there exists a nonzero vector z e Rn, z > 0 such that Az > 0.
```
Proof. Consider in Rn the unit vectors ei,... ,en, together with the rows of
A, to be indicated by a^., i = 1,..., m. Denote the convex hull of these n -f m
vectors by C. Two possibilities exist: either 0 € C or 0 $ C.
```
Assume first that 0 6 C. Then there exist nonnegative coefficients yi (i =
```
```
1,..., m) and r/j (j = 1,..., n), normalized to 1, £2=i yf + £"=i % = 1, such
```
that
26 T. BA§AR AND G. J. OLSDER
```
(iii) The security levels in pure and mixed strategies satisfy the following in-
```
```
equalities:
```
This is further equivalent to the inequality
TWO-PERSON ZERO-SUM FINITE GAMES 27
Componentwise this equation reads Y^Li yiaij + Vj — 0, j' = 1, • • • , n. Since all
```
the i/i's cannot be zero simultaneously (otherwise all r^'s would also be zero—
```
```
which is an impossibility due to the normalization), we have
```
Now, if we take y 6 Rm as the vector whose components are the y^'s defined
```
above, then the validity of possibility (i) of the lemma follows. Assume next that
```
```
0 ^ C. Then, there exists a hyper plane passing through the origin (characterized
```
```
by the equation z'x = 0, where x, z € Rn, and where x is the running variable),
```
```
such that C is on one side of it; furthermore, z'x > 0 for x G C. If we choose
```
```
x = ei (which belongs to C by construction), then it follows that Zi > 0, and
```
hence the vector z is nonzero. Similarly, if we choose x = a^., then z'a'^ > 0 and
```
hence Az > 0, which validates the possibility (ii).
```
```
Theorem 2.4 (The Minimax Theorem) In any matrix game A, the average
```
security levels of the players in mixed strategies coincide, that is,
Proof. We first show, by applying Lemma 2.1, that for a given matrix game
A, at least one of the following two inequalities holds:
Assume that the first alternative of Lemma 2.1 is valid. Then, there exists a
vector y° £ Y such that A'y° < 0, which is equivalent to the statement that the
inequality
holds for all z E Z. This is further equivalent to saying
```
which definitely implies (by also making use of (2.18a))
```
Now, under the second alternative of Lemma 2.1, there exists a vector z° G Z
such that Az° > 0, or equivalently,
28 T. BA§AR AND G. J. OLSDER
```
which finally implies (ii), by also making use of (2.18b), since
```
```
Thus we see that, under the first alternative of Lemma 2.1, inequality (i) holds;
```
```
and under the second alternative, inequality (ii) remains valid.
```
```
Let us now consider a new (m x n)-dimensional matrix B = {bij} obtained
```
by shifting all entries of A by a constant, c, that is, b^ = a^ — c. This results
```
in a shift of the same amount in both Vm(A) and V_m(A), that is,
```
and
```
Since matrix A was arbitrary in (i) and (ii), we replace A with B, as defined
```
above, in these inequalities, and arrive at the following property with regard to
the upper and lower values of a matrix game A, in mixed strategies. For a given
matrix game A and an arbitrary constant c, at least one of the following two
inequalities holds true:
```
But, for this statement to be valid for arbitrary c, it is necessary that V^A) =
```
```
Y_m(A). For, otherwise, the only possibility is Vm(A) = V_m(A) + k, in view of
```
```
the middle inequality of (2.17), where k > 0; and picking c = V_m(A) + (1/2)A;
```
```
in (iii) and (iv), it readily follows that neither (iii) nor (iv) is satisfied. Thus,
```
this completes the proof of the theorem.
```
Corollary 2.3 Let A denote an (m x ri) matrix game. Then,
```
```
(i) A has a saddle point in mixed strategies,
```
```
(ii) a pair of mixed strategies provides a saddle point for A if, and only if, the
```
first of these is a mixed security strategy for PI, and the second one is a
mixed security strategy for P2,
```
(iii) Vm(A) is uniquely given by
```
```
(iv) in case of multiple saddle points, the mixed saddle-point strategies possess
```
the ordered interchangeability property.
Proof. This corollary to Thm. 2.4 is actually an extension of Thm. 2.2 and
Corollary 2.1 to the case of mixed strategies, and its verification is along the
same lines as that of Thm. 2.2. It should only be noted that equality of the
average upper and lower values of the matrix game is a fact, in the present case,
rather than a part of the hypothesis, as in Thm. 2.2.
We have thus seen that, if the players are allowed to use mixed strategies,
matrix games always admit a saddle-point solution which, thereby, manifests
itself as the only reasonable equilibrium solution in zero-sum two-person games
of that type.
TWO-PERSON ZERO-SUM FINITE GAMES 29
2.3 Computation of Mixed Equilibrium
Strategies
We have seen in the previous section that two-person zero-sum matrix games
```
always admit a saddle-point equilibrium in mixed strategies (cf. Thm. 2.4). One
```
important property of mixed saddle-point strategies is that, for each player, the
corresponding one also constitutes a mixed security strategy, and conversely,
```
each mixed security strategy is a mixed saddle-point strategy (Corollary 2.3(ii)).
```
This property, then, strongly suggests a possible way of obtaining the mixed
saddle-point solution of a matrix game, which is to determine the mixed security
```
strategy(ies) of each player.
```
```
To illustrate this approach, let us consider the (2 x 2) matrix game
```
which clearly does not admit a pure-strategy saddle point, since V = 1 and
```
V_ = 0. Let the mixed strategies of PI and P2 be denoted by y = (1/1,2/2)' and
```
```
z = (*i, 22)', respectively, with y» > 0, z» > 0 (i = 1,2), yi + y2 = z\ + z2 = 1,
```
and consider first the average security level of PI. It should be noted that, while
determining the average security level of a player, we can assume, without any
```
loss of generality, that the other player chooses only pure strategies (this follows
```
```
directly from Defs. 2.3 and 2.4). Hence, in the present case, we can take P2 to
```
```
play either (z\ — 1,Z2 = 0) or (z\ =0,22 = 1); and under different choices of
```
mixed strategies for PI, we can determine the average outcome of the game as
shown in Fig. 2.1 by the bold line, which forms the upper envelope to the two
```
straight lines drawn. Now, if the mixed strategy (y\ = \,}fa = f) corresponding
```
to the lowest point of that envelope is adopted by PI, then the average outcome
will be no greater than |. For any other mixed strategy of PI, however, P2 can
```
Figure 2.1: Mixed security strategy of PI for the matrix game (2.20).
```
30 T. BA§AR AND G. J. OLSDER
```
Figure 2.2: Mixed security strategy of P2 for the matrix game (2.20).
```
```
obtain a higher average outcome. (If, for instance, y\ = ^, 7/2 = ^> then P2 can
```
```
increase the outcome by playing z\ = 1, z^ = 0.)
```
```
This then implies that the strategy (y\ — \,yi = f) is a mixed security
```
```
strategy for PI (and his only one), and thereby, it is his mixed saddle-point
```
strategy. The mixed saddle-point value can easily be read off from Fig. 2.1 to
be Vm = f .
To determine the mixed saddle-point strategy of P2, we now consider his
security level, this time assuming that PI adopts pure strategies. Then, for
different mixed strategies of P2, the average outcome of the game can be deter-
mined to be the bold line, shown in Fig. 2.2, which forms a lower envelope to the
two straight lines drawn. Since P2 is the maximizer, the highest point on this
```
envelope is his average security level, which he can guarantee (on the average)
```
```
by playing the mixed strategy (z^ — \,z% = |) which is also his saddle-point
```
strategy.
The computational technique discussed above is known as the ''''graphical
solution" of matrix games, since it makes use of a graphical construction directly
related to the entries of the matrix. Such an approach is practical not only for
```
(2 x 2) matrix games but also for general (2 x n) and (m x 2) matrix games.
```
```
Consider, for example, the (2 x 3) matrix game
```
for which Pi's average security level in mixed strategies has been determined in
```
Fig. 2.3. Assuming again that P2 uses only pure strategies (z\ = 1, z% = 23 = 0
```
```
or z\ = 23 = 0, z-2 — 1 or z\ = Z2 = 0, z% — 1), the average outcome of the
```
game for different mixed strategies of PI is given by the bold line drawn in
```
Fig. 2.3, which is again the upper envelope (this time formed by three straight
```
```
lines). The lowest point on this envelope yields the mixed security strategy,
```
```
and thereby the mixed saddle-point strategy, of PI, which is (yl = |,y| = ?)•
```
TWO-PERSON ZERO-SUM FINITE GAMES 31
```
Figure 2.3: Mixed security strategy of PI for the matrix game (2.21).
```
The mixed saddle-point value, in this case, is Vm — |. To determine the mixed
saddle-point strategy of P2, we first note that the average security level of PI has
been determined by the intersection of only two straight lines which correspond
to two pure strategies of P2, namely "column 1" and "column 2". If PI uses
his mixed security strategy, and P2 knows that before he plays, he would of
course be indifferent between these two pure strategies, but he would never play
the third pure strategy, "column 3", since that will reduce the average outcome
of the game. This then implies that, in the actual saddle-point solution, he
will "mix" only between the first two pure strategies. Hence, in the process of
```
determining P2's mixed security strategies, we can consider the (2 x 2) matrix
```
game
```
which is obtained from (2.21) by deleting the last column. Using the graphi-
```
cal approach, P2's mixed security strategy in this matrix game can readily be
```
determined to be (z\ — |, zZ, = |) which is also his mixed saddle-point strategy.
```
Relation with linear programming
One alternative to the graphical solution when the matrix dimensions are high
```
is to convert the original matrix game into a linear programming (LP) problem
```
and make use of some of the powerful algorithms available for LP. To elucidate
the close connection between a two-person zero-sum matrix game and an LP
```
problem, let us start with an (m x n) matrix game A = {a^} with all entries
```
This is further equivalent to the maximization problem
32 T. BA§AR AND G. J. OLSDER
```
positive (i.e., a^- > 0 Vi = 1,..., m; j = 1,..., n). Then, the average value of
```
this game, in mixed strategies, is given by
which is necessarily a positive quantity by our positivity assumption on A. Let
```
us now consider the min-max operation used in (2.23). Here, first a y € Y is
```
```
given, and then the resulting expression is maximized over Z; that is, the choice
```
```
of z £ Z can depend on y. Hence, the middle expression of (2.23) can also be
```
written as
```
where vi(y) is a positive function of y, defined by
```
```
Since Z is the n-dimensional simplex as defined by (2.13b), the inequality
```
```
in (2.24) becomes equivalent to the vector inequality
```
where
```
Further introducing the notation y = y/v\ (y) and recalling the definition of Y
```
```
from (2.13a), we observe that the optimization problem faced by PI in deter-
```
mining his mixed security strategy is
```
minimize v\ (y) over Rm
```
subject to
subject to
which is a standard LP problem. The solution of this LP problem will give
the mixed security strategy of PI, normalized with the average saddle-point
TWO-PERSON ZERO-SUM FINITE GAMES 33
```
value of the game, Vm(A). Hence, if y* € Y is a mixed saddle-point strategy
```
```
of PI, in the matrix game A, then the quantity y* = y*/Vm(A) solves the LP
```
```
problem (2.25a)-(2.25c), whose optimal value is in fact l/Vm(A). Conversely,
```
every solution of the LP problem will correspond to a mixed strategy of PI in
the same manner.
```
Now, if we instead consider the right-hand expression of (2.23), and introduce
```
and
where c is some constant. Then,
```
(i) every mixed saddle-point strategy pair for matrix game A also constitutes
```
a mixed saddle-point solution for matrix game B, and vice versa,
```
fa)
```
```
Proof. Let (y*,z*) be a saddle-point solution for A, thus satisfying inequal-
```
```
ities (2.16). If A is replaced by B -4- clml'n in (2.16), then it is easy to see that
```
```
(y*,z*) also constitutes a saddle-point solution for 5, since y'lml'nz = I for
```
every y £ Y, z G Z. Since the reverse argument also applies, this completes the
```
proof of part (i). Part (ii), on the other hand, follows from the relation
```
```
Vm(A) = y*'[B + clml'n}z* = y*'Bz* + cy*'lml'nz* = Vm(B) + c.
```
following similar steps and reasoning leads us to the minimization problem
subject to
```
which is the "dual" of (2.25a)-(2.25c). We thus arrive at the conclusion that
```
if z* € Z is a mixed saddle-point strategy of P2, in the matrix game A, then
```
z* — z*/Vm(A) solves the LP problem (2.26a)-(2.26c), whose optimal value is
```
```
again l/Vm(A). Furthermore, the converse statement is also valid.
```
We have thus shown that, given a matrix game with all positive entries,
```
there exist two LP problems (duals of each other) whose solutions yield the
```
```
saddle-point solution(s) of the matrix game. The positivity of the matrix A,
```
however, is only a convention here, and can easily be dispensed with, as the
following lemma shows.
```
Lemma 2.2 Let A and B be two (m x n)-dimensional matrices related to each
```
other by the relation
34 T. BA§AR AND G. J. OLSDER
```
Because of property (i) of the above lemma, we call matrix games which
```
```
satisfy relation (2.27) strategically equivalent matrix games. It should now be
```
apparent that, given a matrix game A, we can always find a strategically equiv-
alent matrix game with all entries positive, and thus the transformation of a
matrix game into two LP problems as discussed prior to Lemma 2.2 is always
valid, as far as determination of saddle-point strategies is concerned. This result
is summarized below in Thm. 2.5.
```
Theorem 2.5 Let B be an (m x n] matrix game, and A be defined by (2.27)
```
with c chosen to make all its entries positive. Introduce the two LP problems
```
with their optimal values (if they exist) denoted by Vp and V&, respectively,
```
```
(i) Both LP problems admit a solution, and
```
```
(ii) If (y*,z*) is a mixed saddle-point solution of the matrix game B, then
```
```
y*/Vm(A) solves (2.28a), and z*/Vm(A) solves (2.28b).
```
```
(in) If y* is a solution of (2.28a), and z* is a solution of (2.28b), the pair
```
```
(y*/Vp, z*/Vd) constitutes a mixed saddle-point solution for matrix game
```
B. Furthermore, Vm(B) = (l/%) - c.
Proof. Since A is a positive matrix and strategically equivalent to B, the
theorem has already been proven prior to the statement of Lemma 2.2. We
should only note that the reason why both LP problems admit a solution is
that the mixed security strategies of the players always exist in the matrix
game A. _
Theorem 2.5 provides one method of making games "essentially" equiva-
lent to two LP problems that are duals of each other, thus enabling the use
of some powerful algorithms available for LP in order to obtain their saddle-
```
point solutions. (The reader is referred to Dantzig (1963), Luenberger (1973),
```
```
Gonzaga (1992), or Karmarkar (1984) for LP algorithms.) There are also other
```
transformation methods available in the literature, which form different kinds
of equivalences between matrix games and LP problems, but the underlying
idea is essentially the same in all these techniques. For one such equivalence
the reader is referred to the next chapter, to Corollary 3.2, where an LP prob-
lem, not equivalent to the one of Thm. 2.5, is obtained as a special case of a
more general result on nonzero-sum matrix games. More details on computa-
```
tional techniques for matrix games can be found in Luce and Raiffa (1957) and
```
```
Singleton and Tyndal (1974).
```
Dominating strategies
We conclude this section by introducing the concept of "dominance" in matrix
games, which can sometimes be useful in reducing the dimensions of a matrix
game by eliminating some of its rows and/or columns which are known from the
very beginning to have no influence on the equilibrium solution. More precisely,
```
given an (m x n) matrix game A = {a^}, we say that "row i" dominates "row
```
fc" if dij < dkj, j = 1,..., n, and if, for at least one j, the strict inequality-sign
holds. In terms of pure strategies, this means that the choice of the dominating
strategy, i.e., "row z", is at least as good as the choice of the dominated strategy,
i.e., "row fc". If, in the above set of inequalities, the strict inequality-sign holds
for all j = 1 , . . . , n, then we say that "row V strictly dominates "row fc", in
```
which case, regardless of what P2 chooses, PI does better with "row i" (strictly
```
```
dominating strategy) than with "row A;" (strictly dominated strategy). It there-
```
fore follows that PI can always dispense with his strictly dominated strategies
and consider only strictly undominated ones, since adoption of a strictly domi-
nated strategy is apt to increase the security level for PI. This argument also
holds for mixed strategies, and strictly dominated rows are always assigned a
```
probability of zero in an optimal (mixed) strategy for PI.
```
```
Analogously for P2, "column j" of A is said to dominate (respectively,
```
```
strictly dominate) "column /", if a^ > an, i = l , . . . , m , and if, for at least
```
```
one i (respectively, for all i}, the strict inequality-sign holds. In an optimal
```
```
(mixed) strategy for P2, strictly dominated columns are also assigned a proba-
```
bility of zero. Mathematical verifications of these intuitively obvious assertions,
as well as some other properties of strictly dominating strategies, can be found
```
in Vorob'ev (1977).
```
```
We have thus seen that, in the computation of (pure or mixed) saddle-point
```
equilibria of matrix games, strictly dominated rows and columns can readily be
deleted, since they do not contribute to the equilibrium solution—and this could
lead to considerable simplifications in the graphical solution or in the solution
```
of the equivalent LP problems, since the matrix is now of smaller dimension(s).
```
```
With (nonstrictly) dominated rows and columns, however, this may not always
```
be so. Even though every saddle-point solution of a matrix game whose domi-
nated rows and columns are deleted is also a saddle-point solution of the original
```
matrix game, there might be other saddle points (of the original game) which are
```
eliminated in this process. As a specific illustration of this possibility, consider
```
the (2 x 2) matrix game
```
which admits two saddle points, as indicated. For PI, "row 2" is clearly dom-
```
inating (but not strictly dominating), and for P2, "column 2" is strictly domi-
```
nating. Thus, by eliminating dominated rows and columns, we end up with the
```
(1 x 1) game corresponding to the dominating strategies of the players, which
```
trivially has a unique saddle point.
TWO-PERSON ZERO-SUM FINITE GAMES 3536 T. BA§AR AND G. J. OLSDER
Motivated by this result, we call the saddle-point solutions of the "reduced"
game which does not have any dominated rows or columns dominant saddle-
point solutions. Even though the original game might have other saddle points,
it may still be reasonable to confine attention only to dominant saddle points,
```
since any game that possesses a saddle point also possesses (by definition) a
```
```
dominant saddle point, and furthermore the saddle-point value (in pure or mixed
```
```
strategies) is unique; in other words, there is nothing essential to gain in seeking
```
saddle-point equilibria which are not dominant.
There exist different mathematical descriptions for two-person zero-sum games
```
with finite strategy sets, the matrix form (also known as the normal form) being
```
one of these. A normal form does not in general provide the full picture of the
underlying decision process, since it only describes the correspondences between
different ordered strategy pairs and outcomes. Some important issues like the
order of play in the decision process, information available to the players at the
time of their decisions, and the evolution of the game in the case of dynamic
situations are suppressed in the matrix description of a zero-sum game.
An alternative to the normal form, which explicitly displays the dynamic
character of the decision problem, is known as the extensive form of a two-
person zero-sum game. An extensive form basically involves a tree structure
with several nodes and branches, providing an explicit description of the order
```
of play and the information available to each player at the time of his decision(s);
```
the game evolves from the top of the tree to the tip of one of its branches. Two
such tree structures are depicted in Fig. 2.4, where in each case PI has two
```
alternatives (branches) to choose from, whereas P2 has three alternatives, and
```
the order of play is such that PI acts before P2 does. The numbers at the end of
```
the lower branches represent the pay-offs to P2 (or equivalently, losses incurred
```
```
to PI) if the corresponding paths are selected by the players. The uppercase
```
letters L, M, R stand for Left, Middle, and Right branches, respectively.
Figure 2.4: Two zero-sum games in extensive form differing only in the infor-
mation available to P2.
2.4 Extensive Forms: Single-Act Games
TWO-PERSON ZERO-SUM FINITE GAMES 37
```
It should be noted that the two zero-sum games displayed in Figs 2.4(a) and
```
```
2.4(b) are equivalent in every aspect other than the information available to
```
P2 at the time of his play, which is indicated on the tree diagrams by dotted
```
lines enclosing an area (known as the information set) including the relevant
```
```
nodes. In Fig. 2.4(a), the two possible nodes of P2 are included in the same
```
dotted area, implying that, even though PI acts before P2 does, P2 does not
have access to his opponent's action. That is to say, at the time of his play,
P2 does not know at what node he really is. This is, of course, equivalent to
the case when both players act simultaneously. Hence, simultaneous play can
also be represented by a tree structure, provided that the relevant information
```
set is chosen properly. The extensive form of Fig. 2.4(a) can now easily be
```
converted into the normal form, with the equivalent matrix game being the
```
one given by (2.21) in Section 2.3. As it has been discussed there, this matrix
```
```
game admits a saddle-point solution in mixed strategies, which is (y*L = |, y*R =
```
```
\''Z*L — \i ZM — |> ZR = 0)> wrth the mixed saddle-point value being Vm = 8/3.
```
```
The extensive form of Fig. 2.4(b), however, admits a different matrix game as
```
its normal form and induces a different behavior on the players. In this case,
each node of P2 is included in a separate information set, thus implying that P2
has perfect information as to which branch of the tree PI has chosen. Hence,
```
if ul denotes the actual choice of PI and 72(-) denotes a strategy for P2, as a
```
maximizer P2's optimal choice will be
Not knowing this situation ahead of time, PI definitely adopts the strategy
with the equilibrium outcome of the game being 3, which is, in fact, the upper
```
value of the matrix game (2.21).
```
```
Thus, we have obtained a solution of the zero-sum game of Fig. 2.4(b) by
```
directly making use of the extensive tree formulation. Let us now attempt to
```
obtain the (same) saddle-point solution by transforming the extensive form into
```
an equivalent normal form. To this end, we first delineate the possible strategies
of the players. For PI, there are two possibilities:
For P2, however, since he observes the action of PI, there exist 32 — 9 possible
```
strategies, which are 72(w1) = u1, 7KW1) = I/, 73 (u1) = M, 7 4 ( u l ) = R,
```
38 T. BA§AR AND G. J. OLSDER
```
where the subscripts i = 1,..., 9 denote a particular ordering (labeling) of the
```
possible strategies of P2. Hence, the equivalent normal form of the zero-sum
```
game of Fig. 2.4(b) is the 2 x 9 matrix game
```
```
which admits two saddle points, as indicated, with the dominant one being {£,
```
```
column 7}. It is this dominant saddle-point solution that corresponds to the
```
```
one given by (2.29a)-(2.29b), and thus we observe that the derivation outlined
```
earlier, which utilizes directly the extensive tree structure, could cancel out dom-
inated saddle points. This, however, does not lead to any real loss of generality,
since, if a matrix game admits a pure-strategy saddle point, it also admits a
dominant pure-strategy saddle point, and furthermore, the saddle-point value
```
is unique (regardless of the number of pure-strategy equilibria).
```
```
We should now note that the notion of "(pure) strategy" introduced above
```
```
within the context of the zero-sum game of Fig. 2.4(b) is somewhat different
```
```
from (and, in fact, more general than) the one adopted in Section 2.2. This dif-
```
ference arises mainly because of the dynamic character of the decision problem
```
of Fig. 2.4(b)—P2 being in a position to know exactly how PI acts. A (pure)
```
strategy for P2, in this case, is a rule that tells him which one of his alternatives
to choose, for each possible action of PI. In mathematical language, it is a map-
ping from the collection of his information sets into the set of his alternatives.
We thus see that there is a rather sharp distinction between a strategy and the
actual action dictated by that strategy, unless there is only one information set
```
(which is the case in Fig. 2.4(a)). In the case of a single information set, the
```
notions of strategy and action definitely coincide, and that is why we have used
these two terms interchangeably in Section 2.2 while discussing matrix games
with pure-strategy solutions.
We are now in a position to give precise definitions of some of the concepts
introduced above.
Definition 2.5 An extensive form of a two-person zero-sum finite game without
chance moves is a finite tree structure with
```
(i) a specificvertexindicating the starting point of the game,
```
```
(ii) a pay-off function assigning a real number to each terminal vertex of the
```
```
tree, which determines the pay-off (respectively, loss) to P2 (respectively,
```
PU
```
(Hi) a partition of the nodes of the tree into two player sets (to be denoted by
```
```
N1 and N2 for PI and P2; respectively),
```
```
(iv) a subpartition of each player set Nl into information sets {??]}, such
```
that the same number of immediate branches emanates from every node
belonging to the same information set, and no node follows another node
in the same information set.7
Remark 2.2 This definition of an extensive form covers more general types
of two-person zero-sum games than the ones discussed heretofore since it also
allows a player to act more than once in the decision process, with the infor-
mation sets being different at different levels of play. Such games are known
as multi-act games, and they will be discussed in Section 2.5. In the remaining
parts of this section we shall confine our analysis to single-act games—games in
which each player acts only once.
Remark 2.3 It is possible to extend Def. 2.5 so that it also allows for chance
moves by a third party called "nature". Such an extension will be incorporated
in Section 2.6. In the present section, and also in Section 2.5, we only consider
the class of zero-sum finite games which do not incorporate any chance moves.
Definition 2.6 Let Nl denote the class of all information sets of Pi, with a
```
typical element designated as jf. Let Ul; denote the set of alternatives of Pi
```
```
at the nodes belonging to the information set rf. Define U1 = (JUli where the
```
union is over rf € Nl. Then, a strategy 7* for Pi is a mapping from Nl into
U1, assigning one element in Ul for each set in Nl, and with the further property
```
that 7l(?7z) €E Uli for each rf € Nl. The set of all strategies for Pi is called his
```
```
strategy set (space), and it is denoted by Tl.
```
Remark 2.4 For the starting player, it is convenient to take Nl = a singleton,
to eliminate any possible ambiguity.
```
Example 2.1 In the extensive form of Fig. 2.4(b), P2 has two information
```
```
sets (N2 = {(node 1), (node 2)}) and three alternatives (U2 = {L, M, R}).
```
Therefore, he has nine possible strategies which have actually been listed earlier
during the normalization phase of this extensive form. Here, by an abuse of
```
notation, r/2 — {u1}.
```
7For tree structures in which an information set contains more than one node from any
```
directed path in the tree or for games in which the underlying graph contains cycles (and
```
```
hence is not a tree anymore), the reader is referred to subsection 2.7.1.
```
TWO-PERSON ZERO-SUM FINITE GAMES 39
```
The quantity J(71*,72*) is known as the saddle-point value of the zero-sum
```
game.
Since a saddle-point equilibrium is defined in terms of the normal form of a
game, a direct method for solving two-person single-act games in extensive form
would be first to convert them into equivalent normal form and solve for the
saddle-point solution of the resulting matrix game, and then to interpret this
solution in terms of the original game through the one-to-one correspondences
that exist between the rows and columns of the matrix and strategies of the
players. This method has already been illustrated in this section in conjunction
```
with the solution of the zero-sum game of Fig. 2.4(b), and as we have observed
```
there, a major disadvantage of such a direct approach is that one has to list
down all possible strategies of the players and consider a rather high dimensional
matrix game, especially when the second-acting player has several information
sets.
An alternative to this approach exists, which makes direct use of the ex-
tensive form description of the zero-sum game. This, in fact, is a recursive
procedure, and it obtains the solution without necessarily considering all strat-
egy combinations, especially when the second-acting player has more than one
information set. The strategies not considered in this procedure are the dom-
inated ones, and this has actually been observed earlier in this section when
```
we employed the technique to arrive at the equilibrium strategy pair (2.29a)-
```
```
(2.29b) for the zero-sum game of Fig. 2.4(b). Before providing a general outline
```
of the steps involved in this derivation, let us first consider another, somewhat
more elaborate, example which is the zero-sum game whose extensive form is
depicted in Fig. 2.5.
To determine the saddle-point strategies associated with this zero-sum game,
```
we first note that if u1 = R, then P2 should choose ^2(R) = L, to result in a loss
```
40 T. BA§AR AND G. J. OLSDER
```
Definition 2.7 A strategy 7Z(-) of Pi is said to be constant if its value does
```
not depend on its argument rf.
Saddle-point equilibria of single-act games
The pure-strategy equilibrium solution of single-act two-person zero-sum games
in extensive form can be defined in a way analogous to Def. 2.1, with appro-
```
priate notational modifications. To this end, let J(71,72) denote the numerical
```
outcome of a game in extensive form, interpreted as the loss incurred to PI
when PI and P2 employ the strategies 71 € F1 and 72 G F2, respectively. This
loss function in fact defines the correspondences between different ordered strat-
egy pairs and outcomes, and thus describes the equivalent normal form of the
zero-sum game. Then, we have the following.
```
Definition 2.8 A pair of strategies {71* e F1^2* e F2} is in saddle-point
```
equilibrium if the following set of inequalities is satisfied for all^1 e F1, 72 e F2:
TWO-PERSON ZERO-SUM FINITE GAMES 41
Figure 2.5: An extensive form that admits a saddle-point equilibrium in pure
strategies.
of 2 to PI. If PI picks L or M, however, P2 cannot differentiate between these
```
two, since they belong to the same information set; and hence now a matrix
```
game has to be solved for that part of the tree. The equivalent normal form is
which clearly admits the unique saddle-point solution ul = M, u2 = R, yielding
a cost of 1 for PI. Hence, in the actual play of the game, PI will always
play u1 = 71 = M, and P2's optimal response will be u2 — R. However, the
saddle-point strategy of P2 is
```
since Nl is a singleton. The reader can now easily check that the pair (2.31)-
```
```
(2.32) satisfies the saddle-point inequalities (2.30).
```
The preceding analysis now readily suggests a method of obtaining the pure-
strategy saddle-point solution of single-act zero-sum games in extensive form,
provided that it exists. The steps involved are as follows.
Derivation of pure-strategy saddle-point solutions of single-act games
in extensive form
```
(1) For each information set of the second-acting player, solve the correspond-
```
```
ing matrix game (assuming that each such matrix game admits a pure-
```
```
strategy saddle point). For the case in which an information set is a
```
```
and not the constant strategy 72(u1) = R, since if P2 adopts this constant
```
strategy, then PI can switch to R to a loss of 0 instead of 1. The strategy for
```
PI that is in equilibrium with (2.31) is still
```
singleton, the matrix game involved is a degenerate one with one of the
```
players having a single choice—but such (essentially one-person) games
```
always admit well-defined pure-strategy solutions.
```
(2) Record the saddle-point value of each of these matrix games. If PI is the
```
starting player, then the lowest of these is the saddle-point value of the
```
extensive form; if P2 is the starting player, then the highest one is.
```
```
(3) The saddle-point strategy of the first-acting player is his saddle-point strat-
```
egy obtained for the matrix game whose saddle-point value corresponds
to the saddle-point value of the extensive game.
```
(4) For the second-acting player, the saddle-point strategy is comprised of his
```
saddle-point solutions in all the matrix games considered, by appropriately
identifying them with each of his information sets.
We leave it to the reader to verify that any pair of equilibrium strategies
obtained by following the preceding procedure does, indeed, constitute a saddle-
```
point strategy pair that satisfies (2.30). The following conclusions can now
```
readily be drawn.
Proposition 2.1 A single-act zero-sum two-person finite game in extensive
```
form admits a (pure-strategy) saddle-point solution if, and only if, each matrix
```
game corresponding to the information sets of the second-acting player has a
saddle point in pure strategies.
Proposition 2.2 Every single-act zero-sum two-person finite game in extensive
form, in which the information sets of the second-acting player are singletons,8
admits a pure-strategy saddle-point solution.
If the matrix game corresponding to an information set does not admit a
saddle-point solution in pure strategies, then it is clear that the strategy spaces
of the players have to be enlarged, in a way similar to the introduction of mixed
strategies in Section 2.2 within the context of matrix games. To pave the way for
such an enlargement in the case of games in extensive form, let us first consider
the single-act two-person zero-sum game whose extensive form is depicted in
Fig. 2.6. Here, P2 is the second-acting player and he has two information sets.
If his opponent picks R, then he can observe that choice, and his best response
```
(strategy) in that case is L, yielding an outcome of 1. If PI chooses L or M,
```
however, then P2 is ignorant about that choice, since both of these nodes are
included in the same information set of P2. The relevant matrix game, in this
case, is
8The node corresponding to each such information set is known as the "state" of the game
```
at that level (stage) of play. Games of this type are known as perfect information games.
```
42 T. BA§AR AND G. J. OLSDERTWO-PERSON ZERO-SUM FINITE GAMES 43
```
Figure 2.6: A game in extensive form that admits a mixed (behavioral)-strategy
```
saddle-point solution.
```
which is the matrix game (2.20) considered earlier in Section 2.3, and it is
```
```
known to admit the mixed saddle-point solution (y*L = |,y^ = f) for PI and
```
```
(z*L — \,ZR = |) for P2, yielding an average outcome of |. Since | < 1, it is
```
clear that PI will prefer to stay on that part of the tree and thus stick to the
strategy
```
since he also has to consider the possibility of Pi playing R (by definition of a
```
```
strategy). The pair (2.33a)-(2.33b) now yields the average outcome of |.
```
```
In order to declare (2.33a)-(2.33b) as the equilibrium solution pair of the
```
extensive form of Fig. 2.6, we have to specify the spaces in which it possesses
such a property. To this end, we first note that PI has three possible pure
```
strategies, 7^ = L, ^\ = M and 73 = R, and in view of this, (2.33a) is indeed a
```
```
mixed strategy for PI (see Def. 2.2). For P2, however, there exist four possible
```
pure strategies, namely
and
Hence, a mixed strategy for P2 is a probability law according to which these
```
four strategies will be "mixed" during the play of the game, and (2.33b) provides
```
which is "mixed" in nature. P2's equilibrium strategy will then be
otherwise ,
otherwise ,
otherwise .
and thus constitutes a saddle-point solution within the class of mixed strategies.
However, the class of mixed strategies is, in general, an unnecessarily large
class for the second-acting player. In the extensive form of Fig. 2.6, for example,
since P2 can tell exactly whether PI has played R or not, there is no reason
why he should mix between his possible actions in case of u1 = R and his
actions otherwise. That is, P2 can do equally well by considering probability
distributions only on the alternatives belonging to the information set including
```
two nodes, and (2.33b) is, in fact, that kind of a strategy. Such strategies are
```
known as behavioral strategies, a precise definition of which is given below.
Definition 2.99 For a given two-person zero-sum finite game in extensive form,
let Yjji denote the set of all probability distributions on U\ , where the latter is
the set of all alternatives of PI at the nodes belonging to the information set
7jl. Analogously, let Z^ denote the set of all probability distributions on U\.
Further define Y = U^iY^i, Z = \JN2ZT12. Then, a behavioral strategy 7* for
```
Pi is a mapping from the class of all his information sets (Nl) into Y, assigning
```
```
one element in Y for each set in Nl, such that 71(//1) G Y^i for each n1 G TV1.
```
A typical behavioral strategy 72 for P2 is defined, analogously, as a restricted
mapping from N2 into Z. The set of all behavioral strategies for Pi is called his
behavioral strategy set, and it is denoted by P.
Remark 2.5 Every behavioral strategy is a mixed strategy, but every mixed
strategy is not necessarily a behavioral strategy, unless the player has a single
information set. D
For games in extensive form, the concept of a saddle-point equilibrium in
behavioral strategies can now be introduced as in Def. 2.8, but with some slight
```
obvious modifications. If .7(71,72) denotes the average outcome of the game
```
```
resulting from the strategy pair {7* G f1,72 G f2 ), then we have the following.
```
```
Definition 2.10 A pair of strategies J71* G F1,^2* G F2} is said to constitute
```
a saddle point in behavioral strategies for a zero-sum game in extensive form,
if the set of inequalities
9This definition is valid not only for single-act games, but also for multi-act games which
are treated in section 2.5.
44 T. BA§AR AND G. J. OLSDER
one such probability distribution which assigns a weight of | to the t pure
strategy and a weight of | to the third one.
```
We have thus verified that both (2.33a) and (2.33b) are well-defined mixed
```
```
strategies for PI and P2, respectively. Now, if J^1,^2) denotes the average
```
outcome of the game when the mixed strategies 7* and 72 are adopted by PI
```
and P2, respectively, the reader can easily check that the pair (2.33a)-(2.33b)
```
indeed satisfies the relevant set of saddle-point inequalities
```
is satisfied for all 7* € F1, 72 <E F2. The quantity J(71*,72*) is known as the
```
saddle-point value of the game in behavioral strategies.
For a given single-act zero-sum game in extensive form, derivation of a
behavioral-strategy saddle-point solution basically follows the four steps out-
lined earlier for pure-strategy saddle points, with the only difference being that
this time mixed equilibrium solutions of the matrix games are also allowed in
this process. The reader should note that this routine extension has already
been illustrated within the context of the extensive form of Fig. 2.6.
```
Since every matrix game admits a saddle point in mixed strategies (cf. Corol-
```
```
lary 2.3), and further, since the above derivation involves only a finite number of
```
comparisons, it readily follows that every single-act zero-sum game in extensive
```
form admits a saddle point in behavioral (or, equivalently, mixed) strategies.
```
We thus conclude this section with a precise statement of this property, and by
recording another immediate feature of the saddle-point solution in extensive
games.
Corollary 2.4 In every single-act two-person zero-sum finite game,
```
(i) there exists at least one saddle point in behavioral strategies,
```
```
(ii) if there exist more than one saddle-point solution in behavioral strategies,
```
then they possess the ordered interchangeability property, and
```
(Hi) every saddle point in behavioral strategies is also a saddle point in the
```
larger class of mixed strategies.
We should note that a finite single-act game in extensive form might also
```
admit a mixed-strategy saddle point (which is not behavioral) in addition to a
```
behavioral saddle point. The corresponding average saddle-point values, how-
ever, will all be the same, because of the interchangeability property, and there-
fore we may confine our attention only to behavioral strategies, particularly in
```
view of Corollary 2.4(i).
```
2.5 Extensive Games: Multi-Act Games
Zero-sum games in which at least one player is allowed to act more than once
and with possibly different information sets at each level of play, are known
as multi-act zero-sum games.10 In the study of extensive forms of such games,
and in accordance with Def. 2.5, we consider the case when the number of
alternatives available to a player at each information set is finite. This leads to
a finite tree structure, incorporating possibly different information sets for each
10Such games are also referred to as "multi-stage zero-sum games" in the literature. They
may be considered also as "dynamic games", since the information sets of a player do, in
general, have a dynamic character, providing information concerning the past actions of the
players. In this respect, the single-act games of the previous section may also be referred to
as "dynamic games" if the second-acting player has at least two information sets.
TWO-PERSON ZERO-SUM FINITE GAMES 45
```
player at each level of play. A (pure) strategy of a player is again defined as a
```
restricted mapping from the collection of his information sets to the finite set
of all his alternatives, exactly as in Def. 2.6, and the concept of a saddle-point
solution then follows the lines of Def. 2.8. This general framework, however,
suppresses the dynamic nature of the decision process and is not constructive as
far as the saddle-point solution is concerned. An alternative to this set-up exists,
which brings out the dynamic nature of the problem, for an important class of
multi-act zero-sum games known as "feedback games", a precise definition of
which is given below.
Definition 2.11 A multi-act two-person zero-sum game in extensive form is
called a two-person zero-sum feedback game in extensive form, if
```
(i) at the time of his action, each player has perfect information concerning
```
the current level of play, i.e., no information set contains nodes of the tree
belonging to different levels of play,
```
(ii) information sets of the first-acting player at every level of play are single-
```
tons, and the information sets of the second-acting player at every level of
play are such that none of them include nodes corresponding to branches
emanating from two or more different information sets of the other player,
i.e., each player knows the state of the game at every level of play.
Figure 2.7: Two multi-act zero-sum games which are not feedback games.
Figure 2.7 displays two multi-act zero-sum games in extensive form, which are
```
not feedback games, since the first one violates (i) above, and the second one
```
```
violates (ii). The one displayed in Fig. 2.8, however, is a feedback game.
```
Now, let the number of levels of play in a zero-sum feedback game be K.
Then, a typical strategy 7* of Pi in such a game can be viewed as composed
```
of K components (7},... , 7/f)> where 7J stands for the corresponding strategy
```
of P^ at his j'th level of action. Moreover, because of the nature of a feedback
game, 7] can be taken, without any loss of generality, to have as its domain
only those information sets of Pi which pertain to the jth level of play. Let us
denote the collection of all such strategies for Pi at level j by H. Then, we
```
can rewrite the saddle-point inequality (2.30), for a feedback game in extensive
```
46 T. BA§AR AND G. J. OLSDERTWO-PERSON ZERO-SUM FINITE GAMES 47
Figure 2.8: A zero-sum feedback game in extensive form.
form, as
Definition 2.12 For a two-person zero-sum feedback game in extensive form
```
with K levels of play, let {ll*,l2*} be a pair of strategies satisfying (2.35)
```
```
and (2.36) for all 7] e Fj, i = 1,2; j = !,...,#. Then, {71#,72*} is said
```
```
to constitute a pair of (pure) feedback saddle-point strategies for the feedback
```
game.
```
Proposition 2.3 Every pair {71*,72*} that satisfies the set of inequalities (2.36)
```
```
also satisfies the pair of saddle-point inequalities (2.35). Hence, the requirement
```
```
for satisfaction of (2.35) in Def. 2.12 is redundant, and it can be deleted without
```
any loss of generality.
```
which is to be satisfied for all 7J € F*;, i = 1,2; j = 1 , . . . , K. Such a decompo-
```
sition of the strategies of the players now allows a recursive procedure for the
```
determination of the saddle-point solution {71*,72*} of the feedback game, if
```
```
we impose some further restrictions on the pair {71*, 72*}. Namely, let the pair
```
```
J71*,72*} also satisfy (recursively) the following set of K pairs of inequalities
```
```
foraU7JGrj,t = l , 2 ; j = l , . . . , K :
```
since they characterize the recursive dynamic programming conditions for a
```
one-person maximization problem (Bellman (1957)). This then establishes the
```
```
validity of the LHS inequality of (2.35) under satisfaction of (2.36). Validity of
```
```
the RHS inequality of (2.35) can likewise be verified by considering the RHS
```
```
inequalities of (2.36).
```
The appealing feature of a feedback saddle-point solution is that it can
be computed recursively, by solving a number of static games at each level
of play. Details of this recursive procedure readily follow from the set of in-
```
equalities (2.36) and the specific structure of the information sets of the game,
```
as it is described below.
```
A recursive procedure to determine the (pure) feedback saddle-point
```
strategies of a feedback game
```
(1) Starting at the last level of play, K, solve each single-act game corre-
```
sponding to the information sets of the first-acting player at that level K.
```
The resulting (pure) saddle-point strategies are the saddle-point strate-
```
```
gies {7^, 7/f } sought for the feedback game at level K. Record the value
```
of each single-act game corresponding to each of the information sets of
the first-acting player, and assign these values to the corresponding nodes
```
(states) of the extensive form at level K.
```
```
(2) Cross out the Kth level of play, and consider the resulting (K — 1) level
```
feedback game. Now solve the last level single-act games of that extensive
```
form, with the resulting saddle-point strategies denoted by {7J£_i, 7J£li},
```
and repeat the remaining deeds of step 1 with K replaced by K — I.
```
(K) Cross out the second level of play, and consider the resulting single-act
```
```
game in extensive form. Denote its saddle-point strategies by {7i*,7i*},
```
48 T. BAS.AR AND G. J. OLSDER
```
Proof. Replace 7 i , - - - , 7 x - i on the LHS inequalities of the set (2.36) by
```
```
7i*,..., 7#_1, respectively. Then, the LHS inequalities of (2.36) read
```
for all 7? € F^, j = 1,..., K. But this set of inequalities reduces to the single
inequality
TWO-PERSON ZERO-SUM FINITE GAMES 49
and its value by V. Then, the original feedback game in extensive form ad-
```
mits a saddle-point solution {(71*, • . - , 7# ), (7i*, • • • , 7x)} and has value
```
J* = V.
Example 2.2 As an illustration of the above procedure, let us consider the
2-level feedback game whose extensive form is depicted in Fig. 2.8. At the
```
last (second) level, there are four single-act games to be solved, which are all
```
equivalent to matrix games. From left to right, we have the normal forms
with the encircled entries indicating the location of the saddle-point solution in
each case. The saddle-point strategies 7^* and 7!* are thus given by
where u\ denotes the choice of Pi at level 1. It is noteworthy that the saddle-
point strategy of P2 is a constant at this level of play. Now, crossing out the
second level of play, but retaining the encircled quantities in the four matrix
games as costs attached to the four nodes, we end up with the equivalent single-
act game
whose normal form is
admitting the unique saddle-point solution
and with the value being V = I, which is also the value of the original 2-level
feedback game. The feedback saddle-point strategies of this feedback game are
```
now given by (2.37a)-(2.37b), with the actual play dictated by them being
```
otherwise,
50 T. BAS.AR AND G. J. OLSDER
If a feedback game in extensive form does not admit a feedback saddle point
in pure strategies, then it could still admit a saddle point in accordance with
```
inequalities (2.35). If that is not possible either, then one has to extend the
```
strategy spaces of the players so as to include also behavioral strategies. Defi-
nition of a behavioral strategy in multi-act games is precisely the one given by
```
Def. 2.9 and the relevant saddle-point inequality is still (2.34), of course un-
```
der the right kind of interpretation. Now, in a zero-sum feedback game with
K levels of play, a typical behavioral strategy 7* of Pi can also be viewed as
```
composed of K components (7i, • • • ,7#), where 7J stands for the correspond-
```
ing behavioral strategy of Pi at his jfth level of play—a feature that follows
readily from Defs. 2.9 and 2.11. By the same token, 7] can be taken, without
any loss of generality, to have as its domain only those information sets of Pi,
which pertain to the jth level of play. Let us now denote the collection of all
such behavioral strategies for Pi at level j by F*. Then, since the saddle-point
```
inequality (2.34) corresponding to a K-level feedback game can equivalently be
```
written as
it follows that a recursive procedure can be devised to determine the behavioral
saddle-point strategies if we further require satisfaction of a set of inequalities
```
analogous to (2.36). Such a recursive procedure basically follows the K steps
```
outlined in this section within the context of pure feedback saddle-point so-
lutions, with the only difference being that now also behavioral strategies are
allowed in the equilibrium solutions of the single-act games involved. Since this
is a routine extension, we do not provide details of this recursive procedure here,
but only illustrate it in the sequel in Example 2.3.
It is worth noting at this point that since every single-act two-person zero-
```
sum game admits a saddle-point solution in behavioral strategies (cf. Corol-
```
```
lary 2.4), every feedback game will also admit a saddle-point solution in behav-
```
ioral strategies. More precisely, we have the following proposition.
Proposition 2.4 Every two-person zero-sum feedback game, which has an ex-
tensive form comprised of a finite number of branches, admits a saddle point in
behavioral strategies.
```
Example 2.3 As an illustration of the derivation of behavioral saddle point(s)
```
in feedback games, let us reconsider the game of Fig. 2.8 with two modifications:
The outcomes of the game corresponding to the paths u\ = R, u\ = L, u\ = L,
11% = L and u\ = R, uf = L, u\ = L, u^ = R, respectively, are now taken
as 0 and —1, respectively. Then, the four single-act games to be solved at the
```
last (second) level are equivalent to the matrix games (respectively, from left to
```
```
right) which all admit pure-strategy solutions, as indicated.
```
TWO-PERSON ZERO-SUM FINITE GAMES 51
```
The saddle-point strategies at this level are, in fact, as given by (2.37a).
```
Now, crossing out the second level of play, but retaining the encircled quan-
tities in the four matrix games as costs attached to the four nodes, we end up
with the equivalent single act game
whose normal form
admits a mixed saddle-point solution:
Furthermore, the average saddle-point value is Vm — 1/2. Since, within the
```
class of behavioral strategies at level 2, the pair (2.37a) can be written as
```
otherwise
```
(2.40) and (2.41) now constitute a set of behavioral saddle-point strategies for
```
the feedback game under consideration, leading to a behavioral saddle-point
value of J* = 1/2.D
For multi-act games which are not feedback games, neither Prop. 2.4 nor
the recursive procedures discussed in this section are, in general, applicable,
and there is no general procedure that would aid in the derivation of pure or be-
havioral saddle-point solutions, even if they exist. If we allow for mixed-strategy
solutions, however, it directly follows from Corollary 2.3 that an equilibrium so-
lution exists, since every multi-act zero-sum finite game in extensive form has an
equivalent normal form which is basically a matrix game with a finite number
of rows and columns. More precisely, we have the following.
Proposition 2.5 Every two-person zero-sum multi-act finite game in extensive
form admits a saddle-point solution in mixed strategies.
52 T. BA§AR AND G. J. OLSDER
The following example now illustrates derivation of mixed-strategy saddle-
point equilibria in multi-act games, which basically involves solution of an ap-
propriate matrix game.
Example 2.4 Let us reconsider the extensive form of Fig. 2.8, but with a single
information set for each player at each level of play, as depicted in Fig. 2.9.
Figure 2.9: A zero-sum multi-act game that does not admit a behavioral saddle-
point solution.
```
Each player has four possible ordered choices (LL,LR,RL,RR), and thus
```
the equivalent normal form of the matrix game is
which admits the unique mixed saddle-point solution
```
with the saddle-point value in mixed strategies being Vm = 7/5. (The reader
```
```
can check this result by utilizing Thm. 2.5.)D
```
Even though every zero-sum multi-act game in extensive form admits a sad-
dle point in mixed strategies, such a result is no longer valid within the class of
behavioral strategies, unless the mixed saddle-point strategies happen to be be-
havioral strategies. A precise version of this property is given below in Prop. 2.6.
Proposition 2.6 Every saddle-point equilibrium of a zero-sum multi-act game
in behavioral strategies also constitutes a saddle-point equilibrium in the larger
class of mixed strategies for both players.
```
11 Note that y\ and j/2 (and also z\ and 23, in the sequel) are picked independently, and
```
hence do not necessarily add up to 1.
TWO-PERSON ZERO-SUM FINITE GAMES 53
```
Proof. Assume, to the contrary, that the behavioral saddle-point value (J*)
```
```
is not equal to the mixed saddle-point value (Jm), where the latter always
```
exists by Prop. 2.5, and consider the case J* < Jm. This implies that PI does
better with his behavioral saddle-point strategy than with his mixed saddle-
point strategy. But this is not possible, since the set of mixed strategies is a
much larger class. Hence, we can only have J* > Jm. Now, repeating the same
argument on this strict inequality from P2's point of view, it follows that only
the equality J* = Jm can hold.D
Proposition 2.6 now suggests a method for derivation of behavioral saddle-
point strategies of zero-sum multi-act games in extensive form. First, determine
all mixed saddle-point strategies of the equivalent normal form, and then inves-
tigate whether there exists, in that solution set, a pair of behavioral strategies.
If we apply this method on the extensive form of Example 2.4, we first note
that it admits a unique saddle-point solution in mixed strategies, which is the
```
one given by (2.42a)-(2.42b). Now, since every behavioral strategy for PI is in
```
the form
for some 0 < yi ^ I j O ^ ^ ^ l ? 1 1 and every behavioral strategy for P2 is given
by
```
for some 0 < z\ < 1, 0 < z^ < 1, it follows (by picking y\ = 1, y^ = 3/5)
```
```
that (2.42a) is indeed a behavioral strategy for PI; however, (2.42b) is not a
```
behavioral strategy for P2.
Hence the conclusion is that the extensive form of Fig. 2.9 does not admit
a saddle point in behavioral strategies, from which we deduce the following
corollary to Prop. 2.6.
Corollary 2.5 A zero-sum multi-act game does not necessarily admit a saddle-
point equilibrium in behavioral strategies, unless it is a feedback game.
We now introduce a special class of zero-sum multi-act games known as
open-loop games, in which the players do not acquire any dynamic information
throughout the decision process, and they only know the level of play that
corresponds to their action. More precisely, see below.
Definition 2.13 A multi-act game is said to be an open-loop game, if, at each
level of play, each player has a single information set.
The multi-act game whose extensive form is depicted in Fig. 2.9 is an open-
loop game. Such games can be viewed as one extreme class of multi-act games in
which both players are completely ignorant about the evolution of the decision
process. Feedback games, on the other hand, constitute another extreme case
```
(with regard to the structure of the information sets) in which both players have
```
full knowledge of their past actions. Hence, the open-loop and feedback versions
of the same multi-act game could admit different saddle-point solutions in pure,
behavioral or mixed strategies. A comparison of Examples 2.2 and 2.4, in fact,
provides a validation of the possibility of such a situation: the extra information
embodied in the feedback formulation actually helps PI in that case. It is of
course possible to devise examples in which extra information in the above sense
instead makes P2 better off or does not change the value of the game at all.
A precise condition for the latter property to hold is given below in Prop. 2.7
whose proof is in the same spirit as that of Prop. 2.6 and is thus left as an
exercise for the reader.
```
Proposition 2.7 A zero-sum open-loop game in extensive form admits a (pure-
```
```
strategy) saddle-point solution if, and only if, its feedback version admits con-
```
stant saddle-point strategies at each level of play.12
Prior and delayed commitment models
Since there is no extra information available to the players throughout the du-
ration of an open-loop game, the players can decide on their actions at the
very beginning, and then there is no real incentive for them to renege dur-
ing the actual play of the game. Such games in which decisions are made at
the outset, with no incentive to deviate from them later, are known as "prior
commitment'''' games. Mixed saddle-point strategies can then be considered to
constitute a reasonable equilibrium solution within such a framework. Feedback
games, on the other hand, are of "delayed commitment" type, since each player
could wait until he finds out at what information set he really is, and only then
announces his action. This then implies that mixed strategies are not the "right
type" of strategies to be considered for such games, but behavioral strategies
are. Since feedback games always admit a saddle point in behavioral strategies
```
(cf. Prop. 2.3), equilibrium is again well defined for this class of games.
```
There are also other classes of multi-act games of the delayed commitment
type, which might admit a saddle point in behavioral strategies. One such class
of multi-act zero-sum games are those in which players recall their past actions
but are ignorant about the past actions of their opponent. As an illustrative
example, let us reconsider the extensive form of Fig. 2.9, but under the present
set-up. The information sets of the players would then look as displayed in
12It is implicit here that these constant strategies might differ from one level to another and
```
that they might not be feedback saddle-point strategies, i.e., they might not satisfy (2.36).
```
54 T. BA§AR AND G. J. OLSDERTWO-PERSON ZERO-SUM FINITE GAMES
Figure 2.10: A multi-act game of the delayed commitment type that admits a
behavioral saddle-point solution.
55
Fig. 2.10. Now, permissible behavioral strategies of the players in this game are
of the form
where 0 < y i < l , 0 < Z i < l , i = l,2,3, and the average outcome of the game
corresponding to these strategies can be determined to be
The reader can now check that the values
satisfy the inequalities
```
for all 0 < yi < 1, 0 < Zi < 1, i = 1,2,3; and hence, when they are used
```
```
in (2.43a)-(2.43d); the resulting behavioral strategies constitute a saddle-point
```
solution for the multi-act game under consideration, with the average value
being 7/5. We thus observe that a modified version of the extensive form of
Fig. 2.9, in which the players recall their past actions, does admit a saddle-
point solution in behavioral strategies.
Derivation of the behavioral saddle-point solutions of multi-act games of
the delayed commitment type involve, in general, solution of a pair of saddle-
```
point inequalities of the type (2.44). For the present case (i.e., for the specific
```
```
example of Fig. 2.10) it so happens that F admits a saddle-point solution which,
```
```
in turn, yields the behavioral strategies sought; however, in general it is not
```
guaranteed that it will admit a well-defined solution. The kernel F will always
be a continuous function of its arguments which belong to a convex compact set
```
(in fact a simplex), but this is not sufficient for existence of a saddle point for
```
```
F (see Section 4.3). The implication, therefore, is that there will exist multi-
```
act games of the delayed commitment type which do not admit a saddle-point
solution in pure or behavioral strategies.
Randomized strategies
As observed above, and also in Example 2.4, not every multi-act finite game
would admit a behavioral saddle-point solution, but whenever it exists, one
method for obtaining the behavioral saddle-point policies would be first to con-
```
struct the average cost of the game on the set of behavioral strategies (such
```
```
as the kernel F defined prior to (2.44)), and then find the saddle point of that
```
```
kernel in pure strategies. The (pure) strategies in this case are in fact the proba-
```
bility weights attached to different actions at each information set, which belong
to closed and bounded subsets of finite dimensional spaces. Let us denote these
subsets for PI and P2 by Y and Z, respectively. For the game of Example 2.4,
for instance, both Y and Z are positive unit squares, whereas for the multi-act
game of Fig. 2.10, they are positive unit cubes. In all cases, the kernel F will be
continuous on Y x Z, and Y and Z will further be convex. We shall study the
derivation of saddle-point equilibria of such continuous-kernel games in Chap-
ter 4, where we will see that F does not necessarily admit a pure-strategy saddle
point ou Y x Z unless it is convex-concave. Clearly, neither F defined earlier
in conjunction with the multi-act game of Fig. 2.10, nor the one that could be
constructed for the game of Example 2.4, is convex-concave,13 which is one of
the reasons why these two games do not admit behavioral saddle points. The
theory of Chapter 4, and particularly of Section 4.3, will tell us, however, that
such continuous-kernel games defined on closed-bounded convex sets will admit
mixed saddle-point equilibria, where mixed strategies in this case are probabil-
ity distributions on Y and Z. For present application, since the elements of Y
and Z already correspond to behavioral strategies for the original finite game
13We leave it as an exercise for the reader to construct the kernel F for the game of Ex-
```
ample 2.4, and show that it is not convex-concave (i.e., convex over Y and concave over
```
```
Z).
```
56 T. BA§AR AND G. J. OLSDER
```
(themselves being special types of mixed strategies), and to avoid confusion,
```
the further mixed strategies defined on Y and Z are often called randomized
```
strategies (for the original finite game). Hence, the message here is that every
```
finite multi-act game admits a saddle point in randomized strategies—a full
verification of which is postponed until Chapter 4. Some further discussion on
randomized strategies, in this chapter, can be found in subsection 2.7.1.
2.6 Zero-Sum Games with Chance Moves
In this section, we briefly discuss the class of two-person zero-sum finite games
which also incorporate chance moves. In such decision problems, the final out-
```
come is determined not only by the decisions (actions) of the two players, but
```
also by the outcome of a chance mechanism whose odds between different al-
ternatives are known a priori by both players. One can also view this situation
as a three-player game wherein the third player, commonly known as "nature",
has a fixed mixed strategy.
Figure 2.11: A zero-sum single-act game in extensive form that incorporates a
chance move.
As an illustration of such a zero-sum game, let us consider the tree structure
```
of Fig. 2.11, where N (nature) picks L w.p. 1/3 and R w.p. 2/3. Not knowing the
```
realized outcome of the chance mechanism, the players each decide on whether
to play L or R. Let us now assume that such a game is played a sufficiently
large number of times and the final outcome is determined as the arithmetic
mean of the outcomes of the individual plays. In each of these plays, the matrix
of possible outcomes is either
TWO-PERSON ZERO-SUM FINITE GAMES 5758 T. BA§AR AND G. J. OLSDER
or
the former occurring w.p. 1/3 and the latter w.p. 2/3. Each of these matrix
games admits a pure-strategy saddle-point solution as indicated. If N's choice
were known to both players, then P2 would always play R, and PI would
play R if JV's choice is L and play L otherwise. However, since Af's choice is
not known a priori, and since the equilibrium strategies to be adopted by the
players cannot change from one individual play to another, the matrix game of
real interest is the one whose entries correspond to the possible average outcomes
of a sufficiently large number of such games. This matrix game can readily be
```
obtained from (2.45a) and (2.45b) to be
```
which admits, as indicated, the unique saddle-point solution
with an average outcome of 1. This solution is, in fact, the only reasonable
equilibrium solution of the zero-sum single-act game of Fig. 2.11, since it also
uniquely satisfies the pair of saddle-point inequalities
```
and ^(T1,!2) denotes the expected (average) outcome of the game correspond-
```
```
ing to a pair of permissible strategies JT1^2} with the expectation operation
```
taken with respect to the probability weights assigned a priori to different
choices of N. For this problem, the correspondences between J and different
```
choices of (T1^2) are in fact displayed as the entries of matrix (2.46a).
```
It is noteworthy that P2's equilibrium strategy is L for the extensive form
```
of Fig. 2.11 (as discussed above), whereas if Ws choice were known to both
```
```
players (Fig. 2.12), he would have chosen the constant strategy R, as shown
```
earlier. Such a feature might, at first sight, seem to be counter-intuitive—after
all, when his information sets can distinguish between different choices of N,
```
P2 disregards N's choice and adopts a constant strategy (R), whereas in the
```
case when this information is not available to him, he plays quite a different
one. Such an argument is misleading, however, because of the following reason:
TWO-PERSON ZERO-SUM FINITE GAMES
Figure 2.12: A modified version of the zero-sum game of Fig. 2.11, with both
players having access to nature's actual choice.
In the tree structure of Fig. 2.12, Pi's equilibrium strategy is not a constant
```
but is a variant of JV's choice; whereas in Fig. 2.11 he is confined to constant
```
strategies, and hence the corresponding zero-sum game is quite a different one.
Consequently there is no reason for P2's equilibrium strategies to be the same
in both games, even if they are constants.
Remark 2.6 It should be noted that the saddle-point solution of the zero-sum
```
game of Fig. 2.12 also satisfies a pair of inequalities similar to (2.47), but this
```
time each player has four permissible strategies, i.e., F* is comprised of four
```
elements. It directly follows from (2.45a)-(2.45b) that the average saddle-point
```
```
value, in this case, is (1/3)(1) 4- (2/3)(l/4) = 1/2, which is less than the average
```
equilibrium value of the zero-sum game of Fig. 2.11. Hence, the increase in
information with regard to the action of N helps PI to reduce his average losses
but works against P2.D
The zero-sum games of both Fig. 2.11 and Fig. 2.12 are in extensive form,
but they do not fit the framework of Def. 2.5 since the possibility of chance
moves was excluded there. We now provide below a more general definition of
a zero-sum game in extensive form that also accounts for chance moves.
Definition 2.14 An extensive form of a two-person zero-sum finite game that
also incorporates chance moves is a tree structure with
```
(i) a specific vertex indicating the starting point of the game,
```
```
(ii) a pay-off function assigning a real number to each terminal vertex of the
```
```
tree, which determines the pay-off (respectively, loss) to P2 (respectively,
```
```
Pi) for each possible set of actions of the players together with the possible
```
choices of nature,
```
(Hi) a partition of the nodes of the tree into three player sets (to be denoted by
```
```
N1 and N2 for PI and P2, respectively, and by N° for nature),
```
59
```
(iv) a probability distribution, defined at each node of N°, among the imme-
```
```
diate branches (alternatives) emanating from this node,
```
```
(v) a subpartition of each player set Nl into information sets {77]} such
```
that the same number of immediate branches emanates from every node
belonging to the same information set, and no node follows another node
in the same information set.
This definition of a zero-sum game in extensive form clearly covers both
single-act and multi-act games, and nature's chance moves can also be at inter-
mediate levels of play. For such a game, let us denote the strategy sets of PI
```
and P2 again as F1 and F2, respectively. Then, for each pair {71 e F1^2 G F2}
```
```
the pay-off function, ./(71, 72), is a random variable, and hence the real quantity
```
```
of interest is its expected value which we denote as J(71,72). The saddle-point
```
equilibrium can now be introduced precisely in the same fashion as in Def. 2.8,
with only J replaced by J. Since this formulation converts the extensive form
into an equivalent normal form, and since both F1 and F2 are finite sets, it
follows that the saddle-point equilibrium solution of such a game problem can
be obtained by basically solving a matrix game whose entries correspond to
```
the possible values of J(jl,j2). This has actually been the procedure followed
```
earlier in this section in solving the single-act game of Fig. 2.11, and it readily
extends also to multi-act games.
When a pure-strategy saddle point does not exist, it is possible to extend
the strategy sets so as to include behavioral or mixed strategies and to seek
equilibria in these larger classes of strategies. Proposition 2.5 clearly also holds
for the class of zero-sum games covered by Def. 2.14, and so does Prop. 2.6.
Existence of a saddle point in behavioral strategies is again not guaranteed, and
even if such a saddle point exists there is no systematic way to obtain it, unless
the players have access to nature's actions and the game possesses a feedback
```
structure (as in Def. 2.11). For games of this specific structure, one can use an
```
appropriate extension of the recursive derivation developed in Section 2.5 for
feedback games, which we do not further discuss here. Before concluding we
should mention that the notions of prior and delayed commitment introduced in
the previous section also fit within the framework of games with chance moves,
and so does the concept of randomized strategies, with the extensions being
conceptually straightforward.
2.7 Two Extensions
In this section we discuss two possible extensions of the extensive form descrip-
```
tion of finite zero-sum games—one where the restriction of (cf. Def. 2.5 and
```
```
Def. 2.14) "no nodes following another node in the same information set" is
```
```
dispensed with (see subsection 2.7.1), and another one where the tree structure,
```
```
viewed as a directed graph, is allowed to have cycles (see subsection 2.7.2).
```
60 T. BA§AR AND G. J. OLSDERTWO-PERSON ZERO-SUM FINITE GAMES 61
Figure 2.13: A zero-sum game in extensive form in which the tree enters an
information set twice.
2.7.1 Games with repeated decisions
We start this subsection with a specific example.
Example 2.5 Consider the zero-sum game in extensive form shown in Fig. 2.13.
```
PI has two information sets, one of which (r?J) contains two nodes from one
```
```
path in the tree; P2 has a single information set. If the probability of going
```
right from r?1 is given by yi, and going left by 1 - yi, and if y^ and z\ are
the corresponding probabilities for r]\ and rj\, respectively, then the behavioral
normal form is given by the kernel14
Now, the function K above is to be minimized by PI and maximized by P2.
```
Toward this end consider the max-min problem: maxZl minyi K(y\, zi), defined
```
```
on the unit square; see Fig. 2.14 for a sketch of the function K. Some calculus
```
```
leads to the result that this max-min is uniquely achieved for z\ — j3 = (170 4-
```
```
40\/6)/386 = 0.694. This behavioral strategy is optimal for P2. To obtain
```
```
14When there are two (or more) nodes following each other in the same information set, a
```
behavioral strategy is defined as one where the odds for different alternatives are the same
```
at each node (since a player cannot differentiate between two nodes belonging to the same
```
```
information set), but the random mechanism that generates the actions (moves) according
```
to these odds is independent from one node to another. Hence, moves from different nodes
belonging to the same information set can be considered as independent random variables
with identical probability distributions.
First minimizing this expression with respect to y^ £ [0,1] leads to the kernel
with a corresponding minimizing y-2 being
62 T. BA§AR AND G. J. OLSDER
Figure 2.14: The function K.
```
the optimal strategy of PI, consider the minimization problem min^ K(yi,0),
```
```
Awhere the minimum is attained at only two points: y\ — a = 5(1—/3)/12/3 = .184
```
and yi = 1. Hence it follows that PI should use two behavioral strategies,
```
viz. y(i) = (j/i = a,y2 = 1) and y(2) = (y\ = 1,3/2 = 0), in an appropriate mix.
```
To find this right "mix", consider the 2 x 2
The solution to this game is given by the following: choose z\ — \ with prob-
```
ability /3 and z\ = 0 with probability 1 — (3; choose y^ with probability
```
```
-l/(6a2 + So; - 7) = 0.17 and y(2) with probability 0.83. Thus it has been
```
established that PI mixes between two behavioral strategies, the result of which
is a randomized strategy.D
The preceding example has shown features which have also surfaced in the
discussion on randomized strategies in Section 2.5. In both cases the starting
point was a behavioral normal form which more generally has the form
```
where the exponents ii(k] and ji(k) are nonnegative integers. This form is some-
```
times called a multinomial. It is to be minimized by PI by choosing a vector
```
(y\i- • • i ym] on [0, l]m and to be maximized by P2 by choosing (zi,..., zn) on
```
[0, l]n. It is fairly easy to see that an arbitrary game in extensive form, of the
form described in this subsection, and without chance moves, has a behavioral
normal form which is a multinomial. The converse is also true: every multino-
mial is the behavioral normal form of an appropriately defined game in extensive
```
form, as shown by Alpern (1988). The theory of Chapter 4 will tell us that the
```
zero-sum game just described in the behavioral normal form does indeed have
a saddle point in mixed strategies. This means that the optimal solution of the
underlying game in extensive form exists in the class of randomized strategies.
```
In Alpern (1988), an even stronger result can be found: for the saddle-point
```
solution of finite games in randomized strategies, each player needs to average
over only a finite number of behavioral strategies.
Counterparts of these results do exist for games in extensive form which also
include chance moves. Since the basic features are the same as above, and no
new difficulties emerge, we will not treat them separately here.
2.7.2 Extensive forms with cycles
In this subsection we consider finite zero-sum dynamic games without chance
```
moves, in which the underlying (directed) graph is no longer a finite tree; it
```
will contain cycles, though the number of nodes remains finite. Moreover, it
is assumed that the players have perfect information, i.e., each information set
consists of a single node. If the players prefer to do so, such games can go on
```
forever by moving and staying within one (or more) cycle(s). Thus infinite play
```
is possible. It is of course possible to 'unfold' the underlying graph with cycles
so as to obtain a tree whose nodes are paths of the original game. However,
the resulting tree will be infinite and no useful general results for such trees are
known.
Games with terminal pay-off
We now first discuss the case when the game has only a terminal pay-off.
Definition 2.15 A two-person zero-sum deterministic graph game with termi-
nal pay-off consists of a finite directed graph of which the nodes are partitioned
into three sets:
```
(i) a specific vertex indicating the starting point of the game,
```
```
(ii) a pay-off function assigning a real number to each terminal vertex of the
```
```
graph, which determines the pay-off (respectively, loss) to P2 (respectively,
```
```
Pi) for each possible set of actions of the players,
```
```
(Hi) a partition of the nonterminal nodes of the graph into two sets (to be
```
```
denoted by Nl and N2 for PI and P2, respectively).
```
If the players choose their actions in such a way that the game does not end in
```
a finite number of steps (i.e., a terminal node is never reached), the pay-off will,
```
by definition, be zero. A pure strategy for Pi in the current context is a rule that
assigns to any path that ends in Nl one of its successor nodes deterministically.
TWO-PERSON ZERO-SUM FINITE GAMES 6364 T. BA§AR AND G. J. OLSDER
A stationary strategy is a pure strategy that only depends on the final node of
the path.
It is possible to devise an algorithm that yields a pair of stationary strategies
which form a saddle-point solution for the game. To introduce such an algo-
```
rithm, we assign a value V to each node x of the graph, with V(x] denoting
```
the value of the game if node x is chosen as the starting point of the game.
The set of nodes for which the value has already been calculated while running
```
the algorithm is denoted by H; note that H will be increasing as long as the
```
algorithm is running, until the entire graph has been covered by H. Let us also
```
introduce the notation a(x] to denote the set of all immediate successor nodes
```
of node x. Then, the algorithm proceeds as follows.
1. For all terminal nodes, the function V(x) equals the pay-off if the game
```
terminates at x; the set of all terminal nodes is denoted by H.
```
2. Consider all x £ H for which a(x] fl N1 6 H. The value for such an x
```
is defined as V(x] = mina(a;) V(a(x}}. The argument(s) for which this
```
minimum is achieved is then part of the saddle-point strategy for PI.
Increase H with new nodes for which a value has been obtained. Repeat
```
this as long as new nodes x G N1 are found such that o~(x) £ H.
```
3. Consider all x $. H for which a(x] n N2 € H. The value for such an x
```
is defined as V(x) = maxCT(x) V(a(x}}. The argument(s) for which this
```
maximum is achieved is then part of the saddle-point strategy for P2.
Increase H with new nodes for which a value has been obtained. Repeat
```
this as long as new nodes x e N2 can be found such that a(x] € H.
```
4. Repeat steps 2 and 3 until no new nodes can be found.
5. Consider those x $ H for which a(x] fl N1 D H ^ 0. Define node y to
```
be that node (or those nodes) of the set cr(z) fl H which has (have) the
```
```
smallest value, i.e., V(y] — mmz€a^x)nffV(z}. If V(y] < 0, then increase
```
```
H by node x and define V(x] = V(y}. The arc between x and y then
```
```
constitutes part of the saddle-point strategy for PI. (If V(y) > 0, then H
```
is not increased: PI prefers nontermination with value 0 over a positive
```
pay-off.) Repeat this as long as H can be increased.
```
6. Consider those x $. H for which cr(x) D N2 n H ^ 0. Define node y to
```
be that node (or those nodes) of the set a(x] n H which has (have) the
```
```
largest value, i.e., V(y] = maxzeo.(x)n// V(z). If V(y] > 0, then increase
```
```
H by node x and define V(x] = V(y). The arc between x and y then
```
```
constitutes part of the saddle-point strategy for P2. (If V(y] < 0, then
```
H is not increased: P2 prefers nontermination with value 0 to a negative
```
pay-off.) Repeat this as long as H can be increased.
```
7. Repeat steps 5 and 6 until no new nodes can be found.
8. Repeat steps 2-7 until no new nodes can be found.
TWO-PERSON ZERO-SUM FINITE GAMES 65
9. The remaining nodes x get assigned the value 0. Any arc departing from
such an x to another node not belonging to H is part of the saddle-point
```
strategies. (The players do not have any desire to terminate the game
```
```
once they reach any one of these remaining nodes.)
```
Intuitively it should be clear that this algorithm indeed yields the saddle-
```
point strategies for the game (the algorithm is in the spirit of dynamic pro-
```
gramming and is identical to the recursive procedures introduced earlier in the
context of feedback games, but here one also takes into account the possibility
```
of nontermination). A formal proof is given in Washburn (1990).
```
Games with local pay-offs
So far we have discussed only games with terminal pay-offs. Other pay-off
structures are also possible, such as assigning a 'local' pay-off to each arc of the
underlying graph. For games in extensive form in which the underlying graph is
a finite tree, this is not a real generalization, since the local pay-offs assigned to
```
arcs can be moved to the terminal nodes (and added together so as to obtain the
```
```
'total' pay-off at each terminal node). For extensive games with cycles in which
```
pay-offs are assigned to arcs, this is not possible and other means are necessary
```
to study saddle-point solutions (if they exist). For such games it brings in no
```
loss of generality to assume that the underlying directed graph does not have
```
terminal nodes (games with some terminal nodes can be modeled with a loop
```
at those nodes: a terminal pay-off can then be assigned to the corresponding
```
loop). Thus all plays are infinite. The total pay-off associated with a play of
```
the game is defined to be the long term average of the local pay-offs obtained
during the play. If necessary one can consider Cesaro limits and/or take the lim
```
m/or lim sup. Note that if a 'terminal' node is reached (if such a node existed
```
```
in the original game) to which a loop has been added, then the total pay-off
```
converges to the local pay-off corresponding to that loop.
For a theory of zero-sum games on graphs without terminal nodes, but with
perfect information and with local pay-offs, the reader is referred to Ehrenfeucht
```
and Mycielski (1979) and Alpern (1991). The theory and results are built around
```
an auxiliary game defined on the same graph and with the same rules except
for the fact that it ends as soon as a node is repeated. This auxiliary game has
therefore a finite number of moves.
2.8 Action-Dependent Information Sets
The classes of zero-sum dynamic games we have seen heretofore all carried one
common feature, which is that the information sets were fixed a priori. There
are other important classes of games, however, where this side condition does
not hold, which means that the information sets change with the actions of the
players. In this section we discuss the theory behind such games, within the
context of two specific examples—a class of duels, and a searchlight game.
66 T. BA§AR AND G. J. OLSDER
2.8.1 Duels
```
Consider a duel in which two men (PI and P2) walk towards each other, taking
```
one step at a time, at time instants t = 0,1,..., N. If nothing intervenes, then
they will meet at t = N. Each player has a pistol with exactly one bullet in
```
it, and he may fire the pistol at any time t € {0,1,..., N}. If one of them
```
fires and hits the other, the duel is immediately over and the one who has fired
successfully is declared the winner. If neither one fires successfully or if both fire
simultaneously and successfully, the duel becomes a stand-off. The probability
of hitting the opponent after the pistol has been fired is a function of the distance
```
between the two men. If a player fires at t G {0,1,..., N}, then the probability
```
of hitting the opponent is t/N.
To complete the formulation of the game, we have to specify whether the
game is 'silent' or 'noisy'. In the silent duel a player does not know whether his
opponent has already fired, unless, of course, he is hit. In a noisy duel a player
knows when his opponent has fired and if he has not yet fired himself and is not
hit he will wait until t = N to fire his own pistol, because at t = N he is certain
to hit his opponent.
Suppose that Pi fires at t1. Then, if t1 < t2, the probability that P2 is hit
```
is tl/N, and the probability that PI will be hit at t2 > t1 is (1 -tl/N}t2/N
```
```
for the silent duel and is 1 — t1 /N for the noisy duel (in which case P2 fires
```
```
at t2 = N). Mutatis mutandis similar results are obtained if t1 > t2. With
```
```
these probabilities one can construct a matrix A of size (N + 1) x (N + 1), the
```
```
(t1 + 1, t2 + l)-th element of which, with t1 < t2 and t\ tj € {0,1,..., TV}, is
```
```
tl/N - (I- tl/N}t2/N for the silent case and (N - 1tl}/N for the noisy case.
```
```
The (t1 + l,t2 + l)-th element with t1 > t2 is t2/N - (1 -t2/N)tl/Nfor the
```
silent case and 1 — 2t2/N for the noisy case. The diagonal elements of A equal
0 in both cases. Finding the optimal strategies for the duel now boils down to
obtaining a saddle-point solution for the matrix game characterized by matrix
```
A in which the minimizer (PI) chooses the row vector and the maximizer (P2)
```
the column vector. We now note that for the extensive-form description of the
noisy game, the extent of a player's knowledge on the evaluation of the game
depends explicitly on his own and/or his opponent's actions. Hence, the noisy
duel is a game with action-dependent information sets.
2.8.2 A searchlight game
In this game, two players, PI and P2, move in a certain closed domain and are
not aware of each other's position unless one of them flashes a searchlight which
illuminates an area of known shape around him. By flashing his searchlight a
player thus discloses his own position to the other player, regardless of their
relative positions. Termination of the game occurs only if a player flashes his
searchlight and the other player finds himself trapped in the area illuminated.
Each player's objective is to catch the other player in his searchlight before he
himself is caught. Therefore flashing has two competing consequences: in order
```
for a player to win, he must flash; if, however, during such a flash the other
```
TWO-PERSON ZERO-SUM FINITE GAMES 67
player is not caught, then the flashing player is in a more vulnerable position
because he has disclosed his location to the other player.
We will provide here some explicit results for the case when the players are
confined to n points on the circumference of a circle, which makes the underlying
```
decision process a finite game. At each time step (the time is considered here
```
```
to be discrete) a player can either move to one of the two adjacent nodes or
```
stay where he is. Even if the players occupy the same position, they will not be
```
aware of this, unless of course one of them (or both) flashes (flash). Flashing
```
illuminates the position of the player who flashes, as well as the two adjacent
positions.
```
In the specific situation when only one of the players (say PI) has a search-
```
light at his disposal, the game becomes a survival game: PI wants to catch the
```
other player (P2), whereas P2 wants to avoid capture. A practical motivation
```
for this game might be the following. P2 is a smuggler who wants to steer his
boat to the shore and PI is a police boat. For obvious reasons P2 wants to
avoid PI, and therefore he performs his practices only during the night, and
that is exactly the reason why the police boat will use a searchlight. Of course
in this case the search domain will not be the circumference of a circle, but
rather a two-dimensional plane.
The basic game
The basic game to be formulated below is a building block for the more general
game described above. Initially both players know each other's position on the
```
circle (but not thereafter if mixed strategies are involved); PI is at position pi
```
```
and P2 is at position p-2,Pi € {1,2,..., n}. Player PI can flash only once during
```
the time instants t — 1, 2 , . . . ,T, where T is the final time which is fixed and
known to both players. P2 does not have any flashes at his disposal. Once the
time proceeds the players do not acquire any new information on each other's
```
new position (unless PI flashes).
```
The optimal strategies of this basic game can be found by solving the matrix
game
where the notation is as follows. At each instant of time, P2 can choose from
```
three options (move to one of the two adjacent nodes or stay where he is)
```
and therefore he has 3T pure strategies. Each component of yp2 denotes the
probability with respect to which one of the 3T pure strategies will be chosen.
The set 53r is the simplex to which the 3T-vector yp2 belongs. The components
of 2P1, on the other hand, indicate Pi's pure strategies, determined by when
and where to flash. After one time step PI can flash at three different positions,
after two time steps at five different positions, etc. Hence the number of pure
```
strategies equals 3 + 5 H + (2T + 1) = T2 + 2T, provided that T < [n/2].
```
```
(For T > [n/2], all points on the circle can be reached by PI.) Vector zpl has
```
d,T — T2 + IT components and SdT denotes the simplex from which zpl must
68 T. BA§ AR AND G. J. OLSDER
be chosen. The size of ^4P1,P2,T is 3T x d,T and the elements of this matrix are
```
either 1 (corresponding to the cases when for the particular strategies capture
```
```
takes place) or 0 (if no capture takes place). The first index of A refers to Pi's
```
initial position, the second index to P2's initial position, and the third index is
the T introduced earlier.
If the saddle-point value of this matrix game is denoted by JP1,P2,T, then it
```
should be intuitively clear (and this has been rigorously proven in Olsder and
```
```
Papavassilopoulos (1988a)) that
```
```
for all p\-,pi € (1,2,..., n}. Hence, in the solution of the game, we can restrict
```
the time horizon to [n/2] steps, without any loss of generality. The idea behind
the proof of this result is that in [n/2] steps each player can reach any point on
the circle with an arbitrary underlying probability distribution.
Now, consider the following generalization of the basic game. At t = 0 P2
```
still knows where PI is (at node pi), but PI only knows the probability dis-
```
tribution of P2's position. The probability for P2 to be at node pi at t = 0 is
```
VP2. The probability vector v = ( t > i , . . . , vn}' is assumed to be known to both
```
players. Of course P2 knows his own position at t = 0. Such a generalization
arises if one considers the following game: two players move on the circumfer-
```
ence of a circle and do not know each other's initial position (other than in a
```
```
probabilistic way). Each player has exactly one flash at his disposal. Suppose
```
that PI flashes first and that he does not capture P2 in this flash. Then P2
knows where PI is at the time of the flash, but PI only knows that P2 is not
in the area just illuminated. If PI does not have any other information about
P2's position, then he may assume a uniform distribution on the nodes which
were not illuminated at the time of the flash. This is the starting point for this
```
generalization (with all v^s corresponding to nonilluminated nodes being equal
```
```
and adding up to one).
```
This generalization can again be formulated as a matrix game:
This minimax problem is not in the standard form treated earlier in Section 2.3,
but it can also be solved by means of LP, viz.,
TWO-PERSON ZERO-SUM FINITE GAMES 69
subject to
The saddle-point value of this game when vi — 0 for i = 1,2,3 and Vi =
```
l/(n — 3) for i = 4 , . . . , n will be indicated by JT, tacitly assuming that n > 3.
```
Both players have one flash each
We will now solve the problem where each player has exactly one flash at his dis-
posal and the players do not know each other's initial position, other than that
they are given by the uniform distribution on the n nodes with no correlation
between the two initial positions.
If both players flash at t = 1, then the probability with which P2 captures
```
PI is 3/n (since three positions are illuminated) and so is the probability for
```
```
PI to capture P2. Thus the pay-off is (3/n) — (3/n) = 0. Let us now consider
```
the situation where P2 flashes at t — I and PI does not. Then the pay-off for
```
P2 is (3/n) - (1 - 3/n) JT-I- In other words, the pay-off equals the probability
```
that P2 captures PI minus the probability that P2 does not capture PI, times
the probability that PI captures P2 during the remaining T — 1 period game.
Similarly, if P2 flashes at t = 2 and PI does not flash at t — 1 or t — 2, then the
```
pay-off for P2 is (3/n) - (1 - 3/n) JT_2, etc. Define ck = (3/n) - (1 - 3/n) JT_fc,
```
and consider the matrix
```
Let s = (si ..., ST)' and r = ( r i , . . . , r^)' represent probability vectors accord-
```
ing to which PI and P2 choose to flash at times t = 1 , . . . , T. Then the players
face the following game:
```
The solution(s) to this latter matrix game provides the solution to our prob-
```
lem. Since the matrix M is skew-symmetric, it turns out that the value of the
70 T. BA§AR AND G. J. OLSDER
```
game is 0 (see Problem 9 in Section 2.9). Some further analysis regarding the
```
signs and monotonicity properties of the elements Ck shows that the optimal
strategies as when to flash are pure strategies. This is no longer true, however,
if the illuminated areas for both players have different sizes. See Olsder and
```
Papavassilopoulos (1988a) for further details and extensions.
```
In conclusion, for the searchlight game as described above to be solved, one
must solve a hierarchy of matrix games. The first matrix game determines the
coefficients c^, which are subsequently used as elements in the second matrix
game defined by means of the matrix M.
2.9 Problems
1. Obtain the security strategies and the security levels of the players in the
```
following matrix game. Does it admit a (pure) saddle-point solution?
```
2. Obtain the mixed security strategies and the average security levels of the
players in each of the following matrix games.
3. Verify that the quantity supz miny y'Az is unique.
4. Prove that the quantity max^ y'Az is a continuous function of y £ Y.
5. Determine graphically the mixed saddle-point solutions of each of the fol-
lowing matrix games.
6. Convert the following matrix games into linear programming problems
and thereby numerically evaluate their saddle-point solutions.
TWO-PERSON ZERO-SUM FINITE GAMES 71
7. The space of all real (m x n) matrices A — {a^} can be considered to be
isomorphic to the mn-dimensional Euclidean space Rmn. Let Smn denote
```
the set of all real (m x n) matrices which, considered as the pay-off matrix
```
of a zero-sum game, admit pure strategy saddle-point equilibria. Then
Smn is isomorphic to a subset of Rmn, to be denoted by Sm'n.
```
(i) Is Sm'n a subspace?
```
```
(ii) Is Sm'n closed, convex?
```
```
(iii) Describe 52'3 explicitly.
```
8. Let Tl(A) denote the set of all mixed saddle-point strategies of Pi in
```
a matrix game A. Show that Tl(A) is nonempty, convex, closed and
```
bounded.
9. A matrix game A, where A is a skew-symmetric matrix, is called a symmet-
ric game. For such a matrix game prove, using the notation of Problem 8,
```
that Tl(A) = T2(A], and Vm(A) = 0.
```
10. Obtain the pure or behavioral saddle-point solutions of the following single-
act games in extensive form.
11. Obtain a feedback saddle-point solution for the feedback game in extensive
form depicted in Fig. 2.15. What is the value of the game? What is the
actual play dictated by the feedback saddle-point solution? Show that
these constant strategies also constitute a saddle-point solution for the
feedback game.
12. Show that the feedback game in extensive form depicted in Fig. 2.16
does not admit a pure-strategy feedback saddle-point solution. Obtain
its behavioral saddle-point solution and the behavioral saddle-point value.
Compare the latter with the value of the game of Problem 11. What is
the actual play dictated by the feedback saddle-point solution?
13. What is the open-loop version of the feedback game of Fig. 2.16? Obtain
its saddle-point solution and value.
14. Prove that if a feedback game in extensive form admits a feedback saddle-
```
point solution with the value Jf, and its open-loop version (if it exists)
```
72 T. BA§AR AND G. J. OLSDER
Figure 2.15: Feedback game of Problem 11.
Figure 2.16: Feedback game of Problem 12.
admits a unique saddle-point solution with the value J0, then Jf = J0.
Furthermore, show that the open-loop saddle-point solution is the actual
```
play dictated by the feedback saddle-point solution. (Hint: make use of
```
```
Prop. 2.7 and the interchangeability property of saddle-point strategies.)
```
15. Verify the following conjecture: If a feedback game in extensive form ad-
mits a behavioral saddle-point solution which actually dictates a random
```
choice at least at one level of play, then its open-loop version (if it exists)
```
cannot admit a pure-strategy saddle-point equilibrium.
16. Investigate whether the following multi-act game of the delayed commit-
ment type admits a behavioral saddle-point solution or not.
TWO-PERSON ZERO-SUM FINITE GAMES 73
17. Determine the pure or behavioral saddle-point solution of the game in
extensive form depicted in Fig. 2.17.
Figure 2.17: The two-person dynamic game of Problem 17.
18. Determine the pure or behavioral saddle-point solution of the game in
extensive form depicted in Fig. 2.18, which also incorporates a chance
move.
Figure 2.18: The game of Problem 18.
19. Determine the solutions to the previous problem in the cases in which (i) PI
```
has access to nature's choice; (ii) P2 has access to nature's choice; and
```
```
(iii) both players have access to nature's choice. What is the value of this
```
extra information to individual players in each case?
20. Construct an example of a matrix with only real eigenvalues, which, when
considered as a zero-sum game, has a value in mixed or pure strategies
that is smaller than the minimum eigenvalue.
21. A matrix game is said to be completely mixed if it admits a mixed saddle-
point solution which assigns positive probability weight to all pure strate-
gies of the players. Show that in a completely mixed game it does not
matter whether PI is minimizing and P2 is maximizing or the other way
around, i.e., PI is maximizing and P2 is minimizing. In both cases the
```
saddle-point value and the strategies are the same. (For further results
```
on completely mixed games the reader is referred to Bapat and Raghavan
```
(1997).)
```
74 T. BA§AR AND G. J. OLSDER
Figure 2.19: The game of Problem 18.
22. Obtain for PI his minimum guaranteed (average) costs in mixed, behav-
ioral and randomized strategies, for the game with incomplete information,
described by the tree of Fig. 2.19.
```
(Answers: In mixed strategies, PI can guarantee an average (minimum)
```
outcome of —3/4, in behavioral strategies his security level is —25/64, and
```
in randomized strategies it is —9/16.)
```
23. 23. Consider the following matrix game.
If the parameters x and y would be known to both players, the matrix
game is a standard one. Parameter x is known to PI only, however,
and y is only known to P2. What both players do know is that both
parameters are stochastic, and independently and uniformly distributed
on the interval [0,T], with T = 1. Show that the optimal solutions are
For x = cl and/or y = c2 the solution is not unique. Investigate the
solution for lim T I 0 and compare the limit behavior with the solution of
the original matrix game with x = y = 0.
```
where the real constants c1 are (uniquely) determined by 0 < cl < 1 and
```
TWO-PERSON ZERO-SUM FINITE GAMES 75
2.10 Notes
Section 2.2. The theory of finite zero-sum games dates back to Borel in the early
```
1920s whose work on the subject was later translated into English (Borel, 1953). Borel
```
introduced the notion of a conflicting decision situation that involves more than one
decision maker, and the concepts of pure and mixed strategies, but he did not really
develop a complete theory of zero-sum games. He even conjectured that the minimax
theorem was false. It was Von Neumann who first came up with a proof of the minimax
```
theorem and laid down the foundations of game theory as we know of today (Von Neu-
```
```
mann, 1928, 1937). His pioneering book with Morgenstern definitely culminates his
```
```
research on the theory of games (Von Neumann and Morgenstern, 1947). A full account
```
of the historical development in finite zero-sum games, as well as possible applications
```
in social sciences, is given in Luce and Raiffa (1957). The reader is also referred to
```
```
McKinsey (1952) and the two edited volumes by Kuhn and Tucker (1950, 1953). The
```
original proof of the minimax theorem given by Von Neumann is nonelementary and
rather complicated. Since then, the theorem has been proven in several different ways,
some being simpler and more illuminating than others. The proof given here seems to
be the simplest one available in the literature.
Section 2.3. Several illustrative examples of the graphical solution of zero-sum
```
matrix games can be found in the nontechnical book by Williams (1954). Graphical
```
solution and the linear programming approach are not the only two applicable in the
```
computation of mixed saddle-point solutions of matrix games; there is also the method
```
```
of Brown and Von Neumann (1950) which relates the solution of symmetric games (see
```
```
Problem 9 in Section 2.9 for a definition) to the solution of a particular differential
```
```
equation. Furthermore, there is the iterative solution method of Brown (1951) by
```
```
fictitious play (see also (Robinson, 1951)). A good account of these two numerical
```
```
procedures, as well as another numerical technique of Von Neumann (1954) can be
```
```
found in Luce and Raiffa (1957, Appendix 6). For a more recent development in this
```
```
area, see Lakshmivarahan and Narendra (1981).
```
Sections 2.4, 2.5 and 2.6. Description of single- and multi-act games in extensive
```
form again goes back to the original work of Von Neumann and Morgenstern (1947),
```
```
which was improved upon considerably by Kuhn (1953). For a critical discussion
```
of prior and delayed commitment approaches to the solution of zero-sum games in
```
extensive form the reader is referred to Aumann and Maschler (1972).
```
```
Section 2.7. The theory presented in this section is based on Alpern (1988, 1991),
```
```
Isbell (1957), and Washburn (1990). Washburn (1990) also discusses computational
```
complexity of the solutions of zero-sum games on finite graphs with cycles and terminal
pay-off. Extensions to discounting are also given in that reference, but extensions to
games with random moves lead to complications in the solution algorithm in an essen-
```
tial way. Alpern (1991), on (not necessarily zero-sum) games with perfect information
```
on finite graphs with cycles and with local pay-offs, gives a constructive way to find
```
optimal stationary strategies. See also Ehrenfeucht and Mycielski (1979) for earlier
```
```
work on this topic. The former reference also discusses m-automated strategies; such
```
strategies can be played by an automaton with m internal states and where inputs
and outputs are coded to the nodes of the graph.
76 T. BA§AR AND G. J. OLSDER
Section 2.8. The two duels presented, the silent and the noisy one, are discretized
```
versions (discretization with respect to time) of duels played in continuous time as
```
```
given in Karlin (1959). For further discussion on extensions of game of timing see
```
the Notes section of Chapter 4, with reference to Section 4.7. The theory of the
searchlight game as presented here has been taken from Olsder and Papavassilopou-
```
los (1988a, 1988b). Such games with a finite state space belong to the realm of Markov
```
```
chain games; for a survey see Parthasarathy and Stern (1977), and Raghavan and Filar
```
```
(1991). For other types of search games, see Gal (1980).
```
Section 2.9. Problem 20: For more results on the relation between games and their
```
spectra, see Weil (1968). Problem 22 has been taken from Isbell (1957).
```
Chapter 3
Noncooperative Finite
```
Games: Af-Person
```
Nonzero-Sum
3.1 Introduction
This chapter develops a general theory for static and dynamic nonzero-sum finite
games under two different types of noncooperative solution concepts—those
named after J. Nash and H. von Stackelberg. The analysis starts in Section 3.2
with bimatrix games which basically constitute a normal form description of
two-person nonzero-sum finite games, and in this context the Nash equilibrium
```
solution (in both pure and mixed strategies) is introduced and its properties
```
and features are thoroughly investigated. The section concludes with some
computational aspects of mixed-strategy Nash equilibria in bimatrix games.
Section 3.3 extends the analysis of Section 3.2 to ./V-person games in normal
```
form and includes a proof for an important theorem (Thm. 3.2) which states
```
that every ./V-person finite game in normal form admits Nash equilibria in mixed
strategies. In Section 3.4, the computational aspects of mixed-strategy Nash
equilibria are briefly discussed, and in particular the close connection between
mixed-strategy Nash equilibria in bimatrix games and the solution of a nonlinear
programming problem is elucidated.
```
Section 3.5 deals with properties and derivation of Nash equilibria (in pure,
```
```
behavioral and mixed strategies) for dynamic TV-person games in extensive form.
```
The analysis is first confined to single-act games and then extended to multi-
act games. In both contexts, the concept of "informational inferiority" between
two finite games in extensive form is introduced, which readily leads to "infor-
mational nonuniqueness" of Nash equilibria in dynamic games. To eliminate
this feature, the additional concept of "delayed commitment" type equilibrium
strategy is introduced, and such equilibria of special types of single-act and
77
78 T. BA§AR AND G. J. OLSDER
multi-act finite games are obtained through recursive procedures. Other refine-
ment schemes discussed in this section are the "perfectness" and "properness"
of Nash equilibria.
Section 3.6 is devoted to an extensive discussion of hierarchical equilibria
in TV-person finite games. First, the two-person games are treated, and in
```
this context both the (global) Stackelberg and feedback Stackelberg solution
```
concepts are introduced, also within the class of mixed and behavioral strategies.
Several examples are included to illustrate these concepts and the derivation of
the Stackelberg solution. The analysis is then extended to many-player games.
The final section of the chapter discusses extension of the results of the
earlier sections to TV-person finite games in extensive form which also incorporate
chance moves.
3.2 Bimatrix Games
Before developing a general theory of nonzero-sum noncooperative games, it
is most appropriate first to investigate equilibria of bimatrix games which in
fact constitute the most elementary type of decision problems within the class
of nonzero-sum games. In spite of their simple structure, bimatrix games still
carry most of the salient features and intricacies of noncooperative decision
making.
A bimatrix game can be considered as a natural extension of the matrix game
of Section 2.2, to cover situations in which the outcome of a decision process does
not necessarily dictate the verdict that what one player gains the other one has
```
to lose. Accordingly, a bimatrix game is comprised of two (ra x n)-dimensional
```
```
matrices, A = {a^} and B = {6^-}, with each pair of entries (ay,6y) denoting
```
the outcome of the game corresponding to a particular pair of decisions made by
the players. As in Section 2.2, we call the alternatives available to the players
strategies. If PI adopts the strategy "row z" and P2 adopts the strategy "column
```
/', then a,ij (respectively, 6^-) denotes the loss incurred to PI (respectively, P2).
```
Being a rational decision maker, each player will strive for an outcome which
provides him with the lowest possible loss.
Stipulating that there exists no cooperation between the players and that
```
they make their decisions independently, let us now investigate which pair(s) of
```
```
strategies to declare as equilibrium pair(s) of a given bimatrix game; in other
```
words, how do we define a noncooperative equilibrium solution in bimatrix
games? To recall the basic property of a saddle-point equilibrium solution in
zero-sum games would definitely help in this investigation: "A pair of strategies
is in saddle-point equilibrium if there is no incentive for any unilateral devia-
```
tion by any one of the players" (Section 2.2). This is, in fact, a most natural
```
property that an equilibrium solution is expected to possess, and surely it finds
relevance also in nonzero-sum games. Hence, we have the following definition.
```
Definition 3.1 A pair of strategies {row i*, column j*} is said to constitute a
```
```
noncooperative (Nash) equilibrium solution to a bimatrix game (A = {a^-}, B =
```
TV-PERSON NONZERO-SUM FINITE GAMES 79
```
{bij}) if the following pair of inequalities is satisfied for all i = 1 , . . . , m and all
```
```
j = l , . . . , n :
```
```
It admits two Nash equilibria, as indicated: {row 1, column 1} and {row 2,
```
```
column 2}. The corresponding equilibrium outcomes are (1,2) and (—1,0). D
```
We now readily observe from the preceding example that a bimatrix game
can admit more than one Nash equilibrium solution, with the equilibrium out-
comes being different in each case. This then raises the question of whether
it would be possible to order different Nash equilibrium solution pairs among
themselves so as to declare only one of them as the most favorable equilibrium
solution. This, however, is not completely possible, since a total ordering does
```
not exist between pairs of numbers; but, notions of "betterness" and "admissi-
```
bility" can be introduced through a partial ordering.
Definition 3.3 A Nash equilibrium strategy pair is said to be admissible if there
exists no better Nash equilibrium strategy pair.
In Example 3.1, out of a total of two Nash equilibrium solutions only one of
```
them is admissible ({row 2, column 2}), since it provides uniformly lower costs
```
for both players. This pair of strategies can therefore be declared as the most
"reasonable" noncooperative equilibrium solution of the bimatrix game. In the
case when a bimatrix game admits more than one admissible Nash equilibrium,
```
however, such a clean choice cannot be made. Consider, for example, the (2 x 2)
```
bimatrix game
```
Furthermore, the pair (ai*j*,bi*j*) is known as a noncooperative (Nash) equi-
```
librium outcome of the bimatrix game.
```
Example 3.1 Consider the following (2 x 2) bimatrix game
```
```
Definition 3.2 A pair of strategies {row i\, column ji} is said to be better
```
```
than another pair of strategies {row 1*2 column j%} if aiij1 < cazj2, biljl < bi2j2
```
and if at least one of these inequalities is strict.
80 T. BA§AR AND G. J. OLSDER
```
which admits two admissible Nash equilibrium solutions: {row 1, column 1},
```
```
{row 2, column 2}, with the equilibrium outcomes being (—2,—!) and (—1, —2),
```
respectively. These Nash equilibria are definitely not interchangeable, and hence
a possibility for the outcome of the game to be a nonequilibrium one in an actual
play emerges. In effect, since there is no cooperation between the players by the
```
nature of the problem, PI might stick to one equilibrium strategy (say, row 1)
```
```
and P2 might adopt the other one (say, column 2), thus yielding an outcome of
```
```
(1,1) which is unfavorable to both players. This is indeed one of the dilemmas of
```
```
noncooperative nonzero-sum decision making (if a given problem does not admit
```
```
a unique admissible equilibrium), but there is really no remedy for it unless one
```
allows some kind of communication between the players, at least to ensure that
a nonequilibrium outcome will not be realized. In fact, a possibility of collusion
saves only that aspect of the difficulty in the above example, since the "best"
choice still remains indeterminate. One way of resolving this indeterminacy is
to allow a repetition and to choose one or the other Nash equilibrium solution
```
at consecutive time points. The average outcome is then clearly (-3/2,-3/2),
```
assuming that the players make their choices dependent on each other. How-
ever, this favorable average outcome is not an equilibrium outcome in mixed
strategies, as we shall see in the last part of this section. The solution proposed
above is acceptable in the "real" decision situation, known as the battle of the
```
sexes, that the bimatrix game (3.2) represents. The story goes as follows.
```
A couple has to make a choice out of two alternatives for an evening's en-
tertainment. The alternatives are a basketball game which is preferred by the
```
husband (PI) and a musical comedy which is preferred by the wife (P2). But
```
they would rather go together, instead of going separately to their top choices.
```
This then implies, also in view of the bimatrix game (3.2), that they will even-
```
tually decide to go together either to the game or to the comedy. It should
be noted that there is a flavor of cooperation in this example, but the solution
is not unique in any sense. If repetition is possible, however, then they can
go one evening to the game, and another evening to the comedy, thus resolv-
ing the problem in a "fair" way. Such a solution, however, does not carry any
game-theoretic meaning as we shall see in the last part of this section. The
message here is that if a bimatrix game admits more than one admissible Nash
equilibrium solution, then a valid approach would be to go back to the actual
decision process that the bimatrix game represents, and see whether a version of
the bimatrix game that also accounts for repetition admits a solution which is
favored by both parties. This, of course, is all valid provided that some commu-
nication is allowed between the players, in which case the "favorable" solution
thus obtained will not be a noncooperative equilibrium solution.
Thus, we have seen that if a bimatrix game admits more than one admissible
Nash equilibrium solution, then the equilibrium outcome of the game becomes
rather ill-defined—mainly due to the fact that multiple Nash equilibria are in
general not interchangeable. This ambiguity disappears, of course, whenever the
equilibrium strategies are interchangeable, which necessarily requires the corre-
sponding outcomes to be the same. Since zero-sum matrix games are special
types of bimatrix games, in which case the equilibrium solutions are known to
```
Remark 3.1 The reader can easily verify that the pair (3.3a)-(3.3b) is indeed
```
an equivalence relation since it is symmetric, reflexive and transitive.
Proposition 3.1 All strategically equivalent bimatrix games have the same Nash
equilibria.
```
Proof. Let (A, B] be a bimatrix game with a Nash equilibrium solution (row
```
```
i*, column j*}. Then, by definition, inequalities (3.1a)-(3.1b) are satisfied. Now
```
```
let (C, D) be any other bimatrix game that is strategically equivalent to (A, B}.
```
```
Then there exist c*i > 0, a2 > 0, J3\, 02 such that (3.3a)-(3.3b) are satisfied for
```
```
all possible i and j. Using these relations in (3.la) and (3.1b), we obtain, in
```
view of the positivity of a\ and 0:2,
for all i — 1,..., m and all j — 1 , . . . , n, which proves the proposition since
```
(A, B} was an arbitrary bimatrix game.
```
```
Proposition 3.2 Multiple Nash equilibria of a bimatrix game (A, B) are inter-
```
```
changeable i f ( A , B ) is strategically equivalent to (A,—A}.
```
Proof. The result readily follows from Corollary 2.1 since, by hypothesis, the
```
bimatrix game (A, B) is strategically equivalent to a zero-sum game A.D
```
The undesirable features of the Nash equilibrium solutions of bimatrix games,
that we have witnessed, might at first sight lead to questioning appropriateness
and suitability of the Nash equilibrium solution concept for such noncooperative
multi-person decision problems. Such a verdict, however, is not fair, since the
undesirable features detected so far are due to the noncooperative nature of the
decision problem under consideration, rather than to the weakness of the Nash
equilibrium solution concept. To supplement our argument, let us consider one
```
extreme case (the so-called "team" problem) that involves two players, identi-
```
```
cal goals (i.e., A — B), and noncooperative decision making. Note that, since
```
JV-PERSON NONZERO-SUM FINITE GAMES
```
be interchangeable (Corollary 2.1), it follows that there exists some nonempty
```
class of bimatrix games whose equilibrium solutions possess such a property.
```
This class is, in fact, larger than the set of all (m x n) zero-sum games, as it is
```
shown below.
```
Definition 3.4 Two (m x n) bimatrix games (A, B} and (C, D} are said to be
```
```
strategically equivalent if there exist positive constants ot\, a^, and scalars (3\,
```
fa, such that
```
for all i = 1 , . . . , m; j = 1 , . . . , n.
```
8182 T. BA§AR AND G. J. OLSDER
possible outcomes are single numbers in this case, a total ordering of outcomes
is possible, which then makes the admissible Nash equilibrium solution the only
possible equilibrium solution for this class of decision problems. Moreover, the
admissible Nash equilibrium outcome will be unique and be equal to the small-
est entry of the matrix that characterizes the game. Note, however, that even
for such a reasonable class of decision problems, the players might end up at
```
a nonequilibrium point in case of multiple equilibria; this being mainly due to
```
the noncooperative nature of the decision process which requires the players
to make their decisions independently. To illustrate such a possibility, let us
```
consider the identical goal game (team problem) with the cost matrices
```
which both parties try to minimize, by appropriate choices of strategies. There
```
are clearly two equilibria, {row 1, column 1} and {row 2, column 2}, both
```
yielding the minimum cost of 0 for both players. These equilibria, however, are
not interchangeable, thus leaving the players at a very difficult position if there
```
is no communication between them. (Perhaps tossing a fair coin and thereby
```
securing an average cost of | would be preferred by both sideny pure
```
strategy which could easily lead to an unfortunate cost of 1.) This then indicates
```
that even the well-established team-equilibrium solution could be frowned upon
in case of multiple equilibria when the decision mode is noncooperative.
```
One can actually make things look even worse by replacing matrix (3.4) with
```
a nonsymmetric one
which admits the same two equilibria as in the previous matrix game. But what
are the players actually going to play in this case? To us, there seems to be only
one logical play for each player: PI would play "row 2", hoping that P2 would
play his second equilibrium strategy, but at any rate securing a loss ceiling of 1.
P2, reasoning along similar lines, would rather play "column 1". Consequently,
the outcome of their joint decision is 1 which is definitely not an equilibrium
outcome. But, in an actual play of this matrix game, this outcome is more
likely to occur than the equilibrium outcome. One might counter-argue at this
point and say that P2 could very well figure out Pi's way of thinking and pick
"column 2" instead, thus enjoying the low equilibrium outcome. But what if PI
thinks the same way? There is clearly no end to this so-called second-guessing
iteration procedure if adopted by both players as a method that guides them to
the "optimum" strategy.
TV-PERSON NONZERO-SUM FINITE GAMES 83
One might wonder at this point as to why, while the multiple Nash equilibria
```
of bimatrix games (including identical-goal games) possess all these undesirable
```
```
features, the saddle-point equilibrium solutions of zero-sum (matrix) games are
```
quite robust—in spite of the fact that both are noncooperative decision prob-
lems. The answer to this lies in the nature of zero-sum games: they are com-
pletely antagonistic, and the noncooperative equilibrium solution fits well within
this framework. In other nonzero-sum games, however, the antagonistic nature
is rather suppressed or is completely absent, and consequently the "noncooper-
ative decision making" framework does not totally suit such problems. If some
cooperation is allowed, or if there is a hierarchy in decision making, then equi-
```
librium solutions of nonzero-sum (matrix) games could possess more desirable
```
features. We shall, in fact, observe this later in Section 3.6 where we discuss
equilibria in nonzero-sum games under the latter mode of decision making.
The minimax solution
One important property of the saddle-point strategies in zero-sum games was
```
that they were also the security strategies of the players (see Thm. 2.2). In
```
bimatrix games, one can still introduce security strategies for the players in the
same way as it was done in Section 2.2, and with the properties cited in Thm. 2.1
being valid with the exception of the third one. It should of course be clear that,
```
in a bimatrix game (A, B) the security strategies of PI involve only the entries
```
of matrix A, while the security strategies of P2 involve only those of matrix B.
In Example 3.1, considered earlier, the unique security strategy of PI is "row 1",
and the unique security strategy of P2 is "column 1". Thus, considered as a pair,
the security strategies of the players correspond to one of the Nash equilibrium
solutions, but not to the admissible one, in this case. One can come up with
examples of bimatrix games in which the security strategies do not correspond
to any one of the equilibrium solutions, or examples of bimatrix games in which
they coincide with an admissible Nash solution. As an illustration of the latter
possibility, consider the following bimatrix game:
This admits a unique pair of Nash equilibrium strategies, as indicated, which are
also the security strategies of the players. Moreover, the equilibrium outcome
is determined by the security levels of the players.
The preceding bimatrix game is known in the literature as the prisoners'
dilemma. It characterizes a situation in which two criminals, suspected of having
committed a serious crime, are detained before a trial. Since there is no direct
evidence against them, their conviction depends on whether they confess or not.
If the prisoners both confess, then they will be sentenced to 8 years. If neither
one confesses, then they will be convicted of a lesser crime and sentenced to
It clearly does not admit a Nash equilibrium solution in pure strategies. The
```
minimax strategies, however, exist (as they always do, by Thm. 2.1 (ii)) and are
```
```
given as {row 1, column 2}.D
```
84 T. BA§AR AND G. J. OLSDER
2 years. If only one of them confesses and puts the blame on the other one, then
he is set free according to the laws of the country and the other one is sentenced
to 30 years. The unique Nash equilibrium in this case dictates that they both
should confess. Note, however, that there is in fact a better solution for both
```
criminals, which is that they both should refuse to confess; but implementation
```
```
of such a solution would require a cooperation of some kind (and also trust),
```
which is clearly out of the question in the present context. Besides, this second
```
solution (which is not Nash) is extremely unstable, since a player will find it to
```
his advantage to unilaterally deviate from this position at the last minute.
Even if the security strategies might not be in noncooperative equilibrium,
they can still be employed in an actual game, especially in cases when there
exist two or more noninterchangeable Nash equilibria or when a player is not
completely sure of the cost matrix, or even the rationality of the other player.
This reasoning leads to the following second solution concept in bimatrix games.
```
Definition 3.5 A pair of strategies {row i, column j} is known as a pair of
```
```
minimax strategies for the players in a bimatrix game (A, B) if the former is
```
a security strategy for PI in the matrix game A, and the latter is a security
strategy for P2 in the matrix game B. The corresponding security levels of the
players are known as the minimax values of the bimatrix game.
Remark 3.2 It should be noted that minimax values of a bimatrix game are
```
definitely not lower (in an ordered way) than the pair of values of any Nash
```
equilibrium outcome. Even if the unique Nash equilibrium strategies correspond
to the minimax strategies, the minimax values could be higher than the values
of the Nash equilibrium outcome, mainly because the minimax strategy of a
player might not constitute optimal response to the minimax strategy of the
other player.D
Mixed strategies
In our discussion of bimatrix games in this section, we have so far encountered
only cases in which a given bimatrix game admits a unique or a multiple of Nash
equilibria. There are other cases, however, in which Nash equilibrium strategies
might not exist, as illustrated in the following example. In such a case we simply
say that a Nash equilibrium does not exist in pure strategies.
```
Example 3.2 Consider the (2 x 2) bimatrix game
```
```
Here, the pair (y*1 Az* ,y*'Bz*} is known as a noncooperative (Nash) equilibrium
```
outcome of the bimatrix game in mixed strategies.
Theorem 3.1 Every bimatrix game has at least one Nash equilibrium solution
in mixed strategies.
Proof. We postpone the proof of this important result until the next section
```
where it is proven in a more general context (Thm. 3.2).
```
Computation of mixed Nash equilibrium strategies of bimatrix games is more
involved than the computation of mixed saddle-point solutions, and in the lit-
erature there are very few generally applicable methods in that context. One
of these methods converts the original game problem into a nonlinear program-
ming problem, and it will be discussed in Section 3.4. What we intend to include
here is an illustration of the computation of mixed Nash equilibrium strategies
```
in (2 x 2) bimatrix games by directly making use of inequalities (3.7a)-(3.7b).
```
Specifically, consider again the bimatrix game of Example 3.2 which is known
not to admit a pure-strategy Nash equilibrium. In this case, since every y S Y
```
can be written as y = (yi,(l — yi))'> with 0 < y\ < 1, and similarly every
```
```
z G Z as z — (21, (1 — 21))', with 0 < z\ < 1, we first obtain the following
```
```
equivalent inequalities for (3.7a) and (3.7b), respectively, for the bimatrix game
```
under consideration:
```
Here (y£, (1 — y l ) ) r and (z$, (1 — z^}}' denote the mixed Nash equilibrium strate-
```
gies of PI and P2, respectively, which have yet to be computed so that the pre-
ceding pair of inequalities is satisfied. A way of obtaining this solution would
```
be to find that value of z\ (if it exists in the interval [0,1]) which would make
```
the RHS of the first inequality independent of yi, and also the value of y\ that
JV-PERSON NONZERO-SUM FINITE GAMES 85
```
Definition 3.6 A pair {y* € Y, z* £ Z} is said to constitute a noncooperative
```
```
(Nash) equilibrium solution to a bimatrix game (A, B] in mixed strategies, if
```
the following inequalities are satisfied for all y G Y and z & Z:
Paralleling our analysis in the case of nonexistence of a saddle point in zero-
```
sum games (Section 2.2), we now enlarge the class of strategies so as to include
```
all mixed strategies, defined as the set of all probability distributions on the set
```
of pure strategies of each player (see Def. 2.2). This extension is in fact sufficient
```
```
to ensure the existence of a Nash equilibrium solution (in mixed strategies, in a
```
```
bimatrix game)—a result which we state below in Thm. 3.1, after making the
```
concept of a Nash equilibrium in this extended space precise. Using the same
notation as in Defs. 2.3-2.5, wherever appropriate, we first have the following
definition.
Remark 3.3 In the above solution to Example 3.2 we observe an interesting,
and rather counter-intuitive, feature of the Nash equilibrium solution in mixed
```
strategies. The solution obtained, i.e., {y*,z*}, has the property that the in-
```
```
equalities (3.7a) and (3.7b) in fact become independent of y £ Y and z 6 Z. In
```
other words, direction of the inequality loses its significance. This leads to the
important conclusion that if the players were instead seeking to maximize their
average costs, then we would have obtained the same mixed equilibrium solution.
This implies that, while computing his mixed Nash equilibrium strategy, each
player pays attention only to the average cost function of his co-player, rather
than optimizing his own average cost function. Hence, the nature of the opti-
```
mization (i.e., minimization or maximization) becomes irrelevant in this case.
```
The same feature can also be observed in mixed saddle-point solutions, yield-
ing the conclusion that a zero-sum matrix game A could have the same mixed
```
saddle-point solution as the zero-sum game with matrix —A. (See Problem 21
```
```
in section 2.9.)
```
The following proposition now makes the conclusions of the preceding remark
precise.
```
Proposition 3.3 Let Y and Z denote the sets of inner points (interiors) of
```
```
Y and Z, respectively. If a bimatrix game (A, B] admits a mixed-strategy Nash
```
```
equilibrium solution {y* £Y,z*£ Z},15 then this also serves as a mixed-strategy
```
```
Nash equilibrium solution for the bimatrix game (—A, —B}.
```
```
Proof. By hypothesis, inequalities (3.7a) and (3.7b) are satisfied by a pair
```
```
{y* £.Y,z* € Z}. This implies that mmyy'Az* is attained by an inner point
```
of Y, and hence, because of linearity of y'Az* in y, this expression has to be
```
independent of y. Similarly, y*'Bz is independent of z. Consequently, (3.7a)-
```
```
(3.7b) can be written as
```
which further leads to the inequality-pair
15Such a solution is also known as a completely mixed Nash equilibrium solution or as an
inner mixed-strategy Nash equilibrium solution.
would make the RHS of the second inequality independent of z\. The solution
```
then readily turns out to be (y\ = \,z\ — |) which can easily be checked to be
```
the unique solution set of the two inequalities. Consequently the bimatrix game
of Example 3.2 admits a unique Nash equilibrium solution in mixed strategies,
which is
86 T. BA§AR AND G. J. OLSDER
which verifies the assertion of the proposition.
Now it readily follows that these inequalities admit the unique inner point so-
```
lution (yl = |, z\ = |) dictating the mixed Nash equilibrium solution {y* =
```
```
(|,|)',z* = (|, |)'}, and an average outcome of ( — I T , — i r ) . Of course, the above
```
```
inequalities also admit the solutions (yl = 1, z\ — 1) and (yl = 0, z\ — 0) which
```
correspond to the two pure Nash equilibria which we have discussed earlier at
some length. Our interest here lies in the nature of the mixed equilibrium so-
lution. Interpreted within the context of the "battle of the sexes", this mixed
solution dictates that the husband chooses to go to the game with probability
| and to the musical comedy with probability |. The wife, on the other ha
decides on the game with probability | and on the comedy with probability
Since, by the very nature of the noncooperative equilibrium solution concept,
these decisions have to be made independently, there is a probability of 13/25
with which the husband and wife will have to spend the evening separately. In
other words, even if repetition is allowed, the couple will be together for less
```
than half the time, as dictated by the unique mixed Nash equilibrium solution;
```
and this so in spite of the fact that the husband and wife explicitly indicate in
their preference ranking that they would rather be together in the evening. The
main reason for such a dichotomy between the actual decision process and the
solution dictated by the theory is that the noncooperative equilibrium solution
concept requires decisions to be made independently, whereas in the bimatrix
game of the "battle of the sexes" our way of thinking for a reasonable equi-
librium solution leads us to a cooperative solution which inevitably asks for
```
dependent decisions. (See Problem 4, Section 3.8, for more variations on the
```
```
"battle of the sexes".)
```
The conclusion we draw here is that bimatrix games with pure-strategy Nash
equilibria could also admit mixed-strategy Nash solutions which, depending on
TV-PERSON NONZERO-SUM FINITE GAMES 87
As a special case of this proposition, we now immediately have the following
result for zero-sum matrix games.
Corollary 3.1 Any inner mixed saddle point of a zero-sum matrix game A also
constitutes an inner mixed saddle point for the zero-sum matrix game —A.
Even though we have introduced the concept of a mixed noncooperative
equilibrium solution for bimatrix games which do not admit a Nash equilibrium
```
in pure strategies, it is quite possible for a bimatrix game that admits (pure)
```
Nash equilibria to admit a mixed Nash equilibrium as well. To verify this
possibility, let us again consider the bimatrix game of the "battle of the sexes",
```
given by (3.2). We have already seen that it admits two noninterchangeable
```
```
Nash equilibria with outcomes (—2, —1) and (—1, —2). Let us now investigate
```
whether it admits a third equilibrium, this time in mixed strategies.
```
For this bimatrix game, letting y — (yi,(l — yi))' G Y and z = (z\,(l —
```
```
z i ) ) r e Z, we first rearrange the inequalities (3.7a)-(3.7b) and write them in the
```
equivalent form
the relative magnitudes of the entries of the two matrices, could yield better or
```
worse (and of course also noncomparable) average equilibrium outcomes.
```
3.3 TV-Person Games in Normal Form
The class of TV-person nonzero-sum finite static games in normal form models a
decision making process similar in nature to that modeled by bimatrix games,
```
but this time with N(> 2) interacting decision makers (players). Decisions are
```
again made independently and out of a finite set of alternatives for each player.
Since there exist more than two players, a matrix formulation on the plane
is not possible for such games, thus making the display of possible outcomes
and visualization of equilibrium strategies rather difficult. However, a precise
formulation is still possible, as it is provided below together with the notation
to be used in describing TV-person finite static games in normal form.
Formulation of an TV-person finite static game in normal form
```
(1) There are N players to be denoted by P 1 , P 2 , . . . , PTV. Let us further
```
```
denote the index set {1,2,..., TV} by N.
```
```
(2) There is a finite number of alternatives for each player to choose from.
```
```
Let ra; denote the number of alternatives available to Pi, and further
```
```
denote the index set {1,2,..., m»} by M;, with a typical element of M»
```
designated as n^.
```
(3) If Pj chooses a strategy rij € M,,-, and this so for all j € N, then the
```
loss incurred to Pi is a single number anljn2 HN' ^e ordered TV-tuple
```
of all these numbers (over i e N), i.e., (aij,...,^,^,...,^,^,...,^),
```
constitutes the corresponding unique outcome of the game.
```
(4) Players make their decisions independently and each one unilaterally seeks
```
the minimum possible loss, of course by also taking into account the pos-
sible rational choices of the other players.
The noncooperative equilibrium solution concept within the context of this
TV-person game can be introduced as follows as a direct extension of Def. 3.1.
```
Definition 3.7 An N-tuple of strategies {n^n^,... ,n^}; with n* 6 M^, i e
```
```
N, is said to constitute a noncooperative (Nash) equilibrium solution for an
```
N-person nonzero-sum static finite game in normal form, as formulated above,
```
if the following N inequalities are satisfied for all n^ € Mi; i e N:
```
88 T. BA§AR AND G. J. OLSDERTV-PERSON NONZERO-SUM FINITE GAMES 89
```
Here, the N-tuple (a1*, a 2 * , . . . , aN*) is known as a noncooperative (Nash) equi-
```
librium outcome of the N-person game in normal form.
There is actually no simple method to determine the Nash equilibrium so-
lutions of TV-person finite games in normal form. One basically has to check
exhaustively all possible combinations of AT-tuples of strategies, to see which
ones provide a Nash equilibrium. This enumeration, though straightforward,
could at times be rather strenuous, especially when N and/or ra^, i E N, are
large. However, given an ./V-tuple of strategies asserted to be in Nash equilib-
rium, it is relatively simpler to verify their equilibrium property, since one then
has to check only unilateral deviations from the given equilibrium solution. To
get a flavor of the enumeration process and the method of verification of a Nash
equilibrium solution, let us now consider the following example.
Example 3.3 Consider a 3-person game in which each player has two alterna-
tives to choose from. That is, N = 3 and m\ = m<2 = m^ = 2. To complete the
description of the game, the 23 = 8 possible outcomes are given as
where the entries of the former matrix correspond to the possible outcomes if
P3's strategy is fixed at n3 = 1, and the latter matrix provides possible outcomes
if his strategy is fixed at n% = 2. We now assert that the "starred" entry is a Nash
equilibrium outcome for this game. A verification of this assertion would involve
```
These possible outcomes can actually be displayed in the form of two (2 x 2)
```
matrices
90 T. BA§AR AND G. J. OLSDER
three separate checks, concerning unilateral deviation of each player. If PI
deviates from this asserted equilibrium strategy HI = 1, then his loss becomes
2 which is not favorable. If P2 deviates from n^ = 1, his loss becomes 1 which
is not favorable either. Finally, if P3 deviates from 713 = 1, his loss becomes 1
which is higher than his asserted equilibrium loss 0. Consequently, the first entry
```
of the first matrix, i.e., (1, —1,0), indeed provides a Nash equilibrium outcome,
```
```
with the corresponding equilibrium strategies being {n\ = \,n\ = 1,77,3 — !)•
```
The reader can now check by enumeration that this is actually the only Nash
equilibrium solution of this 3-person game.
When the Nash equilibrium solution of an TV-person nonzero-sum game is
not unique, we can again introduce the concept of "admissible Nash equilibrium
solution" as direct extensions of Defs. 3.2 and 3.3 to the TV-person case. It is of
course possible that a given TV-person game will admit more than one admissible
Nash equilibrium solution which are also not interchangeable. This naturally
```
leads to an ill-defined equilibrium outcome; but we will not elaborate on these
```
aspects of the equilibrium solution here since they were extensively discussed in
the previous section within the context of bimatrix games, and that discussion
can readily be carried over to fit the present framework. Furthermore, Prop. 3.1
has a natural version in the present framework, which we quote below without
proof, after introducing an extension of Def. 3.4. Proposition 3.2, on the other
hand, has no direct counterpart in an TV-person game with TV > 2.
Definition 3.8 Two nonzero-sum finite static games in normal form are said
to be strategically equivalent if the following three conditions are satisfied:
```
(i) The two games have the same number of players (say N),
```
```
(ii) each player has the same number of alternatives in both games,
```
Proposition 3.4 All strategically equivalent nonzero-sum finite static games in
normal form have the same set of Nash equilibria.
Minimax strategies introduced earlier within the context of bimatrix games
```
(Def. 3.5) find applications also in TV-person games, especially when there exist
```
more than one admissible Nash equilibrium which are not interchangeable. In
the present context their definition involves a natural extension of Def. 3.5,
which we do not provide here.
for all
```
(Hi) »/{(ai1>...,nAf,..-,a£r1,...,n;v),n» e Mi,« € N} is the set of possible out-
```
```
comes in one game, and {(&ni,...,njv>-• • >^i,...,nN )in i ^ Mi,z 6 N} is the
```
set of possible outcomes in the other, then there exist positive constants
oti, i e N, and scalars fa, i G N, such that
^-PERSON NONZERO-SUM FINITE GAMES 91
Example 3.4 In the 3-person game of Example 3.3, the minimax strategies
of PI, P2 and P3 are n\ = 1 or 2, h<2 = 1 and 77,3 = 1, respectively. It
should be noted that for PI any strategy is minimax. The security levels of the
```
players, however, are unique (as they should be) and are 2, 1 and 1, respectively.
```
Note that these values are higher than the unique Nash equilibrium outcome
```
(1,—1,0), in accordance with the statement of Remark 3.2 altered to fit the
```
present context.
Mixed-strategy Nash equilibria
Mixed noncooperative equilibrium solutions of an ./V-person finite static game
in normal form can be introduced by extending Def. 3.6 to N players, i.e., by
```
replacing inequalities (3.7a) and (3.7b) by an iV-tuple of similar inequalities.
```
To this end, let us first introduce the notation Yl to denote the mixed strategy
space of Pi, and further denote a typical element of this space by yz, and its
fcth component by ylk. Then, we have the following definition.
```
Definition 3.9 An N-tuple {yl* G Yl;i e N} is said to constitute a mixed-
```
```
strategy noncooperative (Nash) equilibrium solution for an N-person finite
```
static game in normal form if the following N inequalities are satisfied for all
yi € y', j € N:
```
Here, the N-tuple (J1 *,..., JN*} is known as a noncooperative (Nash) equilib-
```
rium outcome of the N-person game in mixed strategies.
One of the important results of static game theory is that every TV-person
game of the type discussed in this section admits a mixed-strategy Nash equilib-
```
rium solution (note that pure strategies are also included in the class of mixed
```
```
strategies). We now state and prove this result which is also an extension of
```
Thm. 3.1.
Theorem 3.2 Every N-person static finite game in normal form admits a non-
```
cooperative (Nash) equilibrium solution in mixed strategies.
```
we observe that T is a continuous mapping of ITyv^1 into itself. Since each
```
Yl is a simplex of appropriate dimension (see Section 2.2), the product set
```
Tl^Y1 becomes closed, bounded and convex. Hence, by Brouwer's fixed point
```
theorem (see Appendix C), the mapping T has at least one fixed point. We
```
now prove that every fixed point of this mapping is necessarily a mixed-strategy
Nash equilibrium solution of the game, and conversely that every mixed-strategy
Nash equilibrium solution is a fixed point of T, thereby concluding the proof of
the theorem.
```
We first verify the latter assertion: If {y1*; i 6 N} is a mixed-strategy equi-
```
```
librium solution, then, by definition (i.e., from inequalities (3.11)), the function
```
```
V£. (y1*,..., yN*} is nonpositive for every rii e Mi and i € N, and thus cln. = 0
```
for all Hi 6 Mi, i £ N. This readily yields the conclusion
92 T. BA§AR AND G. J. OLSDER
and denoting it by T:
```
for each n» € Mi and i e N. It readily follows that cln.(yl,... ,yN) is also a
```
continuous function on Yl^Y1. Then, introducing the transformation
for each ni € Mi, and i G N. Note that this function is continuous on the
product set HpfY*. Now, let cln. be related to i/j^. by
```
Proof. Let {yl € Yl;i G N} denote an Af-tuple of mixed strategies for the
```
TV-person game, and introduce
```
which is that {yl*; i e N} is a fixed point of T.
```
```
For verification of the former assertion, suppose that {yl',i G N} is a fixed
```
```
point of T, but not a mixed equilibrium solution. Then, for some i € N (say
```
```
i = 1) there exists a yl G Yl such that
```
Now let HI denote an index for which the quantity
```
since Y^M cni > ^- But tn*s contradicts the hypothesis that {yl;i € N} is a
```
fixed point of T.
The proof of Thm. 3.2, as given above, is not constructive. For a constructive
proof, which also leads to a numerical scheme to obtain the equilibrium solution,
```
the reader is referred to Scarf (1967). But, in general, it is not possible to
```
obtain the equilibrium solution explicitly. If a given game admits an inner
mixed-strategy equilibrium solution, however, then a possibility of obtaining the
corresponding strategies in explicit form emerges, as the following proposition
```
(which could also be considered as an extension of Remark 3.3 and Prop. 3.3)
```
indicates.
Proposition 3.5 Let Yl denote the interior ofY1 with i G N. Then, any inner
```
mixed Nash equilibrium solution {yl* e Yl;i € N} of an N-person finite static
```
game in normal form satisfies the set of equations
which can further be seen to be equivalent to
attains its minimum value over n\ e MI. Then, since y1 is a mixed strategy,
```
the RHS of (3.14) can be bounded below by (3.15) with n\ = ni, thus yielding
```
the strict inequality
iV-PERSON NONZERO-SUM FINITE GAMES 93
```
This then implies that c\i = 0, which, when used in (3.12) with i = 1 and
```
^i — ™i> yields the conclusion
This inequality, when used in the definition of c^, implies that c^ > 0. But
since c\li is nonnegative for all n\ € MI, the summation term ^M cni becomes
positive.
```
Now, again referring back to inequality (3.14), this time we let n\ denote an
```
```
index for which (3.15) attains its maximum value over n\ e MI. Then, going
```
through an argument similar to the one used in the preceding paragraph, we
```
bound the LHS of (3.14) from above by (3.15) with ni — n\, and arrive at the
```
strict inequality
94 T. BA§AR AND G. J. OLSDER
Furthermore, these mixed strategies also provide a mixed-strategy Nash equilib-
rium solution for the N-person game structured in a similar way but with some
```
(or all) of the players maximizing their average costs instead of minimizing.
```
Now, since the minimizing y1* is an inner point of Yl by hypothesis, and since
```
the preceding expression is linear in {y^ni e Mx,ni ^ 1}, it readily follows
```
that the coefficient of y^ has to vanish for each HI € MI, n\ ^ \. This
```
condition then yields the first set of equations of (3.17). The remaining ones
```
can be verified analogously, by taking i = 2 , 3 , . . . , AT. Finally, the last part of
Prop. 3.5 follows, as in Prop. 3.3, from the property that at the equilibrium
```
point {yl* € Yl,i € N, i ^ j} Pj's average cost is actually independent of
```
```
yj (which has just been proven), and thus it becomes immaterial whether he
```
minimizes or maximizes his average cost.
Since the hypothesis of Prop. 3.5 requires the mixed-strategy Nash equilib-
rium solution to be an inner point of the product set Y1 x • • • x YN, the set of
```
equations (3.17) is satisfied under quite restrictive conditions, and even more so
```
```
under the conjecture of Problem 5 (Section 3.8), since then it is required that the
```
number of alternatives of each player be the same. Nevertheless, the proposition
still provides a characterization of the equilibrium solution, and enables direct
computation of the corresponding strategies by merely solving coupled algebraic
equations, under conditions which are not totally void. As an illustration of this
point we now provide the following example.
Example 3.5 Consider a 3-person game in which each player has two alterna-
tives to choose from and in which the possible outcomes are given by
Proof. Since
quantity
is an equilibrium solution, y1* minimizes the
over Yl, and with i G N. Let us first take i = 1, and rewrite the preceding
```
expression (in view of the relation £)MI y^ = 1) as
```
3.4 Computation of Mixed-Strategy Nash
Equilibria in Bimatrix Games
We have discussed in Section 3.3 a possible approach toward obtaining mixed-
strategy Nash equilibria of iV-person games in normal form when these equilibria
have the property of being inner solutions. In other words, when a mixed-
strategy equilibrium solution assigns positive probabilities to all possible pure-
strategy choices of a player, and this so for all players, then the corresponding
equilibrium strategies can be determined by solving a set of algebraic equations
```
(cf. Prop. 3.5).
```
For the special case of bimatrix games, this requirement of Prop. 3.5 says
that the pair of mixed Nash equilibrium strategies y* and z* should be elements
of Y and Z, the interiors of Y and Z, respectively, which is a rather restrictive
condition. If this condition is not satisfied, then one natural approach would
be to set some components of the strategies y and z equal to zero and obtain
algebraic equations for the other components, in the hope that the solutions of
these algebraic equations will be nonnegative. If such a solution does not exist,
then one can set some other components equal to zero and look for a nonnegative
solution of the algebraic equation obtained from the other components, and so
on. Since there is only a finite number of possibilities, it is obvious that such an
approach will eventually yield mixed-strategy Nash equilibria of bimatrix games,
also in view of Thm. 3.2. But, then, the natural question that comes to mind
is whether this search can be done in a systematic way. Indeed it can, and this
```
has been established by Lemke and Howson (1964) in their pioneering work.
```
In that paper, the authors' intention has actually been to give an algebraic
TV-PERSON NONZERO-SUM FINITE GAMES 95
```
Since Mj consists of two elements for each i = 1,2,3, the set (3.17) involves
```
only three equations for this game, which can be written as
This coupled set of equations admits a unique solution with the property 0 <
3/2>J/2>2/i < !' which is
Hence, this 3-person game admits a unique inner Nash equilibrium solution in
mixed strategies, which is
It should be noted that this game admits also two pure-strategy Nash equilibria
```
with outcomes (1,—1,0) and (—1,2,0).
```
96 T. BA§AR AND G. J. OLSDER
```
proof of the existence of mixed equilibrium solutions in bimatrix games (i.e.,
```
```
the result of Thm. 3.1), but, as a byproduct, they also obtained an efficient
```
scheme for computing mixed Nash equilibria, which was thereafter referred to
as the "Lemke-Howson" algorithm. The reader should consult the original work
```
(Lemke and Howson, 1964) and the expository article by Shapley (1974) for the
```
essentials of this algorithm and its application in bimatrix games.
Relation to nonlinear programming
Yet another general method for the solution of a bimatrix game is to transform
```
it into a nonlinear (in fact, a bilinear) programming problem, and to utilize the
```
numerical techniques developed for solutions of nonlinear programming prob-
lems. In the sequel, we establish this equivalence between bimatrix games and
```
a specific class of bilinear programming problems with linear constraints (see
```
```
Prop. 3.6); we refer the reader to Luenberger (1973) for algorithms on the nu-
```
merical solution of nonlinear programming problems.
```
Proposition 3.6 A pair {y*,z*} constitutes a mixed-strategy Nash equilibrium
```
```
solution to a bimatrix game (A, B] if, and only if, there exists a pair (p*, q*) such
```
```
that {y*,£*,£>*,<?*} is a solution of the following bilinear programming problem:
```
minV,z,P,q
subject to
Proof. The constraints evidently imply that
which shows that the optimal value of the objective function is nonnegative. If
```
{y*,z*} is an equilibrium pair, then the quadruple
```
is feasible and the corresponding value of the objective function is zero. Hence
```
(ii) is an optimal solution to the bilinear programming problem.
```
```
Conversely, let {y, z,p, q} be a solution to the bilinear programming problem.
```
The fact that feasible solutions to this problem exist has already been established
```
by (ii). Specifically, by Thm. 3.2, the bimatrix game (A,B) has an equilibrium
```
```
pair (y*,2*) and hence (ii) is a feasible solution. Furthermore, (ii) also yields
```
the conclusion that the minimum value of the objective function is nonpositive,
```
which, in view of (i), implies
```
/V-PERSON NONZERO-SUM FINITE GAMES 97
```
This last set of inequalities verifies that {y, z} is indeed a mixed-strategy Nash
```
```
equilibrium solution of the bimatrix game (A, B).
```
For the special case of zero-sum matrix games, the following corollary now
readily follows from Prop. 3.6, and establishes an equivalence between two-
person zero-sum matrix games and LP problems.
3.5 Nash Equilibria of TV-Person Games in
Extensive Form
```
This section is devoted to noncooperative (Nash) equilibria of TV-person finite
```
games in extensive form, which do not incorporate chance moves. Extensive
tree formulation for finite games without chance moves has already been intro-
```
duced in Chapter 2 within the context of zero-sum games (cf. Def. 2.5), and
```
such a formulation is equally valid for finite nonzero-sum games, with certain
appropriate modifications. Hence first, as a direct extension of Def. 2.5, we have
the following definition which covers both single-act and multi-act games.
Definition 3.10 An extensive form of an N-person nonzero-sum finite game
without chance moves is a tree structure with
```
(i) a specific vertex indicating the starting point of the game,
```
and hence,
In particular, we have
and therefore,
Thus
```
Corollary 3.2 A pair {y*,z*} constitutes a mixed-strategy saddle-point solu-
```
tion for a two-person zero-sum matrix game A if, and only if, there exists a pair
```
(p*,<?*) such that {y*,2*,p*,g*} is a solution of the LP problem
```
subject to
Remark 3.4 Note that Prop. 3.6 directly extends to TV-person finite games
in normal form. Furthermore, it is noteworthy that the LP problem of Corol-
lary 3.2 is structurally different from the LP problem considered in Section 2.3
```
(Thm. 2.5), which also provided a means of obtaining mixed-strategy saddle-
```
point solutions for matrix games.
98 T. BA§AR AND G. J. OLSDER
```
(ii) N cost functions, each one assigning a real number to each terminal vertex
```
of the tree, where the ith cost function determines the loss to be incurred
to Pi,
```
(Hi) a partition of the nodes of the tree into N player sets,
```
```
(iv) a subpartition of each player set into information sets {rfj}, such that the
```
same number of branches emanates from every node belonging to the same
information set and no node follows another node in the same information
set.
Figure 3.1: Two typical nonzero-sum finite games in extensive form.
Two typical nonzero-sum finite games in extensive form are depicted in
Fig. 3.1. The first one represents a 3-player single-act nonzero-sum finite game
in extensive form in which the information sets of the players are such that
both P2 and P3 have access to the action of PI. The second extensive form of
Fig. 3.1, on the other hand, represents a 2-player multi-act nonzero-sum finite
game in which PI acts twice and P2 only once. In both extensive forms, the set
of alternatives for each player is the same at all information sets and it consists
of two elements. The outcome corresponding to each possible path is denoted
```
by an ordered TV-tuple of numbers (a1 ,..., a^), where N stands for the number
```
of players and a1 stands for the corresponding cost to Pi.
Pure-strategy Nash equilibria
```
We now introduce the concept of a noncooperative (Nash) equilibrium solution
```
for ./V-person nonzero-sum finite games in extensive form, which covers both
single-act and multi-act games. To this end, let us first recall the definition of
```
a strategy from Chapter 2 (cf. Def. 2.6).
```
Definition 3.11 Let Nl denote the class of all information sets of Pi, with a
typical element designated as rf. Let V%i denote the set of alternatives of Pi
TV-PERSON NONZERO-SUM FINITE GAMES 99
at the nodes belonging to the information set rf. Define Ul — U U^, where
the union is over rf G N*. Then, a strategy 7* for Pi is a mapping from N1
into Ul, assigning one element in Ul for each set in Nl, and with the further
```
property that ^(rf} e U\ for each rf 6 N*. The set of all strategies of Pi is
```
```
called his strategy set (space), and it is denoted by P.
```
```
Let Ji (71 ,... ,7^) denote the loss incurred to Pi when the strategies 71 6
```
```
F 1 , . . . , 7N € FN are adopted by the players. Then, the noncooperative (Nash)
```
equilibrium solution concept for such games can be introduced as follows, as a
direct extension of Def. 3.7.
```
Definition 3.12 An N-tuple of strategies {71*,72*,... ,7^*} with 7" € P,
```
```
i € N, is said to constitute a noncooperative (Nash) equilibrium solution for
```
an N-person nonzero-sum finite game in extensive form, if the following N
inequalities are satisfied for all 7* G P, i 6 N:
```
The N-tuple of quantities (J1 *,..., J^*} is known as a Nash equilibrium out-
```
come of the nonzero-sum finite game in extensive form.
We emphasize the word a in the last sentence of the preceding definition,
since the Nash equilibrium solution could possibly be nonunique with the corre-
sponding set of Nash values being different. This then leads to a partial ordering
in the set of all Nash equilibrium solutions, as in the case of Defs. 3.2 and 3.3
whose extensions to the TV-person case are along similar lines and are therefore
omitted.
Nash equilibria in mixed and behavioral strategies
The concepts of noncooperative equilibria in mixed and behavioral strategies
for nonzero-sum finite games in extensive form can be introduced as straight-
```
forward (natural) extensions of the corresponding ones presented in Chapter 2
```
within the context of saddle-point equilibria. We should recall that a mixed
```
strategy for a player (Pi) is a probability distribution on the set of all his pure
```
strategies, i.e., on P. A behavioral strategy, on the other hand, is an appropri-
ate mapping whose domain of definition is the class of all the information sets of
the player. By denoting the behavioral strategy set of Pi by P, and the average
loss incurred to Pi as a result of adoption of the behavioral strategy TV-tuple
```
{71 € f1 ,...,^ e f N } by J* (7 V - - , 7W )> the definition of a Nash equilibrium
```
100 T. BA§AR AND G. J. OLSDER
solution in behavioral strategies may be obtained directly from Def. 3.12 by
replacing 7*, P and Jl by 7*, f* and J\ respectively.
We now discuss, in the four subsections to follow, properties and derivation
of these different types of Nash equilibria for finite games in extensive form—first
for single-act games, and then for multi-act games.
3.5.1 Single-act games: Pure-strategy Nash equilibria
In single-act games, each player acts only once, and the order in which the
players act could be a variant of their strategies. If the order is fixed a priori,
```
and further if the single-act game is static in nature (i.e., if each player has
```
```
a single information set), then there is no basic difference between extensive
```
and normal form descriptions, and consequently no apparent advantage of one
form over the other. Since such static games in normal form have already been
extensively studied in Sections 3.2 and 3.3, our concern here will be primarily
on dynamic single-act games, that is, games in which at least one of the players
has access to some nontrivial information concerning the action of some other
```
player. Figure 3.1 (a), for example, displays one such game wherein both P2 and
```
P3 have access to Pi's action, but they are ignorant about each other's actions.
Yet another single-act game with dynamic information is the 2-person nonzero-
sum game depicted in Fig. 3.2, wherein P2's information sets can differentiate
between whether PI has played L or not, but not between his possible actions
M and R.
Figure 3.2: A 2-person nonzero-sum single-act game with dynamic information.
One method of obtaining Nash equilibria of a dynamic single-act finite game
in extensive form is to transform it into an equivalent normal form and to make
use of the theory of Sections 3.2 and 3.3. This direct method has already been
discussed earlier in Section 2.4 within the context of similarly structured zero-
sum games, and it has the basic drawback that one has to take into consideration
all possible strategy combinations. As in the case of dynamic zero-sum games,
an alternative to this direct method exists for some special types of nonzero-sum
games, which makes explicit use of the extensive form description, and obtains a
```
subclass of all Nash equilibria through a systematic (recursive) procedure. But
```
N-PERSON NONZERO-SUM FINITE GAMES 101
in order to aid in understanding the essentials and the limitations of such a
recursive procedure, it will be instructive first to consider the following specific
example. This simple example, in fact, displays most of the intricacies of Nash
equilibria in dynamic finite games.
Example 3.6 Consider the 2-person nonzero-sum single-act game whose ex-
tensive form is displayed in Fig. 3.2. The following is an intuitively appealing
recursive procedure that would generate a Nash equilibrium solution for this
game.
```
At the first of his two information sets (counting from left), P2 can tell
```
precisely whether PI has played L or not. Hence, in case ul — L, the unique
decision of P2 would be u2 = L, yielding him a cost of —1 which is in fact the
lowest possible level P2 can hope to attain. Now, if u1 = M or R, however,
P2's information set does not differentiate between these two actions of PI,
thus forcing him to play a static nonzero-sum game with PI. The corresponding
```
bimatrix game (under the usual convention of A denoting the loss matrix of PI
```
```
and B denoting that of P2) is
```
```
This bimatrix game admits (as indicated) a unique Nash equilibrium {R, R}
```
```
with an equilibrium cost pair of ( — 1,0). This then readily suggests
```
as a candidate equilibrium strategy for P2.
PI, on the other hand, has two possible strategies, ul — L and u1 = -R, since
the third possibility u1 = M is ruled out by the preceding argument. Now, the
```
former of these leads (under (3.25a)) to a cost of J1 = 0, while the latter leads
```
to J1 = — 1. Hence, he would clearly decide on playing R, i.e.,
```
The strategy pair (3.25a)-(3.25b) is thus what an intuitively appealing (and
```
```
rather straightforward) recursive procedure would provide us with, as a rea-
```
sonable noncooperative equilibrium solution for this extensive form. It can, in
```
fact, be directly verified by referring to inequalities (3.23) that the pair (3.25a)-
```
```
(3.25b) is indeed in Nash equilibrium. To see this, let us first fix P2's strategy
```
```
at (3.25a), and observe that (3.25b) is then the unique cost-minimizing deci-
```
```
sion for PI. Now, fixing Pi's strategy at (3.25b), it is clear that P2's unique
```
is a candidate equilibrium strategy for PI. The cost pair corresponding to
```
(3.25a)-(3.25b) is
```
102 T. BA§AR AND G. J. OLSDER
```
cost-minimizing decision is u2 = R which is indeed implied by (3.25a) under
```
```
(3.25b).
```
The recursive scheme adopted surely resulted in a unique strategy pair which
is also in Nash equilibrium. But is this the only Nash equilibrium solution that
the extensive form of Fig. 3.2 admits? The reply is, in fact, no! To obtain the
```
additional Nash equilibrium solution (s), we first transform the extensive form
```
of Fig. 3.2 into an equivalent normal form. To this end, let us first note that
```
T1 - {L,M,R} and T2 = {72,722,732,742}, where 7l2(r/2) = I, 7ffo') = Rj
```
Hence, the equivalent normal form is the 3 x 4 bimatrix game
which admits two pure-strategy Nash equilibria, as indicated. The encircled
```
one is {#573} which corresponds to the strategy pair (3.25a)-(3.25b) obtained
```
through the recursive procedure. The other Nash equilibrium solution is the
constant strategy pair
with a corresponding cost pair of
```
A comparison of (3.25c) and (3.26c) clearly indicates that both of these equilibria
```
are admissible.
```
It is noteworthy that, since (3.26a) is a constant strategy, the pair (3.26a)-
```
```
(3.26b) also constitutes a Nash equilibrium solution for the static single-act
```
game obtained from the one of Fig. 3.2 by replacing the dynamic information of
```
P2 with static information (i.e., by allowing him a single information set). It is,
```
in fact, the unique Nash equilibrium solution of this static game which admits
```
the normal form (bimatrix) description
```
```
The actual play (R, R) dictated by (3.25a)-(3.25b), however, does not possess
```
such a property as a constant strategy pair.
AT-PERSON NONZERO-SUM FINITE GAMES 103
The preceding example has displayed certain important features of Nash
equilibria of a single-act game, which we now list below. We will shortly see, in
this section, that these features are, in fact, valid on a broader scale for both
single-act and multi-act nonzero-sum games.
Features of the single-act game of Example 3.6
```
(i) The single-act game admitted multiple (two) Nash equilibria.
```
```
(ii) The (unique) Nash equilibrium solution of the static version of the single-
```
```
act game (obtained by replacing the information sets of the second-acting
```
```
player by a single information set) also constituted a Nash equilibrium
```
solution for the original single-act game with the dynamic information.
```
(iii) A recursive procedure that involves solution of only bimatrix games (which
```
```
could also be degenerate games) at each information set yielded only one
```
of the two Nash equilibria. The actual play dictated by this equilibrium
solution did not constitute an equilibrium solution as constant strategies.
```
We now show that appropriate (and more general) versions of these features
```
are retained in a more general framework for ]V-person single-act games. To this
end, we first introduce some terminology and also a partial ordering of extensive
forms in terms of their information sets.
```
Definition 3.13 A single-act N-person finite game in extensive form (say, I)
```
is the static version of a dynamic single-act N-person finite game in extensive
```
form (say, II), if I can be obtained from II by replacing the information sets of
```
each player with a single information set encompassing all the nodes pertaining
to that player.16 The equivalent normal form of the static game I is called the
static normal form of II.
Definition 3.14 Let I and II be two single-act N-person games in extensive
```
form, and further let F| and Tlu denote the strategy sets of Pi (i e N) in I and
```
II, respectively. Then, I is said to be informationally inferior to II if F| C F|j
for all i 6 N, with strict inclusion for at least one i.
Remark 3.5 In Fig. 3.3, the single-act game I is informationally inferior to II
and III. The latter two, however, do not admit any such comparison.
Proposition 3.7 Let I be an N-person single-act game that is informationally
inferior to some other single-act N-person game, say II. Then,
```
(i) any Nash equilibrium solution of I also constitutes a Nash equilibrium
```
solution for II,
16It should be noted that a single-act finite game admits a static version only if the order
in which the players act is fixed a priori and the possible actions of each player are the same
at all of his information sets.
104 T. BA§AR AND G. J. OLSDER
Figure 3.3: Extensive forms displaying informational inferiority.
```
(ii) if {7*,... ,7^} is a Nash equilibrium solution of II so that jl e F| for all
```
i € N, then it also constitutes a Nash equilibrium solution for I.
```
Proof, (i) If {71* € FI,...^^* e T^} constitutes a Nash equilibrium
```
```
solution for I, then inequalities (3.23) are satisfied for all 7* € F|, i e N. But,
```
since F| C FJj, i e N, we clearly also have 7* 6 F|j, i € N. Now assume, to
```
the contrary, that {71*,..., 7^*} is not a Nash equilibrium solution of II. Then,
```
```
this implies that there exists at least one i (say, i = N, without any loss of
```
```
generality) for which the corresponding inequality of (3.23) is not satisfied for
```
all 7* e F|j. In particular, there exists a 7^ 6 F^f such that
```
Now, the TV-tuple of strategies {71*,..., 7^ 1*,^N} leads to a unique path of
```
action, and consequently to a unique outcome, in the single-act game II. Let us
denote the information set of P7V, which is actually traversed by this path, by
```
fjii, and the specific element (node) of 77^ intercepted by nN. Let us further
```
denote the information set of PN in game I, which includes the node nN, by
```
fji*. Then, there exists at least one element in F^ (say, 7^) with the property
```
```
7^(7?!^) — T^C^n )• ^ thus strategy replaces 7^ on the RHS of inequality (i),
```
the value of JN clearly does not change, and hence we equivalently have
But this inequality contradicts the initial hypothesis that the TV-tuple of policies
```
{71*;...; 7^*} was in Nash equilibrium for the game I. This then completes the
```
```
proof of part (i).
```
```
(ii) Part (ii) of the proposition can be proven analogously.
```
JV-PERSON NONZERO-SUM FINITE GAMES 105
```
Remark 3.6 Proposition 3.7 now verifies a general version of feature (ii) of the
```
single-act game of Example 3.6 for iV-person single-act finite games in extensive
```
form. A counterpart of feature (i) also readily follows from this proposition,
```
which is that such games with dynamic information will in general admit mul-
tiple Nash equilibria. A more definite statement cannot be made, since there
would always be exceptional cases when the Nash equilibrium solution of a
single-act dynamic game is unique and is attained in constant strategies which
definitely also constitute a Nash equilibrium solution for its static version, as-
suming that it exists. It is almost impossible to single out all these special
games, but we can comfortably say that they are "rarely" met, and existence
of multiple Nash equilibria is a rule in dynamic single-act games rather than
an exception. Since this sort of nonuniqueness emerges mainly because of the
dynamic nature of the information sets of the players, we call it informational
nonuniqueness.
```
Now, to elaborate on the third (and the last) feature listed earlier, we have
```
to impose some further structure on the "relative nestedness" of the information
sets of the players, which is already implicit in the extensive form of Fig. 3.2. It
is, in fact, possible to develop a systematic recursive procedure, as an extension
of the one adopted in the solution of Example 3.6, if the order in which the
players act is fixed a priori and the extensive form is in "ladder-nested" form—
a notion that we introduce below.
Definition 3.15 In an extensive form of a single-act nonzero-sum finite game
with a fixed order of play, a player Pi is said to be a precedent of another
player Pj if the former is situated closer to the vertex of the tree than the lat-
ter. The extensive form is said to be nested if each player has access to the
information acquired by all his precedents. If, furthermore, the only difference
```
(if any) between the information available to a player (Pi) and his closest (im-
```
```
mediate) precedent (say Pi — I) involves only the actions of Pi — I , and only at
```
those nodes corresponding to the branches of the tree emanating from singleton
information sets of Pi — 1, and this so for all players, the extensive form is
said to be ladder-nested.17 A single-act nonzero-sum finite game is said to be
```
nested (respectively, ladder-nested,) if it admits an extensive form that is nested
```
```
(respectively, ladder-nested).
```
```
Remark 3.7 The single-act extensive forms of Figs. 3.1(a) and 3.2 are both
```
```
ladder-nested. If the extensive form of Fig. 3.1 (a) is modified so that both
```
nodes of P2 are included in the same information set, then it is only nested,
but not ladder-nested, since P3 can differentiate between different actions of PI
```
but P2 cannot. Finally, if the extensive form of Fig. 3.1 (a) is modified so that
```
```
this time P3 has a single information set (see Fig. 3.4(a)), then the resulting
```
extensive form becomes non-nested, since then even though P2 is a precedent
of P3 he actually knows more than P3 does. The single-act game that this
17Note that in 2-person single-act games the concepts of "nestedness" and "ladder-
nestedness" coincide, and every extensive form is, by definition, ladder-nested.
106
Figure 3.4: Two extensive forms of the same nested single-act game.
extensive form describes is, however, nested since it also admits the extensive
```
form description depicted in Fig. 3.4(b).
```
One advantage of dealing with ladder-nested extensive forms is that they can
recursively be decomposed into simpler tree structures which are basically static
in nature. This enables one to obtain a class of Nash equilibria of such games
recursively, by solving static games at each step of the recursive procedure.
Before providing the details of this recursive procedure, let us introduce some
terminology.
Definition 3.16 For a given single-act dynamic game in nested extensive form
```
(say, I), let 77 denote a singleton information set of Pi's immediate follower
```
```
(say Pj); consider the part of the tree structure of I, which is cut off at 77, has
```
TI as its vertex and has as immediate branches only those that enter into that
information set ofPj. Then, this tree structure is called a sub-extensive form
```
of I. (Here, we adopt the convention that the starting vertex of the original
```
```
extensive form is the singleton information set of the first-acting player.)
```
Remark 3.8 The single-act game depicted in Fig. 3.2 admits two sub-extensive
forms which are as follows:
```
The extensive form of Fig. 3.1 (a), on the other hand, admits a total of four
```
sub-extensive forms which we do not display here. It should be noted that each
sub-extensive form is itself an extensive form describing a simpler game. The
T. BA§AR AND G. J. OLSDERJV-PERSON NONZERO-SUM FINITE GAMES 107
first one displayed above describes a degenerate 2-player game in which PI has
only one alternative. The second one again describes a 2-player game in which
the players each have two alternatives. Both of these sub-extensive forms will
be called static since the first one is basically a one-player game and the second
one describes a static 2-player game. A precise definition follows.
Definition 3.17 A sub-extensive form of a nested extensive form of a single-
act game is static if every player appearing in this tree structure has a single
information set.
We are now in a position to extend the recursive procedure adopted in Exam-
```
ple 3.6 to obtain (some of) the Nash equilibrium solutions of ./V-person single-act
```
games in ladder-nested extensive form.
A recursive procedure to determine pure-strategy Nash equilibria of
AT-person single-act games in ladder-nested extensive form
```
(1) For each fixed information set of the last-acting player (say, PA7"), single
```
out the players who have precisely the same information as PAT, determine
the static sub-extensive form that includes all these players and their single
information sets, and solve the static game that this sub-extensive form
describes. Assuming that each of these static games admits a unique pure-
strategy Nash equilibrium solution, record the equilibrium strategies and
the corresponding Nash outcomes by identifying them with the players
```
and their information sets. (If any one of the static games admits more
```
than one Nash equilibrium solution, then this procedure as well as the
```
following steps are repeated for each of these multiple equilibria.)
```
```
(2) Replace each static sub-extensive form considered at step 1 with the im-
```
mediate branch emanating from its vertex that corresponds to the Nash
```
strategy of the starting player of that sub-extensive form; furthermore,
```
attach the corresponding Ar-tuple of Nash equilibrium values to the end
of this branch.
```
(3) For the remaining game in extensive form, repeat steps 1 and 2 until the
```
extensive form left is a tree structure comprising only a vertex and some
immediate branches with an JV-tuple of numbers attached to each of them.
```
(4) The branch of this final tree structure which corresponds to a minimum
```
loss for the starting player is his Nash equilibrium strategy, and the cor-
responding Ar-tuple of numbers at the end of this branch determines a
Nash outcome for the original game. The corresponding Nash equilib-
rium strategies of the other players in the original single-act game can
then be determined from the solutions of the static sub-extensive forms,
by appropriately identifying them with the players and their information
sets.
108 T. BA§AR AND G. J. OLSDER
Remark 3.9 Because of the ladder-nestedness property of the single-act game,
it can readily be shown that every solution obtained through the foregoing
recursive procedure is indeed a Nash equilibrium solution of the original single-
act game.
A most natural question to raise now is whether the preceding recursive pro-
cedure generates all the Nash equilibrium solutions of a given Af-person single-
act game in ladder-nested extensive form. We have already seen in Example 3.6
that this is, in fact, not so, since out of the two Nash equilibrium solutions,
only one of them was generated by this recursive procedure. Then, the question
may be rephrased as: precisely what subclass of Nash equilibria is generated by
```
the preceding scheme? In other words, what common feature (if any) can we
```
attribute to the Nash equilibria that can be derived recursively? To shed some
light on this matter, it will be instructive to refer again to Example 3.6 and to
take a closer look at the two Nash equilibria obtained:
```
EXAMPLE 3.6 (continued) For the single-act game of Fig. 3.2, the recursive
```
```
procedure generated the solution (3.25a)-(3.25b) with Nash cost pair (—1,0).
```
```
When compared with the other Nash cost pair which is (0, —1), it is clear that
```
the former is more advantageous for PI. Hence, P2 would rather prefer to
```
play the constant strategy (3.26a) and thereby attain a favorable cost level of
```
```
J2 = —1. But, since he acts later, the only way he can ensure such a cost level is
```
by announcing his constant strategy ahead of time and by irrevocably sticking
to it. If he has the means of doing this, and further if PI has strong reasons to
```
believe in such an attitude on the part of P2, then (0, —1) will clearly be the
```
only cost pair to be realized. Hence, the constant-strategy Nash solution in this
case corresponds to a "prior commitment" mode of play on the part of the last
acting player—a notion which has been introduced earlier in Section 2.5 within
the context of multi-act zero-sum games, and which also has connections with
the Stackelberg mode of play to be discussed in the next section of this chapter.
```
Now, yet another mode of play for P2 (the second-acting player) would be
```
to wait until he observes at what information set he is at the time of his play,
and only then decide on his action. This is known as the "delayed commitment"
type of attitude, which has also been introduced earlier in Section 2.5, within the
context of zero-sum games and in conjunction with behavioral strategies. For
the single-act game of Example 3.6, a Nash equilibrium solution of the delayed
```
commitment type is the one given by (3.25a)-(3.25b), which has been obtained
```
using the recursive procedure.
This discussion now leads us to the following definition.
Definition 3.18 A Nash equilibrium solution of a ladder-nested single-act finite
N-person game is of the delayed commitment type if it can be obtained through
the recursive procedure outlined earlier, i.e., by solving only static single-act
games.
IV-PERSON NONZERO-SUM FINITE GAMES 109
Remark 3.10 As a side remark, it is noteworthy that in the single-act game
of Example 3.6 additional information for P2 concerning the action of PI could
```
be detrimental. For, if his information set was a single one (i.e., the static
```
```
version), then the game would admit the unique Nash solution (3.26a)-(3.26b)
```
with P2's Nash cost being J2 = — 1. In the set-up of the extensive form of
Fig. 3.2, however, he could end up with a higher Nash cost of J2 = 0. Hence, in
nonzero-sum games, additional information could be detrimental for the player
who receives it.
We now illustrate, giving two examples, the steps involved in the outlined
recursive procedure to obtain admissible Nash equilibrium strategies of the de-
layed commitment type.
```
Example 3.7 Consider the 3-person single-act game whose extensive form (of
```
```
the ladder-nested type) is depicted in Fig. 3.1 (a). Here, both P2 and P3 have
```
```
access to the action of PI (and to no more information), and hence they are
```
faced with a bimatrix game at each of their information sets. Specifically, if
ul — L, then the corresponding bimatrix game is
where the entries of matrix A denote the possible.losses to P2 and the entries
of matrix B denote the possible losses to P3. This bimatrix game admits a
```
unique admissible Nash equilibrium solution which is {u2* = L,u3* = L}, with
```
the corresponding Nash values being —1 and 0 for P2 and P3, respectively. If
```
u1 = R, on the other hand, the bimatrix game of interest is
```
```
This game admits a unique Nash solution which is {u2* = R, u3* — L} with
```
```
a Nash cost pair of (—2, -1). Hence, regardless of what PI plays, the delayed
```
commitment type admissible Nash equilibrium strategies of P2 and P3 in this
ladder-nested game are unique, and they are given as
```
(Note that P3's equilibrium strategy is a constant.) Now, if PI picks w1 = L,
```
```
his loss under (i) will be 1, but otherwise his loss is 0. Hence he also has a
```
unique equilibrium strategy
```
Strategies (i) and (ii) now constitute the unique delayed commitment type ad-
```
```
missible Nash equilibrium solution of the single-act game of Fig. 3.1 (a), which
```
110 T. BA§AR AND G. J. OLSDER
```
can also be directly verified by again referring to the set of inequalities (3.23).
```
```
The corresponding Nash equilibrium outcome is (0, —2, —1).
```
Figure 3.5: A 4-person single-act ladder-nested game in extensive form.
and this is so if u1 = R, u2 = L, with the corresponding cost quadruple being
```
(0,1,-1,0).
```
The third information set of P4 provides him with the information u1 = R,
```
u2 = R. P3 has access to the same information, and thus the equivalent normal
```
```
Example 3.8 Consider the 4-person single-act game whose extensive form (of
```
```
the ladder-nested type) is depicted in Fig. 3.5. The last-acting player is P4
```
```
and he has three information sets. The first one (counting from left) tells him
```
whether PI has actually played L or not, but nothing concerning the actions
of the other players. The same is true for P2 and P3. The corresponding
three-player static game admits an equivalent normal form which is the one
considered in Example 3.3, with only Pj replaced by Pj + 1 in the present
```
context. Furthermore, we now also have the costs of an additional player (PI)
```
```
showing up at the end of the branches (i.e., in the present context we have a
```
```
quadruple cost instead of a triple). It is already known that this game admits
```
a unique Nash equilibrium solution which, under the present convention, is
and this is so if PI has picked u1 = L, with the corresponding cost quadruple
```
being (1,1,-1,0).
```
The second information set of P4 tells him whether PI has picked R and P2
```
has picked L; and P3 also has access to this information. Hence, we now have a
```
bimatrix game under consideration, which is essentially the first bimatrix game
of Example 3.6, admitting the unique solution
N-PERSON NONZERO-SUM FINITE GAMES 111
form is the second bimatrix game of Example 3.7, which admits the unique Nash
solution
```
with the corresponding cost quadruple being (0,0, —2,1).
```
Using all these, we can now write down the unique Nash equilibrium strate-
gies of P3 and P4, which are given as
Deletion of all the branches of the extensive tree already used now leads to
the simpler tree structure depicted below:
In this game, P2 acts only if ul — R, in which case he definitely picks u2 = R,
```
since this yields him a lower cost (0) as compared with 1 which he would obtain
```
otherwise. This also completely determines the delayed commitment type Nash
equilibrium strategy of P2 in the game of Fig. 3.5, which is unique and is given
by
Then, the final equivalent form that the game takes is
from which it is clear that the optimal unique strategy for PI is
```
The strategies (i)-(iv) constitute the unique delayed commitment type Nash
```
```
equilibrium solution of the extensive form of Fig. 3.8 (which is clearly also
```
```
admissible), and the reader is also encouraged to check this result by verifying
```
```
satisfaction of inequalities (3.23). The corresponding unique Nash equilibrium
```
```
outcome is (0,0, —2,1).
```
112 T. BA§AR AND G. J. OLSDER
Nested extensive forms which are not ladder-nested cannot be decomposed
into static sub-extensive forms, and hence the recursive derivation does not di-
rectly apply to such games. However, some nested extensive forms still admit a
recursive decomposition into simpler sub-extensive forms, some of which will be
dynamic in nature. Hence, an extended version of the recursive procedure ap-
plies for nested games, which involves solution of simpler dynamic sub-extensive
forms. In this context, the delayed commitment mode of play also makes sense
as to be introduced in the sequel.
```
Definition 3.19 A nested extensive (or sub-extensive) form of a single-act
```
game is said to be undecomposable if it does not admit any simpler sub-extensive
form. It is said to be dynamic, if at least one of the players has more than one
information set.
Definition 3.20 For an N-person single-act dynamic game in nested unde-
```
composable extensive form (1) let J denote the set of games which are informa-
```
```
tionally inferior to I. Let J71*,... ,7^*} be an N-tuple of strategies in Nash
```
equilibrium for I, and further let j* denote the number of games in J to which
```
this N-tuple provides an equilibrium solution. Then, J71*, • • • >7^*} is of de-
```
layed commitment type if there exists no other N-tuple of Nash equilibrium
```
strategies of I which constitutes an equilibrium solution to a smaller (than j*)
```
number of games in J.18
The following example now provides an illustration of these notions as well
as a derivation of delayed commitment type Nash equilibria in nested games.
Example 3.9 Consider the 3-person single-act game whose extensive form is
displayed in Fig. 3.6. In this game P3 has access to Pi's action, but P2 does
not. Hence, the game is nested but not ladder-nested. Furthermore, it is both
dynamic and undecomposable, with the latter feature eliminating the possibility
for a recursive derivation of any of its Nash equilibria. Then, the only method
is to bring the single-act game into equivalent normal form and to obtain the
Nash equilibria of this normal form. Toward this end, let us first note that PI
```
and P2 each have two possible strategies: (7* = L^\ = -R;7j = L, 7! = R}.
```
P3, on the other hand, has four possible strategies:
18The reader should verify that every single-act game that admits Nash equilibria in pure
strategies has at least one equilibrium solution of the delayed commitment type, i.e., the
set of such equilibria is not empty unless the game does not admit any pure-strategy Nash
equilibrium solution.
and
JV-PERSON NONZERO-SUM FINITE GAMES 113
Figure 3.6: A nested single-act game in extensive form which is dynamic and
undecomposable.
Using the terminology of Section 3.3, we now display the normal form as two
```
(2 x 4) matrices, one for each strategy of PI:
```
There exists only one game which is informationally inferior to the single-act
game under consideration, which is the one obtained by allowing a single in-
```
formation set to P3 (i.e., the static version). Since the second Nash solution
```
dictates a constant action for P3, it clearly also constitutes a Nash solution for
```
this informationally inferior static game (cf. Prop. 3.7). Hence, for the second
```
set of Nash strategies, j* = 1, using the terminology of Def. 3.20. The first set
of Nash strategies, on the other hand, does not constitute a Nash equilibrium
solution to the informationally inferior game, and hence in this case j* = 0.
This then yields the conclusion that the nested single-act game of Fig. 3.6 ad-
```
mits a unique delayed commitment type Nash equilibria, given by (i) and with
```
```
an outcome of (0,0,0).
```
Here, the integers 1 , 2 , . . . identify particular strategies of the players in accor-
dance with the subscripts attached to 7l's. This normal form admits two Nash
equilibria, as indicated, which correspond, in the original game, to the strategy
triplets
and
114 T. BA§AR AND G. J. OLSDER
For more general types of single-act finite games in nested extensive form,
a recursive procedure could be used to simplify the derivation by decomposing
the original game into static and dynamic undecomposable sub-extensive forms.
Essential steps of such a recursive procedure are given below. Nash equilibria
obtained through this recursive procedure will be called the delayed commitment
type.
A recursive procedure to determine delayed commitment type pure-
strategy Nash equilibria of TV-person single-act games in nested ex-
tensive form
```
(1) Each information set of the last-acting player is included in either a static
```
sub-extensive form or a dynamic undecomposable sub-extensive form. Sin-
gle out these extensive forms and obtain their delayed commitment type
```
Nash equilibria. (Note that in static games every Nash equilibrium is,
```
```
by definition, of the delayed commitment type.) Assuming that each of
```
these games admits a unique pure-strategy Nash equilibrium solution of
the delayed commitment type, record the equilibrium strategies and the
corresponding Nash outcomes by identifying them with the players and
```
their information sets. (If any one of these games admits more than one
```
Nash equilibrium solution of the delayed commitment type, then this im-
plies that the original game admits more than one such equilibrium, and
this procedure as well as the following steps will have to be repeated for
```
each of these multiple equilibria.)
```
```
(2) Replace each sub-extensive form considered at step 1 with the immediate
```
branch emanating from its vertex that corresponds to the delayed com-
```
mitment Nash strategy of the starting player of that sub-extensive form;
```
furthermore, attach the corresponding TV-tuple of Nash equilibrium values
to the end of this branch.
```
(3) For the remaining game in nested extensive form, repeat steps 1 and 2
```
until the extensive form left is a tree structure comprised of only a vertex
and some immediate branches with an TV-tuple of numbers attached to
each of them.
```
(4) The branch of this final tree structure which corresponds to a minimum
```
loss for the starting player is his Nash equilibrium strategy of the delayed
commitment type, and the corresponding TV-tuple of numbers at the end
of this branch determines a Nash outcome for the original game. The
corresponding delayed commitment type Nash equilibrium strategies of
the other players in the original single-act game can then be captured from
```
the solution(s) of the sub-extensive forms considered, by appropriately
```
identifying them with the players and their information sets.
Example 3.10 To illustrate the steps of the foregoing recursive procedure, let
us consider the 3-person single-act game of Fig. 3.7, which is in nested extensive
N-PERSON NONZERO-SUM FINITE GAMES 115
Figure 3.7: The single-act game of Example 3.10.
form. At step 1 of the recursive procedure, we have two 2-person static sub-
extensive forms and one 3-person dynamic undecomposable sub-extensive form.
Counting from the left, the first static sub-extensive form admits the unique
```
Nash equilibrium solution (from Example 3.7):
```
The second static sub-extensive form also admits a unique solution which is
```
(again from Example 3.7):
```
```
It should be noted that (i) corresponds to the information sets rj2 = rj3 — \ul —
```
```
L} and (ii) corresponds to rj2 — r?3 = {ul = Ml}. Now, the dynamic unde-
```
composable sub-extensive form is in fact the single-act game of Example 3.9,
which is already known to admit the unique delayed-commitment type Nash
equilibrium solution
and it readily follows from this tree structure that PI has two Nash equilibrium
```
strategies:
```
At step 2, therefore, the extensive form of Fig. 3.7 becomes
116 T. BA§AR AND G. J. OLSDER
Consequently, the original single-act game admits two Nash equilibria of the
delayed commitment type:
and
```
with equilibrium outcomes being (0, —2, —1) and (0,0,0), respectively.
```
Remark 3.11 As we have discussed earlier in this section, single-act nonzero-
sum finite dynamic games admit, in general, a multiple of Nash equilibria,
mainly due to the fact that Nash equilibria of informationally inferior games
```
also provide Nash solutions to the original game (cf. Prop. 3.7) the so-called in-
```
formational nonuniqueness of Nash equilibria. The delayed commitment mode
of play introduced for nested and ladder-nested games, however, eliminates this
informational nonuniqueness, and therefore strengthens the Nash equilibrium
solution concept for dynamic single-act games.
3.5.2 Single-act games: Nash equilibria in behavioral and
mixed strategies
If an TV-person single-act game in extensive form does not admit a Nash equilib-
```
rium solution (in pure strategies), then an appropriate approach is to investigate
```
Nash equilibria within the enlarged class of behavioral strategies, as has been
done in Section 2.5 for zero-sum games. If the nonzero-sum single-act finite game
under consideration is of the "ladder-nested" type, then it is relatively simpler
```
to obtain Nash equilibria in behavioral strategies (which also fit well within a
```
```
delayed commitment framework), since one still follows the recursive procedure
```
developed in this section for pure strategies, but this time by also considering
the mixed-strategy Nash equilibria of the static sub-extensive forms. Then,
these mixed strategies, obtained for each equivalent normal form, will have to
be appropriately concatenated and written in the form of a behavioral strategy.
The following example now illustrates the steps in the derivation of behavioral
Nash equilibria.
Example 3.11 Consider the 3-person single-act game depicted in Fig. 3.8.
This is of the ladder-nested type, and both P2 and P3 have access to the
iV-PERSON NONZERO-SUM FINITE GAMES 117
Figure 3.8: The 3-person single-act game of Example 3.11.
action of PI, but to no other information. Hence, we first have to consider two
bimatrix games corresponding to the possible actions L and R of PI. If ul — L,
then the bimatrix game is the one treated in Example 3.2, which admits the
unique mixed-strategy Nash equilibrium
```
with the corresponding average cost triple being (0,1/2, 3/2). If ul = R, on the
```
other hand, the bimatrix game is
```
which admits the unique Nash solution (in pure strategies)
```
```
with the corresponding cost triple being (1,1,1). Comparing this outcome with
```
the previous one, it is obvious that the unique equilibrium strategy of PI is
Appropriate concatenation now yields the following unique behavioral Nash
equilibrium strategies for P2 and P3, respectively:
118 T. BA§AR AND G. J. OLSDER
```
The unique average Nash equilibrium outcome in behavioral strategies is (0,1/2,
```
```
3/2).
```
Since the recursive procedure involves solution of static games in normal
form at every step of the derivation, and since every N-person nonzero-sum
finite game in normal form admits a mixed strategy Nash equilibrium solution
```
(cf. Thm. 3.2), it follows that a Nash equilibrium solution in behavioral strate-
```
gies always exists for single-act games in ladder-nested extensive form. This
result is summarized below in Prop. 3.8, which is the counterpart of Corol-
lary 2.4 in the present framework.
Proposition 3.8 In every single-act N-person game which has a ladder-nested
extensive form comprising a finite number of branches, there exists at least one
Nash equilibrium solution in behavioral strategies.
Remark 3.12 It should be noted that, as opposed to Corollary 2.4, Prop. 3.8
does not claim an ordered interchangeability property in case of multiple Nash
equilibria, for reasons which have extensively been discussed in Sections 3.2 and
3.3. Furthermore, the result might fail to hold if the single-act game is not
ladder-nested, in which case one has to consider the even larger class of mixed
strategies.
Remark 3.13 Since the players have only a finite number of possible pure
strategies, it readily follows from Thm. 3.2 that there always exists a Nash
equilibrium in mixed strategies in single-act finite games of the general type.
However, computation of these mixed equilibrium strategies is quite a nontrivial
problem, since a recursive procedure cannot be developed for finite single-act
games which are not ladder-nested.
3.5.3 Multi-act games: Pure-strategy Nash equilibria
In the discussion of the Nash-equilibria of multi-act nonzero-sum finite games in
extensive form, we shall follow rather closely the analysis of Section 2.5 which
was devoted to similarly structured zero-sum games, since the results to be ob-
tained for multi-act nonzero-sum games will be direct extensions of their coun-
terparts in the case of zero-sum games. Not all conclusions drawn in Section 2.5,
however, are valid in the present context and these important differences be-
tween the properties of saddle-point and Nash equilibrium solutions will also be
elucidated in the sequel.
```
In order to be able to develop a systematic (recursive) procedure to obtain
```
Nash equilibria of multi-act nonzero-sum finite games, we will have to impose
```
(as in Section 2.5) some restrictions on the nature of the informational coupling
```
among different levels of play. To this end, we confine ourselves to multi-act
games whose Nash equilibria can be obtained by solving a sequence of single-
act games and by appropriate concatenation of the equilibrium strategies de-
termined at each level of play. This reasoning, then, brings us to the so-called
TV-PERSON NONZERO-SUM FINITE GAMES 119
```
"feedback games", a precise definition of which is given below (as a counterpart
```
```
of Def. 2.11).
```
Definition 3.21 A multi-act N-person nonzero-sum game in extensive form
with a fixed order of play is called an TV-person nonzero-sum feedback game in
extensive form, if
```
(i) at the time of his act, each player has perfect information concerning the
```
current level of play, i.e., no information set contains nodes of the tree
belonging to different levels of play,
```
(ii) information sets of the first-acting player at every level of play are sin-
```
gletons, and the information sets of the other players at every level of
play are such that none of them includes nodes corresponding to branches
emanating from two or more different information sets of the first-acting
player, i.e., each player knows the state of the game at every level of play.
If, furthermore,
```
(Hi) the single-act games corresponding to the information sets of the first-
```
```
acting player at each level of play are of the ladder-nested (respectively,
```
```
nested) type (cf. Def. 3.15), then the multi-act game is called an TV-person
```
```
nonzero-sum feedback game in ladder-nested (respectively, nested,) exten-
```
sive form.
Remark 3.14 It should be noted that, in the case of two-person multi-act
games, every feedback game is, by definition, also in ladder-nested extensive
```
form; hence, the zero-sum feedback game introduced earlier by Def. 2.11 is also
```
ladder-nested.
Now, paralleling the analysis of Section 2.5, let the number of levels of play in
an TV-person nonzero-sum feedback game in extensive form be K, and consider
a typical strategy 7* of Pi in such a game to be composed of K components
```
(7i> • • • '7k)- Here, 7] stands for the corresponding strategy of Pi at his jth
```
level of act, and it can be taken to have as its domain the class of only those
information sets of Pi which pertain to the jth level of play. Let us denote the
collection of all such strategies for Pi at level j by F*-. Then, for such a game,
```
the set of inequalities (3.23) can be written as (as a counterpart of (2.35))
```
```
which are to be satisfied for all 7* € F*,, i — 1,..., TV; j = 1,..., K. On any
```
```
TV-tuple of Nash equilibrium strategies {71*,... ,7^*} that satisfies (3.27), let
```
120 T. BA§AR AND G. J. OLSDER
```
us impose the further restriction that it also satisfies (recursively) the following
```
```
K TV-tuple inequalities for all 7] € F*r, i — 1 , . . . , A/"; j = 1 , . . . , K:
```
level
K
levelK-l
level
1
Definition 3.22 For an N-person nonzero-sum feedback game in extensive form
```
with K levels of play, let J71 *,... ,7^*} be an N-tuple of strategies satisfy-
```
```
ing (3.27) and (3.28) for all 7] € Fj, i = 1,...,N; j = 1,...,K. Then,
```
```
J71*,... ,7^*} constitutes a (pure) feedback Nash equilibrium for the feedback
```
game.
/V-PERSON NONZERO-SUM FINITE GAMES 121
```
Proposition 3.9 Every N-tuple J71 *,... ,7^*} that satisfies the set of inequal-
```
```
ities (3.28) also satisfies the set of N inequalities (3.27). Hence, the requirement
```
```
for satisfaction of (3.27) in Def. 3.22 is redundant, and it can be dispensed with
```
without any loss of generality.
Proof. This basically follows from the lines of argument used in the proof of
Prop. 2.3, with two inequalities at each level of play replaced by N inequalities
in the present case.
```
The set of inequalities (3.28) now readily suggests a recursive procedure for
```
derivation of feedback Nash equilibria in feedback nonzero-sum games. Starting
at level K, we have to solve a single-act game for each information set of the
first-acting player at each level of play, and then appropriately concatenate all
```
the equilibrium strategies (with restricted domains) thus obtained. If the single-
```
```
act games encountered in this recursive procedure admit more than one (pure-
```
```
strategy) Nash equilibrium, the remaining analysis will have to be repeated for
```
each one of these equilibria so as to determine the complete set of feedback Nash
equilibria the multi-act game admits.
For nested multi-act feedback games one might wish to restrict the class of
Nash equilibria of interest even further, so that only those obtained by concate-
nation of delayed commitment type equilibria of single-act games are considered.
```
The motivations behind such a further restriction are: (i) the delayed commit-
```
ment mode of play conforms well with the notion of feedback Nash equilibria
```
and it eliminates informational nonuniqueness, and (ii) Nash equilibria at every
```
level of play can then be obtained by utilizing the recursive technique developed
```
earlier in this section for single-act games (which is computationally attractive,
```
```
especially for ladder-nested games). A precise definition now follows.
```
Definition 3.23 For a nonzero-sum feedback game in extensive form, a feed-
back Nash equilibrium solution is of the delayed commitment type if its appro-
priate restriction at each level of play constitutes a delayed commitment type
```
Nash equilibrium (cf. Defs. 3.18, 3.20) for each corresponding single-act game
```
to be encountered in the recursive derivation.
Heretofore, the discussion on multi-act feedback games has been confined
```
only to feedback Nash equilibria (of the delayed commitment type or otherwise).
```
However, such games could very well admit other types of Nash equilibria which
```
satisfy inequalities (3.23) but which do not fit the framework of Def. 3.22. The
```
main reason for this phenomenon is the "informational nonuniqueness" feature
of Nash equilibria in dynamic games, which has been discussed earlier in this
section within the context of single-act games. In the present context, a natural
extension of Prop. 3.7 is valid, which we provide below after introducing some
terminology and also the concept of informational inferiority in nonzero-sum
feedback games.
Definition 3.24 Letl be an N-person multi-act nonzero-sum game with a fixed
order of play, which satisfies the first requirement of Def. 3.21. An N-person
122 T. BA§AR AND G. J. OLSDER
```
open-loop game in extensive form (say, II) (cf. Def. 3.13) is said to be the static
```
```
(open-loop) version of I if it can be obtained from I by replacing, at each level
```
of play, the information sets of each player with a single information set. The
equivalent normal form of the open-loop version of I is the static normal form
of I.
Definition 3.25 Let I and II be two N-person multi-act nonzero-sum games
with fixed orders of play, which satisfy the first requirement of Def . 3.21. Further
let Fj and FJJ denote the strategy sets of Pi in I and II, respectively. Then I is
informationally inferior to II if Ff C Ffj for all i € N, with strict inclusion for
at least one i.
Proposition 3.10 Let I and II be two N-person multi-act nonzero-sum games
as introduced in Def. 3.25, so that I is informationally inferior to II. Then,
```
(i) any Nash equilibrium solution for I is also a Nash equilibrium solution for
```
II,
```
(ii) if {71,..., 7^} is a Nash equilibrium solution for II so that 7* e F| for all
```
```
i = 1,..., N, then it is also a Nash equilibrium solution for I.
```
Proof. The proof is analogous to the proof of Prop. 3.7, and is therefore
omitted.
Remark 3.15 It should now be apparent why informationally nonunique Nash
equilibria exist in nonzero-sum multi-act feedback games. Every nontrivial
multi-act feedback game in which the players have at least two alternatives at
each level of play admits several informationally inferior multi-act games. These
different games in general admit different Nash equilibria which are, moreover,
not interchangeable. Since every one of these also constitutes a Nash equilib-
```
rium for the original multi-act game (cf. Prop. 3.10), existence of a plethora
```
of informationally nonunique equilibria readily follows. The further restrictions
of Defs. 3.22 and 3.23, imposed on the concept of noncooperative equilibrium,
clearly eliminate this informational nonuniqueness, and they stand out as pro-
viding one possible criterion according to which a further selection can be made.
This "informational nonuniqueness" property of noncooperative equilibrium
in dynamic games is, of course, also featured by zero-sum games, since they
are special types of nonzero-sum games, in which case the Nash equilibrium
solution coincides with the saddle-point solution. However, since all saddle-
point strategies have the ordered interchangeability property and further since
the saddle-point value is unique, nonuniqueness of equilibrium strategies does
not create a problem in zero-sum dynamic games—the main reason why we
have not included a discussion on informational nonuniqueness in Chapter 2.
It should be noted, however, that for two-person zero-sum multi-act feedback
games, Def. 3.23 is redundant and Def. 3.22 becomes equivalent to Def. 2.12,
which clearly dictates a delayed commitment mode of play.
JV-PERSON NONZERO-SUM FINITE GAMES 123
The following example now serves to illustrate both the recursive procedure
and the informational nonuniqueness of Nash equilibria in feedback nonzero-sum
games.
Example 3.12 Consider the multi-act feedback game whose extensive form is
```
depicted in Fig. 3.9. Since this is a 2-person game (of the feedback type) it is
```
```
definitely also ladder-nested (or equivalently, nested), and hence derivation of
```
its delayed commitment type feedback Nash equilibria involves solutions of only
```
static sub-extensive forms. At the second (and the last) levels of play, there are
```
four single-act games, one for each singleton information set of PI. The first of
```
these (counting from left) admits the unique Nash equilibrium solution of the
```
delayed commitment type
```
with the Nash equilibrium outcome being (1,0). The second one admits the
```
unique Nash equilibrium solution
```
with an outcome of (1,1). The third single-act game again has a unique Nash
```
equilibrium, given by
```
with an outcome of (0, —1/2). The last one is the single-act game of Example 3.6,
```
which admits the unique delayed commitment type Nash equilibrium solution
Figure 3.9: The multi-act feedback nonzero-sum game of Example 3.12.
124 T. BA§AR AND G. J. OLSDER
```
with the Nash outcome being (—1,0). When (i)-(iv) are collected together, we
```
obtain the delayed commitment type Nash equilibrium solution of the feedback
game at level 2 to be uniquely given as
whose equivalent normal form is the bimatrix game
In conclusion, the feedback game of Fig. 3.9 admits a unique feedback Nash
```
equilibrium solution of the delayed commitment type, which is given by (3.29a)-
```
```
(3.29b), with the corresponding Nash outcome being (3.29c).
```
This recursively obtained Nash equilibrium solution, although it is unique as
a delayed commitment type feedback Nash equilibrium, is clearly not unique as a
feedback Nash equilibrium solution, since it is already known from Example 3.6
that the fourth single-act game encountered at the first step of the recursive
derivation admits another Nash equilibrium solution which is not of the delayed
commitment type. This equilibrium solution is the constant strategy pair given
```
by (3.26a)-(3.26b), that is,
```
Now, crossing out the second level of play, we end up with the single-act game
which admits, as indicated, the unique Nash equilibrium solution
with the corresponding outcome being
```
which has to replace (iv) in the recursive derivation. The Nash outcome corre-
```
```
sponding to this pair of strategies is (0, —1). The strategy pair (3.29a) will now
```
JV-PERSON NONZERO-SUM FINITE GAMES 125
be replaced by
which also provides a Nash equilibrium solution at level 2. Now, crossing out
the second level of play, this time we have the single-act game
which admits, as indicated, the unique Nash equilibrium solution
with the corresponding outcome being
```
We therefore arrive at the conclusion that (3.30a)-(3.30b) provides another feed-
```
back Nash equilibrium solution for the multi-act game of Fig. 3.9, which is,
however, not of the delayed commitment type. It is noteworthy that the unique
feedback Nash solution of the delayed commitment type is in this case inadmis-
```
sible, which follows immediately by a comparison of (3.29c) and (3.30c).
```
```
To determine the other (non-feedback type) Nash equilibria of this multi-act
```
game, the only possible approach is to bring it into equivalent normal form and
to investigate Nash equilibria of the resulting bimatrix game. This is, however,
quite a strenuous exercise, since the bimatrix game is of rather high dimensions.
Specifically, at level 1 each player has two possible strategies, while at level 2
PI has 23 x 3 = 24 and P2 has 26 — 64 possible strategies. This then implies
that the bimatrix game is of dimensions 48 x 128. Two Nash equilibria of this
```
bimatrix game are the ones given by (3.29a)-(3.29b) and (3.30a)-(3.30b), but
```
there are possibly others which in fact correspond to Nash equilibria of multi-
```
act games which are informationally inferior to the one of Fig. 3.9; one of these
```
games is the open-loop game which is the static version of the original feedback
game.
whose equivalent normal form is
126 T. BA§AR AND G. J. OLSDER
3.5.4 Multi-act games: Behavioral and mixed equilibrium
strategies
Behavioral equilibrium strategies for an TV-person feedback game in ladder-
nested extensive form can be derived by essentially following the recursive pro-
cedure developed in Section 3.5.3 for pure-strategy feedback equilibria, but this
time by also considering the behavioral-strategy Nash equilibria of the single-act
games to be encountered in the derivation. The justification of applicability of
such a recursive procedure is analogous to the one of behavioral saddle-point
equilibria of feedback games discussed in Section 2.5. Specifically, if K denotes
the number of levels of play in the TV-person feedback game of the ladder-nested
type, then a typical behavioral strategy 7* of Pi in such a game can be de-
```
composed into K components 7},... , 7)^, where 7J € F*- is the corresponding
```
behavioral strategy of Pi at his jth level of play. Furthermore, 7J has, as its
domain, only the class of those information sets of Pi which pertain to the jth
level of play. Then, the set of inequalities that determines the behavioral Nash
equilibrium solution can be written as
which have to be satisfied for all 7] € F*,, j — 1,..., K\ i 6 N. Under the delayed
commitment mode of play, we impose the further restriction that they satisfy
```
the K TV-tuple inequalities similar to (3.28), with only 7J replaced by 7] and J1
```
by J1. A counterpart of Prop. 3.9 also holds true in the present framework and
this readily leads to recursive derivation of behavioral-strategy Nash equilibria
in feedback games of the ladder-nested type. This procedure always leads to a
Nash equilibrium solution in behavioral strategies as the following proposition
states.
Proposition 3.11 Every finite N-person nonzero-sum feedback game in ladder-
nested extensive form admits a Nash equilibrium solution in behavioral strategies
which can be determined recursively.
Proof. This result is an immediate consequence of Prop. 3.8, since every
feedback game in ladder-nested extensive form can be decomposed recursively
into ladder-nested single-act games.
For feedback games which are not of the ladder-nested type, and also for
```
other types of (non-feedback) multi-act games, Prop. 3.11 is not in general valid,
```
```
and the Nash equilibrium in behavioral strategies (even if it exists) cannot be
```
obtained recursively. Then, the only approach would be to bring the extensive
form into an equivalent normal form, obtain all mixed-strategy Nash equilibria
of this normal form, and to seek whether any one of these equilibria would
constitute a Nash equilibrium in behavioral strategies. This argument is now
TV-PERSON NONZERO-SUM FINITE GAMES 127
supported by the following two propositions which are direct counterparts of
Props 2.5 and 2.6, respectively.
Proposition 3.12 Every N-person nonzero-sum multi-act finite game in ex-
tensive form admits a Nash equilibrium solution in mixed strategies.
Proof. The result readily follows from Thm. 3.2, since every such game can
be transformed into an equivalent normal form in which each player has a finite
number of strategies.
Proposition 3.13 Every Nash equilibrium of a finite N-person nonzero-sum
multi-act game in behavioral strategies also constitutes a Nash equilibrium in
the larger class of mixed strategies.
```
Proof. Let {7** € f1 ,...,^* e tN} denote an TV-tuple of behavioral
```
strategies stipulated to be in Nash equilibrium, and let fl denote the mixed-
strategy set of Pi, i € N. Since P C P, we clearly have 7** e Fl, for every
```
i € N. Assume, to the contrary, that {71*,... , 7^*} is not a mixed-strategy
```
```
Nash equilibrium solution; then this implies that there exists at least one i (say,
```
```
i — N, without any loss of generality) for which the corresponding inequality
```
```
of (3.31)19 is not satisfied for all 7* € fl . In particular, there exists a 7^ € f N
```
such that
Now, abiding by our standard convention, let TN denote the pure-strategy set
ofPN, and consider the quantity
```
defined for each JN € TN. (This is well denned since TN C TN.) The infimum
```
```
of this quantity over FN is definitely achieved (say, by 7^* € TN), since TN is
```
a finite set. Furthermore, since TN is comprised of all probability distributions
onT",
We therefore have
and also the inequality
```
in view of (i). But this is impossible since
```
thus completing the proof of the proposition.
19Here, by an abuse of notation, we take Jl to denote the average loss to Pi under also
mixed-strategy AT-tuples.
128 T. BA§AR AND G. J. OLSDER
3.5.5 Other refinements on Nash equilibria
As we have seen heretofore in this chapter, existence of multiple Nash equilibria
for a given dynamic game is more a rule rather than an exception, with the
multiplicity arising because of the informational richness of the underlying deci-
sion problem as well as the structure of the players' cost matrices. As a means
of shrinking the set of Nash equilibria in a rational way, we have introduced
heretofore the notions of "delayed commitment", "informational inferiority",
"feedback games" and "admissibility". There are, however, several other refine-
ment schemes which have been introduced in the literature, some of which we
discuss below.
```
To motivate the discussion, let us start with a two-player matrix game (A, B)
```
where the players have identical cost matrices.
```
The game admits two pure-strategy Nash equilibria: (L,L) and (R,R). Note,
```
however, that if we perturb the entries of the two matrices slightly, and inde-
```
pendently:
```
```
where e^-, t, j, k = 1,2, are infinitesimally small (positive or negative) numbers,
```
```
then (L,L) will still retain its equilibrium property (as long as \e^\ < 1/2),
```
```
but (R, R) will not. More precisely, there will exist infinitely many perturbed
```
```
versions of the original game for which (R, R) will not constitute a Nash equilib-
```
```
rium. Hence, in addition to admissibility, (L, L) can be singled out in this case
```
as the Nash solution that is robust to infinitesimal perturbations in the entries
of the cost matrices.
Can such perturbations be induced naturally by some behavioral assump-
tions imposed on the players? The answer is yes, as we discuss next. Consider
```
the scenario where a player who intends to play a particular pure strategy (out
```
```
of a set of n possible alternatives) errs and plays with some small probability
```
```
one of the other n — 1 alternatives. In the matrix game (3.32), for example,
```
```
if both players err with equal (independent) probability e > 0, the resulting
```
```
matrix game is (A6,Be), where
```
```
Note that for all e e (0,1/2) this matrix game admits the unique Nash equi-
```
```
librium (L, L), with a cost pair of (e(2 - e), e(2 - e)), which converges to (0,0)
```
N-PERSON NONZERO-SUM FINITE GAMES 129
as e | 0, thus recovering one of the Nash cost pairs of the original game. A
Nash equilibrium solution that can be recovered this way is known as a perfect
```
equilibrium, which was first introduced in precise terms by Selten (1975), in the
```
context of AT-player games in extensive form. Given a game of perfect recall,
denoted £/, the idea is to generate a sequence of games, Gi,Qz,... ,Gk,-—, a
```
limiting equilibrium solution of which (in behavioral strategies, and as k —> oo)
```
is an equilibrium solution of Q. If Q^ is obtained from Q by forcing the play-
ers at each information set to choose every possible alternative with positive
```
probability (albeit small, for those alternatives that are not optimal), then the
```
```
equilibrium solution(s) of Q that are recovered as a result of the limiting pro-
```
```
cedure above is (are) called perfect equilibrium (equilibria).'20 Selten (1975) has
```
```
shown that every finite game in extensive form with perfect recall (and as a spe-
```
```
cial case in normal form) admits at least one perfect equilibrium, thus making
```
this refinement scheme a legitimate one.
The procedure discussed above, which amounts to "completely" perturbing
```
a game with multiple equilibria, is one way of obtaining perfect equilibria; yet
```
```
another one, as introduced by Myerson (1978), is to restrict the players to use
```
```
completely mixed strategies (with some lower positive bound on the probabili-
```
```
ties) at each information set. Again referring back to the matrix game (A, B]
```
```
of (3.32), let the players' mixed strategies be restricted to the class
```
```
where e < y < 1 — e, e < z < 1 — e, for some (sufficiently small) positive e. Over
```
this class of strategies, the average cost functions of the players will be
```
Such a solution is called an e-perfect equilibrium (Myerson, 1978), which in
```
the limit as e j. 0 clearly yields the perfect Nash equilibrium obtained earlier.
Myerson in fact proves, for TV-person games in normal form, that every perfect
equilibrium can be obtained as the limit of an appropriate e-perfect equilibrium,
with the converse statement also being true. More precisely, using the notation
of Section 3.3, we have the following.
Proposition 3.14 For an N-person finite game in normal form, a mixed-
```
strategy Nash equilibrium {y1* E Yl,i £ N} is a perfect equilibrium if, and
```
```
only if, there exist some sequences {efc}£L1; [y\k e Yl,i E N}^=1 such that
```
```
which admits (assuming that 0 < e < ^) a unique Nash equilibrium:
```
20This is also called "trembling hand equilibrium", as the process of erring at each in-
formation set is reminiscent of a "trembling hand" making unintended choices with small
probability. Here, as k —» oo, this probability of unintended plays converges to zero.
Furthermore, a perfect equilibrium necessarily exists, and every perfect equilib-
rium is a Nash equilibrium.
Even though the notion of perfect equilibrium provides a refinement of the
notion of Nash equilibrium with some appealing properties, it also carries some
undesirable features as the following example of an identical cost matrix game
```
(due to Myerson (1978)) exhibits:
```
```
Note that this is a matrix game derived from (3.32) by adding a completely
```
dominated row and a completely dominated column. It now has three Nash
```
equilibria: (L,L), (M, M), (R,R), the first two of which are perfect equilibria,
```
while the last one is not.21 Hence, inclusion of completely dominated rows and
columns could create additional perfect equilibria not present in the original
game—a feature that is clearly not desirable. To remove this shortcoming of
```
perfect equilibrium, Myerson (1978) has introduced the notion of proper equilib-
```
rium, which corresponds to a particular construction of the sequence of strate-
gies used in Prop. 3.14. Proper equilibrium is defined as in Prop. 3.14, with only
```
the ek-perfect equilibrium in ii) replaced by the notion of e^-proper equilibrium
```
```
to be introduced next. Toward this end, let Jl(j', ye) denote the average cost to
```
```
Pi when he uses his jth strategy (such as jth column or row of the matrix) in
```
the game and all the other players use their mixed strategies y*, k 6 N, k ^ i.
Furthermore, let y\^ be the probability attached to his jth strategy under the
```
mixed strategy y\. Then, the JV-tuple {yl6,i G N} is said to be in e-proper
```
equilibrium if the strict inequality
implies that y\^ < eyl'k, this being so for every j, k € Mj, and every i € N.
In other words, an e-proper equilibrium is one in which every player is giving
```
his better responses much more probability weight than his worse responses (by
```
```
a factor 1/e), regardless of whether those "better" responses are "best" or not.
```
```
Myerson (1978) proves that such an equilibrium necessarily exists as follows.
```
Proposition 3.15 Every finite N-player game in normal form admits at least
one proper equilibrium. Furthermore, every proper equilibrium is a perfect equi-
```
librium (but not vice versa).
```
21 The reader is encouraged to verify this conclusion.
130 T. BA§AR AND G. J. OLSDER
> 0 and linijt-^oo e^ = 0
is an €k-perfect equilibrium
```
i)
```
```
a)
```
```
Hi)
```
JV-PERSON NONZERO-SUM FINITE GAMES 131
```
Remark 3.16 The reader should verify that in the matrix game (3.33) there is
```
```
only one proper equilibrium, which is (I/,L), the perfect equilibrium of (3.32).
```
Another undesirable feature of a perfect equilibrium is that it is very much
```
dependent on whether the game is in extensive or normal form (whereas the
```
```
Nash equilibrium property is form-independent). As it has been first observed by
```
```
Selten (1975), and further elaborated on by van Damme (1984), a perfect equi-
```
librium of the extensive form of a game need not be perfect in the normal form,
and conversely a perfect equilibrium of the normal form need not be perfect
```
in the extensive form. To remove this undesirable feature, van Damme (1984)
```
has introduced the concept of quasi-perfect equilibria for games in extensive
form, and has shown that a proper equilibrium of a normal form game induces
a quasi-perfect equilibrium in every extensive form game having this normal
form. Quasi-perfect equilibrium is defined as a behavioral strategy combina-
tion which prescribes at every information set a choice that is optimal against
```
mistakes ("trembling hands") of the other players; its difference from perfect
```
equilibrium is that here in the construction of perturbed matrices each player
```
ascribes "trembling hand" behavior to all other players (with positive probabil-
```
```
ity), but not to himself.
```
Other types of refinement have also been proposed in the literature, such as
```
sequential equilibria (Kreps and Wilson, 1982), and strategic equilibria (Kohlberg
```
```
and Mertens, 1986), which we do not further discuss here. None of these, how-
```
ever, are uniformly powerful, in the sense of shrinking the set of Nash equilibria
to the smallest possible set. We will revisit the topic of "refinement on Nash
equilibria" later in Chapters 5 and 6, in the context of infinite dynamic games
and with emphasis placed on the issue of time consistency.
3.6 The Stackelberg Equilibrium Solution
The Nash equilibrium solution concept that we have heretofore studied in this
chapter provides a reasonable noncooperative equilibrium solution for nonzero-
sum games when the roles of the players are symmetric, that is to say, when no
single player dominates the decision process. However, there are yet other types
of noncooperative decision problems wherein one of the players has the ability
```
to enforce his strategy on the other player(s), and for such decision problems
```
one has to introduce a hierarchical equilibrium solution concept. Following the
```
original work of H. von Stackelberg (1934), the player who holds the powerful
```
position in such a decision problem is called the leader, and the other players
```
who react (rationally) to the leader's decision (strategy) are called the followers.
```
There are, of course, cases of multi-levels of hierarchy in decision making, with
```
many leaders and followers; but for purposes of brevity and clarity in exposition
```
we will first confine our discussion here to hierarchical decision problems which
```
incorporate two players (decision makers)—one leader and one follower.
```
```
To set the stage to introduce the hierarchical (Stackelberg) equilibrium so-
```
132 T. BA§AR AND G. J. OLSDER
```
lution concept, let us first consider the bimatrix game (A, B] displayed (under
```
```
our standard convention) as
```
This bimatrix game clearly admits a unique Nash equilibrium solution in pure
```
strategies, which is {M, M}, with the corresponding outcome being (1,0). Let us
```
now stipulate that the roles of the players are not symmetric and PI can enforce
his strategy on P2. Then, before he announces his strategy, PI has to take into
```
account possible responses of P2 (the follower), and in view of this, he has to
```
decide on the strategy that is most favorable to him. For the decision problem
whose possible cost pairs are given as entries of A and B, above, let us now work
```
out the reasoning that PI (the leader) will have to go through. If PI chooses
```
```
L, then P2 has a unique response (that minimizes his cost) which is L, thereby
```
yielding a cost of 0 to PI. If the leader chooses M, P2's response is again unique
```
(which is M), with the corresponding cost incurred to PI being 1. Finally, if he
```
picks R, P2's unique response is also R, and the cost to PI is 2. Since the lowest
of these costs is the first one, it readily follows that L is the most reasonable
```
choice for the leader (PI) in this hierarchical decision problem. We then say
```
```
that L is the Stackelberg strategy of the leader (PI) in this game, and the pair
```
```
{L,L} is the Stackelberg solution with PI as the leader. Furthermore, the cost
```
```
pair (0, —1) is the Stackelberg (equilibrium) outcome of the game with PI as the
```
leader. It should be noted that this outcome is actually more favorable for both
players than the unique Nash outcome—this latter feature, however, is not a
```
rule in such games. If, for example, P2 is the leader in the bimatrix game (3.34),
```
```
and PI the follower, then the unique Stackelberg solution is {L,R} with the
```
```
corresponding outcome being (3/2, —2/3) which is clearly not favorable for PI
```
```
(the follower) when compared with his unique Nash cost. For P2 (the leader),
```
however, the Stackelberg outcome is again better than his Nash outcome.
The Stackelberg equilibrium solution concept introduced above within the
```
context of the bimatrix game (3.34) is applicable to all two-person finite games
```
in normal form, but provided that they exhibit one feature which was inherent to
```
the bimatrix game (3.34) and was used implicitly in the derivation: the follower's
```
response to every strategy of the leader should be unique. If this requirement is
not satisfied, then there is ambiguity in the possible responses of the follower
and thereby in the possible attainable cost levels of the leader. As an explicit
example to demonstrate such a decision situation, consider the bimatrix game
JV-PERSON NONZERO-SUM FINITE GAMES 133
```
and with PI acting as the leader. Here, if PI chooses (and announces) L, P2
```
has two optimal responses L and M, whereas if PI picks R, P2 again has two
optimal responses, L and R. Since this multiplicity of optimal responses for the
follower results in a multiplicity of cost levels for the leader for each one of his
strategies, the Stackelberg solution concept introduced earlier cannot directly be
applied here. However, this ambiguity in the attainable cost levels of the leader
can be resolved if we stipulate that the leader's attitude is toward securing his
possible losses against the choices of the follower within the class of his optimal
responses, rather than toward taking risks. Then, under such a mode of play,
Pi's secured cost level corresponding to his strategy L would be 1, and the
one corresponding to R would be 2. Hence, we declare 7** = L as the unique
```
Stackelberg strategy of PI in the bimatrix game of (3.35), when he acts as the
```
```
leader. The corresponding Stackelberg cost for PI (the leader) is J1* = 1. It
```
should be noted that, in the actual play of the game, PI could actually end up
with a lower cost level, depending on whether the follower chooses his optimal
response 72 = L or the optimal response 72 = M. Consequently, the outcome
```
of the game could be either (1,0) or (0,0), and hence we cannot talk about
```
```
a unique Stackelberg equilibrium outcome of the bimatrix game (3.35) with
```
PI acting as the leader. Before concluding our discussion on this example, we
```
finally note that the admissible Nash outcome of the bimatrix game (3.35) is
```
```
( — !,—!) which is more favorable for both players than the possible Stackelberg
```
outcomes given above.
We now provide a precise definition for the Stackelberg solution concept
introduced above within the context of two bimatrix games, so as to encompass
all two-person finite games of the single-act and multi-act type which do not
incorporate any chance moves. For such a game, let F1 and F2 again denote the
```
pure-strategy spaces of PI and P2, respectively, and Jl(71,72) denote the cost
```
```
incurred to Pi corresponding to a strategy pair {7* e F1^2 e F2}. Then, we
```
have the following definitions.
```
Definition 3.26 In a two-person finite game, the set R2^1) C F2 defined for
```
each 71 € F1 by
```
is the optimal response (rational reaction) set o/P2 to the strategy 71 G F1 of
```
PI.
Definition 3.27 In a two-person finite game with PI as the leader, a strategy
71* € F1 is called a Stackelberg equilibrium strategy for the leader, if
The quantity J1* is the Stackelberg cost of the leader. If, instead, P2 is the
leader, the same definition applies with only the superscripts 1 and 2 inter-
changed.
134 T. BA§AR AND G. J. OLSDER
Theorem 3.3 Every two-person finite game admits a Stackelberg strategy for
the leader.
```
Proof. Since F1 and F2 are finite sets, and R2(il) is a subset of F2 for each
```
```
71 e F1, the result readily follows from (3.37).
```
Remark 3.17 The Stackelberg strategy for the leader does not necessarily have
to be unique. But nonuniqueness of the equilibrium strategy does not create
```
any problem here (as it did in the case of Nash equilibria), since the Stackelberg
```
cost for the leader is unique.
```
Remark 3.18 If R2(^1} is a singleton for each 71 € F1, then there exists a
```
```
mapping T2 : F1 -> F2 such that 72 6 R2^1) implies 72 = T2^1. This
```
```
corresponds to the case in which the optimal response of the follower (given
```
```
by T2) is unique for every strategy of the leader, and it leads to the following
```
```
simplified version of (3.37) in Def. 3.27:
```
```
Here J1* is no longer only a secured equilibrium cost level for the leader (PI),
```
but it is the cost level that is actually attained.
From the follower's point of view, the equilibrium strategy in a Stackelberg
game is any optimal response to the announced Stackelberg strategy of the
leader. More precisely, we have the following.
```
Definition 3.28 Let 71* € F1 be a Stackelberg strategy for the leader (PI).
```
```
Then, any element 72* e R2(^1*} is an optimal strategy for the follower (P2)
```
```
that is in equilibrium with 71*. The pair J71*,72*} is a Stackelberg solution for
```
```
the game with PI as the leader, and the cost pair («/1(71*,72*),«/2(71*,72*)) is
```
the corresponding Stackelberg equilibrium outcome.
```
Remark 3.19 In the preceding definition, the cost level «/1(71*,72*) could in
```
fact be lower than the Stackelberg cost J1*—a feature which has already been
```
observed within the context of the bimatrix game (3.35). However, if .R2^1*)
```
is a singleton, then these two cost levels have to coincide.
For a given two-person finite game, let J1* again denote the Stackelberg cost
```
of the leader (PI), and J^ denote any Nash equilibrium cost for the same player.
```
```
We have already seen within the context of the bimatrix game (3.35) that J1*
```
is not necessarily lower than Jjy, in particular, when the optimal response of
the follower is not unique. The following proposition now provides one sufficient
condition under which the leader never does worse in a "Stackelberg game" than
in a "Nash game".
TV-PERSON NONZERO-SUM FINITE GAMES 135
Proposition 3.16 For a given two-person finite game, let J1* and Jjy be as
```
defined before. If R2(~fl) is a singleton for each 7* G F1, then
```
Proof. Under the hypothesis of the proposition, assume to the contrary that
```
there exists a Nash equilibrium solution J710 G FI, 72° G F2} whose correspond-
```
ing cost to PI is lower than J1*, i.e.,
```
Since R2(l1} is a singleton, let T2 : F2 —>• F1 be the unique mapping introduced
```
```
in Remark 3.18. Then, clearly, 72° — T27l0, and if this is used in (i), together
```
```
with the RHS of (3.38), we obtain
```
which is a contradiction.
Remark 3.20 One might be tempted to think that if a nonzero-sum game
admits a unique Nash equilibrium solution and a unique Stackelberg strategy
```
(71*) for the leader, and further if R2^1*) is a singleton, then the inequality of
```
Prop. 3.16 still should hold. This, however, is not true as the following bimatrix
game demonstrates:
Here, there exists a unique Nash equilibrium solution, as indicated, and a unique
```
Stackelberg strategy 71* = L for the leader (PI). Furthermore, the follower's
```
```
optimal response to 71* = L is unique (which is 72 = L). However, 0 =
```
J1* > Jjy = —1. This counterexample indicates that the sufficient condition of
Prop. 3.16 cannot be relaxed any further in any satisfactory way.
Since the Stackelberg equilibrium concept is nonsymmetric as to the roles
```
of the players, there is no recursive procedure that can be developed (as in
```
```
Section 3.5) to determine the Stackelberg solution of dynamic games. The only
```
possibility is the brute-force method, that is, to convert the original two-person
```
finite dynamic game in extensive form into an equivalent normal form (which
```
```
is basically a bimatrix game), to exhaustively work out the possible outcomes
```
of the game for each strategy choice of the leader, and to see which of these is
the most favorable one for the leader. Since this is a standard application of
Def. 3.27 and is no different from the approach adopted in solving the bimatrix
```
games (3.34) and (3.35), we do not provide any further examples here. However,
```
the reader is referred to Section 3.8 for problems that involve derivation of
Stackelberg equilibria in single-act and multi-act dynamic games.
```
The feedback (stagewise) Stackelberg solution
```
Within the context of multi-act dynamic games, the Stackelberg equilibrium
concept is suitable for the class of decision problems in which the leader has the
ability to announce his decisions at all of his possible information sets ahead
of time—an attitude which naturally forces the leader to commit himself ir-
revocably to the actions dictated by these strategies. Hence, in a sense, the
Stackelberg solution is of the prior commitment type from the leader's point of
```
view. There are yet other types of hierarchical multi-act (multi-stage) decision
```
problems, however, in which the leader does not have the ability to announce
and enforce his strategy at all levels of play prior to the start of the game, but
```
can instead enforce his strategy on the follower(s) at every level (stage) of the
```
game. Such a hierarchical equilibrium solution, which has the "Stackelberg"
```
property at every level of play (but not globally), is called a feedback Stackelberg
```
solution.22 We now provide below a precise definition of this equilibrium solu-
tion within the context of two-person nonzero-sum feedback games in extensive
```
form (cf. Def. 3.21). To this end, let us adopt the terminology and notation of
```
Section 3.5, and consider I to be a two-person nonzero-sum feedback game in
extensive form with K levels of play and with PI being the first-acting player
at each level of play. Let a typical strategy 7* e F* of Pi be decomposed as
```
(7! € r|,..., 7Jf € I\), where 7] stands for the corresponding strategy of Pi
```
at the jth level of play. Furthermore, for a given strategy 7* € P, let 7z>fc and
7i k denote two truncated versions of 7*, defined as
22We shall henceforth refer to the Stackelberg equilibrium solution introduced through
Defs. 3.26-3.28 as the global Stackelberg equilibrium solution whenever it is not clear from
the context which type of equilibrium solution we are working with.
Then, we have the following.
```
Definition 3.29 Using the preceding notation and terminology, a pair {/31 €
```
```
F1,/?2 € F2} constitutes a feedback Stackelberg solution for I with PI as the
```
leader if the following two conditions are fulfilled:
and
136 T. BA§AR AND G. J. OLSDER
```
(ii) R"k((3',/3/J:, 7i,/c) is a singleton, with its only element being (3%, k = 1 , . . . , K.
```
```
The quantity J1(/31,/32) is the corresponding feedback Stackelberg cost of
```
the leader.
```
Relations (3.40a) and (3.40b) now indicate that the feedback Stackelberg
```
solution features the "Stackelberg" property at every level of play, and this can
in fact be verified recursively—thus leading to a recursive construction of the
equilibrium solution. Before we go into a discussion of this recursive procedure,
```
let us first prove a result concerning the structures of R^(-) and J1(-)-
```
Proposition 3.17 For a two-person feedback game that admits a feedback Stack-
elberg solution with PI as the leader,
```
(i) -R|(/3;7i fc+i) is not a variant 0/71/5, and hence it can be written as
```
```
Rl(fcri),
```
```
(ii) J1(7i,fc,/31>fe;72,fc,/32'fc) depends onji^ and^2,k only through the singleton
```
information sets 77^ of PI at level k.
Proof. Both of these results follow from the "feedback" nature of the two-
person dynamic game under consideration, which enables one to decompose
the /f-level game recursively into a number of single-act games each of which
```
corresponds to a singleton information set of PI (the leader). In accordance
```
with this observation, let us start with k — K, in which case the proposition
```
is trivially true. Relation (3.40a) then defines the (global) Stackelberg solution
```
of the single-act game obtained for each information set 17^ of PI, and the
```
quantity on the RHS of (3.40a) defines, for k = K, the Stackelberg cost for the
```
leader in each of these single-act games, which is actually realized because of
```
condition (ii) of Def. 3.29 (see Remark 3.19). Then, the cost pair transferred
```
```
to level k = K — 1 is well defined (not ambiguous) for each information set
```
r]lK of PI. Now, with the Kth level crossed out, we repeat the same analysis
```
and first observe that J2(-) in (3.40) depends only on the information set TJ]^_I
```
of PI and on the strategies 7^_1 and 7x_1 of PI and P2, respectively, at
level k ~ K — 1, since we again have essentially single-act games. Consequently,
```
^K-I(') depends only on 7^_1. If the cost level of PI at k = K — 1 is minimized
```
```
subject to that constraint, (3}<_l is by hypothesis a solution to that problem,
```
to which a unique response of the follower corresponds. Hence, again a well-
defined cost pair is determined for each game corresponding to the singleton
information sets r]lK_l of PI, and these are attached to these nodes to provide
TV-PERSON NONZERO-SUM FINITE GAMES 137
```
defined bycost pairs for the remaining (K — 2)-level game. An iteration on this analysis
```
then proves the result.
The foregoing proof already describes the recursive procedure involved to
obtain a feedback Stackelberg solution of a two-person feedback game. The re-
```
quirement (ii) in Def. 3.29 ensures that the outcomes of the Stackelberg games
```
considered at each level are unambiguously determined, and it imposes an indi-
rect restriction on the problem under consideration, which cannot be tested a
priori. This then leads to the conclusion that a two-person nonzero-sum feed-
back game does not necessarily admit a feedback Stackelberg solution.
There are also cases in which a feedback game admits more than one feedback
Stackelberg solution. Such a situation might arise if, in the recursive derivation,
one or more of the single-act Stackelberg games admit multiple Stackelberg
strategies for the leader whose corresponding costs to the follower are not the
same. The leader is, of course, indifferent to these multiple equilibria at that
```
particular level where they arise, but this nonuniqueness might affect the (over-
```
```
all) feedback Stackelberg cost of the leader. Hence, if {/21,/?2} and {C1,^2} are
```
two feedback Stackelberg solutions of a given game, it could very well happen
```
that J1^1,/?2) < J1(C1,^2)) in which case the leader definitely prefers (3l over
```
£* and attempts to enforce the components of 01 at each level of the game. This
argument then leads to a total ordering among different feedback Stackelberg
solutions of a given game, which we formalize below.
```
Definition 3.30 A feedback Stackelberg solution {/51, /32} of a two-person nonzero-
```
sum feedback game is admissible if there exists no other feedback Stackelberg
```
solution i^1,^2} with the property Jl(£l,£2) < Jl(/3l,(32).
```
It should be noted that if a feedback game admits more than one feedback
Stackelberg solution, then the leader can definitely enforce the admissible one
on the follower since he leads the decision process at each level. A unique
feedback Stackelberg solution is-clearly admissible. We now provide below an
example of a 2-level two-person nonzero-sum feedback game which admits a
unique feedback Stackelberg solution. This example also illustrates recursive
derivation of this equilibrium solution.
Example 3.13 Consider the multi-act feedback nonzero-sum game of Fig. 3.9.
To determine its feedback Stackelberg solution with PI acting as the leader, we
first attempt to solve the Stackelberg games corresponding to each of the four
```
information sets of PI at level 2. The first one (counting from the left) is a
```
trivial game, admitting the unique equilibrium solution
```
with the corresponding outcome being (1,0). The next two admit, respectively,
```
the following bimatrix representations whose unique Stackelberg equilibria are
T. BA§AR AND G. J. OLSDER138JV-PERSON NONZERO-SUM FINITE GAMES 139
indicated on the matrices:
whose unique Stackelberg solution with PI as leader is as indicated.
To recapitulate, the leader's unique feedback Stackelberg strategy in this
feedback game is
```
The unique feedback Stackelberg outcome of the game is (0, -1/2), which follows
```
```
from (vi) by inspection.
```
Finally, the fourth one admits the trivial bimatrix representation
together with the 2 x 2 bimatrix representation
with the unique Stackelberg solutions being as indicated, in each case. By a
```
comparison of the leader's cost levels at (iv) and (v), we conclude that he plays
```
R at the fourth of his information sets at level 2. Hence, in going from the
```
second to the first level, we obtain from (i), (ii), (iii) and (v) the unique cost
```
```
pairs at each of Pi's information sets to be (counting from the left): (1,0),
```
```
(1,1), (0, —1/2) and (—1,0). Then, the Stackelberg game at the first level has
```
the bimatrix representation
and the follower s unique optimal response strategy is
Before concluding our discussion on the feedback Stackelberg equilibrium
solution, we should mention that the same concept has potential applicability
to multi-act decision problems in which the roles of the players change from one
level of play to another, that is, when different players act as leaders at different
stages. An extension of Def. 3.29 is possible so that such classes of decision
```
problems are also covered; but since this involves only an appropriate change
```
of indices and an interpretation in the right framework, it will not be covered
here.
Mixed and behavioral Stackelberg equilibria
The motivation behind introducing mixed strategies in the investigation of
```
saddle-point equilibria (in Chapter 2) and Nash equilibria (in Section 3.2) was
```
that such equilibria do not always exist in pure strategies, whereas within the
enlarged class of mixed strategies one can ensure existence of noncooperative
equilibria. In the case of the Stackelberg solution of two-person finite games,
```
however, an equilibrium always exists (cf. Thm. 3.3), and thus, at the outset,
```
there seems to be no need to introduce mixed strategies. Besides, since the
leader dictates his strategy on the follower, in a Stackelberg game, it might at
first seem to be unreasonable to imagine that the leader would ever employ a
mixed strategy. Such an argument, however, is not always valid, and there are
```
cases in which the leader can actually do better (in the average sense) with a
```
proper mixed strategy, than the best he can do within the class of pure strate-
```
gies. As an illustration of such a possibility, consider the bimatrix game (^4, B]
```
displayed below:
140 T. BA§AR AND G. J. OLSDER
If PI acts as the leader, then the game admits two pure-strategy Stackelberg
```
equilibrium solutions, which are {L, L} and {R, R}, the Stackelberg outcome in
```
```
each case being (1,1/2). However, if the leader (PI) adopts the mixed strategy
```
which is to pick L and R with equal probability 1/2, then the average cost
```
incurred to PI will be equal to 1/2, quite independent of the follower's (pure
```
```
or mixed) strategy. This value J1 -- 1/2 is clearly lower than the leader's
```
Stackelberg cost in pure strategies, which can further be shown to be the unique
Stackelberg cost of the leader in mixed strategies, since any deviation from
```
(1/2,1/2) for the leader results in higher values for J1, by taking into account
```
the optimal responses of the follower.
The preceding result then establishes the significance of mixed strategies in
the investigation of Stackelberg equilibria of two-person nonzero-sum games,
and demonstrates the possibility that a proper mixed-strategy Stackelberg so-
lution could lead to a lower cost level for the leader than the Stackelberg cost
level in pure strategies. To introduce the concept of mixed-strategy Stackel-
berg equilibrium in mathematical terms, we take the two-person nonzero-sum
TV-PERSON NONZERO-SUM FINITE GAMES 141
```
finite game to be in normal form (without any loss of generality) and associate
```
```
with it a bimatrix game (A, J3). Abiding by the notation and terminology of
```
Section 3.2, we let Y and Z denote the mixed-strategy spaces of PI and P2,
respectively, with their typical elements designated as y and z. Then, we have
the following.
```
Definition 3.31 For a bimatrix game (A,B), the set
```
```
is the optimal response (rational reaction,) set o/P2 in mixed strategies to the
```
mixed strategy y G Y of PI.
```
Definition 3.32 In a bimatrix game (A, B] with PI acting as the leader, a
```
mixed strategy y* G Y is called a mixed Stackelberg equilibrium strategy for the
leader if A
The quantity J1* is the Stackelberg cost of the leader in mixed strategies.
```
It should be noted that the "maximum" in (3.43) always exists since, for each
```
```
y G Y, y'Az is continuous in z, and R2(y) is a closed and bounded subset of Z
```
```
(which is a finite dimensional simplex). Hence, J1* is a well-defined quantity.
```
```
The "infimum" in (3.43), however, cannot always be replaced by a "minimum",
```
unless the problem admits a mixed Stackelberg equilibrium strategy for the
leader. The following example now demonstrates the possibility that a two-
person finite game might not admit a mixed-strategy Stackelberg strategy even
though J1* < J1*.
Example 3.14 Consider the following modified version of the bimatrix game
```
of (3.41):
```
With PI as the leader, this bimatrix game also admits two pure-strategy Stackel-
```
berg equilibria, which are {!/,!/} and {R, R}, the Stackelberg cost for the leader
```
```
being J1* = 1. Now, let the leader adopt the mixed strategy y = (yi, (1 — yi))',
```
under which J2 is
```
where z — (z\, (1 - z\)]' denotes any mixed strategy of P2. Then, the mixed-
```
strategy optimal response set of P2 can readily be determined as
Hence, for y\ > 4/7, the follower chooses "column 1" with probability 1, and
this leads to an average cost of J1 = y\ for PI. For y\ < 4/7, on the other hand,
P2 chooses "column 2" with probability 1, which leads to an average cost level
```
of J1 = (1 — ?/i) for PI. Then, clearly, the leader will prefer to stay in this latter
```
```
region; in fact, if he employs the mixed strategy y = (4/7—e, 3/7+e)' where e > 0
```
is sufficiently small, his realized average cost will be J1 = 3/7 4- e, since then P2
will respond with the unique pure-strategy 72 = R. Since e > 0 can be taken as
small as possible, we arrive at the conclusion that J1* = | < 1 = J1*.
of this fact, the leader does not have a mixed Stackelberg strategy since, for the
```
only candidate y° = (4/7,3/7), R2(y°) = Z, and therefore maxz€^2(J/0) y° Az =
```
4/7 which is higher than J1*.
The preceding example thus substantiates the possibility that a mixed Stack-
elberg strategy might not exist for the leader, but he can still do better than his
```
pure Stackelberg cost J1* by employing some sub-optimal mixed strategy (such
```
```
as the one y = (4/7 — e, 3/7 + e)' in Example 3.14, for sufficiently small e > 0).
```
In fact, whenever J1* < J1*, there will always exist such an approximating
mixed strategy for the leader. If J1* = J1*, however, it is, of course, reasonable
to employ the pure Stackelberg strategy which always exists by Thm. 3.3. The
following proposition now verifies that J1* < J1* and J1* = J1* are the only
```
two possible relations we can have between J1* and J1*; in other words, the
```
inequality J1* > J1* never holds.
Proposition 3.18 For every two-person finite game, we have
Proof. Let YQ denote the subset of Y consisting of all one-point distributions.
Analogously, define ZQ as comprised of one-point distributions in Z. Note that
YQ is equivalent to F1, and ZQ is equivalent to F2. Then, for each y 6 IQ
since any minimizing solution in Z can be replaced by an element of ZQ. This
```
further implies that, for each y e YQ, elements of R2(y) are probability distri-
```
```
butions on R2(y}, where the latter set is defined by (3.42) with Z replaced by
```
ZQ. Now, since YQ C Y,
```
and further, because of the cited relation between R2(-) and R2(-), the latter
```
quantity is equal to
```
which, by definition, is J1*, since R2(y) is equivalent to the pure-strategy op-
```
```
timal response set of the follower, as defined by (3.36). Hence, J1* < J1*.
```
142 T. BA§AR AND G. J. OLSDER
E
```
Computation of a mixed-strategy Stackelberg equilibrium (whenever it ex-
```
```
ists) is not as straightforward as in the case of pure-strategy equilibria, since
```
the spaces Y and Z are not finite. The standard technique is first to determine
```
the minimizing solution(s) of
```
as functions of y € Y. This will lead to a decomposition of Y into subsets
```
(regions), on each of which a reaction set for the follower is defined. (Note that
```
```
in the analysis of Example 3.14, Y has been decomposed into three regions.)
```
Then, one has to minimize y'Az over y e y, subject to the constraints imposed
by these reaction sets, and under the stipulation that the same quantity is
maximized on these reaction sets whenever they are not singletons. This brute-
force approach also provides approximating strategies for the leader, whenever
a mixed Stackelberg solution does not exist, together with the value of J1*.
If the two-person finite game under consideration is a dynamic game in
extensive form, then it is more reasonable to restrict attention to behavioral
```
strategies (cf. Def. 2.9). Stackelberg equilibria within the class of behavioral
```
strategies can be introduced as in Defs. 3.31 and 3.32, by replacing the mixed-
strategy sets with the behavioral-strategy sets. Hence, using the terminology
and notation of Def. 2.9, we have the following counterparts of Defs. 3.31 and
3.32, in behavioral strategies.
Definition 3.33 Given a two-person finite dynamic game with behavioral-strat-
```
egy sets (Tl^T2) and average cost functions (J1, J2) the set
```
```
is the optimal response (rational reaction) set of P2 in behavioral strategies to
```
the behavioral strategy -y1 e f x of PI.
Definition 3.34 In a two-person finite dynamic game with PI acting as the
leader, a behavioral strategy 71* € F1 is called a behavioral Stackelberg equilib-
rium strategy for the leader if
The quantity J1* is the Stackelberg cost of the leader in behavioral strategies.
```
Remark 3.21 The reason why we use "supremum" in (3.46), instead of "max-
```
```
imum", is because the set R2^1) is not necessarily compact. The behavioral
```
Stackelberg cost for the leader is again a well-defined quantity, but a behavioral
Stackelberg strategy for the leader does not necessarily exist.
We now conclude our discussion on the notion of behavioral Stackelberg
equilibrium by presenting an example of a single-act dynamic game that admits
a Stackelberg solution in behavioral strategies.
TV-PERSON NONZERO-SUM FINITE GAMES 143144 T. BA§AR AND G. J. OLSDER
Figure 3.10: The two-person single-act game of Example 3.15.
Example 3.15 Consider the two-person single-act game whose extensive form
is depicted in Fig. 3.10. With P2 acting as the leader, it admits two pure-
strategy Stackelberg solutions:
and
```
with the unique Stackelberg outcome in pure strategies being (0, |). Now, let
```
us allow the leader to use behavioral strategies. The bimatrix game at the first
```
(counting from the left) of the leader's information sets is
```
for which J2* = 1/2. The mixed Stackelberg strategy exists for the leader, given
as
and the behavioral Stackelberg cost for the leader is
```
and any mixed strategy is an optimal response strategy for PI (the follower)
```
yielding him an average cost level of J1 — 3/2.
We thus observe that if the leader can force the follower to play L or M, his
average cost can be pushed down to 1/2 which is lower than J2* = 2/3, and
the follower's cost is then pushed up to 3/2 which is higher than the cost level
of J1 = 0 which would have been attained if P2 were allowed to use only pure
```
strategies. There indeed exists such a (behavioral) strategy for P2, which is
```
given as
```
It should be noted that the follower (PI) does not dare to play u1 — R since,
```
```
by announcing (i) ahead of time, the leader threatens him with an undesirable
```
cost level of J1 = 2 .
```
We now leave it to the reader to verify that the behavioral strategy (i) is
```
```
indeed a Stackelberg strategy for the leader and the quantity (ii) is his Stack-
```
elberg cost in behavioral strategies. This verification can be accomplished by
converting the original game in extensive form into equivalent normal form and
```
then applying Def. 3.34. Another alternative method is to show that (ii) is the
```
```
mixed-strategy Stackelberg cost for the leader, and since (i) attains that cost
```
level, it is also a behavioral Stackelberg strategy, by the result of Problem 21
```
(Section 3.8).
```
Many-player games
Heretofore, we have confined our investigation on the Stackelberg concept in
nonzero-sum games to two-person games wherein one of the players is the leader
and the other one is the follower. Extension of this investigation to A^-player
```
games could be accomplished in a number of ways; here we discuss the possible
```
extensions to the case when N = 3, with the further generalizations to N > 3
being conceptually straightforward.
For three-person nonzero-sum games, we basically have three different pos-
sible modes of play among the players within the framework of noncooperative
decision making.
```
(1) There are two levels of hierarchy in decision making—one leader and two
```
followers. The followers react to the leader's announced strategy by play-
```
ing according to a specific equilibrium concept among themselves (for in-
```
```
stance, Nash).
```
```
(2) There are still two levels of hierarchy in decision making—now two leaders
```
and one follower. The leaders play according to a specific equilibrium con-
cept among themselves, by taking into account possible optimal responses
of the follower.
```
(3) There are three levels of hierarchy. First PI announces his strategy, then
```
P2 determines his strategy by also taking into account possible responses
of P3 and enforces this strategy on him, and finally P3 optimizes his cost
```
function in view of the announced strategies of PI and P2.
```
Let us now provide precise definitions for the three types of hierarchical
```
equilibria outlined above. To this end, let Jt(71,72,73) denote the cost function
```
```
of Pi corresponding to the strategy triple {7* e F1^2 € F2,73 € FS}. Then,
```
we have the following.
```
Definition 3.35 For the three-person finite game with one leader (PI) and two
```
```
followers (PI and P3), 71* € F1 is a hierarchical equilibrium strategy for the
```
leader if
JV-PERSON NONZERO-SUM FINITE GAMES 145
```
where RF(^1} is the optimal response set of the followers' group and is defined
```
for each 71 G F1 by
```
Any (7^*, 7"**) G R* (71*) is a corresponding optimal strategy pair for the fol-
```
lowers ' group.
```
Definition 3.36 For the three-person finite game with two leaders (PI andP2)
```
```
and one follower (P3), 7** G Fz is a hierarchical equilibrium strategy for Pi
```
```
(i = 1,2) if
```
```
where JR3(71;72) is the optimal response set of the follower and is defined for
```
```
each (71,72) G T1 x T2 by
```
```
Any strategy j3* G R3^1*;^2*) is a corresponding optimal strategy for the
```
```
follower (P3).
```
Definition 3.37 For the three-person finite game with three levels of hierarchy
```
in decision making (with PI enforcing his strategy on P2 and P3, and P2
```
```
enforcing his strategy on P3), 71* G F1 is a hierarchical equilibrium strategy
```
for PI if
where
```
Any 72* G S2^1*) is a corresponding optimal strategy for P2, and any 73 G
```
```
53(71*,72*) is an optimal strategy for P3 corresponding to the pair (71*,72*).
```
Several simplifications in these definitions are possible if the optimal response
sets are singletons. In the case of Def. 3.35, for example, we would have the
following.
146 T. BA§AR AND G. J. OLSDER.Y-PERSON NONZERO-SUM FINITE GAMES 147
```
Proposition 3.19 In the three-person finite game of Def. 3.35, if RF (^} is a
```
singleton for each 7* G F1, then there exist unique mappings T2 : F1 —> F2 and
```
T3 : F1 -> F3 such that
```
and
and for each 71 e F1.
Analogous results can be obtained in the cases of Defs. 3.36 and 3.37, if the
optimal response sets are singletons. The following example now illustrates all
these cases with singleton optimal response sets.
Example 3.16 Using the notation of Section 3.3, consider a three-person game
with possible outcomes given as
```
and determine its hierarchical equilibria (of the three different types discussed
```
```
above).
```
1. One leader (PI) and two followers (P2 and P3).
```
For the first alternative of PI (i.e., HI = 1), the bimatrix game faced by P2 and
```
P3 is
```
which clearly admits the unique Nash equilibrium solution {n% = 1,713 = 1}
```
whose cost to the leader is J1 = 1. For n\ = 2, the bimatrix game is
```
with a unique Nash equilibrium solution [n^ — 2,71,3 = 1} wnose cost to PI
```
is J1 = 2. This determines the mappings T2 and T3 of Prop. 3.19 uniquely.
Now, since 1 < 2, the leader has a unique hierarchical equilibrium strategy that
```
satisfies (3.50a), which is 71* = "alternative 1", and the corresponding unique
```
optimal strategies for P2 and P3 are 72* = "alternative 1", 73* — "alternative
```
1", respectively. The unique equilibrium outcome is (1,0,0).
```
2. Two leaders (PI and P2) and one follower (P3).
There are four pairs of possible strategies for the leaders' group, which are listed
below together with the corresponding unique optimal responses of the follower
and the cost pairs incurred to PI and P2:
The two leaders are faced with the bimatrix game
```
whose Nash solution is uniquely determined as {n\ = 2,n^ = 2}. Hence, the
```
hierarchical equilibrium strategies of the leaders are unique: 71* = "alternative
```
2"} y2* _ "alternative 2". The corresponding unique optimal strategy of P3 is
```
```
73* = "alternative 1", and the unique equilibrium outcome is (2, —1,1).
```
3. Three levels of hierarchy (first PI, then P2 and finally P3).
```
We first observe that 52(71) is a singleton and is uniquely determined by the
```
mapping P2 : F1 —> F2 defined by
Hence, if PI chooses HI — 1, his cost will be J1 — 1, and if he chooses n\ = 2, his
cost will be J1 = 2. Since 1 < 2, this leads to the conclusion that Pi's unique
hierarchical equilibrium strategy is 71* = "alternative 1" and the corresponding
unique optimal strategies of P2 and P3 are 72* = "alternative 1" and 73* =
```
"alternative 1", respectively. The unique equilibrium outcome is (1,0,0).
```
3.7 Nonzero-Sum Games with Chance Moves
In this section, we briefly discuss an extension of the general analysis of the
previous sections to the class of nonzero-sum games which incorporate chance
moves, that is, games wherein the final outcome is determined not only by the
actions of the players, but also by the outcome of a chance mechanism. A
special class of such decision problems was already covered in Section 2.6 within
the context of zero-sum games, and the present analysis in a way builds on the
material of Section 2.6 and extends that analysis to nonzero-sum games under
the Nash and Stackelberg solution concepts. To this end, let us first provide a
148 T. BA§AR AND G. J. OLSDER
precise definition of an extensive form description of a nonzero-sum finite game
with chance moves.
Definition 3.38 An extensive form of an N-person nonzero-sum finite game
with chance moves is a tree structure with
```
(i) a specific vertex indicating the starting point of the game,
```
```
(ii) N cost functions, each one assigning a real number to each terminal vertex
```
of the tree, where the ith cost function determines the loss incurred to Pi
for each possible set of actions of the player together with the possible
choices of nature,
```
(Hi) a partition of the nodes of the tree into N + 1 player sets (to be denoted
```
```
by Nl for Pi, i € N, and by N° for nature),
```
```
(iv) a probability distribution, defined at each node of N°, among the imme-
```
```
diate branches (alternatives) emanating from that node,
```
```
(v) a subpartition of each player set Nl into information sets {r/j} such that the
```
same number of immediate branches emanates from every node belonging
to the same information set and no node follows another node in the same
information set.
```
For such a game, let us again denote the (pure) strategy space of Pi by
```
P, with a typical element designated as 7*. For a given TV-tuple of strategies
```
j-yj g F^; j 6 N} the cost function Jl (71 ,.. - ,7^) of Pi is in general a random
```
quantity, and hence the real quantity of interest is the one obtained by taking
the average value of Jl over all relevant actions of nature and under the a priori
```
known probability distribution. Let us denote that quantity by Ll (7*,... ,7^)
```
which in fact provides us with a normal form description of the original nonzero-
sum game with chance moves in extensive form. Apart from a difference in
notation, there is no real difference between this description and the normal
form description studied heretofore in this chapter, and this observation readily
leads to the conclusion that all concepts introduced and general results obtained
in the previous sections on the normal form description of nonzero-sum games
are also equally valid in the present context. In particular, the concept of Nash
equilibrium for /^-person nonzero-sum finite games with chance moves can be
```
introduced through Def. 3.12 by merely replacing Jl with L* (i = 1,...,N).
```
Analogously, the Stackelberg equilibrium solution in two-person nonzero-sum
finite games with chance moves can be introduced by Defs. 3.26-3.29. Rather
than list all these apparent analogies, we shall consider, in the remaining part of
this section, two examples which illustrate derivation of Nash and Stackelberg
equilibria in static and dynamic games that incorporate chance moves.
Example 3.17 Consider the two-person static nonzero-sum game depicted in
Fig. 3.11. If nature picks the left branch, then the bimatrix game faced by the
JV-PERSON NONZERO-SUM FINITE GAMES 149150 T. BA§AR AND G. J. OLSDER
Figure 3.11: A single-act static nonzero-sum game that incorporates a chance
move.
players is
whereas if nature picks the right branch, the relevant bimatrix game is
Since the probabilities with which nature chooses the left and right branches
are 1/4 and 3/4, respectively, the final bimatrix game of average costs is
This game admits a unique Nash equilibrium solution in pure strategies, which
is
```
with an average outcome of (—1, —2).
```
To determine the Stackelberg solution with PI as the leader, we again fo-
```
cus our attention on the bimatrix game (iii) and observe that the Stackelberg
```
solution is in fact the same as the unique Nash solution determined above. D
Example 3.18 Let XQ be a discrete random variable taking the values
We view XQ as the random initial state of some system under consideration. PI,
```
whose decision variable (denoted by u1) assumes the values —1 or +1, acts first,
```
```
and as a result the system is transferred to another state (x\) according to the
```
equation
```
Then P2, whose decision variable u2 can take the same two values (—1 and +1),
```
```
acts and transfers the system to some other state (x-2) according to
```
It is further assumed that, in this process, PI does not make any observation
```
(i.e., he does not know what the real outcome of the chance mechanism (value
```
```
of XQ) is), whereas P2 observes sgn (xi), where
```
This observation provides P2 with partial information concerning the value of
XQ and the action of PI. The cost structure of the problem is such that Pi
wishes to minimize J*, i = 1,2, where
with
We now seek to obtain Nash equilibria and also Stackelberg equilibria of this
single-act game with either player as the leader.
To this end, let us first note that this is a finite nonzero-sum game which
incorporates a chance move, but it has not been cast in the standard framework.
Though, by spelling out all possible values of J1 and J2, and the actions of the
players leading to those values, an extensive tree formulation can be obtained for
the game, which is the one depicted in Fig. 3.12. This extensive form also clearly
displays the information sets and thereby the number of possible strategies for
each player. Since PI gains no information, he has two possible strategies:
P2, on the other hand, has eight possible strategies which can be listed as
AT-PERSON NONZERO-SUM FINITE GAMES 151152 T. BA§AR AND G. J. OLSDER
Figure 3.12: Extensive tree formulation for the single-act two-person game of
Example 3.18.
Then, the equivalent bimatrix representation in terms of average costs is
Since the first row of matrix A strictly dominates the second row, PI clearly
```
has a unique (permanent) optimal strategy 71* = — 1 regardless of whether it
```
is a Nash or a Stackelberg game. P2 has four strategies that are in Nash or
Stackelberg equilibrium with 71*, which are 7§, 74, 75 and 7?. Hence, we have
four sets of Nash equilibrium strategies, with the equilibrium outcome being
```
either (—1, —1) or (—3/2, —1). If PI is the leader, his Stackelberg strategy is
```
```
71* = — 1 and the Stackelberg average cost (as defined in Def. 3.27) is L1* = — 1.
```
He may, however, also end up with the more favorable average cost level of
```
Ll = —3/2, depending on the actual play of P2. If P2 is the leader, he has four
```
```
Stackelberg strategies (7!, 74, 75 and 7^), with his unique Stackelberg average
```
cost being Z/2* = — 1.
To summarize the results in terms of the terminology of the original problem,
```
regardless of the specific mode of play (i.e., whether it is Nash or Stackelberg),
```
PI always chooses u1 — —I, and P2 chooses u2 — +1 if x\ < 0, and is indifferent
```
between his two alternatives (—1 and + l ) i f x i > 0 .
```
TV-PERSON NONZERO-SUM FINITE GAMES 153
3.8 Problems
1. Determine the Nash equilibrium solutions of the following bimatrix game,
and investigate the admissibility properties of these equilibria:
2. Verify that minimax values of a bimatrix game are not lower (in an ordered
```
way) than the pair of values of any Nash equilibrium outcome. Now,
```
construct a bimatrix game wherein minimax strategies of the players are
also their Nash equilibrium strategies, but in which the minimax values
are not the same as the ordered values of the Nash equilibrium outcome.
3. Obtain the mixed Nash equilibrium solution of the following bimatrix
```
game:
```
```
What is the mixed Nash equilibrium solution of the game (—A, —B}1
```
4. If a bimatrix game (A, B) admits multiple minimax strategies for a player,
```
then these can be ordered according to the notion of admissibility (which
```
```
is different from the admissibility of Nash equilibrium a la Def. 3.3). A
```
minimax strategy for PI, say "row i", is said to be admissible if there
exists no other minimax strategy, say "row j", with the property
with the inequality being strict for at least one k. An admissible minimax
strategy for P2 can be defined analogously.
```
(i) Does every bimatrix game admit admissible minimax strategies for PI
```
and P2? If they do, are they necessarily unique?
```
(ii) Obtain the set of all admissible minimax strategies for PI and P2 in
```
the following bimatrix game:
5. Let (A, B) be an (m x n) bimatrix game admitting a unique Nash equi-
```
librium solution in mixed strategies, say (y*, z*}. Prove that the number
```
154 T. BA§AR AND G. J. OLSDER
of nonzero entries of y* and z* are equal. How would this statement have
to be modified if the number of mixed-strategy equilibrium solutions is
greater than one?
6. Prove (or disprove) the following conjecture: Every bimatrix game that
admits more than one pure Nash equilibrium solution with different equi-
librium outcomes necessarily also admits a Nash equilibrium solution in
```
proper mixed strategies (that is, mixed strategies that are not pure).
```
7. Determine all Nash equilibria of the three-person nonzero-sum game, with
```
mi = 77i2 = ma = 2, whose possible outcomes are given below:
```
8. Obtain the minimax strategies of the players in the three-person game of
Problem 7. What are the security levels of the players?
9. Consider the following bimatrix game, which admits three pure-strategy
Nash equilibria:
```
Which of these three Nash equilibria are (i) perfect, (ii) proper?
```
```
(Note: See the discussion in subsection 3.5.5.)
```
10. This problem is concerned with a specific TV-person zero-sum game23 (N >
```
2) which is called Mental Poker after Epstein (1967). Each of the N players
```
```
chooses independently an integer out of the set {1,2,..., n}. The player
```
who has chosen the lowest unique integer is declared the winner and he
receives N — 1 units from the other players each of whom loses one unit.
If there is no player who has chosen a unique integer, then no one wins
the game and everybody receives zero units. If for instance N = 3, n = 5
23In terms of the notation of Section 3.3, the game is zero-sum if X/ PN a «i.--->n N = ^'
Vn,- € MJ, j 6 N.
TV-PERSON NONZERO-SUM FINITE GAMES 155
and the integers chosen are 2, 2 and 5, then the player who has chosen
5 is the winner. Show that for N = 2, n arbitrary, there exists a unique
saddle-point strategy for both players, which is to choose integer 1 with
probability 1.
Next consider the case TV = 3. A mixed strategy for Pi will be denoted by
```
y* = ( r / j , . . . , y£), i = 1,2,3, with y] > 0 and £?=1 y] = 1- we seek a Nash
```
```
equilibrium solution {y1*,^2*,?/3*} with the property yl* = y2* = y3*.
```
Show that there exists a unique such Nash equilibrium solution, which is
```
given by yj* = (1/2)', j = 1 , . . . , n - 1, y% = (1/2)71-1. Generalize these
```
results to N > 3.
11. Obtain all pure-strategy Nash equilibria of the following single-act games
```
in extensive form. Which of these equilibria are (i) admissible, (ii) of the
```
delayed commitment type?
12. Obtain all pure-strategy Nash equilibria of the following three-player single-
act game wherein the order in which P2 and P3 act is a variant of Pi's
```
strategy. (Hint: A recursive procedure still applies here for each fixed
```
```
strategy of PI.)
```
13. Investigate whether the three-person single-act game in nested extensive
form depicted in Fig. 3.13 admits a Nash equilibrium of the delayed com-
mitment type.
156 T. BA§AR AND G. J. OLSDER
14. Determine all multi-act nonzero-sum feedback games which are informa-
tionally inferior to the feedback game whose extensive form is depicted in
Fig. 3.9. Obtain their delayed commitment type feedback Nash equilibria.
15. Investigate whether the multi-act game of Fig. 3.14 admits pure-strategy
Nash equilibria, by transforming it into equivalent normal form.
16. Determine the perfect Nash equilibria (in pure and mixed strategies) of
the multi-act game of Problem 15.
17. Determine the Stackelberg strategies for the leader (PI) and also his
Stackelberg costs in the single-act games of Problem 11. Solve the same
game problems under the Stackelberg mode of play with P2 acting as the
leader.
18. Let I be a two-person single-act finite game in extensive form in which P2
is the first-acting player, and denote the Stackelberg cost for the leader
```
(PI) as Jj1*. Let II be another two-person single-act game that is infor-
```
```
rnationally inferior to I, and whose Stackelberg cost for the leader (PI) is
```
Jji*. Prove the inequality J/* < Jjj*.
Figure 3.13: Single-act game of Problem 13.
Figure 3.14: Multi-act game of Problem 15.
TV-PERSON NONZERO-SUM FINITE GAMES 157
19. Determine the Stackelberg solution of the two-person dynamic game of
```
Problem 15 with (i) PI as the leader, (ii) P2 as the leader.
```
20. Obtain the pure, behavioral and mixed Stackelberg costs with P2 as the
leader for the single-act game of Fig. 3.15. Does the game admit a behav-
```
ioral Stackelberg strategy for the leader (P2)?
```
Figure 3.15: Single-act game of Problem 20.
21. Let 71* € F1 be a mixed Stackelberg strategy for the leader (PI) in a
two-person finite game, with the further property that 71* € F1, i.e.,
it is a behavioral strategy. Prove that 71*
Stackelberg strategy for the leader.
also constitutes a behavioral
22. Prove that the hierarchical equilibrium strategy 71* introduced in Def. 3.37
always exists. Discuss the reasons why 71* introduced in Def. 3.35, and
71* and 72* introduced in Def. 3.36, do not necessarily exist.
23. Determine hierarchical equilibria in pure or behavioral strategies of the
```
single-act game of Fig. 3.4(b) when (i) P2 is the leader, PI and P3 are
```
```
followers, (ii) Pi and P2 are leaders, P3 is the follower, (iii) there are
```
three levels of hierarchy with the ordering being P2, PI, P3.
24. In the four-person single-act game of Fig. 3.5, let P3 and P4 form the
leaders' group, and PI and P2 form the followers' group. Stipulating that
the noncooperative Nash solution concept is adopted within each group
and the Stackelberg solution concept between the two groups, determine
the corresponding hierarchical equilibrium solution of the game in pure or
behavioral strategies.
25. Determine the pure-strategy Nash equilibria of the three-person single-act
game in extensive form depicted in Fig. 3.16, which incorporates a chance
```
move. Also determine its (pure-strategy) Stackelberg equilibria with (i)
```
```
PI as leader, P2 and P3 as followers (i.e., two levels of hierarchy), (ii) P2
```
```
as leader, PI as the first follower, P3 as the second follower (i.e., three
```
```
levels of hierarchy).
```
158 T. BA§AR AND G. J. OLSDER
Figure 3.16: Three-person single-act game of Problem 25.
26. Let XQ be a discrete random variable taking the values
Consider the two-act game whose evolution is described by
```
where UQ (respectively, u\) denotes the decision (action) variable of Pi
```
```
at level 1 (respectively, level 2), i — 1,2, and they take values out of
```
```
the set {—1,0,1}. At level 1 none of the players make any observation
```
```
(i.e., they do not acquire any information concerning the actual outcome
```
```
of the chance mechanism (XQ)}. At level 2, however, P2 still makes no
```
observation, while PI observes the sign of x\. At this level, both players
also recall their own past actions. Finally, let
denote the cost function of Pi which he desires to minimize, where
```
(i) Formulate this dynamic multi-act game in the standard extensive
```
form and obtain its pure-strategy Nash equilibrium.
```
(ii) Determine its pure-strategy Stackelberg equilibria with PI as leader.
```
3.9 Notes
Sections 3.2 and 3.3. The first formulation of nonzero-sum finite games dates
```
back to the pioneering work of Von Neumann and Morgenstern (1947). But the non-
```
cooperative equilibrium solution discussed in these two sections within the context of
```
TV-person finite games in normal form was first introduced by Nash (1951) who also
```
gave a proof of Thm. 3.2. For interesting discussions on the reasons and implications of
```
occurrence of nonunique Nash equilibria, see Howard (1971). A nontechnical treatment
```
of the "prisoner's dilemma" phenomenon can be found in the recent book by Pound-
```
stone (1992). Some other selected texts on nonzero-sum (as well as zero-sum) games
```
```
are Binmore (1992), Fudenberg and Tirole (1991), Kreps (1990), Myerson (1991) and
```
```
Szep and Forgo (1985).
```
Section 3.4. For further discussion on the relationship between bimatrix games
```
and nonlinear programming, see Section 7.4 of Parthasarathy and Raghavan (1971).
```
Section 7.3 of the same reference also presents the Lemke-Howson algorithm which is
further exemplified in Section 7.6 of that reference.
Sections 3.5 and 3.7. A precise formulation for finite nonzero-sum dynamic
```
games in extensive form was first given by Kuhn (1953), who also proved that every
```
```
TV-person nonzero-sum game with perfect information (i.e., with singleton information
```
```
sets) admits a pure-strategy Nash equilibrium solution (note that this is a special case
```
```
of Prop. 3.11 given in subsection 3.5.4). He also showed that in every game in exten-
```
```
sive form with perfect recall, every behavioral-strategy mixture (and hence a mixed
```
```
strategy) for a player can be replaced by a realization-equivalent behavioral strategy,
```
and hence equilibrium can be sought in the class of behavioral strategies. The notion
of ladder-nested extensive forms and the recursive derivation of the Nash equilibrium
solution of the delayed commitment type are introduced here for the first time. The
"informational nonuniqueness" property of the Nash equilibrium solution in finite dy-
namic games is akin to a similar feature in infinite nonzero-sum games observed by
```
Ba§ar (1974, 1976a, 1977b); see also Section 6.3. A special version of this feature was
```
```
observed earlier by Starr and Ho (1969a,b) who showed that, in nonzero-sum finite
```
```
feedback games, the feedback Nash equilibrium outcome could be different (for all play-
```
```
ers) from the open-loop Nash equilibrium outcome. The result of Prop. 3.10 was also
```
```
proved in Dubey and Shubik (1981), and later included in the text by Shubik (1983).
```
```
This latter reference calls the static (open-loop) version of an extensive form the most
```
```
coarsened form, and its (open-loop) Nash equilibria information-insensitive equilibria,
```
as they also constitute equilibria for all refined versions. The material of subsec-
```
tion 3.5.5 is based on Selten (1975) and Myerson (1978). Further refinements on Nash
```
```
equilibrium can be found in Kreps and Wilson (1982), Kohlberg and Mertens (1986)
```
```
and van Damme (1987, 1989).
```
A detailed study of TV-player nonzero-sum games on finite graphs with cycles,
viewed as "nonzero-sum extensions" of the zero-sum theory treated in subsection 2.7.1,
```
can be found in Alpern (1991). The cost TV-tuples corresponding to an infinite path
```
on the graph in such dynamic games are the long term averages of the local cost TV-
tuples. Results have been obtained for the existence of Nash equilibria confined to
certain classes of pure strategies.
```
If a static game (such as the game of the prisoners' dilemma) is played repeatedly,
```
one speaks of repeated games. In such games one gets to know about the past behavior
```
of the other player(s) which may therefore influence the current decision to be made.
```
TV-PERSON NONZERO-SUM FINITE GAMES 159
With this role of information about the past actions of the players, the repeated game
must be viewed as a dynamic one. It is possible to derive conditions for the existence of
a Nash equilibrium for such a dynamic game, which, if viewed at the individual time
```
steps, converges to a Pareto solution of the underlying static ("one shot") game as
```
```
time evolves. The reason why a "sequence of Pareto-like solutions" can form a (stable)
```
Nash equilibrium is the ability to perform threats, which, in its turn, is possible by
applying strategies which include history. Many results exist in this direction. One
```
can for instance consult Radner (1981, 1985), Mertens (1985, 1991) and the references
```
therein.
Sections 3.6 and 3.7. The Stackelberg solution in nonzero-sum static games
```
was first introduced by H. Von Stackelberg (1934) within the context of economic
```
competition and it was later extended to dynamic games primarily in the works of
```
Chen and Cruz, Jr (1972) and Simaan and Cruz, Jr (1973a,b) who also introduced
```
the concept of a feedback Stackelberg solution, but their formulation is restricted by
the assumption that the follower's response is unique for every strategy of the leader.
The present analysis is free from such a restriction. The feature of the mixed-strategy
```
Stackelberg solution exhibited in (Counter-) Example 3.14 is reported here for the first
```
time. Its counterpart in the context of infinite games was first observed in Ba§ar and
```
Olsder (1980a); see also Section 4.4. For a discussion on a unified solution concept,
```
```
from which Nash, Stackelberg, minimax solutions (and also cooperative solutions)
```
```
follow as special cases, see Blaquiere (1976).
```
160 T. BA§AR AND G. J. OLSDER
Static Noncooperative
Infinite Games
4.1 Introduction
This chapter deals with static zero-sum and nonzero-sum infinite games, i.e.,
static noncooperative games wherein at least one of the players has at his dis-
posal an infinite number of alternatives to choose from. In Section 4.2, the
concept of e equilibrium solutions is introduced, and several of its features
are discussed. In Section 4.3, continuous-kernel games defined on closed and
bounded subsets of finite dimensional spaces are treated, and general questions
of the existence and uniqueness of Nash and saddle-point equilibrium solutions
```
in such games are addressed; furthermore, some iterative algorithms are devel-
```
oped for their computation.
Section 4.4 is devoted to the Stackelberg solution of continuous-kernel games,
```
in both pure and mixed strategies; it presents some existence and uniqueness
```
results, together with illustrative examples. Section 4.5 focuses on the con-
```
sistent conjectural variations (CCV) equilibrium in continuous-kernel games,
```
and introduces different levels of approximation to the CCV solution. Finally,
Section 4.6 deals with quadratic games and economic applications. Here, the
theory developed in the previous sections is specialized to games whose kernels
are quadratic functions of the decision variables, and in that context the equi-
librium strategies are obtained in closed form. These results are then applied
to different price and quantity models of oligopoly. Section 4.7 deals with the
Braess paradox, which shows that the Nash equilibrium solution concept could
lead to rather counter-intuitive results in certain situations.
161
Chapter 4
4.2 e Equilibrium Solutions
Abiding by the notation and terminology of the previous chapters, we have N
players, PI,..., PAT, where Pi has the cost function Jl whose value depends
not only on his action but also on the actions of some or all of the other play-
ers. Pa's action is denoted by w% which belongs to his action space U1.24 For
infinite games, the action space of at least one of the players has infinitely many
elements.
In Chapters 2 and 3, we have only dealt with finite games in normal and
extensive form, and in that context we have proven that noncooperative equilib-
```
rium solutions always exist in the extended space of mixed strategies (cf. Thm. 3.2).
```
The following example now shows that this property does not necessarily hold
```
for infinite games in normal form; it will then motivate the introduction of a
```
weaker equilibrium solution concept — the so-called e equilibrium solution.
Example 4.1 Consider the zero-sum matrix game A represented by
Here, PI has two pure strategies, whereas P2 has countably many alternatives.
As in finite zero-sum games, let us denote the mixed strategies of PI and P2 by
```
yi, i = 1,2; and Zj, j = 1,2,..., respectively; that is, yi denotes the probability
```
with which PI chooses row i, and Zj denotes the probability with which P2
chooses column j. Obviously we should have yi > 0, Zj > 0, y\ + 7/2 = 1,
v^°° _ _ i2^j=izj ~ L-
If we employ a natural extension of the graphical solution method described
in Section 2.3, to determine the mixed security strategy of PI, we arrive at
Fig. 4.1. The striking feature here is that the upper envelope is not well defined.
Suppose that the matrix A is truncated to Ak, where Ak comprises the first k
```
columns of A. The mixed security strategy for PI is then y\ = (k — l)/(3k — 2);
```
```
2/2 = (2k — I)/(3k — 2), and the average security level of PI is Vm(Ak) =
```
```
2(k - l)/(3fc - 2). If we let k -> oo, we obtain yl = ±, y% = f, Vm(A) = f.
```
```
Thus it is easily seen that (yl = |, y% = f) is a mixed security strategy for PI
```
in the matrix game A. The mixed security strategy for P2 in the truncated
```
game Ak is (z$ — (k — 2)/(3k - 2), z£ = 2k/(3k - 2)) and the average security
```
```
level of P2 is V_m(Ak) = 2(k — l)/(3Ar - 2). In the limit as k —> oo, we obtain
```
```
(zi — \iz*oo ~ |) and Y.m(A) = |. But, P2 cannot guarantee for himself the
```
```
average lower value V_m (^4), since he cannot choose column "oo". We observe,
```
however, that if he chooses column k with probability |, k being sufficiently
large, then he can secure for himself an average lower value arbitrarily close to
```
Y.m(A) = |. In accordance with this observation, if P2 chooses k such that
```
24In this chapter, we deal with static problems only, and in that context we do not distin-
guish between strategy space and action space, since they are identical.
162 T. BA§AR AND G. J. OLSDERSTATIC NONCOOPERATIVE INFINITE GAMES 163
```
Figure 4.1: Graphical solution to the (2 x oo) matrix game of Example 4.1.
```
```
Y_m(Ak) > | — e, then the corresponding mixed security strategy is call
```
c-mixed security strategy, which is also an e-mixed saddle-point strategy for P2.
The reason for the observed disparity between finite and infinite games lies
in the fact that the set of mixed strategies of P2 is not compact in the latter
```
case. Consequently, an (m x oo) matrix game might not admit a saddle point
```
in mixed strategies, in spite of the fact that the outcome is continuous with
respect to mixed strategies of P2.
The preceding example has displayed the non-existence of a saddle point
within the class of mixed strategies. It is even possible to construct examples
```
of zero-sum (semi-infinite) matrix games which admit e saddle-point strategies
```
within the class of pure strategies. This happens, for instance, in the degenerate
matrix game A, where A = [0 ^ | |...]. Similar features are also exhibited
nonzero-sum matrix games under the Nash solution concept. We now provide
below a precise definition of an t equilibrium solution in ./V-person games within
the context of pure strategies. Extension to the class of mixed strategies is
then immediate, and the last part of this section is devoted to a discussion on
that extension, as well as to some results on existence of mixed e equilibrium
solutions.
```
Definition 4.1 For a given e > 0, an N-tuple {u\ , . . . ,u^ }, with u\ e Ul,
```
```
i € N7 is called a (pure) e Nash equilibrium solution for an N-person nonzero-
```
sum infinite game if
```
Fore = 0, one simply speaks of "equilibrium" instead of "0 equilibrium" solution,
```
in which case we denote the equilibrium strategy of Pi by ul .
For zero-sum two-person games the terminology is somewhat different. For
N — 2 and J1 = — J2 = J, we have the following definition.
```
Definition 4.2 For a given c>0, the pair {u^' ,u2*} E Ul x U2 is called an e
```
saddle point if
```
for all {u1,^2} E U1 x U2. For € = 0 one simply speaks of a "saddle point".
```
By direct analogy with the treatment given in Section 2.2 for finite games,
the lower value of a zero-sum two-person infinite game is defined by
```
which is also the security level of P2 (the maximizer). Furthermore, the upper
```
value is defined by
which is also the security level of PI. Since the strategy spaces are fixed, and
in particular the structure of U1 does not depend on uj, i ^ j, i,j = 1,2,
```
it can easily be shown, as in the case of Thm. 2.1(iii), that V_ < V. If V_ =
```
V, then V = V_ = V is called the value of the game. We should note that
```
existence of the value of a game does not necessarily imply existence of (pure)
```
```
equilibrium strategies; it, however, implies existence of an e saddle point—a
```
property that is verified in Thm. 4.1 below, which proves, in addition, that the
converse statement is also true.
```
Theorem 4.1 A two-person zero-sum (infinite) game has a finite value if, and
```
only if, for every e > 0, an e saddle point exists.
```
Proof. First, suppose that the game has a finite value (V = V_ = V). Then,
```
given an e > 0, one can find u\ € U1 and u2' e U2 such that
which follow directly from the definitions of V_ and V, respectively. Now, since
```
V_ = V = V, by adding e to both sides of the first inequality we obtain
```
```
where the latter inequality follows from (ii) by letting u2 = u2'. Analogously,
```
```
if we now add —e to both sides of (ii), and also make use of (i) with u1 = u**,
```
we obtain
164 T. BA§AR AND G. J. OLSDER
```
If (iii) and (iv) are collected together, the result is the set of inequalities
```
for all n1 € U1, u2 e U2, which verifies the sufficiency part of the theorem, in
view of Def. 4.2.
Second, suppose that for every e > 0, an e saddle point exists, that is, a pair
```
(uf e Ul,u2* e U2} can be found satisfying (v) for all n1 E C71, u2 e U2. Let
```
```
the middle term be denoted as Je. We now show that the sequence {J€l, JC 2 ,...},
```
with €i > £2 > • • • > 0 and lim^oo Cj = 0, is Cauchy.25 Toward this end, let
```
us first take e = e^ and <L = €j, j > k, in subsequent order in (v) and add the
```
resulting two inequalities to obtain
```
Now, substituting first {u1 — u\. , u2 — u2k} and then {u1 — u\k, u2 — u2 } in
```
the preceding inequality, we get
```
for any finite k and j, with j > fc, which proves that { J € k } is indeed a Cauchy
```
sequence. Hence, it has a limit in R, which is the value of the game.
```
It should be noted that, although the sequence {«/£fc} converges, the se-
```
```
quences {u\k } and {u2k } need not have limits.
```
Extension to mixed strategies
All definitions, as well as the result of Thm. 4.1, of this section have been
presented for pure strategies. We now outline possible extensions to mixed
strategies.
We define a mixed strategy for Pi as a probability distribution /j,1 on t/z,
and denote the class of all such probability distributions by Ml. For the case in
which U1 is a finite set, we have already seen in Chapters 2 and 3 that Ml is a
simplex of an appropriate dimension. For an infinite t/z, however, its description
will be different. Let us consider the simplest case in which Ul is the unit interval
[0,1]. Then, elements of Ml are defined as mappings ^ : [0,1] —» [0,1] with the
properties
that is, Ml is the class of all probability distribution functions defined on [0,1].
More generally, if Ul is a closed and bounded subset of a finite dimensional
space, a mixed strategy for Pi can likewise be defined since every such U"1 has
the cardinality of the continuum and can therefore be mapped into the unit
25For a definition of a Cauchy sequence, the reader is referred to Appendix A.
STATIC NONCOOPERATIVE INFINITE GAMES 165
interval in a one-to-one manner. However, such a definition of a mixed strategy
leads to difficulties in the construction of the average cost function, and therefore
```
it is convenient to define $ as a probability measure (see Ash (1972)) on Ul,
```
which is valid even if Ul is not closed and bounded. The average cost function
of Pi is then defined as a mapping Jl : M1 x • • • x MN —> R by
whenever this integral exists, which has to be interpreted in the Lebesgue-
```
Stieltjes sense (see Appendix B.3).
```
```
With the average cost functions defined as in (4.2) the mixed e Nash equi-
```
librium solution concept can be introduced as in Def. 4.1, by merely replacing
Jl by Jl, Ul by Mz, u1 by IJLI and ul€' by //**. Analogously, Def. 4.2 can be
extended to mixed strategies, and Thm. 4.1 finds a direct counterpart in mixed
strategies, with V_ and V replaced by
and
respectively, and with V replaced by Vm.
We devote the remainder of this section to the mixed e Nash equilibrium
solution concept within the context of semi-infinite bimatrix games. The two
matrices, whose entries characterize the possible costs to be incurred to PI and
```
P2, will be denoted by A = {a^-} and B = {6^}, respectively, and both will
```
```
have size (ra x oo). If m < oo, then we speak of a semi-infinite bimatrix game,
```
and if m = oo, we simply have an infinite bimatrix game. The case when the
```
matrices both have the size (oo x n) follows the same lines and will not be
```
treated separately.
```
A mixed strategy for PI, in this case, is an m-tuple {yi, 3/2? • • • > 2/m} satisfying
```
Sili 2/i — 1? y* > 0- The class of such m-tuples is denoted by Y. The class Z is
```
defined, similarly, as the class of all sequences {21, z-i,...} satisfying Xli^i zi — 1>
```
Zi > 0. Each element of Z defines a mixed strategy for P2. Now, for each
```
pair {y 6 Y,z € Z}, the average costs incurred to PI and P2 are defined,
```
respectively, by
assuming that the infinite series corresponding to these sums are absolutely
convergent. If the entries of A and B are bounded, for example, then such a
requirement is fulfilled for every y G Y", z 6 Z. The following theorem now
verifies the existence of mixed e equilibrium strategies for the case m < oo.
166 T. BA§AR AND G. J. OLSDER
```
Theorem 4.2 For each e > 0, the semi-infinite bimatrix game (A,B], with
```
```
A — {aij}™=l°^l,B = {bij}1£=l<jL1,m < oo and the entries a^ and bij being
```
bounded, admits an e equilibrium solution in mixed strategies.
Proof. Let N denote the class of all positive integers. For each j € N, define
```
Cj as the jth column of B, i.e., GJ = Bej. The set C — {c^, j € N} is a bounded
```
subset of Rm. Let e > 0 be given. Then, a finite subset C C C exists such that
to each c e C a c E C corresponds with the property \\c — c||oo < e, which means
```
that the magnitude of each component of (c — c) is less than e. Without loss of
```
```
generality, we assume that C = {ci, 0 2 , . . . , cn} with n < oo. If necessary, the
```
pure strategies of P2 can be rearranged so that C consists of the first n columns
ofB.
```
Define An = {alj}^l^1,Bn = {bij}™^^. By Thm. 3.1, the "truncated"
```
```
bimatrix game (An,Bn} has at least one equilibrium point in mixed strategies,
```
```
to be denoted by ( y , z ) . We now prove that (y, z) is a mixed e equilibrium
```
```
solution for (A, B), where z is defined as (£', 0,0,...)'. For all y G Y we have
```
```
For each r G N, we choose a 6(r) G {!,...,n} such that \\cr — c^Hoo < e.
```
```
For each j € {1,... , n}, let R(j) = {r G N : 6(r) = j}. Next, for each z G Z,
```
```
we define z = (zi,...,zn) by Zj = ^2r€R^zr. Then Zj > 0 and ^Zj — 1.
```
Subsequently,
```
for all z € Z. Inequalities (i) and (ii) verify that (y, z) constitutes a mixed e
```
```
equilibrium solution for (A,B).
```
Corollary 4.1 Every bounded semi-infinite zero-sum matrix game has a value
in the class of mixed strategies.
Proof. This result is a direct consequence of Thm. 4.2, and constitutes a
natural extension of Thm. 4.1 to mixed strategies.
```
For (oo x oo) bimatrix games such powerful results do not exist as shown in
```
```
the following (counter-) example.
```
Example 4.2 Each one of two players chooses a natural number, independently
of the other. The player who has chosen the highest number wins and receives
```
where lm = (1,..., 1)' G Rm. Hence
```
STATIC NONCOOPERATIVE INFINITE GAMES 167168 T. BA§AR AND G. J. OLSDER
one unit from the other player. If both players choose the same number, then
the outcome is a draw. The matrix corresponding to this zero-sum game is
```
whose size is (oo x oo). For this game, V_ = — 1 and V = +1 and consequently
```
the average value Vm does not exist, in spite of the fact that the matrix entries
are bounded.
4.3 Continuous-Kernel Games: Reaction Curves,
and Existence and Uniqueness of Nash and
Saddle-Point Equilibria
This section deals with static games in which the number of alternatives avail-
able to each player is a continuum and the cost functionals are continuous. In
particular, we shall consider the class of games for which a pure strategy of each
player can be represented as an element of a finite-dimensional space, and hence
take Ul C Rmi, where raj is an integer denoting the dimension of the decision
vector of Pi. We will sometimes speak of "games on the square", which will
refer to two-player games where Ul = [0,1], i = 1, 2.
We first discuss the role of reaction curves in the construction of pure-
strategy Nash equilibria in continuous-kernel games, and present some classifi-
cation of Nash equilibria based on the topological classifications of the reaction
curves. Then, we state and prove some results on the existence of Nash equilib-
ria, and as a special case the existence of saddle-point solutions in continuous-
kernel games, in both pure- and mixed-strategy spaces. Finally, we discuss some
computational algorithms motivated by the notion of stability of Nash equilibria.
Reaction curves and classification of Nash equilibria
Pure-strategy Nash equilibrium solution in infinite static games can be obtained
as the common intersection point of the reaction curves of the players. The con-
```
cept of a reaction set was already introduced in Chapter 3 (Def. 3.26) within the
```
context of finite games, and its counterpart in infinite games describes a curve,
or a family of curves, with some properties like continuity and differentiability,
depending on the structure of the action sets and the cost functionals. We now
make this notion precise for AT-person games.
STATIC NONCOOPERATIVE INFINITE GAMES 169
Definition 4.3 In an N-person nonzero-sum game, let the minimum of the
```
cost function of PI, Jl(ul, • • • , U N ) , with respect to ul £ Ul be attained for
```
```
each u_i 6 U-\, where u_i = {u2 ,... ,UN} and £/_i = U2 x • • • x UN. Then,
```
```
the set RI(U-I) C Ul defined by
```
```
is called the optimal response or rational reaction set of PI. If RI(U-I} is a
```
singleton for every w_i € U-\, then it is called the reaction curve or reaction
```
function of PI, and is denoted by l\(u-i). The reaction sets and curves of Pi,
```
```
i = 2 , . . . , N are similarly defined (simply by replacing the index 1 by i).
```
Figure 4.2: Constant level curves for J1 and J2, and the corresponding reaction
```
curves (l\ and I?} of PI and P2, respectively.
```
To illustrate the role of reaction curves in the derivation of Nash equilibria,
we have drawn in Fig. 4.2, the "constant level" or iso-cost curves corresponding
to two cost functions J1 and J2 for a specific two-person game with Ul = U2 =
R. For fixed it1, say ul — ul, the best P2 can do is to minimize J2 along the line
```
ul = ul. Assuming that this minimization problem admits a unique solution,
```
the said optimal response of P2 is determined, in the figure, as the point where
the line ul — ul is tangent to iso-cost curve J2 = constant. For each different
ft1, a possibly different unique optimal response can thus be found for P2, and
the collection of all these points forms the reaction curve of P2, indicated by
/2 in the figure. The reaction curve of PI is similarly constructed: it is the
```
collection of all points (u1, u2) where horizontal lines are tangent to the iso-cost
```
curves of J1, and it is indicated by /i in the figure. By definition, the Nash
solution must lie on both reaction curves, and therefore, if these curves have
only one point of intersection, as in the figure, the Nash solution exists and is
unique.
The configuration of the reaction curves is not always as depicted in Fig. 4.2.
In Fig. 4.3, some other possibilities have been displayed. In all these cases it has
been assumed that Ul = U2 — R. In Fig. 4.3a, l\ and /2 are parallel straight
lines and neither a Nash nor an e Nash solution exists. In Fig. 4.3b, l\ and 1%
170 T. BA§AR AND G. J. OLSDER
Figure 4.3: Possible configurations of the reaction curves.
partly coincide and there exists a continuum of Nash solutions. In Fig. 4.3c,
there is a finite number of Nash solutions. In Fig. 4.3d, one of the reaction
curves is not connected and neither a Nash solution nor an c. Nash solution
exists.
The nonzero-sum games associated with Figs. 4.3b and 4.3c both exhibit
multiple Nash equilibria, in which case, as discussed earlier in Section 3.2, the
Nash concept is weak as an equilibrium solution concept. Unless other crite-
ria are imposed, there is no reason why players should prefer one particular
```
equilibrium solution over the other(s). In fact, since the players make their de-
```
```
cisions independently, it could so happen that the outcome of their joint (but
```
```
noncooperative) choices is not an equilibrium point at all. Such nonequilibrium
```
outcomes of unilateral equilibrium strategies are shown in Figs. 4.3b and 4.3c,
where PI picks a Nash strategy u1 and P2 chooses a Nash strategy u2.
In the case of nonunique Nash equilibrium solutions, we can classify them in
a number of ways. One such classification is provided by the notion of "robust-
```
ness" , which is introduced below for (two-person) games on the square, which,
```
however, easily extends to general TV-person games covered by Def. 4.3.
```
Definition 4.4 Given two connected curves u2 = ^(u1) andu1 — l\(u2} on the
```
square, denote their weak 6-neighborhoods by N2 andN$, respectively.26 Then,
a point P of intersection of these two curves is said to be robust if, given e > 0,
there exists a 60 > 0 so that every ordered pair selected from N2Q and N$o has
an intersection in an ^-neighborhood of P.
As specific examples, consider the two-person nonzero-sum games for which
the reaction functions are depicted in Figs. 4.4a and 4.4b. In Fig. 4.4a, the
26A weak ^-neighborhood Ns,ofl: [0,1]
```
[0,1] such that |/(£) - l(£)\ < 6, Vf € [0,1].
```
[0,1] is defined as the set of all maps I : [0,1]
STATIC NONCOOPERATIVE INFINITE GAMES 171
Figure 4.4: Games on the square, illustrating robust Nash solutions.
point PI, constitutes a robust Nash solution, while point PI is not robust. In
Fig. 4.4b, however, all the Nash equilibrium solutions are robust. It should be
noted that the number of robust intersections of two connected curves is actually
odd, provided that possible points of intersections at the corners of the square
are excluded. Hence, we can say that nonzero-sum two-person static games
on the square, with well-defined reaction functions for both players, essentially
admit an odd number of Nash equilibrium solutions. An exception to this rule
is the case when the two reaction curves intersect only at the corners of the
square.
Yet another classification within the class of multiple Nash equilibrium so-
lutions of a two-person nonzero-sum static game is provided by the notion of
```
"stability" of the solution(s) of the fixed point equation. Given a Nash equilib-
```
```
rium solution, consider the following sequence of moves: (i) One of the players
```
```
(say PI) deviates from his corresponding equilibrium strategy, (ii) P2 observes
```
```
this and minimizes his cost function in view of the new strategy of Pi, (iii) PI
```
```
now optimally reacts to that (by minimizing his cost function), (iv) P2 optimally
```
reacts to that optimum reaction, etc. Now, if this infinite sequence of moves
converges to the original Nash equilibrium solution, and this being so regardless
of the nature of the initial deviation of PI, we say that the Nash equilibrium
solution is stable. If convergence is valid only under small initial deviations,
then we say that the Nash equilibrium solution is locally stable. Otherwise, the
Nash solution is said to be unstable. A nonzero-sum game can of course admit
more than one locally stable equilibrium solution, but a stable Nash equilibrium
solution has to be unique.
The reaction functions of two different nonzero-sum games on the square
```
are depicted in Fig. 4.5. In the first one (Fig. 4.5a) the equilibrium solution is
```
stable, whereas in the second one it is unstable.
The notion of stability, as introduced above for two-person games, brings in
a refinement to the concept of Nash equilibrium, which finds natural extensions
to the TV-player case. Essentially, we have to require that the equilibrium be
"restorable" under any rational readjustment scheme when there is a deviation
from it by any player. For N > 2 this will depend on the specific scheme adopted,
which brings us to the following formal definition of a stable Nash equilibrium.
172 T. BA§AR AND G. J. OLSDER
```
Figure 4.5: Illustration of (a) a stable and (b) an unstable Nash solution.
```
```
Definition 4.5 A Nash equilibrium u1 ,i £ N, is (globally,) stable with respect
```
to an adjustment scheme S if it can be obtained as the limit of the iteration:
/ & \where the superscript $k indicates that the precise choice of u_£ depends on
the readjustment scheme selected.
One possibility for the scheme above is
```
which corresponds to the situation where the players update (readjust) their
```
actions simultaneously, in response to the most recently determined actions of
the other players. Yet another possibility is
```
where the players do the update in a predetermined (in this case numerical)
```
order. A third possibility is
where m* fc is an integer-valued random variable, satisfying the bounds
which corresponds to a situation where Pi receives action update information
from Pj at random times, with the delay not exceeding d time units.
Clearly, if the iteration of Def. 4.5 converges under any one of the readjust-
```
ment schemes above (or any other readjustment scheme where a player receives
```
```
update information from every other player infinitely often), then the Nash
```
equilibrium solution is unique. Every unique Nash equilibrium, however, is not
necessarily stable, nor is a Nash equilibrium that is stable with respect to a
particular readjustment scheme necessarily stable with respect to some other
```
scheme. Hence stability is generally given with some qualification (such as "sta-
```
```
ble with respect to scheme <S" or "with respect to a given class of schemes"),
```
```
except when the number of players is two, in which case all schemes (with at
```
```
most a finite delay in the transmission of update information) lead to the same
```
condition of stability, as one then has the simplified recursions
where r^, r2,i, rs^, .. .denote the time instants when Pi receives new action
update information from Pj, j ^ i, i, j = 1,2.
Existence of Nash equilibria
In view of the discussion in the previous subsection, the existence of pure-
strategy Nash equilibria in TV-person continuous-kernel games can be established
by proving existence of well-defined reaction functions with a common point of
intersection. The following theorem does precisely that, and provides a set
of sufficient conditions under which TV-person nonzero-sum games admit pure-
strategy Nash equilibria.
Theorem 4.3 For each i G N, let Ul be a closed, bounded and convex subset
of a finite-dimensional Euclidean space, and the cost functional J1 : Ul x • • • x
UN —>• R be jointly continuous in all its arguments and strictly convex in ul for
every w-7 € f/-7', j G N7 j ^ i. Then, the associated N-person nonzero-sum game
admits a Nash equilibrium in pure strategies.
Proof. Let us take i = 1. By strict convexity, there exists a unique mapping
```
/i : C7_i —> Ul such that u1 = h(u2,... ,UN] uniquely minimizes J1 (u1 ,... ,UN)
```
```
for any given (N — l)-tuple {it2,..., UN}.
```
The mapping l\ is actually the reaction function of PI in this TV-person game.
Similarly, reaction functions ^, i — 2 , . . . , TV, can be defined as unique mappings
from U-i into U\ Using vector notation, these relations can be written in
```
compact form as u = £(u), where u = (it1 ,..., UN) € U = Ul x • • • x UN, and
```
```
L — ( / i , . . . ,/#). It will be shown in the sequel that the individual reaction
```
functions li are continuous in their arguments, and hence L is a continuous
mapping. Since L maps a closed and bounded subset U of a finite-dimensional
space into the same subset, this then implies, by utilization of Brouwer's fixed
```
point theorem (see Appendix C), that there exists a u* E U such that u* = L(u*}
```
```
(that is, u* is a fixed point of L). Obviously, the individual components of it*
```
constitute a Nash equilibrium solution.
To complete the proof of the theorem, what remains to be shown is the
continuity of ^. Let us take i — 1 and assume that, to the contrary, l\ is dis-
```
continuous at (WQ, . . . , itj^). Further, let /1(1/0, . . . , UQ) = UQ. Then there exists
```
STATIC NONCOOPERATIVE INFINITE GAMES 173
```
a sequence of vectors {uj = ( w 2 , . . . , Uj1)'] j — 1,2,...} such that (WQ, • • • , UQ)'
```
```
is the limit of this sequence but UQ is not the limit of / i ( u 2 , . . . , w^) as j —> oo.
```
```
Because of compactness of the action spaces, there exists a subsequence of {v>j},
```
```
say {ujk}, such that li(ujk] converges to a limit UQ ^ UQ, and simultaneously
```
the following inequality holds:
```
Now, by taking limits with respect to the subsequence of indices {jk}, one
```
obtains the inequality
which, together with UQ ^ UQ, forms a contradiction to the initial hypothesis that
```
UQ is the unique u1 which minimizes Jl(ul, UQ, ..., uff). Hence, /i is continuous.
```
The continuity of /j, i > 1, can analogously be proven.
One of the conditions of Thm. 4.3 was the compactness of the action spaces
Ul, which was needed in the proof of the theorem to ensure that for each u_j G
C/_j there exists a u1 G Ul that minimizes J1, and hence that the reaction curve
```
of each player is well defined. If the Ul are unbounded (such as Ul = Rmi),
```
then the same end result can be obtained provided that the cost function of the
```
corresponding player becomes arbitrarily large (positive) as its action variable
```
becomes arbitrarily large in norm. Under such a condition, the player's action
variable can be restricted to a closed and bounded set, without any loss of
generality, and hence the theorem again applies. This reasoning then leads to
the following corollary to Thm. 4.3.
Corollary 4.2 For each i G N, let U* = Rm% the cost functional J1 : U1 x
• • • x UN —»• R be jointly continuous in all its arguments and strictly convex in
u1 for every u^ G U^, j G N, j ^ i. Furthermore, let
Then, the associated N-person nonzero-sum game admits a Nash equilibrium in
pure strategies.
```
The notion of stability introduced in the previous subsection (cf. Def. 4.5)
```
can be used to sharpen the result above for two-person games, so as to establish
unicity of equilibria under some additional conditions. Toward that end, let
J1 and J2 be twice continuously differentiate on Rmi x Rm2, with J1 strictly
convex on Rmi for each u2 G Rm2, and J2 strictly convex on Rm2 for each
u1 G Rmi. Consider the parallel readjustment scheme whereby each player
```
responds (optimally) to the previously selected action of the other player, which
```
can be written as
```
where l\ and l<i are continuous (reaction) functions, uniquely solvable from
```
174 T. BA§AR AND G. J. OLSDER
```
Hence, if (u1*,^2*) is a stable Nash equilibrium, then
```
where u is generated from
Now introduce the matrix
```
where w is an arbitrary mi-dimensional vector, v — l2(w), and u = l\(v}. If the
```
```
operator norm of T (equivalently, its spectral radius) is bounded above by a,
```
```
0 < a < 1, for all w G Rmi, it can be shown (see Li and Ba§ar, 1987) that for
```
some a, a < a < 1,
```
which implies (by a contraction mapping argument) that u —> u1*, and hence
```
```
u2 — /2(« ) —> u2*—which proves existence of a stable Nash equilibrium,
```
which by definition is also unique. Hence, we have the following.
Proposition 4.1 For a two-person nonzero-sum game, in addition to the hy-
potheses of Corollary 4-2, assume that Jl is twice continuously differentiable in
u1 for each u-7 € Ttmj, j ^ z, i, j = 1, 2. Further assume that the matrix function
```
(4-8) has operator norm strictly less than 1. Then the game admits a unique
```
Nash equilibrium, which is also stable.
```
Remark 4.1 One class of games for which the last condition of Prop. 4.1 (that
```
```
is, the one involving the operator norm of (4.8)) is satisfied is that with weakly
```
coupled players, that is one with cost functions:
where e is a sufficiently small scalar. More precisely, there exists an ei > 0
```
such that for all e 6 (—ei, ei), this class of convex-kernel games admits a unique
```
stable Nash equilibrium, which can be obtained as the limit point of the parallel
```
update scheme introduced earlier (see Srikant and Ba§ar, 1992).
```
One important class of games not covered by Thm. 4.3 are those where the
```
action spaces (or the constraint sets) of the players are not rectangular. As an
```
example of such a scenario consider the two-person scalar game on the unit circle
```
(instead of the unit square), where the scalar decision variables of the two players
```
```
are coupled through the inequality (u1}2 + (u2)2 < 1. Note that in such a case,
```
STATIC NONCOOPERATIVE INFINITE GAMES 175
there does not exist a U1 and a U2 from which the actions of the players can be
```
chosen independently, but rather a single set U = {(u1, w2) : (u1)2 + (w2)2 < 1},
```
```
to which the pair (ul,u2} belongs. Extending this to the A/"-person case, we
```
```
have the coupled constraint set U C Rm, where the TV-tuple u = (u1, w 2 , . . . , UN)
```
```
belongs, with m = £)i=i mi. The concept of Nash equilibrium is still well defined
```
in this general framework, with the obvious modification that u G U is a Nash
equilibrium if
```
where Ul(u-{) is a subset of Rm% obtained by the projection
```
```
Note that (4.9) can equivalently be written as
```
where
a form that we will have occasion to use in the sequel. The following theorem
now generalizes the result of Thm. 4.3 to such games.27
Theorem 4.4 Let U be a closed, bounded and convex subset of Rm, and for
each i € N the cost functional J1 : U —> R be continuous on U and convex in u1
for every u-7 € U^, j 6 N, j ^ i. Then, the associated N-person nonzero-sum
game admits a Nash equilibrium in pure strategies.
Proof. First note that under the hypotheses of the theorem the function
```
J(M; v) defined prior to the statement of the theorem is continuous in u and v
```
```
and is convex in v for every fixed v, with (u, v) G U x U. Introduce the reaction
```
set for the game:
which, by the continuity and convexity property of J cited above, is an upper
semicontinuous mapping that maps each point u in the convex and compact
set U into a closed convex subset of U. Then, by the Kakutani fixed point
```
theorem (see Appendix C), there exists a point u* € U such that w* € Tu*, or
```
```
equivalently such that it minimizes J(u*;v) over v € U. Such a point indeed
```
27This theorem is of course also valid for the special case in which the action constraint
sets are rectangular, and for this class it relaxes the assumption of strict convexity of the cost
functions to simple convexity.
176 T. BA§AR AND G. J. OLSDER
constitutes a Nash equilibrium, because if it does not, then this would imply
```
that for some i £ N there would be a ul £ ^(w!^) such that
```
```
which would in turn imply (by adding Ji(u*) to both sides and summing over
```
```
j G N, j ^ i) the strict inequality
```
```
contradicting the initial hypothesis that u* minimizes J ( u * ; v ) over v e U.
```
```
Since two-person zero-sum games are special types of (two-person) nonzero-
```
sum games, the statements of Thms. 4.3 and 4.4 are equally valid for saddle-
point solutions. The first one can even be sharpened to some extent. We first
```
recall the ordered interchangeability property of multiple saddle points (viewed
```
```
as a direct extension of Corollary 2.1), which we will need in the sequel.
```
Property 4.1 For a given two-person zero-sum game with rectangular action
sets U1 and U2 for the players, multiple saddle points satisfy the ordered in-
```
terchangeability property; that is, if (ul*,u2*) and (ul,u2) are two saddle-point
```
```
equilibrium solutions, so are the pairs (ul*,u2) and (ul,u2*}.
```
Theorem 4.5 Consider a two-person zero-sum game on convex finite-dimensional
```
action sets U1 x U2, defined by the continuous kernel J(ul,u2}. Suppose that
```
```
J(w1, w2) is strictly convex in u1 for each u2 £ U2 and strictly concave in u2 for
```
each u1 £ U1. Suppose that either
```
(i) U1 and U2 are closed and bounded, or
```
Then, the game admits a unique pure-strategy saddle-point equilibrium.
Proof. Existence of a saddle-point equilibrium is a direct consequence of
Thm. 4.3 and Corollary 4.2. Furthermore, by strict convexity and concavity,
there can be no saddle-point solutions outside the class of pure strategies. Hence
only uniqueness within the class of pure strategies remains to be proven, which,
however, follows readily from the ordered interchangeability property of multiple
```
saddle points (cf Property 4.1).
```
The next theorem, which follows as a special case of Thm. 4.4, provides a
generalization of the preceding theorem to coupled constraint sets, but in the
absence of uniqueness of equilibria.
```
Theorem 4.6 Let J(ul, u2) be a functional defined on the (not necessarily rect-
```
```
angular) convex and compact action set U. If J is continuous on U, concave in
```
```
u2 for each u1 £ Ul(u2) and convex in u1 for each u2 £ U2(ul), then a saddle
```
point exists in pure strategies, but it is not necessarily unique.
STATIC NONCOOPERATIVE INFINITE GAMES 177178 T. BA§AR AND G. J. OLSDER
We now focus attention on nonzero-sum games whose cost functionals are
continuous but not necessarily convex. For such games one cannot, in general,
```
hope to obtain pure-strategy Nash equilibria; however, in the enlarged class
```
of mixed strategies, the Nash equilibrium solution exists as it is stated in the
following theorem.
Theorem 4.7 An N-person nonzero-sum game in which the finite-dimensional
```
action spaces Ul (i e N) are compact and the cost functionals J1 (i G N) are
```
continuous on U1 x • • • x UN admits a Nash equilibrium in mixed strategies.
```
Proof. A proof of this theorem can be found in Owen (1974). The under-
```
lying idea is to make the kernels Jl discrete so as to obtain an AT-person finite
matrix game that suitably approximates the original game in the sense that a
```
mixed-strategy solution of the latter (which always exists by Thm. 3.1) is ar-
```
bitrarily close to a mixed equilibrium solution of the former. Compactness of
the action spaces ensures that a limit to the sequence of solutions obtained for
approximating finite matrix games exists. For yet another proof of this result,
```
see Glicksberg (1950).
```
As a special case of Thm. 4.7 we now have the following.
Corollary 4.3 Every continuous-kernel two-person zero-sum game with com-
```
pact action (strategy) spaces admits a saddle point in mixed strategies.
```
The question of the weakest conditions under which a mixed-strategy saddle
point exists naturally arises. Corollary 4.3 says that continuity is a sufficient
condition. But it is by no means necessary, and, in fact, it can be replaced by
```
semi-continuity conditions (see Glicksberg, 1950). However, it is not possible to
```
relax the semi-continuity conditions any further and still retain existence of a
```
mixed-strategy saddle point within a general enough framework (see Sion and
```
```
Wolfe, 1957).
```
We conclude this section with an example of a zero-sum game whose cost
functional is continuous but not convex-concave, and which admits a mixed
saddle-point equilibrium.
```
Figure 4.6: Reaction curves for the zero-sum game with kernel J = (u1 — u2}2.
```
Example 4.3 Consider the two-person zero-sum game on the square [0,2]2
```
characterized by the kernel J = (u1 — u2)2. In Fig. 4.6, the two reaction curves
```
11 and l^ have been drawn, which clearly do not intersect. It can readily be
verified that the upper value V = 1 and the lower value V_ = 0, i.e., a pure-
strategy saddle point does not exist. In the extended class of mixed strategies,
however, a candidate saddle-point solution directly follows from Fig. 4.6, which
is
It can readily be verified that this pair of strategies indeed provides a mixed
saddle-point solution.
4.4 Stackelberg Solution of Continuous-Kernel
Games
This section is devoted to the Stackelberg solution of static nonzero-sum game
problems in which the number of alternatives available to each player is not a
finite set and the cost functions are described by continuous kernels. For the
sake of simplicity and clarity in exposition, we shall deal primarily with two-
person static games. A variety of possible extensions of the Stackelberg solution
concept to TV-person static games with different levels of hierarchy will be briefly
mentioned towards the end of the section, with the details left to the interested
reader as exercises.
Let ul € Ul denote the action variable of Pi, where his action set Ul is
assumed to be a subset of an appropriate metric space and, the cost function
Jl of Pi be denned as a continuous function on the product space Ul x U2.
Then we can give the following general definition of a Stackelberg equilibrium
solution, which is the counterpart of Def. 3.27 for infinite games.
Definition 4.6 In a two-person game, with PI as the leader, a strategy ul G
Ul is called a Stackelberg equilibrium strategy for the leader if
```
for all u1 € U1. Here, R2(u1} is the rational reaction set of the follower as
```
introduced in Def. 4-3.
```
Remark 4.2 If R2^1) is a singleton for each u1 € [71, in other words, if it is
```
```
described completely by a reaction curve 1% : U1 —> t/2, then inequality (4.11)
```
in the above definition can be replaced by
for all u1 e U1.
STATIC NONCOOPERATIVE INFINITE GAMES 179
If a Stackelberg equilibrium strategy exists for the leader, then the LHS of
```
inequality (4.11) is known as the leader's Stackelberg cost, and is denoted by
```
J1 . A more general definition for J1 is, in fact,
which also covers the case when a Stackelberg equilibrium strategy does not
exist. It follows from this definition that the Stackelberg cost of the leader is a
well-defined quantity, and that there will always exist a sequence of strategies
for the leader which will ensure him a cost value arbitrarily close to J1 . This
observation brings us to the following definition of t Stackelberg strategies.
Definition 4.7 Let e > 0 be a given number. Then, a strategy u\* € U1 is
```
called an e Stackelberg strategy for the leader (PI) if
```
The following two properties of 6 Stackelberg strategies now readily follow.
Property 4.2 In a two-person game, let J1' be a finite number. Then, given
an arbitrary e > 0, an e Stackelberg strategy for the leader necessarily exists.
```
Property 4.3 Let {u**} be a given sequence of e Stackelberg strategies in U1,
```
and with ^ > 6j for i < j and limj-.oo €j = 0. Then, if there exists a con-
```
vergent subsequence {ulik} in U1 with its limit denoted as ul , and further ifsu
```
```
Pu2€R2(ul) Jl(ul,u2} is a continuous function of u1 in an open neighborhood
```
of ul 6 U1, u1 is a Stackelberg strategy for PI.
The equilibrium strategy for the follower, in a Stackelberg game, would be
```
any strategy that constitutes an optimal response to the one adopted (and
```
```
announced) by the leader. Mathematically speaking, if u1 (respectively, ul )
```
```
is adopted by the leader, then any u2 e R2^1} (respectively, u2 e R2(u\'}}
```
will be referred to as an optimal strategy for the follower that is in equilibrium
```
with the Stackelberg (respectively, e Stackelberg) strategy of the leader. This
```
```
pair is referred to as a Stackelberg (respectively, e Stackelberg) solution of the
```
```
two-person game with PI as the leader (see Def. 3.28). The following theorem
```
now provides a set of sufficient conditions for two-person nonzero-sum games to
```
admit; a Stackelberg equilibrium solution.
```
Theorem 4.8 Let U1 and U2 be compact metric spaces, and J1 be continuous
on Ui x U2, i = 1,2. Further let there exist a finite family of continuous
```
mappings l^ : U1 —*• U2, indexed by a parameter i 6 / = {1,..., M}, so that
```
```
R2(u1} = {u2 e U2 : u2 = l^(ul},i G I}. Then, the two-person nonzero-sum
```
static game admits a Stackelberg equilibrium solution.
180 T. BA§AR AND G. J. OLSDER
Proof. It follows from the hypothesis of the theorem that J1 , as defined by
```
(4.13), is finite. Hence, by Property 4.2, a sequence of Stackelberg strategies
```
exists for the leader, and it admits a convergent subsequence whose limit lies
```
in C/1, due to compactness of Ul. Now, since -R2(-) can be constructed from a
```
```
finite family of continuous mappings (by hypothesis),
```
and the latter function is continuous on U1. Then, the result follows from
Property 4.3.
```
Remark 4.3 The assumption of Thm. 4.8, concerning the structure of R2(-),
```
```
imposes some severe restrictions on J2; but such an assumption is inevitable as
```
the following example demonstrates. Take U1 = U2 = [0,1], J1 = —u1^2 and
```
J2 — (ul — i)w2. Here, R2(-} is determined by a mapping /(•) which is continuous
```
```
on the half-open intervals [0, |),(|,1], but is multivalued at ul — \. The
```
Stackelberg cost of the leader is clearly J1 = — |, but a Stackelberg strategy
does not exist because of the "infinitely multivalued" nature of /. D
```
If /^(u1) is a singleton for every ul 6 U1, the hypothesis of Thm. 4.8 can
```
definitely be made less restrictive. One such set of conditions is provided in the
following corollary to Thm. 4.8 under which there exists a unique / which is
```
continuous (by an argument similar to the one used in the proof of Thm. 4.3).
```
Corollary 4.4 Every two-person nonzero-sura continuous-kernel game on the
```
square, for which J2(u1, •) is strictly convex for all ul € U1 and PI acts as the
```
leader, admits a Stackelberg equilibrium solution.
It should be noted that the Stackelberg equilibrium solution for a two-person
game exists under a set of sufficiency conditions which are much weaker than
```
those required for existence of Nash equilibria (cf. Thm. 4.3). It should fur-
```
ther be noted, however, that the statement of Thm. 4.8 does not also rule out
the existence of a mixed-strategy Stackelberg solution which might provide the
leader with a lower average cost. We have already observed occurrence of such
a phenomenon within the context of matrix games, in Section 3.6, and we now
investigate to what extent such a result could remain valid in continuous-kernel
games.
If mixed strategies are also allowed, then permissible strategies for Pi will
be probability measures // on the space U1. Let us denote the collection of all
such probability measures for Pi by Ml. Then, the quantity replacing J1 will
be the average cost function
and the reaction set R2 will be replaced by
STATIC NONCOOPERATIVE INFINITE GAMES 181
Hence, we have the following.
Definition 4.8 In a two-person game with PI as the leader, a mixed strategy
/41 € M1 is called a mixed Stackelberg equilibrium strategy for the leader if
for all p,1 £ M1, where J1 is known as the average Stackelberg cost of the
leader in mixed strategies.
Proposition 4.2
```
Proof. Since Ml also includes all one-point measures, we have (by an abuse
```
```
of notation) Ul C M1. Then, for each u1 6 Ul, considered as an element of M1,
```
where the last equality follows since any infimizing sequence in M2 can be
replaced by a subsequence of one-point measures. This implies that, for one
```
point measures in M1, R2 (p,1) coincides with the set of all probability measures
```
```
denned on R2(u1). Now, since M1 D C/1,
```
```
and because of the cited relation between R2(p,1) and J?2^1), the last expression
```
can be written as
We now show, by a counter-example, that, even under the hypothesis of
```
Thm. 4.8, it is possible to have strict inequality in (4.16).
```
Example 4.4 Consider a two-person continuous-kernel game with U1 = U2 =
[0,1], and with cost functions
where e > 0 is a sufficiently small parameter. The unique Stackelberg solution
```
of this game, in pure strategies, is u1* = 0, u2* = (u1)2, and the Stackelberg
```
cost for the leader is J1* = 0. We now show that the leader can actually do
better by employing a mixed strategy.
182 T. BA§AR AND G. J. OLSDER
First note that the follower's unique reaction to a mixed strategy of the
```
leader is u2 = E[(u1)2] which, when substituted into J1, yields the expression
```
Now, if the leader uses the uniform probability distribution on [0,1], his average
cost becomes
which clearly indicates that, for e sufficiently small,
The preceding example has displayed the fact that even Stackelberg games
with strictly convex cost functionals may fail to admit only pure-strategy solu-
tions, and the mixed Stackelberg solution may in fact be preferable.28 However,
if we further restrict the cost structure to be quadratic, it can then be shown
that only pure-strategy Stackelberg equilibria exist.
Proposition 4.3 Consider the two-person nonzero-sum game with Ul — Rmi,
```
U2 = Rm\ and
```
where Rlu > Q, R^, R\j, R1^ are matrices of appropriate dimensions, and r\, r*
are vectors of appropriate dimensions. This "quadratic" game can only admit a
pure-strategy Stackelberg solution, with either PI or P2 as the leader.
Proof. Without any loss of generality take PI as the leader, and assume,
to the contrary, that the game admits a mixed-strategy Stackelberg solution,
and denote the leader's optimal mixed strategy by /u1 . Furthermore, denote
the expectation operation under p,1 by E[-]. If the leader announces this mixed
strategy, then the follower's reaction is unique and is given by
```
By substituting this in J1 = E[J1}, we obtain
```
28In retrospect, this should not be surprising since for the special case of zero-sum games
```
(without pure-strategy saddle points) we have already seen that the minimizer could further
```
```
decrease his guaranteed expected cost by playing a mixed strategy; here, however, it holds
```
even if J1 ^ — J2.
STATIC NONCOOPERATIVE INFINITE GAMES 183
184 T. BAS.AR AND G. J. OLSDER
where
```
Now, applying the Cauchy-Schwarz inequality (see Appendix B.4) on the first
```
term of J1 , we further obtain the bound
which depends only on the mean value of u1. Hence,
This implies that enlargement of the strategy space of the leader, so as to
include mixed strategies as well, does not yield him any better performance.
```
In fact, since E[ul'u1} > E[ul]'E[u1}, whenever the probability distribution is
```
```
not one-point, it follows that the inequality in (i) is actually strict for the case
```
of a proper mixed strategy. This then implies that, outside the class of pure
strategies, there can be no Stackelberg solution.
Graphical display of Stackelberg solutions and the notion of rela-
tive leadership
```
If U1 and U2 are one-dimensional spaces, and if, in addition, R2(ul) is singleton
```
for each u1 € U1, then the Stackelberg equilibrium solution can easily be visu-
alized on a graph. In Fig. 4.7, iso-cost curves for J1 and J2 have been drawn,
together with the reaction curves /i and /2- With PI as the leader, the Stack-
```
elberg solution will be situated on /2) at the point where it is tangent to the
```
Figure 4.7: Graphical construction of the Stackelberg solution in a two-person
game—a "stalemate" solution.
STATIC NONCOOPERATIVE INFINITE GAMES 185
appropriate iso-cost curve of J1. This point is designated as Si in Fig. 4.7. The
coordinates of Si now correspond to the Stackelberg solution of this game with
```
PI as the leader. The point of intersection of /i and 12 (which is denoted by
```
```
N in the figure) denotes the unique Nash equilibrium solution of this game. It
```
should be noted that the Nash costs of both players are higher than their corre-
sponding equilibrium costs in the Stackelberg game with PI as the leader. This
is, though, only a specific example, since it would be possible to come up with
situations in which the follower is worse off in the Stackelberg game than under
```
the associated "Nash game". However, when the reaction set R2^1} is a sin-
```
gleton for every u1 e U1, the leader cannot do worse in the "Stackelberg game"
than his best performance in the associated "Nash game" since he can, at the
```
worst, play the strategy corresponding to the most favorable (from the leader's
```
```
point of view) Nash equilibrium solution pair, as discussed earlier in Section 3.6
```
```
(see, in particular, Prop. 3.14). To summarize, we have the following.
```
Proposition 4.4 For a two-person nonzero-sum game, let V^ denote the infi-
```
mum of all the Nash equilibrium costs of PI. Then, if R2(u1} is a singleton for
```
every u1 e C71, V^ > J1*.
Hence, in two-person nonzero-sum games with unique follower responses, the
leader never prefers to play the "Nash game" instead of the "Stackelberg game",
whereas the follower could prefer to play the "Nash game", if such an option is
open to him. In some games, even the option of who should be the leader and
who should follow might be open to the players, and in such situations there
is the question of whether it is profitable for either player to act as the leader
rather than be the follower. For the two-person game depicted in Fig. 4.7, for
```
example, both players would prefer to act as the follower (in the figure, point
```
```
52 characterizes the Stackelberg solution with P2 as the leader). There are
```
other cases, however, in which both players prefer the leadership of only one of
```
the players (see Fig. 4.8a, where leadership of P2 is more advantageous to both
```
```
players) or in which each player wants to be the leader himself (Fig. 4.8b).
```
```
Figure 4.8: Different types of Stackelberg equilibrium solutions, (a) Concurrent
```
```
solution, (b) Nonconcurrent solution.
```
186 T. BA§AR AND G. J. OLSDER
When the players mutually benefit from the leadership of one of them, then
this constitutes a stable situation since there is no reason for either player
to deviate from the corresponding Stackelberg solution which was computed
under mutual agreement—such a Stackelberg equilibrium solution is called a
"concurrent" solution.
If each player prefers to be the leader himself, then the Stackelberg solution
is called "nonconcurrent". In such a situation each player will try to dictate
to the other player his own Stackelberg strategy and thus force him to play the
"Stackelberg game" under his leadership. In this case, the one who can pro-
cess his data faster will certainly be the leader and announce his strategy first.
However, if the slower player does not actually know that the other player can
process his data faster than he does, and/or if there is a delay in the information
exchange between the players, then he might tend to announce a Stackelberg
strategy under his own leadership quite unaware of the announcement of the
other player—which certainly results in a nonequilibrium situation.
```
Finally, for the case in which neither player wants to be the leader (as in
```
```
Fig. 4.7), the "Stackelberg game" is said to be in a "stalemate", since each
```
player will wait for the other one to announce his strategy first.
4.5 Consistent Conjectural Variations Equilib-
rium
A third noncooperative solution concept for nonzero-sum games is the consistent
```
conjectural variations (CCV) equilibrium, which we introduce here for the two-
```
player case. This solution concept is in a sense a "double-sided Stackelberg
equilibrium", where an equilibrium is sought in the class of reaction functions
```
(or, more generally, reaction sets). For a precise mathematical formulation, let
```
```
TI x TV be the class of all mappings (Ti,T2), Ti : Uj -> U\ j ^ i, i,j = 1,2,
```
with the property that the composite maps T\ o T2 and T2 o Ti have unique fixed
```
points. For a given pair (Tf ,T|) € 7i x T2, let (ul c ,u2 C ) 6 U1 x U2 denote the
```
unique fixed points, satisfying
Introduce the unique pair
Let
defined by
```
Note that AT?(uJ') is the "differential" reaction of Pi under T? to a deviation
```
AT?from u30 to u3. Furthermore, g
i 3 can be interpreted as a reaction function of
STATIC NONCOOPERATIVE INFINITE GAMES 187
Pi when Pj uses a policy that is additively composed of a predetermined policy
```
(w-7) and a policy determined by the policy choice of Pi under the reaction
```
```
function A7J. Clearly, u1 introduced by (4.18) can also be obtained from
```
This now brings us to the following definition.
```
Definition 4.9 A pair of reaction functions (T^T-f) £ T\ x ?2, along with
```
```
their unique fixed point (uic,u2°) G U1 x U2, constitute a consistent conjectural
```
```
variations equilibrium (CCVE) if we have the consistency of reaction functions:
```
and consistency of policies
Remark 4.4 In the most general case, it may be impossible to validate the
consistency of the reaction functions, though this may be possible to any "order"
```
under appropriate smoothness conditions on J1 and J2 (this will be further
```
```
discussed in the sequel). Consistency of policies, on the other hand, is more
```
readily testable.
To gain more insight into CCVE, let us take U1 = U2 — R, and J1 and J2 to
```
be continuously differentiate and (jointly) strictly convex in their arguments.
```
```
Furthermore, let 7i x T2 be chosen such that J1 (-,T2 (-)) and J2 (Ti(-),-) are
```
```
strictly convex in their arguments, for every pair (Ti,Tz) e 71 x 72. To simplify
```
```
the notation, let ul = w, u2 = v. Then, two reaction functions (Tf, T^) G T\ x T^
```
```
are in CCV equilibrium if (and only if)
```
Note that these are two coupled partial differential equations which are, in
general, difficult to solve. To gain some further insight, we expand these around
```
the CCV solution (u°,vc) and perform a local analysis. First, to third order in
```
```
v:
```
and likewise, to third order in u:
188 T. BA§AR AND G. J. OLSDER
```
second order (arguments at u = uc, v = vc)
```
zeroth order
```
first order (arguments at u = uc, v = v°)
```
```
second order (arguments at u = uc, v — vc)
```
```
Now, rewriting (i):
```
```
and using the above expansion for T£(v) around v = i>c, we arrive at the follow-
```
```
ing:
```
zeroth order
```
first order (arguments at u = uc, v — vc]
```
where we assume that derivatives of all required orders exist. Likewise, rewrit-
```
ing (ii)
```
```
and using the above expansion for T^u) around u = uc, we have the following:
```
STATIC NONCOOPERATIVE INFINITE GAMES 189
In view of these relationships, we now refine the definition of CCVE for thrice
continuously differentiable cost functions J1 and J2.
```
Definition 4.10 A pair of conjectured response functions (Tf,T|) is in CCV
```
```
equilibrium to zeroth order if (0) and (0') are satisfied. It is CCV equilibrium
```
```
to first order if (O)-(l) and (O')-(l') are satisfied, and is in CCV equilibrium to
```
```
second order z/(0)-(2) and (0')-(2') are satisfied.
```
It should now be clear from the above that this refinement of CCVE can be
extended to any order, provided that J1 and J2 are continuously differentiable
```
up to that order. An nth-order expansion (of CCVE) involves 2n 4- 2 separate,
```
recursively solvable equations, which yield derivatives of Tf and T| up to order
```
n, evaluated at (u°,vc}. If a pair of response functions (T^Tj) satisfy these
```
2n + 2 equations, then we say that it is "in CCVE to nth-order", which provides
```
an nth-order approximation to the true (if it exists) CCVE. In some cases one
```
need not go to higher orders, as they may vanish beyond a certain "n", in
which case we clearly have the true CCVE. Games with quadratic cost functions
constitute one such class of problems, as we shall see in the next section. Another
point worth mentioning here is that the definition of an nth-order CCVE can
```
be given also if Ul and U2 are higher (than one) dimensional Euclidean spaces,
```
```
and even if they are (infinite-dimensional) Banach spaces. In the latter case one
```
```
deals with Frechet derivatives (Luenberger, 1969) instead of regular (Euclidean)
```
differentials.
An observation we can make from the analysis given above in the context
```
of scalar two-player games is that the Nash equilibrium (u*,*u*), where each
```
```
player takes the policy of the other player as given (and fixed), is a zeroth-order
```
CCVE, with
It is also a first-order CCVE under the set of restrictive conditions
```
For (u*,t>*) to be a second-order CCVE, also the set of conditions
```
has to be satisfied. Since the latter two conditions are overly restrictive, we
can say that generically the Nash equilibrium solution is a zeroth-order CCVE.
Clearly this holds not only for the scalar game, and hence we have the following.
Proposition 4.5 If a two-player nonzero-sum game admits a unique Nash equi-
```
librium, say (u1 , u2 ), then this pair is a zeroth-order CCVE, where ul and
```
u2 are the constant reaction functions. The Nash equilibrium is not necessarily
a higher-order CCVE.
190 T. BA§AR AND G. J. OLSDER
Remark 4.5 The notion of a CCVE can be extended from two- to many-
player games, at least at the conceptual level. Computationally, however, the
difficulties would be compounded here, as a player's response now depends on
```
the policies or actions of more than one player (cf. Def. 4.3).
```
4.6 Quadratic Games with Applications in Mi-
croeconomics
In this section, we obtain explicit expressions for the Nash, Stackelberg and
consistent conjectural variations equilibrium solutions of static nonzero-sum
games in which the cost functions of the players are quadratic in the decision
```
variables—the so-called quadratic games. The action (strategy) spaces will be
```
taken as Euclidean spaces of appropriate dimensions, but the results are also
```
equally valid (under the right interpretation) if the strategy spaces are taken as
```
infinite-dimensional Hilbert spaces. In that case, the Euclidean inner products
will have to be replaced by the inner product of the underlying Hilbert space,
and the positive-definiteness requirements on some of the matrices will have to
be replaced by strong positive definiteness of the corresponding self-adjoint op-
erators. This section will also include some discussion on iterative algorithms for
the computation of Nash equilibria, and an illustration of the quadratic model
and its Nash and Stackelberg solutions within the context of noncooperative
economic equilibrium behavior of firms in oligopolistic markets.
A general quadratic cost function for Pi, which is strictly convex in his action
variable, can be written
where MJ € U^ = Rmj is the m,j-dimensional decision variable of Pj, Rjk is
```
an (raj x mfc)-dimensional matrix with Rlu > 0, rj is an raj-dimensional vector
```
```
and c1 is a constant. Without loss of generality, we may assume that, for j ^ A;,
```
Rjk — Rkj , since if this were not the case, the corresponding two quadratic
terms could be written as
```
and redefining Rl-k as (Rljk + Rjk )/2, a symmetric matrix could be obtained.
```
By an analogous argument, we may take RJJ to be symmetric, without any loss
of generality.
Quadratic cost functions are of particular interest in game theory, firstly
because they constitute second-order approximation to other types of nonlinear
cost functions, and secondly because they are analytically tractable, admitting,
in general, closed-form equilibrium solutions which provide insight into the prop-
erties and features of the equilibrium solution concept under consideration.
STATIC NONCOOPERATIVE INFINITE GAMES 191
To determine the Nash equilibrium solution in strictly convex quadratic
```
games, we differentiate J* with respect to ul (i e N), set the resulting expres-
```
sions equal to zero, and solve the set of equations thus obtained. This set of
equations, which also provides a sufficient condition because of strict convexity,
is
Proposition 4.6 The quadratic N-player nonzero-sum static game defined by
```
the cost functions (4-23) and with Rlu > 0, admits a Nash equilibrium solution
```
```
if, and only if, (4-26a) admits a solution, say u*; this Nash solution is unique
```
```
if the matrix R defined by (4-26b) is invertible, in which case it is given by
```
Remark 4.6 Since each player's cost function is strictly convex and continuous
in his action variable, quadratic nonzero-sum games of the type discussed above
cannot admit a Nash equilibrium solution in mixed strategies. Hence, in strictly
convex quadratic games, the equilibrium analysis can be confined to the class
of pure strategies.
We now investigate the stability properties of the unique Nash solution of
quadratic games, where the notion of stability was introduced in Section 4.3.
Assuming N — 2, and directly specializing Prop. 4.1 to the quadratic case, we
arrive at the following iteration:
which can be written in compact form as
where
This then leads to the following proposition.
192 T. BA§AR AND G. J. OLSDER
with an arbitrary starting choice u , where
```
This iteration corresponds to the sequential (Gauss-Seidel) update scheme where
```
PI responds to the most recent past action of P2, whereas P2 responds to the
```
current action of PI. The alternative to this is the parallel (Jacobi) update
```
```
scheme where (4.28) is replaced by
```
```
starting with arbitrary initial choices (u , u ). Then, the question of stabil-
```
```
ity of the Nash solution (4.27), with N = 2, reduces to the question of stability
```
```
of the fixed point of either (4.28) or (4.29). Note that, apart from a relabeling
```
of indices, stability of these two iterations is equivalent to the stability of the
single iteration:
Since this is a linear difference equation, a necessary and sufficient condition for
```
it to converge (to the actual Nash strategy of PI) is that the eigenvalues of the
```
matrix C\C2, or equivalently those of C<iC\, should be in the unit circle, i.e.,
```
where p(A) is the spectral radius of the matrix A. This spectral radius condi-
```
tion is precisely the one given in Prop. 4.1, which shows that the condition of
Prop. 4.1 is tight for the quadratic case.
Note that the condition of stability is considerably more stringent than the
condition of existence of a unique Nash equilibrium, which is
The question we address now is whether, in the framework of Gauss-Seidel or
```
Jacobi iterations, this gap between (4.30) and (4.31) could be shrunk or even
```
totally eliminated, by allowing players to incorporate memory into the iterations.
While doing this, it would be desirable for the players to need to know as little
```
as possible regarding the reaction functions of each other (note that no such
```
```
information is necessary in the Gauss-Seidel or Jacobi iterations given above).
```
```
To study this issue, consider the Gauss-Seidel iteration (4.28), but with a
```
```
one-step memory for (only) PI. Then, the "relaxed" algorithm will be (using
```
```
the simpler notation u1 — Uk, u — Vk):
```
```
where A is a gain matrix, yet to be chosen. Substituting the second (for Vk)
```
into the first, we obtain the single iteration
```
2 (0)
```
```
1 (0) 2 (0)
```
```
(k) (k)2
```
STATIC NONCOOPERATIVE INFINITE GAMES 193
where
By choosing
```
where the required inverse exists because of (4.31), we obtain an immediate
```
convergence, assuming that the true value of C? is known to PI. If the true
value of 6*2 is not known, but a nominal value is given in a neighborhood of
```
which the true value lies, the scheme (4.32) along with the choice (4.33) and
```
```
using the nominal value, still leads to convergence (but not in a finite number
```
```
of steps) provided that the neighborhood is sufficiently small (see Ba§ar, 1987).
```
```
Now, if the original scheme is instead the parallel (Jacobi) scheme, then a
```
one-step memory for PI will not be sufficient to obtain a finite-step convergence
```
result as above. In this case we replace (4.32) by
```
where B is another gain matrix. Note that here Pi uses, in the computation
of u/c+i, not Uk but rather u^-i- Now, substituting for v^ from the second into
```
the first equation of (4.34), we arrive at the iteration
```
which again shows immediate convergence, with B chosen as
Again, there is a certain neighborhood of nominal Cz or equivalently of the
```
nominal C, where the iteration (4.34) is convergent.
```
In general, however, the precise scheme according to which P2 responds
to Pi's policy choices may not be common information, and hence one would
like to develop relaxation-type algorithms for PI which would converge to the
```
true equilibrium solution regardless of what particular scheme P2 adopts (for
```
```
example, Gauss-Seidel or Jacobi). Consider, for example, the scheme where
```
```
P2's responses for different k are modeled by (see also (4.7))
```
where i^ > 0 is an integer denoting the delay in the receipt of current policy
information by P2 from PI. The choice i^ — 0 for all k, would correspond to the
Gauss-Seidel iteration, and the choice i^ — 1 for all fc, to the Jacobi iteration
```
— assuming that u^+i is still determined according to (4.28). An extreme case
```
```
would be the totally asynchronous communication where {ik}k>o could be any
```
sequence of positive integers. Under the assumptions that PI communicates
194 T. BA§AR AND G. J. OLSDER
```
new policy choices to P2 infinitely often, and he uses the simple ("nonrelaxed")
```
iteration
```
it is known from the work of Chazan and Miranker (1969) that such a scheme
```
converges if, and only if,
where \C\ is the matrix derived from C by multiplying all its negative entries
by-1.
This condition can be improved upon, however, by incorporating relaxation
```
terms in (4.37), as well as in (4.36), such as
```
```
where a and (3 are some scalars. A sufficient condition for convergence of any
```
```
asynchronously implemented version of (4.39) is (see Ba§ar(1987) for details)
```
where
```
Clearly, there are values of a ^ 0, /? ^ 0, for which (4.40) requires a less
```
```
stringent condition (on C\ and C%) than (4.38). For example, if C\ and C-2 are
```
```
positive scalars, and a = (3 = |, inequality (4.40) dictates
```
```
while (4.38) requires that CiC2 < 1.
```
From a game-theoretic point of view, each of the iteration schemes discussed
above corresponds to a game with a sufficiently large number of stages and with
a particular mode of play among the players. Moreover, the objective of each
player is to minimize a kind of an average long horizon cost, with costs at each
stage contributing to this average cost. Viewing this problem overall as a multi-
act nonzero-sum game, we observe that the behavior of each player at each
stage of the game is rather "myopic", since at each stage the players minimize
their cost functions only under past information, and quite in ignorance of the
possibility of any future moves. If the possibility of future moves is also taken
into account, then the rational behavior of each player at a particular stage could
be quite different. For an illustration of this possibility the reader is referred to
Example 4.5 which is given later in this section. Such myopic decision making
could make sense, however, if the players have absolutely no idea as to how many
stages the game comprises, in which case there is the possibility that at any stage
a particular player could be the last one to act in the game. In such a situation,
risk-aversing players would definitely adopt "myopic" behavior, minimizing their
current cost functions under only the past information, whenever given the
opportunity to act.
STATIC NONCOOPERATIVE INFINITE GAMES 195
Two-person zero-sum games
Since zero-sum games are special types of two-person nonzero-sum games with
```
Ji = — J2 (PI minimizing and P2 maximizing), in which case the Nash equilib-
```
rium solution concept coincides with the concept of saddle-point equilibrium, a
special version of Prop. 4.6 will be valid for quadratic zero-sum games. To this
```
end, we first note that the relation J\ — — J% imposes in (4.23) the restrictions
```
```
under which matrix R defined by (4.26b) can be written as
```
which has to be nonsingular for existence of a saddle point. Since R can also be
written as the sum of two matrices
the first one being positive definite and the second one skew-symmetric, and
since eigenvalues of the latter are always imaginary, it readily follows that R is
a nonsingular matrix. Hence we arrive at the conclusion that every quadratic
strictly convex-concave zero-sum game admits a unique saddle-point equilibrium
in pure strategies—a result that also follows as a special case of Thm. 4.5.
Corollary 4.5 The strictly convex-concave quadratic zero-sum game with cost
function
admits a unique saddle-point equilibrium in pure strategies, which is given by
Remark 4.7 The positive-definiteness requirements on R\1 and R^2 m Corol-
lary 4.5 are necessary and sufficient for the game kernel to be strictly convex-
strictly concave, but this structure is clearly not necessary for the game to admit
```
a saddle point. If the game is simply convex-concave (that is, if the matrices
```
```
above are nonnegative definite, with a possibility of zero eigenvalues), then in
```
view of Thm. 4.6 a saddle point will still exist provided that the upper and lower
values are bounded.29 If the quadratic game is not convex-concave, however,
```
then either the upper or the lower value (or both) will be unbounded, implying
```
that a saddle point will not exist.
29For a convex-concave quadratic game, the upper value will not be bounded if, and only
if, there exists a v € Rm2 such that v'R^2v — 0 while v'r\ 7^ 0. A similar result applies to
the lower value.
30This result may fail to hold true for team problems with strictly convex but nondifferen-
```
tiable kernels (see Problem 10, Section 4.8).
```
196 T. BA§AR AND G. J. OLSDER
Team problems
Yet another special class of nonzero-sum games is the team problems in which
```
the players (or equivalently, members of the team) share a common objective.
```
Within the general framework, this corresponds to the case J1 = J2 = • • • =
```
JN = J, and the objective is to minimize this cost function over all ul €
```
```
Ul, i = I,... ,N. The resulting solution ./V-tuple (u1 ,u2 , . . . ,UN ) is known
```
as the team-optimal solution. The Nash solution, however, corresponds to a
```
weaker solution concept in team problems, the so-called person-by-person (pbp)
```
optimality. In a two-member team problem, for example, a pbp optimal solution
```
(u1 , u2 ) dictates satisfaction of the pair of inequalities
```
```
whereas a team-optimal solution (u1 , u2 ) requires satisfaction of a single in-
```
equality
A team-optimal solution always implies pbp optimality, but not vice versa. Of
course, if J is quadratic and strictly convex on the product space U1 x • • • x
UN, then a unique pbp optimal solution exists, and it is also team-optimal.30
However, for a cost function that is strictly convex only on individual spaces £/"*,
but not on the product space, this latter property may not be true. Consider,
for example, the quadratic cost function
which is strictly convex in u1 and u2, separately. The matrix corresponding to
```
R defined by (4.26b) is
```
which is clearly nonsingular. Hence a unique pbp optimal solution will exist.
```
However, a team-optimal solution does not exist since the said matrix (which
```
```
is also the Hessian of J) has one positive and one negative eigenvalue. By
```
cooperating along the direction of the eigenvector corresponding to the negative
eigenvalue, the members of the team can make the value of J as small as possible.
In particular, taking u2 = — fw1 and letting u1 —> +00, drives Jto -co.
The Stackelberg solution
```
We now elaborate on the Stackelberg solutions of quadratic games of type (4.23)
```
but with N = 2, and PI acting as the leader. We first note that since the
STATIC NONCOOPERATIVE INFINITE GAMES 197
quadratic cost function Jl is strictly convex in ul, by Prop. 4.3 we can confine
our investigation of an equilibrium solution to the class of pure strategies. Then,
to every announced strategy u1 of PI, the follower's unique response will be as
```
given by (4.25) with N = 2, i = 2:
```
Now, to determine the Stackelberg strategy of the leader, we have to minimize
J1 over t/1 and subject to the constraint imposed by the reaction of the follower.
Since the reaction curve gives u2 uniquely in terms of u1, this constraint can best
```
be handled by substitution of (4.42) in J1 and by minimization of the resulting
```
```
functional (to be denoted by J1) over U1. To this end, we first determine J1:
```
For the minimum of J1 over U1 to be unique, we have to impose a strict con-
vexity condition on J1. Because of the quadratic structure of J1, this condition
amounts to having the coefficient matrix of the quadratic term in w1 positive
definite, which is
Under this condition, the unique minimizing solution can be obtained by setting
the gradient of J1 equal to zero, which yields
```
Proposition 4.7 Under condition (4-43), the two-person version of the quadratic
```
```
game (4-23) admits a unique Stackelberg strategy for the leader, which is given
```
```
by (4-44)- The follower's unique response is then given by (4-4%)-
```
Remark 4.8 The reader can easily verify that a sufficient condition for condi-
```
tion (4.43) is strict convexity of J1 on the product space U1 x U2.
```
The consistent conjectural variations equilibrium
Again working with the two-player version of the quadratic game defined by
```
the cost functions (4.23), but with R^ — I and R22 = I for simplicity in
```
```
notation (and without any loss of generality), let us "conjecture" linear reaction
```
```
functions:
```
198 T. BA§AR AND G. J. OLSDER
where Ki is an m^ x rrij dimensional matrix, and ki e IRmi. Substituting this
```
into (4.21) we arrive at
```
These subsequently lead to the pair of equations
```
which are required to hold for all (ul,u2} e Rmi x Rm2. A sufficient condition
```
would be the existence of KI, KI, k\, ki that satisfy
Note that what we have here is a pair of quadratic equations for K\ and KI,
and after K\ and K% are determined the next two linear equations yield the
corresponding choices for k\ and ki- This shows that a CCVE equilibrium
in the form of linear reaction functions is plausible in the quadratic case, but
existence is by no means guaranteed. For some numerical results on the scalar
```
(R1) version of this problem we refer the reader to Ba§ar , Turnovsky and D'Orey
```
```
(1986), and Turnovsky, Ba§arand D'Orey (1988).
```
Applications in microeconomics
We now consider specific applications of the quadratic nonzero-sum game the-
ory in microeconomics, in establishing economic equilibria in certain types of
markets. The markets we have in mind are defined with respect to a specific
```
good or a collection of goods, and an individual participant (player) in these
```
markets is either a buyer or a seller. The traded goods are produced by the
```
seller(s) and are consumed by the buyer(s).
```
An important characteristic of such markets is the number of participants
involved. Markets in which there are a large number of both buyers and sellers,
and in which each participant is involved with only a negligible fraction of the
```
transactions, are called (bilaterally) competitive. A key characteristic of such
```
competitive markets is that no single participant can, by his own actions, have
a noticeable effect on the prices at which the transactions take place. There is
a market price, and no seller needs to sell for less, and no buyer needs to pay
more.
Our interest in the sequel will be on the so-called oligopoly markets, which
comprise a few sellers and many competitive buyers. In the special case of
STATIC NONCOOPERATIVE INFINITE GAMES 199
two sellers this market is known as a duopoly. In an oligopoly, the buyers
cannot influence the price or quantities offered, and it is assumed that the
collective behavior of the buyers is fixed and known. The sellers, however, have
```
an influence via the price they ask and the output (production) they realize.
```
Example 4.5 This example refers to what is known as a Cournot duopoly
situation. There are two firms with identical products, which operate in a
market in which the market demand function is known. The demand function
relates the unit price of the product to the total quantity offered by the firms.
It is then assumed that the whole production is sold at the prevailing price
dictated by the demand function. Let ql denote the production level of firm
```
i (Pi), i = 1,2, and p denote the price of the commodity. Then, assuming a
```
linear structure for the market demand curve, we have the relation
```
where a and (3 are positive constants known to both firms. Furthermore, if we
```
assign quadratic production costs to both firms, the profit functions are given
by
for PI and P2, respectively, where k\, k-2 are nonnegative constants. PI wants
to maximize P1 and P2 wants to maximize P2. Under the stipulation that the
firms act rationally and that there is no explicit collusion, the Nash equilibrium
concept seems to fit rather well within this framework. Formulated as a nonzero-
sum game, the Nash equilibrium solution of this decision problem is
```
which has been obtained by solving equation (4.25). The corresponding (Nash
```
```
equilibrium) level of price is
```
To obtain the Stackelberg solution with, for instance, PI as the leader, we
first compute the reaction curve of P2,
and then substitute this into P1 and maximize the resulting expression over ql.
The result is the following quantity level for the leader:
200 T. BA§AR AND G. J. OLSDER
The corresponding quantity level for the follower, and the price level are, re-
spectively,
```
The CCV solution can be obtained by making direct use of (4.46)-(4.49). For
```
simplicity in computation, let us consider the symmetric case where k\ = k^ — k,
```
and let ft = ft/k. Then, (4.46)-(4.47) admit two pairs of solutions:
```
```
with the corresponding values of ki and k^ from (4.48)-(4.49) being
```
```
These two determine completely the optimum reaction functions (4.45) (in
```
```
the sense of CCVE). Now letting
```
and solving for q1 and q2, we obtain the following unique quantity levels for this
symmetric duopoly under the CCVE:
Example 4.6 This is a somewhat more sophisticated, and extended version of
Example 4.5. There will be N firms or players in the oligopoly problem to be
defined, and it will be possible for the price of the product to vary from one
firm to another. Let us first introduce the following notation:
```
Pl = the profit for the ith firm (Pi),
```
```
pl = the price per unit product charged by Pi,
```
```
p = average price = (^ p*) /JV,
```
```
ft = price sensitivity of overall demand,
```
```
V = the price at which demand is zero,
```
```
c = fixed costs per unit product of Pi,
```
```
dl = demand for the product of the ith firm.
```
Suppose that demand and price of the product of Pi are related through the
relation
STATIC NONCOOPERATIVE INFINITE GAMES 201
where 7 > 0 is a constant, or equivalently through
where we have replaced d1 with ql since the quantity produced by each firm is
assumed to be equal to the demand for the product of that firm under the price
dictated by this relation. The profit of Pi can then be written as
```
where dl is to be interpreted as being equal to ql if (4.51) is used. Note that
```
in this example all firms enter symmetrically into the model. One could in-
corporate an asymmetry by including weighting factors in the determination of
the average price p or by making c firm-dependent. This modified model would
```
definitely be more realistic; however, the structural properties of equilibria are
```
not that sensitive to such changes, and therefore we have avoided such a more
complicated model for the sake of simplicity in exposition.
```
For this example, we again seek the Nash equilibrium solution; but, from
```
the model, it is not clear at the outset which set of variables should be taken as
```
the decision (action) variables: the prices or the quantities? We shall elaborate
```
here on both possibilities. In the so-called "price game" the Nash solution must
satisfy the set of equations
and in the so-called "quantity game" the first-order conditions for Nash equilib-
rium are
In the price game it is assumed that the demands are determined via the prices
```
according to (4.50), whereas in the quantity game it is assumed that the prices
```
```
satisfy (4.51). In both cases the profit function of each firm is quadratic in
```
the decision variables, and strictly concave in his own decision variable, and
therefore it suffices to consider only the first order conditions.
```
For the price game, the set of equations (4.52) becomes
```
and since the p*'s appear symmetrically in this set of equations, we may substi-
tute p — p1 = p2 = • • • = pN and solve for p, to obtain
202 T. BA§AR AND G. J. OLSDER
```
If this solution is substituted into (4.50), the following equilibrium values for
```
the demands are obtained:
```
Now, for the quantity game, the set of equations (4.53) can be rewritten as
```
```
In order to express pl as a function of g1,. • • > tf^ only, we must solve (4.51), which
```
as it stands is an implicit equation in the variables pl. Because of symmetry, the
solution should have the form p1 = ot\ +a^d1+#3 Y^j^i $, where the coefficients
Qfc, k = 1,2,3, do not depend on i. Some analysis then leads to
Because of symmetry reasons, in the equilibrium situation we have p = p1 =
• • • = pN and q = q1 = • • • = qN and hence
```
Now evaluating the quantity dp1 /dql from (4.57), we obtain
```
```
Substitution of (4.58) and (4.59) into (4.56) yields the following unique equilib-
```
rium value for q1:
```
If this value is further substituted into (4.58), we obtain
```
```
A comparison of (4.54) with (4.61) readily yields the conclusion that the Nash
```
equilibria of price and quantity games are not the same. For the trivial game
with only one firm, i.e., N = 1, however, we obtain in both cases the same
result, which is
STATIC NONCOOPERATIVE INFINITE GAMES 203
Figure 4.9: Graphical illustration of the effect of a coordinate transformation
on Nash equilibria.
Also, in the limit as N —> oo, the corresponding limiting equilibrium values of
```
p and q(d} in both games are again the same
```
For 1 < N < oo, however, the Nash equilibria differ, in spite of the fact that the
economic background in both models is the same. Therefore, it is important to
know a priori in this market as to which variable is going to be adopted as the
decision variable. The price game with its equilibrium is sometimes named after
```
Edgeworth (1925), whereas the quantity game is named after Cournot who first
```
```
suggested it in the nineteenth century (Cournot, 1838).
```
We now present a graphical explanation for occurrence of different Nash
equilibria when different variables are taken as decision variables in the same
model. Suppose that the iso-cost curves for a nonzero-sum game are as depicted
in Fig. 4.2, which have been redrawn in Fig. 4.9, where the decision variables
are denoted by p1 and p2, and the reaction curves corresponding to these de-
cision variables are shown as l\ and l^-, respectively. If a transformation to a
```
new set of decision variables dl(p1,p2)^ d 2 ( p l , p 2 ) is made, then it should be
```
apparent from the figure that we obtain different reaction curves mi and 7712
and hence a different Nash solution. As a specific example, in Fig. 4.9 we have
```
chosen dl = p1, d2 — p1 + p2, in which case mi — /i; but since m.2 ^ /2 Nash
```
equilibrium becomes a variant of the coordinate transformation. It should be
```
noted, however, that if the transformation is of the form d l ( p l ) , d2(p2), then
```
the Nash equilibrium remains invariant.
4.7 Braess Paradox
In this section we present a nonzero-sum game for which the Nash equilibrium
solution leads to a surprising phenomenon, called the "Braess paradox", after
```
Dietrich Braess who was the first to publish about it (Braess, 1968). Braess'
```
field of application concerned traffic behavior, but the Braess paradox has since
204 T. BA§AR AND G. J. OLSDER
```
been observed in other fields of application as well (Cohen and Horowitz, 1991).
```
We will more or less follow here Braess' original paper.
Figure 4.10: The routes of the Braess paradox.
Consider a network of roads as given in Fig. 4.10. All these roads allow only
one-way traffic, as indicated by the arrows. The drivers on this network all want
to go from point B to point E. They have to choose one of the three possible
```
routes:
```
```
route 1 consists of two segments: from B via C to E;
```
```
route 2 consists of two segments: from B via D to E;
```
route 3 consists of three segments: from B via C and D to E.
There are many cars and this leads to congestion on the roads. The time needed
to drive along a segment depends on the intensity i, i.e., the number of cars per
time unit that choose this segment. Measurements have shown that the time
duration t needed to traverse each segment is as follows:
```
along segment BC: t = 10 x i;
```
```
along segment BD: t = 50 -f i;
```
along segment CE: t = 50 4- i\
along segment CD: t = 10 + i]
along segment DE: t = 10 x i.
Suppose that Xj drivers choose route j, j = 1,2,3. The total time needed to go
from B to E is then
```
along route 1: 10 x (xi + xs) + 50 + xi;
```
```
along route 2: 50 + x2 + 10 x (x2 + £3);
```
```
along route 3: 10 x (xi -f xs) + 10 + x3 + 10 x (x2 + x3).
```
STATIC NONCOOPERATIVE INFINITE GAMES 205
```
Each driver will individually (and independently) choose that route for which
```
the total driving time is the smallest. At equilibrium, this will lead to all
three total driving times being equal. If we pick an arbitrary normalization,
```
say x\ + x-2 + £3 = 6 (this normalization leads automatically to integer-valued
```
```
solutions), then these driving times being equal leads to the unique Nash solution
```
```
xi = x-2 — £3 = 2, and the total driving time along each of the three routes
```
equals 92 time units.
Now reconsider the problem described above with one change: segment CD
```
is not available (due to roadworks) and the drivers must now choose between
```
route 1 and route 2 only. The unique Nash solution in this new game turns out
```
to be x\ — £2 = 3 (x^ = 0) and the total driving time along each of the two
```
routes equals 83 time units. Apparently, reducing the number of possibilities
```
has led to a uniformly better result (the driving time for each driver being now
```
```
83 rather than 92). Or, if we reason in the other direction, increasing capacity
```
```
leads to a worse equilibrium—a rather counter-intuitive result (Cohen, 1988).
```
If in the original problem, with three routes available, all drivers together
would agree upon minimizing the maximum of the three total driving times, i.e.,
minimizing the maximum of the three expressions above for the total driving
```
time, then the result turns out to be (the elementary calculation is left to the
```
```
reader) x\ = x% — 3, £3 = 0. Hence in this group optimum, equivalently
```
```
Pareto optimum (see Chapter 1), route 3 is not used at all even though it is
```
available. However, this solution is cheating-prone as each individual driver
would be tempted to use route 3, since if he were the only one doing so, his
total driving time would be reduced to approximately 70. This, of course, is not
an equilibrium situation.
4.8 Problems
1. Solve the following zero-sum semi-infinite matrix games:
2. The kernel of a zero-sum game on the unit square is given by J(w1,n2) =
```
(u1)3 — 3ulu2 + (w2)3. Determine the pure or mixed saddle-point strate-
```
```
gies. (Hint: Note that J is strictly convex in it1, and that for any u1 the
```
maximum value of J with respect to u2 is attained either at u2 = 0 or at
```
u2 = 1.)
```
3. Consider the zero-sum continuous-kernel game on the unit square, defined by
```
J(u1 ,u2 ) = (u1 — u2)2 —a(u2 )2 , where a is a scalar parameter. Determine
```
5. Consider the two-person nonzero-sum game with cost functions
206 T. BA§AR AND G. J. OLSDER
```
its pure or mixed saddle-point solution when (i) 1 < a < 2 and (ii) 0 <
```
a< 1.
4. Consider the following zero-sum game, known as a duel. Two men, starting
at t = 0, walk toward each other at such a speed that they meet each
other at t = 1 if nothing intervenes. Each one has a pistol with exactly
one bullet, and may fire it at any time t € [0,1]. If one of them hits the
other, the duel is immediately over, and the one who has fired successfully
is declared the winner. If neither one fires successfully or if both fire
simultaneously and successfully, then the duel becomes a stand-off. The
probability of a hit after firing is inversely proportional with the distance
between the two duelists, and assuming a uniform distribution we can let
```
this probability (of hitting the other player) to be t at time t € [0,1]. This
```
is a continuous-time version of the duel described in Section 2.8. Consider
now the following two versions of the duel:
```
(i) Silent duel. Assuming that the players do not know whether their
```
```
opponents have fired (unless, of course, a particular player is hit),
```
```
first show that an appropriate zero-sum game modeling this (silent)
```
duel is one with the kernel
where u1 and u2 are the instants at which the pistols are fired by
PI and P2, respectively. Then, noting that this is a skew-symmetric
```
game (a la Problem 9 of Section 2.9), show that its value in mixed
```
```
strategies (Fm) is zero, and the corresponding mixed saddle-point
```
policies are:
```
(ii) Noisy duel. Now consider the case where the pistols are noisy, i.e.,
```
a player knows whether his opponent has already fired or not. Show
that this version of the duel game can be described by the kernel
and that it admits a saddle point in pure strategies, given by
STATIC NONCOOPERATIVE INFINITE GAMES 207
where u and v are the action variables of PI and P2, respectively, belong-
ing to the coupled constraint set
and £,0:1,0:2 are positive constants, and T,ft\,fii are each positive and
smaller than 1.
```
(i) Using Thm. 4.4, show that the game admits a pure-strategy Nash
```
equilibrium.
```
(ii) Using Prop. 4.1, obtain a set of sufficient conditions on the six param-
```
eters characterizing the game, under which the pure-strategy Nash
equilibrium is unique. Show that this condition is satisfied for the
set of values:
```
(iii) For the same set of parameter values given above, obtain the unique
```
```
Nash equilibrium solution. Is it (globally) stable? If yes, can you
```
obtain the solution using an iterative procedure?
```
Partial answer for part (iii): The Nash equilibrium solution is
```
```
(u* = 0.3,v* =0.9).
```
6. Prove the statement of Remark 4.1, and find the range of values of the scalar
parameter e for which the Nash equilibrium solution of the following game
```
(with weakly coupled players) can be obtained using the Gauss-Seidel
```
readjustment scheme
where
7. Consider the two-person Stackelberg game defined by the cost functions
```
where o^, i = 1,2, are scalar parameters; the decision variables ul and u2
```
```
are also scalars. For what values of (01,02) is the game (i) concurrent,
```
```
(ii) nonconcurrent, or (iii) stalemate?
```
8. Consider the two-person Stackelberg game, with PI as the leader, defined
on the unit square, and with the cost functions
208 T. BA§AR AND G. J. OLSDER
```
where (3 and k are scalars. For what values of (3 and k does a mixed
```
Stackelberg solution for the leader yield a lower average cost for him than
the pure Stackelberg solution?
9. In a three-person game with three levels of hierarchy, first PI announces his
```
strategy and dictates it on both P2 and P3; subsequently P2 announces
```
his strategy and dictates it on P3. The cost functions are given by
where x = u1 4- u2 + u3 and ul € R, i — 1,2,3.
Determine the hierarchical equilibrium solution of this game. Compare
this with the Nash equilibrium solution of a nonzero-sum game having the
```
same cost structure. In which one (the three-level hierarchy or the Nash
```
```
game) does PI attain a better performance?
```
10. Consider a two-person game with J1 = J2 = J. Show that the cost
```
function J : R2 —> R defined by
```
```
is strictly convex on R2. What are the reaction sets JR2(w1) and Rl(u2)?
```
```
Prove that infinitely many person-by-person (pbp) optimal solutions exist,
```
but that the team solution is unique.
Now prove that if J is quadratic and strictly convex, the reaction curves
can have at most one point in common, and therefore the pbp optimal
solution and the team solution are unique and identical.
11. The purpose of this problem is to show that the notion of stability of Nash
equilibrium, as introduced in Def. 4.5, indeed depends on the particular
scheme used when the number of players is three or more. Consider the
scalar three-person game with the cost functions
where e is a scalar parameter. The following two numerical schemes for
```
the determination of Nash equilibrium correspond to (4.5) and (4.6), re-
```
```
spectively:
```
Scheme 1 :
STATIC NONCOOPERATIVE INFINITE GAMES 209
Scheme 2 :
where, by a slight abuse of notation, uk denotes the value of ul at the kth
step of iteration.
```
(i) Prove that, for e = 0.36, if ulk is calculated according to scheme 1
```
```
(which is the Cournot-Jacobi iteration), the sequences {uk}, i =
```
1,2,3, do not converge, whereas they do converge if calculated ac-
```
cording to scheme 2 (which is a particular Gauss-Seidel iteration).
```
```
(ii) Obtain the complete range of values of t for which (a) scheme 1 is
```
```
convergent, (6) scheme 2 is convergent.
```
```
(iii) Repeat (ii) for the following variation of scheme 2:
```
Scheme 3 :
12. Consider the following "relaxed" version of scheme 1 in Problem 11, where
```
a,/3,7 are the relaxation parameters (a la Section 4.6) yet to be chosen:
```
Relaxed Scheme 1 :
Let e = 0.36, and show that there exist nonzero values of a, /?, 7 such that
the sequences generated by the scheme above converge.
13. Relaxation algorithms of the type used in Problem 12 above can also be
used in the computation of saddle-point solutions in zero-sum games, not
only to assure convergence of a particular scheme, but also to improve the
speed of convergence to equilibrium. To explore this possibility, consider
the scalar zero-sum two-person game with reaction functions
```
(i) First consider the Gauss-Seidel update scheme, written as:
```
VQ arbitrarily chosen, k — 0,1,
210 T. BA§AR AND G. J. OLSDER
Verify that with the starting choice picked as VQ = 0, the sequence
```
generated by this algorithm converges (to the nearest six figures) to
```
```
the unique saddle-point solution (w* = 0.9547778, v* = 0.94110625)
```
in approximately 120 iterations.
```
(ii) Now consider the "relaxed" version:
```
```
ufe+i = auk + (1 - a)ll(vk), vk+i = /3vk + (1 - (3)l2(uk+i);
```
VQ arbitrarily chosen, k = 0,1,...
and show by numerical experimentation that by choosing the values
```
of a and (3 appropriately the speed of convergence can be improved
```
significantly. Consider, for example, the values of a = I,/? = 0.67,
for which the improvement is by a magnitude of 10.
14. Consider the "price" and "quantity" models of oligopoly as introduced in
Example 4.6, but with the roles of the players such that PI is the leader
and P2,..., PN are the followers. Further assume that the followers play
according to the Nash solution concept among themselves. Obtain the so-
lution of this "leader-followers" game, and study its asymptotic properties
as the number of followers goes to infinity.
15. Consider a two-person nonzero-sum game with the cost functions defined
```
by J1 = (u1)2 + (u2)2; J2 = (n1 - I)2 + (u2 - I)2.
```
```
(i) Determine the Nash solution, the consistent conjectural variations
```
solution and the Stackelberg solution with PI as the leader.
```
(ii) Now assume that PI has access to the action variable u2 of P2, and
```
can announce a strategy in the form
where a and fi are constants yet to be determined. Assuming that P2
is a rational player seeking to minimize his own cost function J2, how
should PI choose the values for a and /?? Show that the resulting
value of J1 is its global minimum, i.e., zero. Provide a graphical
interpretation for this result.
4.9 Notes
Section 4.2. The concept of e equilibrium solution is as old as the concept of
equilibrium solution itself. The results presented in this section are standard, and our
```
proof of Thm. 4.2 follows closely the one given in Burger (1966) where additional results
```
on equilibrium solutions can be found. Corollary 4.1, which follows from Thm. 4.2 in
```
our case, was originally proven by Wald (1945) in a different way. More recently, the
```
```
concept of € equilibrium solution has received attention in the works of Tijs (1975)
```
```
and Rupp (1977).
```
STATIC NONCOOPERATIVE INFINITE GAMES 211
```
Section 4.3. The notion of a reaction curve (also called reaction function) was first
```
introduced in an economics context and goes as far back as 1838, to the original work
of Cournot. There are several results in the literature, known as minimax theorems,
```
which establish existence of the (saddle-point) value in pure strategies under different
```
conditions on Ul, U2 and J. Thoerem 4.5 provides only one set of conditions which
are sufficient for existence of a value. Some other minimax theorems have been given
```
(in more general spaces) by Sion (1958), Fan (1953), Blackwell and Girshick (1954),
```
```
Balakrishnan (1976) and Bensoussan (1971). The result on the existence of Nash
```
```
equilibria in pure strategies (i.e., Thm. 4.3) is due to Nikaido and Isoda (1955), and
```
```
extensions to mixed strategies were first considered by Ville (1938). The extension
```
```
of Thm. 4.3 to coupled constraint sets (i.e., Thm. 4.4) is due to Rosen (1965), who
```
```
also introduces a dynamic (differential equation-based) model for the computation of
```
unique stable equilibrium. For extensions of these existence results to more general
```
topological spaces, with applications to economics, see Aubin (1980). The discretiza-
```
tion procedure alluded to in the proof of Thm. 4.7, in the text, suggests a possible
method for obtaining mixed Nash equilibria of continuous-kernel games in an approx-
imate way, if an explicit derivation is not possible. In this case, one has to solve for
mixed Nash equilibria of only finite matrix games, with the size of these matrices
determined by the degree of approximation desired.
Section 4.4. The Stackelberg solution concept was first introduced in economics
```
through the work of H. Von Stackelberg in the early 1900s (cf. Von Stackelberg, 1934).
```
But the initial formulation involved a definition in terms of reaction curves, with
```
the generalization to nonunique reaction curves (or the so-called reaction sets) being
```
```
relatively recent (cf. Ba§arand Selbuz, 1979a; Leitmann, 1978). A different version
```
of Thm. 4.8, which accounts only for unique reaction curves, was given by Simaan
```
and Cruz, Jr (1973b), but the more general version given here is due to Ba§arand
```
```
Olsder (1980a). The latter reference is also the first source for extension to mixed
```
strategies. A classification of different static Stackelberg solutions as given here is
```
from Ba§ar(1973), but the concept of "relative leadership" goes as far back as the
```
```
original work of Von Stackelberg (1934). See also Ho and Olsder (1981).
```
```
Section 4.5. The material in this section is based on Ba§ar (1986a). Other, more
```
```
restrictive definitions of CCVE can be found in Bresnahan (1981), and Kamien and
```
```
Schwartz (1983). See also Olsder (1987) for another perspective on CCVE.
```
Section 4.6. Quadratic games have attracted considerable attention in the litera-
```
ture partly because of analytic tractability (reaction functions are always affine). For
```
further discussion on asynchronous implementation of Gauss-Seidel and Jacobi algo-
rithms, and their generalized versions, for two as well as three-player games we refer the
```
reader to Ba§ar (1987). Some other selective references which deal with asynchronous
```
```
algorithms in other contexts are Bertsekas and Tsitsiklis (1989a,b, 1991) and Tsitsik-
```
```
lis (1987, 1989). Two good sources for economic applications of (quadratic) games
```
```
are Friedman (1977) and Case (1979). Example 4.5 is the so-called Cournot's quan-
```
```
tity model and can be found in the original work of Cournot (1838). Several later
```
```
publications have used this model in one way or another (see e.g., Intriligator (1971)
```
```
and Ba§arand Ho (1974)). Example 4.6 is a simplified version of a model presented
```
```
in Levitan and Shubik (1971), where the authors also provide a simulation study on
```
some data obtained from the automobile industry in the USA.
212 T. BA§AR AND G. J. OLSDER
Section 4.7. The Braess paradox is named after Braess who apparently was the
```
first to write about a paradoxal phenomenon in traffic control (Braess, 1968). This
```
phenomenon can also occur in other fields of application such as electric networks and
```
mechanical constructions (Cohen and Horowitz, 1991).
```
```
Section 4.8. Problem 4 can be found in Karlin (1959). Extensions of the game of
```
timing involve the cases of PI having m bullets and P2 having n bullets, where m and n
```
are positive integers. The general silent (ra,n) case was solved by Restrepo (1957) and
```
```
the general noisy duel with m and n bullets was solved by Fox and Kimeldorf (1969).
```
The case of PI having m noisy bullets and P2 having n silent bullets remains unsolved
```
for m > 1. Problem 5 is taken from Li and Ba§ar (1987), which the reader can
```
consult with motivation and more theoretical as well as numerical results. A theoretical
development for weakly coupled games, in which context Problems 11 and 12 arise, can
```
be found in Srikant and Ba§ar (1992). Problem 15 (ii) exhibits a dynamic information
```
```
structure and can therefore also be viewed as a dynamic game; for more details on such
```
Stackelberg games the reader is referred to Chapter 7 of this book, and in particular
to Section 7.4.
Part II
This page intentionally left blank
Chapter 5
General Formulation of
Infinite Dynamic Games
5.1 Introduction
This chapter provides a general formulation and some background material for
the class of infinite dynamic games to be studied in the remaining chapters of
the book. In these games, the action sets of the players comprise an infinite
```
number (in fact a continuum) of elements (alternatives), and the players gain
```
some dynamic information throughout the decision process. Moreover, such
```
games are defined either in discrete time, in which case there exists a finite (or
```
```
at most a countable) number of levels of play, or in continuous time, which
```
corresponds to a continuum of levels of play.
Quite analogous to finite dynamic games, infinite dynamic games can also
```
be formulated in extensive form which, however, does not lead to a (finite)
```
tree structure, firstly because the action sets of the players are not finite, and
secondly because of existence of a continuum of levels of play if the game is
defined in continuous time. Instead, the extensive form of an infinite dynamic
```
game involves a difference (in discrete time) or a differential (in continuous
```
```
time) equation which describes the evolution of the underlying decision process.
```
In other words, possible paths of action are not determined by following the
```
branches of a tree structure (as in the finite case) but by the solutions of these
```
functional equations which we also call "state equations". Furthermore, the
information sets, in such an extensive formulation, are defined as subsets of
```
some infinite sets on which some further structure is imposed (such as Borel
```
```
subsets). A precise formulation which takes into account all these extensions as
```
well as the possibility of chance moves is provided in Section 5.2 for discrete-
time problems and in Section 5.3 for game problems defined in continuous time.
In a discrete-time dynamic game, every player acts only at discrete instants of
time, whereas in the continuous-time formulation the players act throughout
a time interval which is either fixed a priori or determined by the rules of
215
216 T. BA§AR AND G. J. OLSDER
the game and the actions of the players. For finite games, the case in which
the duration of the game is finite, but not fixed a priori, has already been
included in the extensive tree formulation of Chapters 2 and 3. These chapters,
in fact, included examples of multi-act games wherein the number of times a
```
player acts is explicitly dependent on the actions of some other player(s)—
```
and these examples have indicated that such decision problems do not require
a separate treatment, and they can be handled by making use of the theory
developed for multi-act games with an a priori fixed number of levels. In infinite
dynamic games, however, an a priori fixed upper bound on the duration may
sometimes be lacking, in which case "termination" of the play becomes a more
delicate issue, as already shown in Section 2.7. Therefore, we devote a portion of
Section 5.3 to a discussion on this topic, which will be useful later in Chapter 8.
Even though we shall not be dealing, in this book, with mixed and behavioral
strategies in infinite dynamic games, we devote Section 5.4 to a brief discussion
on this topic, mainly to familiarize the reader with the difficulties encountered
in extending these notions from the finite to the infinite case, and to point out
the direction along which such an extension would be possible.
Section 5.5 deals with certain standard techniques of one-person single-goal
optimization and, more specifically, with optimal control problems. Our objec-
```
tive in including such a section is twofold; first, to introduce the reader to our
```
notation and terminology for the remaining portions of the book, and second,
to summarize some of the tools for one-person optimization problems which will
be heavily employed in subsequent chapters in the derivation of noncooperative
equilibria of infinite dynamic games. Section 5.6 deals with the notion of "rep-
resentations of strategies on trajectories", and the issue of "time consistency",
both of which are of prime importance in infinite dynamic games, as it will
become apparent in Chapters 6 and 7.
Finally, Section 5.7 deals with so-called viscosity solutions. An important
standard technique for continuous-time problems, as described in Section 5.5,
is the solution of a partial differential equation. Among all possible solutions of
```
this equation, the viscosity solution is a particular one; it is quite often unique
```
and has a clear interpretation in terms of a generalized optimal control problem
to which some stochastic perturbations have been added.
5.2 Discrete-Time Infinite Dynamic Games
In order to motivate the formulation of a discrete-time infinite dynamic game in
extensive form, we now first introduce an alternative to the tree model of finite
dynamic games—the so-called loop model. To this end, consider an AT-person
```
finite dynamic game with strategy sets {T*;i e N} and with decision (action)
```
```
vectors {ul € Ul;i € N}, where the kth block component (ulk) of ul designates
```
```
Pi's action at the kth level (stage) of the game and no chance moves are allowed.
```
```
Now, if such a finite dynamic game is defined in extensive form (cf. Def. 3.10),
```
```
then for any given TV-tuple of strategies {7* e F*;z e N} the actions of the
```
GENERAL FORMULATION 217
players are completely determined by the relations
where rf denotes the information set of Pi. Moreover, since each r\l is completely
determined by the actions of the players, there exists a point-to-set mapping31
```
gi : Ul x U2 x - • • x UN -> N1 (i € N) such that rf = gl(u\. . . , U N ] (i e N)
```
```
and when substituted into (5.1), yields the "loop" relation
```
This clearly indicates that, for a given strategy ./V-tuple, the actions of the
players are completely determined through the solution of a set of simultaneous
equations which admits a unique solution under the requirements of Kuhn's
```
extensive form (Kuhn, 1953). If the cost function of Pi is defined on the action
```
spaces, as Ll : Ul x U2 x • • • x UN —> R, then substitution of the solution of
```
(5.2) into Ll yields Ll(ul,..., u1*) as the cost incurred to Pi under the strategy
```
```
JV-tuple {7* € F*; i € N}. If the cost function is instead defined on the strategy
```
spaces, as Jl : F1 x F2 x • • • x F^ —-> R, then we clearly have the relation
```
Jt (71 ,... ,7^) = Ll(ul,... ,UN] (i 6 N). Now, if a finite dynamic game
```
```
is described through a set of equations of the form (5.2), with {pl]i £ N}
```
belonging to an appropriately specified class, together with an ./V-tuple of cost
```
functions {Ll;i e N}, then we call this a loop model for the dynamic game
```
```
(Witsenhausen, 1971 a, 1975b). The above discussion has already displayed the
```
steps involved in going from a tree model to a loop model in finite deterministic
dynamic games. Conversely, it is also possible to start with a loop model and
derive a tree model out of that, provided that the functions p1 : f/1 x t/2 x • • • x
```
UN —> Ul (i £ N) are restricted to a suitable class satisfying conditions like
```
```
causality, unique solvability of the loop equation (5.2), etc. In other words, if
```
```
the sets Pl (i 6 N) are chosen properly, then the set of equations
```
```
together with the cost structure {Ll;i € N} leads to a tree structure that
```
satisfies the requirements of Def. 3.10.
If the finite dynamic game under consideration also incorporates a chance
```
move, with possible alternatives for nature being u) € fi, then the loop equation
```
```
(5.3) will accordingly have to be replaced by
```
where, by an abuse of notation, we again let Pl be the class of all permissible
```
mappings pl : U1 x U2 x • • • x UN x Q -> U\ The solution of (5.4) will
```
```
now be a function of a;; and when this is substituted into the cost function
```
```
Ll : U1 x • • • x UN x Q —>• R and expectation is taken over the statistics of u;,
```
the resulting quantity determines the corresponding expected loss to Pi.
31 Here Nl denotes the set of all rf as in Def. 3.11.
218 T. BA§AR AND G. J. OLSDER
Now, for infinite dynamic games defined in discrete time, the tree model
of Def. 3.10 is not suitable since it cannot accommodate infinite action sets.
```
However, the loop model defined by relations such as (5.3) does not impose
```
```
such a restriction on the action sets; in other words, in the loop model we can
```
take each £/"*, to which the action variable u1 belongs, to be an infinite set.
```
Hence, let us now start with a set of equations of the form (5.3) with Ul (i € N)
```
taken as infinite sets. Furthermore let us decompose ul into K blocks such that,
considered as a column vector,
```
where K is an integer denoting the maximum number of stages (levels) in the
```
```
game,32 and ulk denotes the action (decision or control) variable of Pi corre-
```
sponding to his move during the fcth stage of the game.
In accordance with this decomposition, let us decompose each pl € P* in its
```
range space, so that (5.3) can equivalently be written as
```
Under the causality assumption which requires u\ to be a function of only the
```
past actions of the players, (5.6) can equivalently be written as (by an abuse of
```
```
notation)
```
By following an analysis parallel to the one used in system theory in going
```
from input-output relations for systems to state space models (Zadeh, 1969),
```
we now assume that P£ is structured in such a way that there exist sets Tlk, Y£,
```
X (i € N, k G K), with the latter two being finite dimensional, and functions
```
```
A : X x Ul x • • • x UJ? -> X, hi : X -+ Y£ (i € N, k € K), and for each
```
```
p\ € Pl (i G N,fc € K) there exists a 7^ € r£ (i € N, A; € K), such that (5.7)
```
can equivalently be written as
where 77^ is a subcollection of
and
for some x\ € X.
32In other words, no player can make more than K moves in the game, regardless of the
strategies picked.
GENERAL FORMULATION 219
Adopting the system theory terminology, we call Xk the state of the game, y\
```
the (deterministic) observation of Pi and rfk the information available to Pi, all
```
```
at stage fc; and we shall take relations (5.8), (5.9a)-(5.9b) as the starting point
```
in our formulation of discrete-time infinite dynamic games. More precisely, we
define an iV-person discrete-time infinite dynamic game of prespecified fixed
duration as follows.
Definition 5.1 An A^-person discrete-time deterministic infinite dynamic game33
of prespecified fixed duration involves
```
(i) An index set N = {1,..., N} called the players' set.
```
```
(ii) An index set K = {1,..., K} denoting the stages of the game, where K
```
is the maximum possible number of moves a player is allowed to make in
the game.
```
(Hi) An infinite set X with some topological structure, called the state set
```
```
(space) of the game, to which the state of the game (x^ belongs for all
```
```
k£K(J{K + l}.
```
```
(iv) An infinite set Uk with some topological structure, defined for each k G K
```
```
and i € N, which is called the action (control) set of Pi at stage k. Its
```
elements are the permissible actions ulk of Pi at stage k.
```
(v) A function fk : X x Uk x • • • x Uk —> X, defined for each k 6 K, so that
```
for some x\ e X which is called the initial state of the game. This differ-
ence equation is called the state equation of the dynamic game, describing
the evolution of the underlying decision process.
```
(vi) A set Yk with some topological structure, defined for each k € K and i G
```
N, and called the observation set of Pi at stage k, to which the observation
ylk of Pi belongs at stage k.
```
(vii) A function hlk : X —> Yk, defined for each k € K and i 6 N , so that
```
```
which is the state-measurement (-observation) equation of Pi concerning
```
the value of Xk-
```
(viii) A finite set rfk, defined for each k 6 K and i G N as a subset of
```
```
{y\,..., j / J ; . . . ; y f , . . . , y£; w j , . . . , u j . j j . . . ; u f , . . . , uJLj, which deter-
```
mines the information gained and recalled by Pi at stage k of the game.
Specification of jfk for all k e K characterizes the information structure
```
(pattern) of Pi, and the collection (over i 6 Nj of these information
```
structures is the information structure of the game.
33Also known as an 'W-person deterministic multi-stage game".
220 T. BA§AR AND G. J. OLSDER
```
(ix) A set Nk> defined for each k e K and i e N as an appropriate subset of
```
```
{(Y? x ... x yfc!) x ... x (Y? x - x YkN) x (C/!1 x • • • x C^i) x • • • x (U? x
```
```
• • •x £^-i)}> compatible with r\lk. Nk is called the information space of Pi
```
at stage k, induced by his information rfk.
```
(x) A prespecified class T], of mappings 7^ : Nk —>• Uk which are the permissi-
```
```
ble strategies of Pi at stage k. The aggregate mapping 7* = J7i, 72, • • • , JK}
```
is a strategy for Pi in the game, and the class F* of all such mappings 7*
```
so that 7|. € Tzk, k £ K, is the strategy set (space) of Pi.
```
```
(xi) A functional U : (X x U} x • • • x U?) x (X x U\ x • • • x U?) x • • • x
```
```
(X x U f t X , . . . , *UK) —> R defined for each i £ N, and ca//ed i/ie cost
```
functional o/Pi in the game of fixed duration.
The preceding definition of a deterministic discrete-time infinite dynamic
game is clearly not the most general one that could be given, first because
the duration of the game need not be fixed, but be a variant of the players'
strategies, and second because a "quantitative" measure might not exist to
reflect the preferences of the players among different alternatives. In other
```
words, it is possible to relax and/or modify the restrictions imposed by items (ii)
```
```
and (xi) in Def. 5.1, and still retain the essential features of a dynamic game. A
```
```
relaxation of the requirement of (ii) would involve introduction of a termination
```
```
set A C X x {1,2,...}, in which case we say that the game terminates, for
```
a given N-tuple of strategies, at stage k, if k is the smallest integer for which
```
(xk, k) € A.34 Such a more general formulation clearly also covers fixed duration
```
```
game problems in which case A = X x {K}, where K denotes the number of
```
```
stages involved. A modification of (xi), on the other hand, might for instance
```
```
involve a "qualitative" measure (instead of the "quantitative" measure induced
```
```
by the cost functional), thus giving rise to the so-called qualitative games (as
```
```
opposed to "quantitative games" covered by Def. 5.1). Any qualitative game
```
```
(also called game of kind) can, however, be formulated as a quantitative game
```
```
(also known as game of degree) by assigning a fixed cost of zero to paths and
```
strategies leading to preferred states, and positive cost to the remaining paths
and strategies. For example, in a two-player game, if PI wishes to reach a
certain subset A of the state set X after K stages and P2 wishes to avoid it, we
can choose
and thus consider it as a zero-sum quantitative game.
Now, returning to Def. 5.1, we note that it corresponds to an extensive form
description of a dynamic game, since the evolution of the game, the information
gains and exchanges of the players throughout the decision process, and the
interactions of the players among themselves are explicitly displayed in such a
34It is, of course, implicit here that Xf. is determined by the given TV-tuple of strategies, and
the strategies are defined as in Def. 5.1, but by taking K sufficiently large.
GENERAL FORMULATION 221
formulation. It is, of course, also possible to give a normal form description
of such a dynamic game, which in fact readily follows from Def. 5.1. More
specifically, for each fixed initial state x\ and for each fixed TV-tuple permissible
```
strategies (7* E Fz;i 6 N} the extensive form description leads to a unique set
```
```
of vectors [ulk = 7|.(?7jc), £fc+r, i £ N, k € K} because of the causal nature of the
```
information structure and because the state evolves according to a difference
```
equation. Then, substitution of these quantities into Ll (i e N) clearly leads to
```
a unique TV-tuple of numbers reflecting the corresponding costs to the players.
This further implies existence of a composite mapping Jl : F1 x • • • x TN —> R,
```
for each i e N, which is also known as the cost functional of Pi (i e N). Hence,
```
```
the permissible strategy spaces of the players (i.e., F1 ,... ,FN] together with
```
```
these cost functions (J1 ,..., J^) constitute the normal form description of the
```
dynamic game for each fixed initial state vector x\.
It should be noted that, under the normal form description, there is no es-
sential difference between infinite discrete-time dynamic games and finite games
```
(the complex structure of the former being disguised in the strategy spaces
```
```
and the cost functionals), and this permits us to adopt all the noncooperative
```
equilibrium solution concepts introduced in Chapters 2 and 3 directly in the
present framework. In particular, the reader is referred to Defs 2.8, 3.12 and
3.27 which introduce the saddle-point, Nash and Stackelberg equilibrium solu-
tion concepts, respectively, which are equally valid for infinite dynamic games
```
(in normal form). Furthermore the feedback Nash (cf. Def. 3.22) and feedback
```
```
Stackelberg (cf. Def. 3.29) solution concepts are also applicable to discrete-time
```
infinite dynamic games under the right type of interpretation, and these are
discussed in Chapters 6 and 7, respectively.
Before concluding our discussion on the ingredients of a discrete-time dy-
namic game as presented in Def. 5.1 we now finally classify possible information
structures that will be encountered in the following chapters, and also introduce
a specific class of cost functions—the so-called stage-additive cost functions.
Definition 5.2 In an N-person discrete-time deterministic dynamic game of
```
prespecified fixed duration, we say that Pi's information structure is a(n)
```
```
(i) open-loop (OL) pattern if rfk = {xi}, k € K,
```
```
(ii) closed-loop perfect state information (CLPS) pattern ifrjk = {xi,..., Xk},
```
A r e K ,
```
(Hi) closed-loop imperfect state information (CLIS) pattern ifrik = {y\,..., yk},
```
keK,
```
(iv) memoryless perfect state information (MPS) pattern if rjk = {xi,Xfc},
```
k£K,
```
(v) feedback (perfect state) information (FB) pattern if' rfk = {x^, k G K,
```
```
(vi) feedback imperfect state information (FIS) pattern if r)k = { y k } , k € K,
```
222 T. BA§AR AND G. J. OLSDER
```
(vii) one-step delayed CLPS (1DCLPS) pattern if rfk — {xi,... ,Xk-i},
```
keK,k^ I ,
```
(viii) one-step delayed observation sharing (IDOS) pattern ifrfk = {yi,..., yk-i,
```
```
yl}, keK, where yj = (yj, y ? , . . . , yj^}.
```
Definition 5.3 In an N-person discrete-time deterministic dynamic game of
```
prespecified fixed duration (i.e., K stages), Pi's cost functional is.said to be
```
```
stage-additive if there exist glk : X x X x Uk x • • • x Uk —> R, (k € K), so that
```
where
```
Furthermore, if Ll(ul,..., UN) depends only on XK+I (the terminal state), then
```
we call it a terminal cost functional.
Remark 5.1 It should be noted that every stage-additive cost functional can be
converted into a terminal cost functional, by introducing an additional variable
```
zk (k € K) through the recursive relation
```
and by adjoining Zk to the state vector x^. as the last component. Denoting the
```
new state vector as Xk — (x'k,Zk)', the stage-additive cost functional (5.10) can
```
then be written as
which is a terminal cost functional.
Games with chance moves: Stochastic games
```
Infinite discrete-time dynamic games which also incorporate chance moves (the
```
```
so-called stochastic games) can be introduced by modifying Def. 5.1 so that
```
we now have an additional player, called "nature", whose actions influence the
evolution of the state of the game. These are, in fact, random actions which
obey an a priori known probability law, and hence what replaces the state
```
equation (5.9b) in stochastic games is a conditional probability distribution
```
```
function of the state given the past actions of the players and the past values
```
of the state. An equivalent way of saying this is that there exists a function
```
Fk : X x Uk x • • • x Uk x 6 -* X, defined for each k € K, so that
```
GENERAL FORMULATION 223
```
where Ok is the action variable of nature at stage fc, taking values in 6; the
```
initial state 0:1 is also a random variable and the joint probability distribution
```
function of {#i,0i, . . . , # « • } is known. A precise formulation of a stochastic
```
dynamic game in discrete time would then be as follows.
Definition 5.4 An N-person discrete-time stochastic infinite dynamic game of
```
prespecified fixed duration involves all but items (v) and (xi) of Def. 5.1, and in
```
addition
```
(0) A finite or infinite set 9, with some topological structure, which denotes
```
```
the action set of the auxiliary (N+lst) player, nature. Any permissible
```
action 6k of nature at stage k is an element o/G.
```
(v) A function Fk : X x U% x • • • x U^ x 6 —> X, defined for each k G K, 50
```
that
where x\ is a random variable taking values in X, and the joint probability
```
distribution function of {xi, #1,..., Ok} is specified.
```
```
(xi) A functional U : (X x U\ x • • • x U? x 6) x (X x U% x • • • x [72N x 6) x
```
```
• • • x (X x UK x • • • x U$ x O) —> R defined for each i G N, and called
```
the cost functional of Pi in the stochastic game of fixed duration.
To introduce the noncooperative equilibrium solution concepts for stochastic
games as formulated in Def. 5.4, we again have to transfer the original game in
```
extensive form into equivalent normal form, by first computing Ll(-} (i G N)
```
```
for each AT-tuple of permissible strategies {7* G Tl,i G N} and as a function
```
```
of the random variables {xi, 9\,..., QN}I and then by taking expectation of the
```
resulting iV-tuple of cost functions over the statistics of these random variables.
```
The resulting deterministic functions J*(71,... , 7N) (i G N) are known as the
```
```
expected (average) cost functionals of the players, and they characterize the
```
normal form of the original stochastic game together with the strategy spaces
```
{r*; i G N}. We now note that, since such a description is free of any stochastic
```
nature of the problem, all the solution concepts applicable to the deterministic
dynamic game are also applicable to the stochastic dynamic game formulated
in Def. 5.4, and hence the stochastic case need not be treated separately while
introducing the equilibrium solution concepts.
Remark 5.2 Definition 5.4 does not cover the most general class of stochastic
```
dynamic games since (i) the state measurements of the players could be of
```
```
stochastic nature by taking hlk to be a mapping: X x 6 —>• Y£, (ii) the duration
```
of the game might not be fixed a priori, but be a variant of the players' actions
```
as well as the outcome(s) of the chance move(s), and (iii) the order in which
```
the players act might not be fixed a priori, but again depend on the outcome of
```
the chance move as well as on the actions of the players (Witsenhausen, 1971a).
```
Such extensions will, however, not be considered in this book. Yet another
224 T. BA§AR AND G. J. OLSDER
```
possible extension which is also valid in deterministic dynamic games (and which
```
is included in the formulation of finite games in extensive form in Chapters 2
```
and 3) is the case when the action sets U^ (i e N, k € K) of the players are
```
structurally dependent on the history of the evolution of the game. Such games
will also not be treated in the following chapters. D
5.3 Continuous-Time Infinite Dynamic Games
Continuous-time infinite dynamic games, also known as differential games in
the literature, constitute a class of decision problems wherein the evolution of
the state is described by a differential equation and the players act throughout a
time interval. Hence, as a counterpart of Def. 5.1, we can formulate such games
of prespecified fixed duration as follows.
Definition 5.5 A quantitative TV-person differential game of prespecified fixed
duration involves the following:
```
(i) An index set N = {!,..., N} called the players' set.
```
```
(ii) A time interval [0,T] which is specified a priori and which denotes the
```
duration of the evolution of the game.
```
(Hi) An infinite set So with some topological structure, called the trajectory
```
```
space of the game. Its elements are denoted as {x(t),0 < t < T} and
```
constitute the permissible state trajectories of the game. Furthermore, for
```
each fixed t 6 [0, T], x(t] € S°, where S° is a subset of a finite dimensional
```
vector space, say Rn.
```
(iv) An infinite set U1 with some topological structure, defined for each i €
```
```
N and which is called the control (action) space of Pi, whose elements
```
```
{ul(t),0 < t < T} are the control functions or simply the controls of Pi.
```
```
Furthermore, there exists a set S* C Rm; (i 6 N) so that, for each fixed
```
```
te [0,T], !/*(*) (=5*.
```
```
(v) A differential equation
```
whose solution describes the state trajectory of the game corresponding to
```
the N-tuple of control functions {ul(t), 0 < t < T} (i € N) and the given
```
initial state XQ .
```
(vi) A set-valued function ril(-} defined for each i 6 N as
```
```
where elt is nondecreasing in t, and rf(t] determines the state information
```
```
gained and recalled by Pi at time t G [0, T]. Specification of ril(-} (in fact,
```
GENERAL FORMULATION 225
```
f.\ in this formulation) characterizes the information structure (pattern)
```
```
of Pi, and the collection (over i G Nj of these information structures is
```
the information structure of the game.
```
(mi) A sigma-field Nl, in SQ, generated for each i G N by the cylinder sets
```
```
{x G SQ,X(S) G B} where B is a Borel set in S° and 0 < s < e\. The
```
sigma-field /Nf, t >t0, is called the information field of Pi.
```
(viii) A prespecified class P of mappings 7* : [0,T] x S0 —> Sl, with the property
```
```
thatul(t] = 7l(i,x) is N}-measurable (i.e., it is adapted to the information
```
```
field NI). Tl is the strategy space of Pi and each of its elements 7* is a
```
permissible strategy for Pi.
```
(ix) Two functionals ql : 5° -> R, gi : [0, T] x 5° x Sl x • • • x SN -> R defined
```
for each i G N, so that the composite functional
```
is well defined 35 for every u^(t) = 7J(£,x),7JI G FJ (j G N), and for
```
each i G N. Ll is the cost function of Pi in the differential game of fixed
duration.
A differential game, as formulated above, is yet not well defined unless we im-
pose some additional restrictions on some of the terms introduced. In particular,
```
we have to impose conditions on / and P(i € N), so that the differential equa-
```
```
tion (5.12) admits a unique solution for every TV-tuple {ux(t) = 7*(t,x), i G N},
```
```
with 7* e P. A nonunique solution to (5.12) is clearly not allowed under the
```
extensive form description of a dynamic game, since it corresponds to nonunique
```
state trajectories (or game paths) and thereby to a possible nonuniqueness in
```
the cost functions for a single ./V-tuple of strategies. We now provide below in
Thm. 5.1 a set of conditions under which this uniqueness requirement is ful-
filled. But first we list down some information structures within the context of
deterministic differential games, as a counterpart of Def. 5.2.
Definition 5.6 In an N-person continuous-time deterministic dynamic game
```
(differential game) of prespecified fixed duration [0,T], we say that Pi's infor-
```
```
mation structure is a(n)
```
```
(i) open-loop (OL) pattern ifr/l(t) = (xo},t G [0,T],
```
```
(ii) closed-loop perfect state (CLPS) pattern if
```
35This term will be made precise in the sequel.
226 T. BA§AR AND G. J. OLSDER
```
(Hi) e-delayed closed-loop perfect state (eDCLPS) pattern if
```
where e > 0 is fixed,
```
(iv) memoryless perfect state (MPS) pattern ifrf(t) = {xo,x(t}}, t G [0,T],36
```
```
(v) feedback (perfect state) (FB) pattern ifrf(t] = {x(t)}, t G [0,T\.
```
Theorem 5.1 Within the framework of Def. 5.5, let the information structure
for each player be any one of the information patterns of Def. 5.6. Furthermore,
let SQ = Cn[0,T]. Then, if
```
(i) f ( t , x, u1,..., UN) is continuous in t G [0, T] for each x G S°, i G N,
```
```
(ii) f ( t , x, u 1 , . . . , UN) is uniformly Lipschitz in x, u 1 , . . . , UN; i.e., for some
```
k > O,37
```
(Hi) for 7* G F* (i G N), 7*(£,z) is continuous in t for each x(-) G <7n[0,T]
```
```
and uniformly Lipschitz in x(-) G Cn[0, T],
```
```
the differential equation (5.12) admits a unique solution (i.e., a unique state
```
```
trajectory) for every 7* G T1 (i G N), so that ul(t) = jl(t,x), and furthermore
```
this unique trajectory is continuous.
Proof. It follows from a standard result on the existence of unique con-
tinuous solutions to differential equations. See for instance Coddington and
```
Levinson (1955).
```
Remark 5.3 Theorem 5.1 provides only one set of conditions which are suf-
ficient to ensure existence of a unique state trajectory for every TV-tuple of
```
strategies {7* G P;i G N}, which necessarily implies a well-defined differential
```
game problem within the framework of Def. 5.5. Since these conditions are all
related to existence of a unique solution to
```
they can definitely be relaxed (but slightly) by making use of the available the-
```
```
ory on functional differential equations (see, for example, Hale (1977)). But,
```
```
36Note that (iv) and (v) are not covered by (5.13) in Def. 5.5.
```
37\v\ denotes here the Euclidean norm for the vector v.
GENERAL FORMULATION 227
inevitably, these conditions all involve some sort of Lipschitz-continuity on the
permissible strategies of the players. However, although such a restriction could
```
be reasonable in the extreme case of one-person differential games (i.e., optimal
```
```
control problems), it might be quite demanding in an TV-player (N > 2) differ-
```
ential game. To illustrate this point, consider, for instance, the one-player game
described by the scalar differential equation
```
and adopt the strategy 71(t,x) = sgn (x(t)). The solution to (i) with ul(t} =
```
```
71(t,x) is clearly not unique; a multitude of solutions exists. If we adopt yet
```
```
another strategy, viz. 71(t,x) = — sgn (x(t)), then the solution to (i) does not
```
even exist in the classical sense. Hence, a relaxation of the Lipschitz-continuity
condition on the permissible strategies could make an optimal control prob-
lem quite ill-defined. In such a problem, the single player may be satisfied
```
with smooth (but sub-optimal) strategies. In differential games, however, it
```
is unlikely that players are willing to restrict themselves to smooth strategies
voluntarily. If one player would restrict his strategy to be Lipschitz, the other
```
player(s) may be able to exploit this. Specifically in pursuit evasion games (see
```
```
Chapter 8) the players play "on the razor's edge", and such a restriction could
```
result in a drastic change in the outcome of the game. Extensive discussions on
these issues, and on various different definitions of "solution", can be found in
```
the book by Krasovskii and Subbotin (1988).
```
In conclusion, non-Lipschitz strategies cannot easily be put into a rigorous
mathematical framework. On the other hand, in many games, we do not want
the strategy spaces to comprise only smooth mappings. These difficulties may
show up especially under the Nash equilibrium solution concept. In two-person
Stackelberg games, however, non-Lipschitz strategies can more easily be handled
```
in the general formulation, since one of the player's (follower's) choice of strategy
```
```
is allowed to depend on the other player's (leader's) announced strategy. D
```
The saddle-point, Nash, Stackelberg and consistent conjectural variations
equilibrium concepts introduced earlier for finite games are equally valid for
```
(continuous-time) differential games if we bring them into equivalent normal
```
form. To this end, we start with the extensive form description of a differential
game, as provided in Def. 5.5 and under the hypotheses of Thm. 5.1, and for
```
each fixed TV-tuple of strategies {7* € Fl; i e N} we obtain the unique solution
```
```
of the functional differential equation (5.15) and determine the corresponding
```
```
action (control) vectors u l ( - ) — jl(-,x),i G N,x € S°. Substitution of these
```
```
into (5.14), together with the corresponding unique state trajectory, thus yields
```
```
an TV-tuple of numbers {Lz; i 6 N}, for each choice of strategies by the players
```
```
— assuming of course that functions gl (i e N) are integrable, so that (5.14)
```
```
are well defined. Therefore, we have mappings Jl : F1 x • • • x TN —> R (i e N)
```
for each fixed initial state vector XQ, which we call the cost functional of Pi
in a differential game in normal form. These cost functionals, together with
```
the strategy spaces {ri;i € N} of the players, then constitute the equivalent
```
228 T. BA§AR AND G. J. OLSDER
normal form description of the differential game, which is the right framework to
introduce noncooperative equilibrium solution concepts, as we have done earlier
for other classes of dynamic games.
Termination
```
Definition 5.5 covers differential games of fixed prespecified duration; however,
```
as discussed in Section 5.2 within the context of discrete-time games, it is pos-
sible to extend such a formulation so that the end point in both state and time
is a variable. Let S° again denote a subset of Rn, and R+ denote the half-open
```
interval [0, oo). Let a closed subset A C 5° x R+ be given, which we call a
```
```
terminating (target) set, so that (xo,0) ^ A. Then, we say that the differential
```
game terminates, for a given N-tuple of strategies, at time T 6 R+ if T is the
```
smallest element o/R+ with the property (x(T),T) e A, i.e.,
```
This positive number T is called the terminal time of the differential game,
corresponding to the given JV-tuple of strategies. Terminal time is sometimes
```
defined in a slightly different way, as the smallest time at which x(-) penetrates,
```
A, i.e.,
where A denotes the interior of A. The two definitions can only differ in sit-
```
uations where the trajectory x(-) belongs to the boundary of A for a while or
```
only touches A at one instant of time. Unless stated differently, we shall adopt
```
(5.16) as the definition of the terminal time.
```
The question of whether a given differential game necessarily terminates is
one that requires some further discussion. If there is a finite time, say ti, such
```
that (x,ii) € A for all x € S°, then the game always terminates, in at most t\
```
units of time. Such a finite ti, however, does not always exist, as elucidated by
```
the following optimal control (one-player differential game) problem.
```
Example 5.1 Consider the optimal control problem described by the two-
dimensional state equation
```
where u(-) is the control (action) variable satisfying the constraint 0 < u(t) < 3
```
```
for all t G [0, oo). Let the terminal time T be defined as T = min{t G [0, oo) :
```
```
x2(t) = 2}, and a permissible OL strategy 7 be defined as a mapping from [0, oo)rri
```
```
into [0,3]. Now, if the cost function is L(u) = J0 1 dt = T, then the minimizing
```
```
strategy is 7*(£) = 3, t > 0, with the corresponding terminal time (and thereby
```
```
the minimum value of L) being In 2.
```
GENERAL FORMULATION 229
```
If the cost function to be minimized is L(u) — xi(T), however, the player
```
may not have an incentive to terminate this "one-player game" since, as long as
termination has not been achieved, his cost is not defined. So, on the one hand,
```
he does not want to terminate the game; while on the other hand, if he has an
```
incentive to terminate it, he should do it as soon as possible, which dictates him
```
to employ the strategy j * ( t ) — 3, t > 0.
```
The player is faced with a dilemma here, since the latter cost function is
not well defined for all strategies available to the player. This ambiguity can,
```
however, be removed by either (i) restricting the class of permissible strategies
```
to the class of so-called playable strategies which are those that terminate the
```
game in finite time, or (ii) extending the definition of the cost functional so that
```
which eliminates any possible incentive for the player not to terminate the game.
```
In both cases, the optimal strategy will be 7*(t) = 3, t > 0.
```
The difficulties encountered in the preceding optimal control example, as well
as the proposed ways out of the dilemma, are also valid for differential games,
and this motivates us to introduce the following concept of "playability".
Definition 5.7 For a given N-person differential game with a target set A, a
```
strategy N-tuple is said to be playable (at (to, XQ)) if it generates a trajectory x(-)
```
```
such that (x(t),t) € A for finite t. Such a trajectory x(-) is called terminating.
```
Differential games with chance moves38
Chance moves can be introduced in the formulation of differential games by
```
basically following the same lines as in the discrete-time case (cf. Def. 5.4), but
```
this time one has to be mathematically more precise. In particular, if we assume
```
the chance player (nature) to influence the state trajectory throughout a given
```
time interval, then actions of this additional player will be realizations of a
```
stochastic process {9t,t > 0} whose statistics are known a priori. If we adjoin
```
```
such a stochastic process to (5.12), then the resulting "differential equation"
```
might not be well defined, in the sense that even though its solution might be
```
unique for each realization (sample path) of {8t,t > 0}, it might not exist as
```
a stochastic process. To obtain a meaningful formulation and tractable results,
```
one has to impose some restrictions of F and {9t,t > 0}. One particular such
```
38Here it is assumed that the reader has some prior knowledge of stochastic processes. If
```
this is not the case, either this part may be skipped (without much loss, since this formulation
```
```
is used later only in Section 6.7) or the standard references may be consulted for background
```
```
knowledge (Fleming and Rishel, 1975; Gikhman and Skorohod, 1972; Wong and Hajek, 1985).
```
230 T. BA§AR AND G. J. OLSDER
assumption is to consider equations of the form
where F satisfies the conditions imposed on / in Def. 5.5, the function a satisfies
```
similar conditions in its arguments s and xs and {dt,t > 0} is a special type
```
```
of a stochastic process called the Wiener process. Equation (5.17a) can also be
```
written symbolically as
where we have used Wt, instead of Ot, to denote that it is the Wiener process.
```
It should further be noted that in both (5.17a) and (5.17b), the function #(•) is
```
```
written as #(.), to indicate explicitly that it now stands for a stochastic process
```
```
instead of a deterministic function. Equation (5.17b) is known as a stochastic
```
differential equation, and existence and uniqueness properties of its solution,
```
whenever ul(-} = jl(-,x),Y 6 F* (i € N), are elucidated in the following
```
```
theorem, whose proof can be found in Gikhman and Skorohod (1972).
```
Theorem 5.2 Let F1,. • • ,TN denote the strategy spaces of the players under
any one of the information patterns of Def. 5.6. Furthermore let SQ = Cn[0,T],
```
F satisfy the requirements imposed on f in Thm. 5.1, andj1 € Tl (i G N) satisfy
```
```
the restrictions imposed on 7* in Thm. 5.1 (in). If, further, a is a nonsingular
```
```
(n x n) matrix, whose elements are continuous in t and uniformly Lipschitz in x,
```
```
the stochastic differential equation (5.17b) with ul(-) = 7* (•,£), 7* € P (i € N),
```
admits as its solution a unique stochastic process with continuous sample paths,
for every such N-tuple of strategies.
```
To complete the formulation of a differential game with chance moves (i.e., a
```
```
stochastic differential game), it will now be sufficient to replace Ll in Def. 5.5 (ix)
```
with the expected value of the same expression, where the expectation operation
```
is taken with respect to the statistics of the Wiener process {wt, t > 0} and
```
the initial state XQ. The game can then be converted into normal form by
```
determining, in the usual sense, functions Jl : F1 x • • • x FN —> R (i € N), which
```
is now the suitable form to introduce the solution concepts already discussed.
5.4 Mixed and Behavioral Strategies in Infinite
Dynamic Games
In Chapter 2, we have defined a mixed strategy for a player as a probability
distribution on the space of his pure strategies, or equivalently, as a random vari-
```
able whose values are the player's pure strategies (cf. Def. 2.2), which was also
```
adopted in Chapters 3 and 4, for finite games and static infinite games, respec-
tively. Defined in this way, a mixed strategy is a mathematically well-established
GENERAL FORMULATION 231
object, mainly because the strategy spaces are finite for the former class of prob-
lems, and at most a continuum for the latter class of games, thereby allowing one
```
to introduce (Borel-) measurable subsets of these strategy spaces, on which prob-
```
ability measures can be defined. An attempt to extend this directly to infinite
dynamic games, however, is hampered by several measure-theoretic difficulties
```
(Aumann, 1964). To illustrate the extent of these difficulties, let us consider a
```
simple two-stage two-person dynamic game defined by the state equation
where —1 < u1 < 0, 0 < u2 < 1, and P2 has access to closed-loop perfect
```
state information (i.e., he knows the value of £2). A mixed strategy for PI
```
can easily be defined in this case, since his pure strategy space is [—1,0] which
is endowed with a measurable structure, viz. its Borel subsets. However, for
```
P2, the permissible (pure) strategies are measurable mappings from [0,1] into
```
[0,1], and thus the permissible strategy space of P2 is F2 = I1, where / denotes
the unit interval. In order to define a probability distribution on I1, we have
to endow it with a measurable structure, but no such structure exists which is
```
suitable for the problem under consideration (see Aumann, 1961). Intuitively,
```
such a difficulty arises because the set of all probability distributions on / is
already an extremely rich class, so that if one wants to define a similar object
on a domain I1 whose cardinality is higher than that of /, such an increase in
cardinality cannot reflect itself on the set of probability distributions.
An alternative, then, is to adopt the "random variable" definition of a mixed
strategy. This involves a sample space, say Q, and a class of measurable map-
pings from £1 into /', with each of these mappings being a candidate mixed
strategy. The key issue here, now, is the choice of the sample space £1. In
fact, since fJ stands for a random device, the question can be rephrased as the
choice of a random device whose outcomes are rich enough to be compatible
with I1. But the richest one is the continuous roulette wheel which corresponds
to a sample space $1 as a copy of the unit interval 7.39 Such a consideration
```
then leads to the conclusion that the random variable (mixed strategy) should
```
be a measurable mapping / from O into I1. Unfortunately, this approach also
leads to difficulties, since one still has to define a measurable structure on the
range space I1. But now, if we note that, to every function /: £1 —» I7, there
```
corresponds a function g: O x 7 —> / defined by g(u,x) = f(u:)(x), then the
```
"measurability" difficulty is resolved since one can define measurable mappings
from fJ x / into /. This conclusion of course remains valid if the action set /
is replaced by any measurable space C7, and the information space / is replaced
by any measurable space N. Moreover, an extension to multi-stage infinite
dynamic games is also possible as the following definition elucidates.
```
39This follows from the intuitive idea of a random device (Aumann, 1964). It is, of course,
```
possible to consider purely abstract random devices which are even richer.
232 T. BA§AR AND G. J. OLSDER
Definition 5.8 Given a K-stage discrete-time infinite dynamic game within
the framework of Def. 5.1, let Ufa, Nfc be measurable spaces for each i G N,
```
k G K. Then, a mixed strategy 7* for Pi is a sequence 7* = (7],... ,7]^) of
```
```
measurable mappings 7] : fi x TVj —> f/j (j G K), 50 that 7z(u;, •) G P for every
```
```
(jj G O, w/iere fHs a /ized sample space (taken as a copy of the unit interval).
```
A direct extension of this definition to continuous-time systems would be as
follows.
Definition 5.9 Given a differential game that fits the framework of Def. 5.5,
```
let SQ and Sl(i G N) be measurable spaces. Then, a mixed strategy 7* for Pi,
```
in this game, is a measurable transformation 7* : fi x [0, T] x 5o —* Sl, so
```
that 7*(u;, •, •) G Fz for every u) G O, where Q is a fixed sample space (taken as
```
```
a copy of the unit interval). Equivalently, 7z (-,-,or) is a stochastic process for
```
each x £ SQ.
```
A behavioral strategy, on the other hand, has been defined (cf. Def. 2.9) as a
```
collection of probability distributions, one for each information set of the player.
```
In other words, using the notation of Section 2.4, it is a mixed strategy 7(-) with
```
```
the property that 7(771) and 7(772) are independent random variables whenever
```
TJI and 772 belong to different information sets. If we extend this notion directly
```
to multi-stage infinite dynamic games (and within the framework of Def. 5.8), we
```
```
have to define a behavioral strategy as a mixed strategy 7*(-, •) with the property
```
```
that the collection of random variables {7J(-,77J-),?7* G NJ-;j e K} is mutually
```
independent. But, since TV- is in general a non-denumerable set, in infinite
dynamic games this would imply that we have a non-denumerable number of
mutually independent bounded random variables on the same sample space.
This is clearly not possible, since any bounded random variable defined on our
```
sample space f£ should have a countable basis (Loeve, 1963). Then, a way out
```
of this difficulty is to define a behavioral strategy as a mixed strategy which
is independent from stage to stage, but not necessarily stagewise. Aumann
```
actually discusses that stagewise correlation is quite irrelevant (Aumann, 1964),
```
and the expected cost function is invariant under such a correlation. Therefore,
we have the following definition.
Definition 5.10 Given a K-stage discrete-time infinite dynamic game within
the framework of Def. 5.1, let Uj., Nfc be measurable spaces for each i G N,
```
A; G K. Then, a behavioral strategy 7* for Pi is a mixed strategy (cf. Def. 5.8)
```
```
with the further property that the sequence of random variables {7J(-, ?/!•), j £ K}
```
```
is mutually independent for every fixed r/j G NJ- (j G K).
```
This definition of a behavioral strategy can easily be extended to multi-stage
```
games with a (countably) infinite number of stages; however, an extension to
```
continuous-time infinite dynamic games is not possible since the time set [0, T]
is not denumerable.
GENERAL FORMULATION 233
5.5 Tools for One-Person Optimization
Since optimal control problems constitute a special class of infinite dynamic
games with one player and one criterion, the mathematical tools available for
such problems may certainly be useful in dynamic game theory. This holds par-
ticularly true if the players adopt the noncooperative Nash equilibrium solution
concept, in which case each player is faced with a single criterion optimization
```
problem (i.e., optimal control problem) with the strategies of the remaining
```
players taken to be fixed at their equilibrium values. Hence, in order to verify
whether a given set of strategies is in Nash equilibrium, we inevitably have to
utilize the tools of optimal control theory. We, therefore, present in this section
some important results on dynamic one-person optimization problems so as to
provide an introduction to the theory of subsequent chapters.
The section comprises three subsections. The first two deal with the dynamic
```
programming (DP) technique applied to discrete-time and continuous-time op-
```
timal control problems, and the third is devoted to the "minimum principle".
For more details on, and a rigorous treatment of, the material presented in these
```
subsections the reader is referred to Fleming and Rishel (1975), Fleming and
```
```
Soner (1993) and Boltyanski (1978).
```
5.5.1 Dynamic programming for discrete-time systems
The method of dynamic programming is based on the principle of optimality
which states that an optimal strategy has the property that, whatever the initial
```
state and time are, all remaining decisions (from that particular initial state
```
```
and particular initial time onwards) must also constitute an optimal strategy.
```
To exploit this principle, we work backwards in time, starting at all possible
final states with the corresponding final times. Such a technique has already
```
been used in this book within the context of finite dynamic (multi-act) games,
```
specifically in Sections 2.5 and 3.5, in the derivation of noncooperative equilibria,
though we did not refer explicitly to dynamic programming. We now discuss
the principle of optimality within the context of discrete-time systems that fit
```
the framework of Def. 5.1, but with only one player (i.e., N = 1). Toward that
```
```
end, we consider equation (5.9b), assume feedback perfect state information and
```
```
a stage-additive cost functional of the form (5.10), all for N — 1, i.e.,
```
```
where u = {UK, k 6 K}, Wfc = u\ = 7fc(xfc); 7fc(-) denotes a permissible (control)
```
strategy at stage k G K, and K is a fixed positive integer. In order to determine
the minimizing control strategy, we shall need the expression for the minimum
```
(or minimal] cost from any starting point at any initial time. This is also called
```
234 T. BA§AR AND G. J. OLSDER
the value function, and is defined as
```
with Ui — ji(xi) G Ui and Xk = x. A direct application of the principle of
```
optimality now readily leads to the recursive relation
```
If the optimal control problem admits a solution u* = {u^k G K}, then the
```
```
solution V(l,xi) of (5.19) should be equal to L(u*}, and furthermore each u£
```
```
should be determined as an argument of the RHS of (5.19).
```
Affine-quadratic problems
```
As a specific application, let us consider the so-called affine-quadratic (or linear-
```
```
quadratic) discrete-time optimal control problem, which is described by the state
```
equation
and cost functional
Let us further assume that Xk G Rn, Uk G Rm, Ck G Rn, Rk > 0, Qk+i > 0
for all A: G K, and Ak, Bk are matrices of appropriate dimensions. Here, the
corresponding expression for fk is obvious, but the one for gk is not uniquely
```
defined; though, it is convenient to take it as
```
```
We now obtain, by inspection, that V(k, x] should be a general quadratic func-
```
```
tion of x for all A; G K, and that V(K + I,XK+I) = \X'K+\QK+IXK+I- This
```
```
leads to the structural form V(k,x) = ^x'SkX + x'sk + qk- Substituting this in
```
```
the recursive relation (5.19) we obtain the unique solution of the optimal control
```
```
problem (5.20a)-(5.20b) as follows.
```
```
Proposition 5.1 The optimal control problem (5.20a)-(5.20b) admits the unique
```
solution
```
Furthermore, the minimum value of (5.20b) is
```
where
Infinite horizon linear-quadratic problems
To formulate a possibly meaningful linear-quadratic optimal control problem
when K —> oo, we take c^ = 0, and the matrices Ak,Bk,Qk and Rk to be
```
independent of k (which will henceforth be written without the index k). Thus
```
the optimization problem is
GENERAL FORMULATION 235
for all k £ K, where
Remark 5.4 If the requirement Qk > 0 is not satisfied, a necessary and suf-
```
ficient condition for (5.21a)-(5.21b) to provide a unique solution to the affine-
```
quadratic discrete-time optimal control problem is
subject to
It is natural to assume here that the problem is well defined, in the sense that
there exists at least one control sequence that renders it a finite cost. Conditions
```
which ensure this are stabilizability of the matrix pair (A, J5), and detectability
```
```
of the matrix pair (A, D), where D is a matrix such that D'D — Q.40 These
```
notions of stabilizability and detectability belong to the realm of the theory of
```
linear systems; see for instance Kailath (1980) or Anderson and Moore (1989).
```
```
The pair (A, B] is stabilizable if an m x n matrix F exists such that (A+BF] is a
```
stable matrix, i.e., all its eigenvalues lie strictly within the unit circle. The pair
```
(A,D) is detectable if its "dual pair", (A',D') is stabilizable. The following
```
result is now a standard one, which can be found in any textbook on linear
control systems.
40Using standard terminology, we will also refer to the latter condition as detectability of
```
the pair (A, Q).
```
236 T. BA§AR AND G. J. OLSDER
```
Proposition 5.2 Assume that the pair (A, B) is stabilizable and the pair (A, Q)
```
is detectable. Then, there exists an n x n nonnegative-definite matrix S such
that
```
(K\(i) for fixed k, 5j>. —> S as K —> oo; where for each K, and arbitrary
```
```
QK+I > 0, S!.K) is recursively defined by (5.21a)-(5.21b);
```
```
(ii) S is the unique solution of the algebraic Riccati equation (ARE)
```
```
S = Q + A'S[I-B(R + B'SB}-1B'S]A
```
```
within the class of nonnegative-definite matrices;
```
```
(Hi) the (closed-loop) matrix A — B(R + B'SB)~1B'S is stable, i.e., all its
```
```
eigenvalues lie strictly within the unit circle;
```
```
(iv) the minimum value of the cost functional is ^x'^Sxi;
```
```
(v) the stationary optimal control law is
```
```
Under the stronger assumption that (A, D) is observable (a sufficient condition
```
```
for which is Q > 0), the solution of the ARE is positive definite.
```
5.5.2 Dynamic programming for continuous-time systems
The dynamic programming approach, when applied to optimal control problems
```
defined in continuous time, leads to a partial differential equation (PDE) which
```
```
is known as the Hamilton-Jacobi-Bellman (HJB) equation. Toward this end
```
we consider the optimal control problem defined by
where / is a scalar function, defining an n-dimensional smooth manifold in the
product space Rn x R+, and F is taken to be the class of all admissible feedback
strategies.
```
The minimum cost-to-go from any initial state (x) and any initial time (t)
```
is described by the so-called value function which is defined by
satisfying the boundary condition
```
which takes (5.24b) as the boundary condition.
```
```
In general, it is not easy to compute V(£, x). Moreover, the continuous differ-
```
```
entiability assumption imposed on V(t, x) is rather restrictive (see, for instance,
```
```
Example 5.2 in subsection 5.5.3, for which it does not hold). Nevertheless, if
```
```
such a function exists, then the HJB equation (5.25) provides a means of ob-
```
taining the optimal control strategy. This "sufficiency" result is now proven in
the following theorem.
```
Theorem 5.3 If a continuously differentiable function V(t,x] can be found that
```
```
satisfies the HJB equation (5.25) subject to the boundary condition (5.24b), then
```
```
it generates the optimal strategy through the static (pointwise) minimization
```
```
problem defined by the RHS of (5.25).
```
```
Proof. If we are given two strategies, 7* e F (the optimal one) and 7 6 F
```
```
(an arbitrary one), with the corresponding terminating trajectories x* and x,
```
```
and terminal times T* and T, respectively, then (5.25) reads
```
where 7* and 7 have been replaced by the corresponding controls u* and u,
```
respectively. Integrating (5.26a) from 0 to T and (5.26b) from 0 to T*, we
```
obtain
GENERAL FORMULATION 237
```
A direct application of the principle of optimality on (5.24a), under the
```
assumption of continuous differentiability of V", leads to the HJB equation
```
Elimination of V(0,xo) yields
```
from which it readily follows that u* is the optimal control, and therefore 7* is
the optimal strategy.
238 T. BA§AR AND G. J. OLSDER
```
Remark 5.5 If, in the problem statement (5.23), the time variable t does not
```
appear explicitly, i.e., if /, g, q and / are time-invariant, the corresponding
value function will also be time-invariant. This then implies that dV/dt — 0,
```
and the resulting optimal strategy can be expressed as a function of only x(t),
```
```
i.e., u*(t) = 7*(x(£)). In such cases, we will write V(x] for V(t,x).
```
```
Remark 5.6 There exists an alternative derivation of the HJB equation (5.25),
```
which is of a geometrical nature. This will not be discussed here, since it is a
special case of the more general derivation to be given in Section 2 of Chapter 8
for two-player zero-sum differential games.
Affine-quadratic problems
We now consider an important class of problems—the so-called affine- quadratic
```
(or linear-quadratic) continuous-time optimal control problems—for which V(t, x)
```
is continuously differentiable, so that Thm. 5.3 applies. Toward that end, let
```
the system be described (as a continuous-time counterpart of (5.20a)-(5.20b))
```
by
and the cost functional to be minimized be given as
```
where x(t) € Rn, u(t) € Rm, 0 < t < T and T is fixed. A(-), £(•), Q(-) > 0,
```
```
R(-) > 0 are matrices of appropriate dimensions and with continuous entries on
```
```
[0,T]. The matrix Qf is nonnegative-definite, and c(-) and p(-) are continuous
```
vector-valued functions, taking values in Rn. Furthermore, we adopt the feed-
back information pattern and take a typical control strategy as a continuous
mapping 7 : [0,T] x Rn —> Rm. Denote the space of all such strategies by F.
Then the optimal control problem is to find a 7* € F such that
where
Several methods exist to obtain the optimal strategy 7* or the optimal control
```
function w*(-) = 7*(-, x). We shall derive the former by making use of Thm. 5.3.
```
```
Simple arguments (see Anderson and Moore, 1989) lead to the conclusion that
```
```
minp J(7) is quadratic in XQ. Moreover, it can be shown that, if the system is
```
positioned at an arbitrary t G [0, T] at an arbitrary point x € Rn, the minimum
cost-to-go, starting from this position, is quadratic in x. Therefore, we may
assume existence of a continuously differentiable value function of the form
Thus we arrive at the following proposition.
```
Proposition 5.3 The linear-quadratic optimal control problem (5.30a)-(5.30b)
```
```
admits a unique optimum feedback controller 7* which is given by (5.34), where
```
```
S(-), k(-) and ra(-) uniquely satisfy (5.35a)-(5.35c). The minimum value of the
```
cost functional is
GENERAL FORMULATION 239
```
that satisfies (5.25). Here S(-} is a symmetric (n x n) matrix with continuously
```
```
differentiable entries, k(-) is a continuously differentiable n-vector and ra(-) is
```
```
a continuously differentiable function. If we can determine such S(-), k(-) and
```
```
ra(-) so that (5.32) satisfies the HJB equation (5.25), then Thm. 5.3 justifies the
```
assumption of the existence of a value function quadratic in x. Substitution of
```
(5.32) into (5.25) leads to
```
Carrying out the minimization on the RHS yields
```
substitution of which into (5.33) leads to an identity relation which is readily
```
satisfied if
```
Proof. Except for existence of a unique S(-) > 0, k(-} and m(-) that satisfy
```
```
(5.35a)-(5.35c), the proof has been given prior to the statement of the propo-
```
```
sition. Furthermore, if there exists a unique S(-} that satisfies (5.35a), which
```
is known as the matrix Riccati equation, existence of unique solutions to the
```
remaining two differential equations of (5.35a)-(5.35c) is assured since they are
```
linear in k and m, respectively. Then, what remains to be proven is that a
```
unique solution S(-} > 0 to (5.35a) exists on [0, T]. This can be verified by
```
```
either making use of the theory of differential equations (cf. Reid, 1972) or uti-
```
```
lizing the specific form of the optimal control problem with Q/ and Q(-) taken
```
```
to be nonnegative-definite (Anderson and Moore, 1989).
```
```
Remark 5.7 The solution (5.34) can be obtained by other methods as well.
```
```
Two of these are the minimum principle (to be discussed shortly; see also
```
```
(Bryson and Ho, 1975), and the "completion of squares" method (see Brock-
```
```
ett, 1970). Since the latter will be discussed in Chapter 6 in the context of
```
affine-quadratic differential games, it will not be covered in this chapter.
240 T. BA§AR AND G. J. OLSDER
```
Remark 5.8 The nonnegative-definiteness requirements on Qf and Q(-) may
```
be relaxed, but then we have to assume from the outset the existence of a
```
unique bounded solution to (5.35a) in order to ensure the existence of a unique
```
```
minimizing control as given by (5.34).41 Otherwise (5.35a) might not admit
```
```
a solution; more precisely its solution may exhibit finite escape (depending on
```
```
the length of the time interval), implying in turn that the "optimal" cost might
```
tend to —oo. To exemplify this situation consider the scalar example:
```
on the interval (T — l,T]. Hence, for T > 1, a continuously differentiate
```
```
solution on the interval [0, T] to (5.37) does not exist. This non-existence of a
```
solution to the matrix Riccati equation is directly related to the well-posedness
```
of the optimal control problem (5.36), since it can readily be shown that, for
```
```
T > 1, L(u) can be made arbitrarily small (negative) by choosing a proper u(t).
```
In such a case we say that the Riccati equation has a conjugate point in [0, T]
```
(Sagan (1969) and Brockett (1970)). This topic will be revisited in Chapter 6
```
in the context of affine-quadratic zero-sum differential games.
Infinite horizon linear-quadratic problems
Meaningful continuous-time linear-quadratic optimal control problems where
```
T —> oo can also be formulated. Toward that end we take c(t) = 0, and the
```
```
matrices A(t),B(t),Q(t) and R(t) to be independent of t. Then the problem
```
becomes
```
Conditions which ensure this problem to be well defined (with a finite mini-
```
```
mum) are precisely those of the discrete-time analogue of this problem, i.e., the
```
```
matrix pair (A, B) must be stabilizable and the matrix pair (A, D), where D is a
```
```
matrix such that D'D = Q, must be detectable; see for instance Kailath (1980)
```
41 See Chapter 6, and particularly Lemma 6.4 and Remark 6.15 for further elaboration of
this point.
which admits the solution
```
The Riccati equation (5.35a), for this example, becomes
```
subject to
GENERAL FORMULATION 241
```
or Davis (1977) for discussions on these notions in continuous time.42 The pair
```
```
(A, B} is stabilizable if an ra x n matrix F exists such that (A + BF} is stable,
```
```
i.e., all its eigenvalues lie in the open left half of the complex plane (note the
```
difference of this definition of stability from the one for its discrete-time coun-
```
terpart). Furthermore, the pair (A,D) is detectable if its "dual pair" ( A ' , D f )
```
is stabilizable. The following proposition, which can be found in any textbook
on linear control systems, summarizes the main result.
```
Proposition 5.4 Assume that the pair (A, B) is stabilizable and the pair (A, Q)
```
is detectable. Then, there exists an n x n nonnegative definite matrix S such
that
```
(i) for fixed t, and for every Q/ > 0, ST(t) —> S (uniformly in t < T)
```
```
as T —> oo, where ST(t) is the unique nonnegative definite solution to
```
```
(5.35a), parameterized in T;
```
```
(ii) S is the unique solution of the algebraic Riccati equation (ARE)
```
```
in the class of nonnegative definite matrices;
```
```
(Hi) the closed-loop matrix A — BR~1B'S is stable, i.e., all its eigenvalues lie
```
```
in the open left half of the complex plane;
```
```
(iv) the minimum value of the cost functional is ^x'0SxQ;
```
```
(v) the stationary optimum control law is
```
```
Under the stronger condition that (A, D) is observable (a sufficient condition
```
```
for which is Q > Q), the solution to the ARE is positive definite.
```
5.5.3 The minimum principle
Continuous-time systems
```
In this subsection our starting point will be the HJB equation (5.25) under
```
the additional assumption that V is twice continuously differentiate, and we
shall convert it into a series of pointwise optimization problems, indexed by the
parameter t. This new form is in some cases more convenient to work with and
it is closely related to the so-called minimum principle, as also discussed here.
Toward this end we first introduce the function
42As in the discrete-time case, we will refer to the latter condition also as detectability of
```
the pair (A,Q).
```
In conclusion, we have arrived at the following necessary condition for the op-
```
timal control u*(-): under the assumption that the value function V(t,x) is
```
```
twice continuously differentiate, the optimal control u* (t) and corresponding
```
242 T. BA§AR AND G. J. OLSDER
```
in terms of which equation (5.25) can be written as
```
Being consistent with our earlier convention, the minimizing u will be denoted
by u*. Then
Since this is an identity in x, its partial derivative with respect to x is also zero.
This leads to, by also interchanging the orders of second partial derivatives
```
(which is allowed if V is twice continuously differentiate):
```
If there are no constraints on u, then dH/du = 0 for u — u* according to
```
equation (5.40). If there are constraints on w, and u* is not an interior point,
```
```
then it can be shown that (dH/du)-(du*/dx) = 0 (because of optimality, dH/du
```
```
and du* f d x are orthogonal; for specific problems we may have du* f d x = 0). In
```
```
view of this, equation (5.42) becomes
```
```
By introducing the so-called costate vector, p'(t] = dV(t,x*(t))/dx, where x*
```
```
denotes the state trajectory corresponding to w*, (5.43) can be re-written as
```
where H is defined by
```
The final time T is determined by the scalar relation /(T,x) = 0 and hence can
```
```
be regarded as a function of the state: T(x). The boundary condition for p(t)
```
is determined from
GENERAL FORMULATION 243
```
trajectory x*(t) must satisfy the following so-called canonical equations
```
```
In the derivation of (5.47), the controls have been assumed to be functions
```
```
of time and state; i.e., u(t) — 7(t,x(t)). If, instead, one starts with the class
```
```
of control functions which depend only on time, i.e., u(t) — 7(t), the set of
```
```
necessary conditions (5.47) can be derived in quite a different way (by using
```
perturbation functions, which is a standard procedure in the classical calculus
```
of variations), and under milder assumptions. Following such a derivation, one
```
obtains the following result which can, for instance, be found in Pontryagin,
```
et al. (1962) and which is referred to as the minimum principle.
```
```
Theorem 5.4 Consider the optimal control problem defined by (5.23) and un-
```
der the OL information structure. If the functions f , g, q and I are continuously
```
differentiate in x and continuous in t and u, then relations (5.47) provide a set
```
of necessary conditions for the optimal control and the corresponding optimal
trajectory to satisfy.
Remark 5.9 A particular class of optimal control problems which are not cov-
ered by Thm. 5.4 are those with a fixed terminal condition, i.e., with the terminal
```
time T and the terminal state x(T) prescribed, since the function I is not differ-
```
entiable in this case. However, the necessary conditions for the optimal control
```
are still given by (5.47), with the exception that the condition on p(T) is absent
```
```
and instead the terminal state constraint x(T) = Xf replaces it. We also note,
```
in passing, that in the theory of zero-sum differential games, reasonable prob-
```
lems with state space dimension > 2 will (almost) never be formulated with a
```
fixed prescribed terminal state, since it is not possible to enforce this end-point
constraint on both players if they have totally conflicting objectives.
The following example now illustrates how Thm. 5.4 can be employed to
obtain the solution of an optimal control problem for which the value function
V is not continuously differentiate. It also exhibits some features which are
frequently met in the theory of differential games, and introduces some termi-
nology which will be useful later in Chapter 8.
```
assuming, of course, that T > 1. Now, by integrating (5.48) backwards in time
```
```
from an arbitrary terminal condition on the line xi = x^ (which we take as
```
```
x\(T] = X2(T) — a where a is the parameter), and by replacing u(t] with u*(t)
```
```
as given by (5.51), we obtain
```
244 T. BA§AR AND G. J. OLSDER
Example 5.2 Consider an open-loop control problem with systems dynamics
described by
```
where the scalar control satisfies the constraint —1 < u(t) < I for all t > 0. The
```
objective is to steer the system from an arbitrary but known initial point in the
```
(£1,2:2) plane to the line x\ = x-z in minimum time. Hence, the cost function
```
can be written as
where T is defined as the first instant for which the function
becomes zero. In the derivation to follow, we shall restrict ourselves to initial
points satisfying x\ — Xi > 0. Initial points with the property x\ — x^ < 0
```
can be dealt with analogously. Now, application of relations (5.47) yields (with
```
```
P= (Pl,P2)'):
```
```
where the costate variable P2(t) here is also known as the switching function,
```
```
since it determines the sign of u*(t). For points on the line x\ = #2, which can be
```
reached by optimal trajectories, obviously I = 0 and therefore the costate vector
```
p(T] = (pi(T),p2(T}}f will be orthogonal to the line x\ — x^ and point in the
```
direction "south-east". Since the magnitude of the costate vector is irrelevant
in this problem, we take
which leads to
```
for T - 1 < t < T. Fort = T-l we get xi(T - 1) = \, x2(T) = a - 1. The line
```
x\ = lj,X2 = a — 1 is called, for varying a, the switching line.
GENERAL FORMULATION 245
Since we only consider the "south-east" region of the state space, the tra-
```
jectory (xi(t),X2(t)} should move into this region from £i(T) — x^(T) — a if t
```
goes backwards in time, which is only true for a < 1. For a > 1, the trajecto-
ries first move into the region x^ > £i, in retrograde time. Those trajectories
cannot be optimal, since we only consider the region x^ < £1, and, moreover,
```
boundary conditions (5.50) are not valid for the region x% > x\. Hence only
```
```
points Xi(t) = x^(T) = a with a < 1 can be terminal points. The half line
```
```
xi = £2 = a, a < 1, is called the usable part (UP) of the terminal manifold or
```
target set. Integrating the state equations backwards in time from the UP, and
substituting t ~ T — 1, we observe that only the line segment Xi = - 2 i , £ 2 = a — 1,
a < 1, is a switching line. Using the points on this switching line as new ter-
minal state conditions, we can integrate the state equations further backwards
```
in time with u*(t] = —I. The complete picture with the optimal trajectories is
```
shown in Fig. 5.1. These solutions are the only ones which satisfy the necessary
conditions of Thm. 5.4 and therefore they must be optimal, provided that an
optimal solution exists.
Figure 5.1: The solution of Example 5.2. The dotted lines denote lines of "equal
time-to-go".
```
Once the optimal trajectories are known, it is not difficult to construct V(x}.
```
Curves of "equal time-to-go", also called isochrones, have been indicated in
```
Fig. 5.1. Such a curve is determined from the equation V(x) = constant. It is
```
```
seen that along the curve AB the value function V(x) is discontinuous. Curves
```
```
along which V(x) is discontinuous are called barriers. The reason behind the
```
usage of such a terminology is that no optimal trajectory can ever cross a curve
```
along which V(x) is discontinuous in the direction from large V to small V.
```
```
Note that, because of the discontinuity in V(x), Thm. 5.3 cannot be used in
```
246 T. BA§AR AND G. J. OLSDER
this case.
```
The corresponding feedback strategy 7*(i,x), which we write as 7*(rr) since
```
```
the system is autonomous (i.e., time-invariant), can readily be obtained from
```
```
Fig. 5.1: to the right of the switching line and barrier, we have 7*(x) — —1, and
```
to the left of the switching line and barrier, and also on the barrier, we have
```
7* (or) = +1.
```
Discrete-time systems
We now conclude this subsection by stating a counterpart of Thm. 5.4 for
discrete-time optimal control problems. Such problems have earlier been for-
```
mulated by (5.18a) and (5.18b), but now we also assume that fk is continu-
```
ously differentiable in Xk, and §k is continuously differentiate in x& and Xk+i
```
(k e K). Then, the following theorem, whose derivation can be found in either
```
```
Canon et al. (1970) or Boltyanski (1978), provides a minimum principle for such
```
systems.
```
Theorem 5.5 For the discrete-time optimal control problem described by (5.18a)
```
```
and (5.18b), let
```
```
(i) fk(-,Uk) be continuously differentiable on Rn, (A; € K),
```
```
( ii) 9k('iuki •) be continuously differentiable on Rn x Rn; (k € K),
```
```
(Hi) /fe(-, •) be convex on Rn x Rm, (k & K).
```
```
Then, if {u%,k 6 K} denotes an optimal control sequence, and {x^+1,k 6
```
```
K} denotes the corresponding state trajectory, there exists a finite sequence
```
```
{P2i • • • tPK+i} of n-dimensional costate vectors so that the following relations
```
are satisfied:
The reader should now note that Prop. 5.1 also follows as a special case of
this theorem.
```
It should be noted that (5.54a) is an open-loop strategy since it only depends
```
```
on the discrete-time parameter k and the initial state x\ (through (5.54b)). We
```
```
can also interpret (5.54a) as the realization of the feedback strategy (5.53) on
```
```
the control set C7fc, that is to say, (5.54a) is the action dictated by (5.53). For
```
```
this reason, we also sometimes refer to (5.54a) as the open-loop realization or
```
```
open-loop value of the feedback strategy (5.53).
```
```
The question now is whether we can obtain other solutions (optimal strate-
```
```
gies) for the optimal control problem under consideration by extending the strat-
```
egy space to conform with some other information structure, say the closed-loop
perfect state pattern. If we denote by P^ the class of all permissible closed-loop
```
strategies (under the CLPS pattern) at stage k (i.e., strategies of the form
```
```
7 f c ( x f c , X f c _ i , . . . ,Xi)), then (5.53) is clearly an element of that space, and it
```
GENERAL FORMULATION 247
5.6 Representations of Strategies Along Trajec-
tories, and Time Consistency of Optimal Poli-
cies
In our discussion of some important results of optimal control theory in the
previous section, we have confined our treatment to two specific information
```
patterns and thereby to two types of strategies; namely, (i) the open-loop in-
```
formation pattern dictates strategies that depend only on the initial state and
```
time, (ii) the feedback information pattern forces the permissible strategies to
```
depend only on the current value of the state and time. The former class of
strategies is known as open-loop strategies, while the latter class is referred to
as feedback strategies.
Referring back to the discrete-time afrine-quadratic optimal control problem
```
described by (5.20a)-(5.20b) (and assuming that Cfc = 0 for all k € K, for the
```
```
sake of brevity in the discussion to follow), the solution presented in Prop. 5.1,
```
i.e.,
is a feedback strategy, and in fact it constitutes the unique solution of that
optimal control problem within the class of feedback strategies. If the optimal
control problem had been formulated under the open-loop information pattern,
```
then (5.53) would clearly not be a solution candidate. However, the unique
```
```
optimal open-loop strategy, for this problem, can readily be obtained from (5.53)
```
as
```
where x£ is the value of the state at stage k, obtained by substitution of (5.53)
```
```
in the state equation; in other words, x* can be solved recursively from the
```
difference equation
where Fk — Ak — BkPk$k+iAk, and Ck is a matrix of appropriate dimensions.
```
It should be noted that all such strategies have the same open-loop value (which
```
```
is the RHS of (5.55)) and generate the same state trajectory (5.54b), both of
```
```
which follow from (5.55). Because of these properties, we call them different
```
representations of a single strategy, a precise definition of which is given be-
low. Since the foregoing analysis also applies to continuous-time systems, this
definition covers both discrete-time and continuous-time systems.
Definition 5.11 For a control problem with a strategy space T, a strategy 7 6 F
is said to be a representation of another strategy 7 G T if
```
(i) they both generate the same unique state trajectory, and
```
```
(ii) they both have the same open-loop value on this trajectory.
```
Remark 5.10 The notion of representation of a strategy, as introduced above,
involves no optimality. As a matter of fact, it is a property of the strategy
space itself, together with the control system for which it is defined, and no cost
functional has to be introduced.
A significance of the notion of "representations" in control problems is that
```
it enables one to construct equivalence classes (of equal open-loop value control
```
```
laws) in the general class of closed-loop strategies, so that there is essentially no
```
difference between the elements of each equivalence class. In fact, for an optimal
control problem, we can only talk about a unique optimal "equivalence class",
instead of a unique optimal "strategy", since as we have discussed earlier within
```
the context of the optimal control problem (5.20a)-(5.20b), every representation
```
```
of the feedback strategy (5.53) constitutes an optimal solution to the optimal
```
```
control problem, and there are uncountably many such (optimal) strategies.
```
As a converse to the italicized statement, we can say that every solution of
```
the optimal control problem (5.20a)-(5.20b) is a representation of the feedback
```
```
strategy (5.53). Both of these statements readily follow from Def. 5.1, and they
```
```
are valid not only for the specific optimal control problem (5.20a)-(5.20b) but
```
for the general class of control problems defined in either discrete or continuous
time.
248 T. BA§AR AND G. J. OLSDER
```
constitutes an optimal solution to the optimal control problem (5.20a)-(5.20b)
```
also on that enlarged strategy space. As a matter of fact, any element 7^ of
```
Tk (k G K), which satisfies the boundary condition
```
also constitutes an optimal solution. One such optimal policy is the one-step
memory strategy
GENERAL FORMULATION 249
We now address the problem of constructing equivalence classes of repre-
```
sentations of strategies for a control problem with a given space (F) of closed-
```
loop strategies, and first for discrete-time systems. The procedure, which fol-
lows directly from Def. 5.11, is as follows. For such a system, first determine
```
the set (FQL) of all elements of F, which are strategies that depend only on
```
the initial state xi, and the discrete time parameter k. The set FOL> thus
constructed, is the class of all permissible open-loop controls in F. Now, let
```
7 = |7fc(^i); k G K} be a chosen element of FQL, which generates (by substitu-
```
```
tion into (5.18a)) a (necessarily) unique trajectory {x^, k £ K}. Then, consider
```
```
all elements 7 = {7fc(-)> k € K} of F with the properties
```
```
(i) 7 generates the same state trajectory as 7, and
```
```
(ii) 7fc(xf c ,xf c _1 ,...,x2 ,xi) =7f c (xi),fc € K.
```
The subset of F thus constructed constitutes an equivalence class of representa-
tions which, in this case, have the open-loop value 7. If this procedure is exe-
cuted for every element of 7oL5 then the construction of all equivalence classes
of F becomes complete.
For continuous-time systems, essentially the same procedure can be followed
to construct equivalence classes of representations of strategies. In both discrete-
time and continuous-time infinite dynamic systems such a construction leads,
in general, to an uncountable number of elements in each equivalence class
which necessarily contains one, and only one, open-loop strategy. The main
reason for the "non-denumerable" property of equivalence classes is that in de-
```
terministic problems the CLPS information pattern (which involves memory)
```
exhibits redundancy in information—thus giving rise to existence of a plethora
of different representations of the same open-loop policy. This "informational
nonuniqueness" property of the CLPS pattern is of no real concern to us in op-
timal control problems, since every representation of an optimal control strategy
also constitutes an optimal solution. However, in dynamic games, the concept of
"representations" plays a crucial role in the characterization of noncooperative
equilibria, and the preceding italicized statements are no longer valid in a game
situation. We defer details of an investigation in that direction to Chapters 6
and 7, and only provide here an extension of Def. 5.11 to dynamic games.
```
Definition 5.12 For an N-person dynamic game with strategy spaces {Tl;i €
```
```
N} let the strategies of all players, except Pi, be fixed at 7^ € F-3, j e N, j ^ i.
```
Then, a strategy 7* € F* for Pi is a representation of another strategy 7* G Tl,
```
with -yj e F-7' (j e N, j ^ i} fixed, if
```
```
(i) the N-tuples (7t,7J; j £ N, j ^ i} and {7%7J;j € N, j ^ i} generate the
```
same unique state trajectory, and
```
(ii) 7* and 7* have the same open-loop value on this trajectory.
```
Time consistency
```
In (one-player) optimal control problems, even though all representations of
```
an optimal control strategy constitute optimal solutions, as discussed above,
denote, respectively, the truncations of 7 € F and 7* € P, to the time interval
[s,t] C [0,T], and let
43 By an abuse of notation, we will let this also denote the discrete interval for the discrete-
```
time problem, in which case [0, T] = {1,..., K}.44
```
In an optimal control problem, for example, sol could stand for minimization or maxi-
```
mization, or reachability to a target set; in a dynamic game it could stand for saddle-point,
```
Nash or Stackelberg equilibrium.
250 T. BA§AR AND G. J. OLSDER
some of these representations may be preferred over others due to additional
appealing properties they carry, such as robustness to modeling and decision
errors. One such refinement scheme that brings in a selection among different
representations of a given optimal strategy is provided by the notion of time
consistency, which is valid not only for one-player dynamic decision problems
```
(such as optimal control), but also for dynamic games in both discrete and
```
continuous time. This notion manifests itself in two forms—as weak and strong
time consistency, which we introduce below in the general framework of dynamic
games. Toward that end, let us first introduce the notation
to denote an JV-person dynamic game, where F is the product strategy space,
[0, T] is the decision interval,43 and sol stands for any particular solution concept
```
according to which (optimal or equilibrium) policies are determined.44 Further-
```
more, let
```
denote a version of D(T; [0, T1]; sol), where the policies of all players in the inter-
```
```
vals [0, s) and (t, T] are fixed as f3L SN, ffit T,, i € N. An underlying assumption
```
```
here is that the cost functions of the players are of additive type, such as (5.10)
```
```
or (5.14). Then, we have the following two definitions on the "time consistency"
```
of a solution obtained for the original game according to the concept "sol".
Definition 5.13 An N-tuple of policies 7* € F solving the dynamic game
```
D(T; [0,T]; sol) is weakly time consistent (WTC) if its truncation to the inter-
```
```
val [s, T], 7r*s Ti, solves the truncated game D7 j,,, this being so for all s £ (0, T].
```
If a solution 7* 6 F is not WTC, then it is time inconsistent.
Definition 5.14 An N-tuple of policies 7* € F solving the dynamic game
```
D(T; [0,T]; sol) is strongly time consistent (STC) if its truncation to the in-
```
```
terval [s,T], 7? t i, solves the truncated game D?T,, for every /?[o,s) £ F[o,s)>
```
```
this being so for every s € (0, T].
```
Both these refinements essentially say that for a solution to be time consis-
```
tent (in the weak or strong sense), the players should have no rational reason,
```
GENERAL FORMULATION 251
at any future stage of the game, to deviate from the adopted policies. The
```
difference between weak and strong time consistency lies in the consistency (or
```
```
inconsistency) of the past actions with the adopted policies. In the former (that
```
```
is, weak time consistency) there is no ground for reneging at future stages only
```
```
if the past actions are consistent with the original solution, 7*; whereas in the
```
latter this is true even if there have been deviations in the past from the actions
which are dictated by the original solution.
Solutions of optimal control problems of the types discussed in Section 5.5
```
(and of course also their representations) are weakly time consistent—a fact
```
that easily follows from Def. 5.13. Not every solution is strongly time consistent,
```
however; a case in point being the optimal open-loop control u?Q Ti = J7*(t),t G
```
```
[0,T]} obtained using the minimum principle (cf. subsection 5.5.3). If a non-
```
```
optimal open-loop policy W[o,s) is applied to the system in the time interval
```
```
[0,s), which brings the system to a state, say x(s), that is not on the optimal
```
trajectory, then the control wf T, will not necessarily be optimal for the new
truncated control problem Df T1. The reason for this is that u? ^ will no longerI ' J t ' J
satisfy the necessary conditions given in Thm. 5.4 or Thm. 5.5 on the interval
[s,T], as the initial state at t = s is not on the optimal trajectory. Hence,
optimal open-loop control is only WTC. The optimal feedback control, obtained
```
from the dynamic programming equation (5.19) or (5.25),45 however, is STC—a
```
fact that follows from Def. 5.14. For the discrete-time linear-quadratic problem
```
(with Ck = 0 ) , for example, the policy (5.53), which is obtained from a dynamic
```
```
programming equation, is STC, whereas its open-loop version (5.54a) is only
```
```
WTC. For the more general affine-quadratic problem described by (5.20a), the
```
```
optimal feedback policy (5.2la) is STC.
```
In a way, an STC policy is one that is not only independent of the initial
state XQ but also independent of other past values of the state. Not every op-
timal policy that depends only on the current value of the state is necessarily
STC, however, especially if the initial state is a fixed known quantity. To make
```
the point here, consider the optimal feedback policy 7* — {7^, k 6 K} ob-
```
```
tained from (5.19), for a discrete-time system whose initial state x\ is fixed and
```
```
given, under which the optimal trajectory, {x£+1, k e K}, is just a sequence
```
of known vectors. Now, if F is a policy space compatible with the feedback
```
(FB) information pattern, and we seek an optimal solution in the class F, then
```
```
not only 7*, but every 7 € F given by 7fc(£fc) = if)k(xk,x*k), k 6 K, will be
```
an optimal policy, where ipk is anY function satisfying the boundary condition
```
i^k(x*k,x*k} — 7fc(££), k G K. Different choices of ip lead to different represen-
```
```
tations of the same (optimal) policy on the optimal trajectory, and they all (in
```
```
this case) depend only on the current value of the state. Because of this feature,
```
one may be tempted to call all these different representations "optimal feedback
policies", but this would not be correct because only one of them, namely 7*, is
```
STC. The others cannot be obtained directly from (5.19) and do not meet the
```
"permanent optimality" property associated with an STC optimal solution.
```
45Here, the feedback control is obtained by minimization of the right-hand side of (5.19) or
```
```
(5.25), depending on whether we have a discrete-time or a continuous-time problem.
```
```
with the boundary condition being V(K + I,XK+I,XK) = 0. Clearly, an
```
```
STC solution will have to be of the form Uk = ^(xk^Xk-i)- If the information
```
pattern is FB, then the problem may not admit a solution at all.
This concludes our discussion on WTC and STC optimal solutions in one-
person dynamic decision problems. The topic will be revisited later in Chap-
ters 6 and 7, in the context of saddle-point, Nash and Stackelberg equilibria in
dynamic games.
Stochastic systems
We devote the remaining portion of this section to a brief discussion on the
representation of closed-loop strategies in stochastic control problems. In par-
252 T. BA§AR AND G. J. OLSDER
There are two points worth mentioning here in connection with the notion
of an STC optimal policy. The first one is that it is quite possible for a con-
```
stant policy to be STC, which would arise if the optimization problem in (5.19)
```
```
(or (5.25)) admits a solution which is independent of x for every k (or t}. In
```
such a case, the optimal feedback and the optimal open-loop solutions would
coincide, and hence the open-loop solution would also be STC. The second
point is that a given dynamic optimization problem may not admit an STC
optimal solution, even under the CL information pattern. This would arise
if the policy space has additional structural restrictions imposed on the per-
missible policies. For example, one may require the policies to depend only
linearly on the current value of the state, with no such restriction imposed on
their dependence on past history. If the unique optimal FB solution obtained
```
from (5.19) (or (5.25)), i.e., 7*, is not linear in its argument, then the problem
```
would not admit an STC solution. However, the problem would still admit a
```
WTC optimal solution, such as (in the discrete-time case) any policy of the form
```
```
1k(xk,xi) — MkXk+ik(x*k) ~-^fc^fc, k e K, where {Mk} is an arbitrary matrix
```
```
sequence (of compatible dimensions) and {x^ } is the trajectory generated by 7*,
```
in general as a function of x\.
The notions of weak and strong time consistency are of course also valid in
systems which incorporate lag variables, either in the state or in the control.
```
Consider, for example, the system where the evolution is governed by (as a
```
```
counterpart of (5.18a)):
```
```
This could be converted into the form (5.18a) by introducing a new state vector
```
```
£jt which corresponds to the pair (xfc,Xfc-i), thus making our earlier discussion
```
apply here, with Xk replaced by £&. Note that, in view of this, an STC solution
```
will exist for the discrete-time optimal control problem (under the cost function
```
```
(5.18a)) only if one allows the permissible policies at time k to depend not
```
only on x^ but also on Xk-i- This can best be seen if one writes down the
```
counterpart of the dynamic programming (DP) equation (5.19) in terms of the
```
original variables:
```
where 7^ (k 6 K) is a random vector taking values in Rn, and the joint proba-
```
```
bility distribution function of {xi, # 1 , . . . , 9^} is known. Then, given a feedback
```
```
strategy {~fk('), k G K}, it is in general not possible to write down a representa-
```
tion of this strategy that also involves past values of the state, since now there
is no redundancy in the CLPS information, due to existence of the disturbance
vector 9k- In other words, while in deterministic systems Zfc+i can easily be
recovered from x^ if one knows the strategy employed, this is no longer possible
in stochastic systems because £fc+i also contains some information concerning
9k- This is particularly true if the random vectors xi, $1,..., OK are Gaussian,
```
mutually independent and cov (Ok) > O46 for all k e K. Hence, we have the
```
following proposition.
```
Proposition 5.5 //, for the stochastic system (5.59), all random vectors are
```
Gaussian, mutually independent and the covariance of 6k is of full rank for all
k G K, then every equivalence class of representations is a singleton.
Remark 5.11 The reason why we do not allow some random vectors to be
```
statistically dependent and/or cov (Ok) to have lower rank is because then
```
some components of Zfc+i can be expressed in terms of some components of x^,
Zfc_i, etc., independent of 0^, 9k-i, • • • , which clearly gives rise to nonunique
representations of a strategy. Furthermore, we should note that the requirement
of having a Gaussian probability distribution with positive definite covariance
can be replaced by having any probability distribution that assigns positive
probability mass to every nonempty open subset of Rn. The latter requirement
is important because, if any open subset of Rn receives zero probability, then
nonunique representations are possible on that particular open set.
```
Optimal control of stochastic systems described by (5.59) can be performed
```
by following the basic steps of dynamic programming applied to the determinis-
```
tic system (5.18a)-(5.18b). Toward this end, we first replace the cost functional
```
```
(5.18b) by
```
```
where {7fc;fc 6 K} is any permissible closed-loop strategy, and E[-] denotes
```
the expectation operation defined with respect to the underlying statistics of
Zi, #1,..., OK which we assume to be mutually independent. Then, the value
```
function V(fc,z) is defined by
```
```
46 cov (0fe) stands for the covariance matrix of 9k, i.e., E[(0k - E[6k})(9k - E[9k])']
```
GENERAL FORMULATION 253
ticular, we consider, as a special case of Def. 5.4, the class of control problems
described by the state equation
254 T. BA§AR AND G. J. OLSDER
Following the steps of dynamic programming, and by utilizing the assumption
of independence of the random vectors involved, it can readily be shown that
the value function satisfies the recursive relation47
where all terms are defined in Prop. 5.1.
Remark 5.12 The solution of the stochastic linear-quadratic optimal control
```
problem depends only on the mean value (cjt, k € K) of the additive distur-
```
```
bance term but not on its covariance (Afc,fc 6 K).48 Moreover, the solution
```
coincides with the optimal feedback solution of the deterministic version of the
```
problem (as presented in Prop. 5.1), which takes the mean value of the Gaussian
```
disturbance term as a constant known input. Finally, in view of Remark 5.11,
uniqueness of the solution is valid under the assumption that the covariance
```
matrix A^ is positive definite for all A; € K.
```
47Egk denotes the expectation operation with respect to statistics of 0*..
```
48The minimum expected value of the cost function does, however, depend on {A&, k € K}.
```
```
which also leads to the conclusion that the optimal strategy 7^ (k € K) depends
```
only on the current value of the state, i.e., is a feedback strategy. This is
also a strongly time consistent solution under the CLPS information pattern,
```
where the notion (introduced by Def. 5.14) applies equally to stochastic dynamic
```
decision problems, since its definition used strategies, and not actions.
We now apply this result to the so-called linear-quadratic discrete-time
stochastic optimal control problem described by the linear state equation
and the quadratic cost functional
```
where Rk+i > 0, Qk+i > 0; (k e K); 0^ (k € K) are n-dimensional mutually
```
independent Gaussian vectors with mean value c^, and covariance A^ > 0, and
```
7fc (k e K) are permissible closed-loop strategies. For this special case, V(k, x)
```
again turns out to be a general quadratic function in or, and this leads to the
following counterpart of Prop. 5.1.
Proposition 5.6 For the linear-quadratic stochastic control problem described
```
by (5.62a)-(5.62b), and under the closed-loop perfect state information pattern,
```
```
there exists a unique minimizing solution (which is a feedback strategy) given by
```
```
where £ is a (small) positive parameter. The first term of this expression is also
```
written compactly as sAV. In the same way one could add the same term with
```
the second derivatives to (5.66):
```
GENERAL FORMULATION 255
Remark 5.13 Optimal solutions to stochastic control problems can be ob-
tained under other deterministic information patterns also, but they are not
representations of each other under the full-rank condition of the independent
additive noise. Hence, each different information pattern requires a different
```
derivation for the optimal solution. These solutions are not all ST(7, but they
```
are necessarily WTC. See Problem 5 in Section 5.8 for an illustrative example.
5.7 Viscosity Solutions
```
In this section another solution approach to the HJB equation (5.25) will be
```
given. This approach has its origin in the theory of stochastic optimal control
```
theory (in continuous time) as will be explained soon.
```
```
If in (5.25), or in its time-invariant analogue
```
```
the minimizing u, as a function of dV/dx, x (and t for (5.25)), is substituted
```
into the same expressions, the resulting equations will be written formally as
and
respectively. For both equations we assume appropriate boundary conditions.
```
Sometimes finding a solution of (5.65) is referred to as a Cauchy problem and
```
```
of (5.66) as a Dirichlet problem. Note that, strictly speaking, the function H
```
```
in (5.65) and (5.66) does not directly depend on the argument V in our case
```
```
and that the format of (5.65) and (5.66) is more general than strictly necessary.
```
```
Rather than studying (5.65) it pays sometimes to study
```
The theory of partial differential equations tells us that existence and unicity
```
results are easier to obtain for (5.67) than for (5.65). One may hope to get results
```
```
where E stands for the expectation operator (see Appendix B) and where u(t] =
```
```
7(t,xt) 6 5, i.e., only feedback strategies are allowed. The following result from
```
```
Fleming (1969) (see also Fleming and Rishel (1975)) provides a set of sufficient
```
conditions and a characterization for the optimal solution of this stochastic
control problem.
```
Proposition 5.7 Let there exist a suitably smooth solution W(t,x) of the semi-
```
linear parabolic partial differential equation
256 T. BA§AR AND G. J. OLSDER
```
for (5.67) and then study their limits as e tends to zero. Hopefully the solution
```
```
of (5.67) (respectively, (5.68)) will resemble a solution of (5.65) (respectively,
```
```
(5.66)) in some sense if £ J, 0. Quite often solutions to (5.65), (5.66), viewed
```
as abstract mathematical equations, are not unique, among others depending
on the definition of solution that one adopts. With the addition of a proper
understanding of the underlying optimal control problem, the solution becomes
usually unique. Before continuing along these lines, we will first indicate that
```
(5.67) is not only an auxiliary equation, but that it represents a condition for
```
the value function of a related stochastic optimal control problem.
Consider the one-person version of the TV-person stochastic differential equa-
```
tion as described by (5.17b):
```
and the cost functional
where a^ denotes the ijth element of the matrix ad', and Xi denotes the ith
```
component of the vector x. If there is a function u°(t,x,p) that minimizes
```
```
\p'F(t,x,u) + g(t,x,u}} on S, then j*(t,x) — u°(t,x,VxW) is an optimal con-
```
```
trol, with the corresponding expected cost given by J* = W(0, XQ).
```
This proposition yields a partial differential equation which the value func-
tion of the stochastic optimal control problem should satisfy. The resemblance
```
between (5.67) and (5.71) will be obvious. Thus (5.67), for the time-invariant
```
case, can also be considered as a condition which the value function of an ap-
propriately defined stochastic optimal control problem should satisfy. The for-
mulation of the stochastic optimal control problem and the statement of the
proposition have been rather informal. The rigorous underlying mathematics is
beyond the scope of this book and can for instance be found in the references
```
already mentioned and in Lions (1982) and in Crandall and Lions (1983).
```
```
W(T,x) = q(T,x),
```
Those vectors p which satisfy this latter equation are sometimes referred to as
subdifferentials. A similar and somewhat more intuitive definition of a viscosity
supersolution can also be given. The equivalences of these definitions have been
```
shown in Crandall, Evans and Lions (1984).
```
Remark 5.14 It should be noted that, in the definitions of viscosity sub- and
```
supersolutions, in the references cited the inequalities (5.72) and (5.73) are both
```
```
reversed in sign. The definitions given here, however, are consistent with (5.68),
```
in which it is assumed that e is positive. In the references cited, the second-order
```
term in (5.68) is preceded by a minus sign. We have not adopted the addition
```
of this minus sign because of the fact that the second-order term is directly
GENERAL FORMULATION 257
```
We now return to (5.67) and will study its solution when E j 0. To express
```
```
the fact that the solution of (5.67) depends on £, it will be indicated by Ve.
```
```
If lim£j_oV^(x) exists, this limit might be what is called a viscosity solution
```
```
of (5.66). The name "viscosity solution" comes from a celebrated method in
```
fluid dynamics where the term E £V - Q®.QX. represents physically a viscosity.
In optimal control theory, however, the term "viscosity solution" is reserved for
```
a particular solution of (5.66), which may equal the limit solution (with e | 0)
```
```
of (5.68) if it exists. More properly, it then could have been called the vanishing
```
viscosity solution.
Definition 5.15 Suppose that H is continuous in its arguments. A continuous
```
function V(x) is called a viscosity subsolution of (5.66) provided that for all
```
```
continuously differentiate <j>(x), which map Rn or an appropriate subset o/Rn
```
```
into R (as does V(x)), the following holds: if V — 4> attains a local maximum
```
at XQ, then
```
A continuous function V(x) is called a viscosity supersolution of (5.66) pro-
```
```
vided that for all continuously differentiate 0(x), which map Rn or an appro-
```
```
priate subset o/Rn into R (as does V(x)), the following holds: ifV — (j) attains
```
a local minimum at XQ, then
```
A continuous function V(x] is called a viscosity solution of (5.66) if it is both
```
a viscosity subsolution and a viscosity supersolution.
An equivalent, somewhat more intuitive, definition of a viscosity subsolution
```
is as follows. A continuous function V(x) is a viscosity subsolution of (5.66) if
```
```
H(x, V ( x ) , p ) > 0 for all p € Rn such that
```
258 T. BA§AR AND G. J. OLSDER
```
related to the covariance of the noise in the system equations (and hence has
```
```
to be nonnegative). Another way to make the definitions given here and in the
```
references cited fully equivalent is to change H into —H.
The following simple result establishes the consistency of the notions of viscosity
and "classical solutions".
Theorem 5.6 // the continuously differentiable function V is a classical solu-
```
tion of (5.66), then it is a viscosity solution.
```
```
The following theorem relates the solutions of (5.66) and (5.67).
```
```
Theorem 5.7 Suppose that V£ is a continuously differentiable solution of (5.67)
```
and that it converges uniformly, as e \. 0, to some continuous function V. Then
```
V is a viscosity solution of (5.66).
```
```
Proof. Let us check (5.72) first for twice continuously differentiable 4>. As-
```
suming that V — 0 has a local maximum at XQ, a function C is chosen, differen-
```
tiable a sufficient number of times and with 0 < £ < 1 for x ^ XQ and C(^o) = 1?
```
```
such that V — (</> — £) has a strict local maximum at XQ . Thus for e sufficiently
```
```
small, Vs — (0 — £) has a local maximum at some x£ and x£ —» XQ as e j 0. Then,
```
and therefore
```
Now, if we take the limit e j 0, this expression becomes (5.72).
```
Suppose now that </> is only once continuously differentiable. The first part of
the proof, as just given, is now repeated for 0^, where <t>k is twice continuously
differentiable and 4>k —* </> as k —> oo. This concludes the part of the proof that
V is a viscosity subsolution. The fact that V is also a viscosity supersolution can
analogously be shown, which then, together with V being a viscosity subsolution,
concludes the proof.
A weaker version of Thm. 5.7 exists in the sense that the function H may also
depend on £, to be indicated by H£. In the formulation of the extended version
of Thm. 5.7 it is then necessary that H£ converges to H uniformly on compact
subsets of Rn x R x R n , as e [ 0. This weaker version is useful, for instance, when
the continuous-time formulation of the problem is approximated by a discretized
```
one (for numerical purposes, for instance). Such a "discretized version" of H
```
```
could then coincide with He; see Lions (1982), Bardi and Sartori (1992) and
```
```
Pourtallier and Tolwinski (1992).
```
An issue not dealt with heretofore is that of uniqueness of viscosity solutions.
```
Consider (5.67) again. For special forms of the function H the uniqueness has
```
```
been shown (see Lions (1982) or Crandall, Evans and Lions (1984)). In those
```
GENERAL FORMULATION 259
cases it is essential that H depends explicitly on V, which is for instance the
case if H has the following form:
where the nonnegative constant A is the so-called discount factor. Such cost
functions can be treated in the standard way but offer some advantages if T = oo
and the terminal term is not present. Under some mild conditions, see Bardi
```
and Soravia (1991), the following proposition can be shown to hold.
```
```
Proposition 5.8 If V is a viscosity solution of (5.66), where H does not depend
```
```
on the second argument V, and if there exists a constant go such that g ( x , u ) >
```
go > 0, then v is a viscosity solution of
where
```
for some scalar functions a and (3. However, as remarked in the beginning of
```
this section, H as introduced does not depend on V. It is, however, possible to
create such a dependence by means of the so-called Kruzkov-transform of the
value function V. This transform of the value function is defined as
elsewhere,
```
where ijj(V) = I — exp(—V). It is important to realize that v is bounded and is
```
itself the value function of an optimal control problem, viz.,
```
with L defined in (5.23). The transformed cost function ^(L(u)} can be rewrit-
```
ten in the "standard form" of integral and terminal part as
As a side remark, this latter cost function resembles cost functions with discount
factors, which have the form
```
A lower semicontinuous function V is called a viscosity supersolution of (5.66)
```
provided that for all continuously differentiate 4>, which map Rn or an appro-
```
priate subset of Rn into R (as does V), the following holds: ifV — ^> attains a
```
local minimum at XQ, then
5.8 Problems
1. A company has x(t) customers at time t and makes a total profit of
260 T. BA§AR AND G. J. OLSDER
This proposition shows how the original equation H = 0, where H does not
```
depend on the value function V directly, can be transformed into (5.74), which
```
has a linear term in the transformed value function v. For the latter kind of
partial differential equations, it is easier to prove uniqueness of solution. It also
```
offers a starting point for numerical procedures to solve for v (and hence for V).
```
Hitherto in this section, we have taken V to be continuous. For the case in
which it is discontinuous the definition of viscosity solution can be modified as
follows.
Definition 5.16 Suppose that H is continuous in its arguments. An upper
```
semicontinuous function V is called a viscosity subsolution of (5.66) provided
```
that for all continuously differentiable 4>, which map Rn or an appropriate subset
```
of Rn into R (as does V), the following holds: if V — (j) attains a local maximum
```
at XQ, then
```
A function V is called a viscosity solution of (5.66) if the upper and lower
```
semicontinuous envelopes ofV, defined, respectively, by
are respectively a viscosity subsolution and a viscosity supersolution.
For value functions with discontinuities relatively few results are available
```
as yet; see the notes section 5.9.
```
```
up to time t, where c is a positive constant. The function u(t), restricted
```
```
by u(t) > 0, f ( t ) > 0, represents the money put into advertising. The
```
```
restriction f ( t ) > 0 indicates that the company is not allowed to borrow
```
GENERAL FORMULATION 261
money. Advertisement helps to increase the number of customers accord-
ing to
The company wants to maximize the total profit during a given time
period [0,T], where T is fixed. Obtain both the open-loop and feedback
optimal solutions for this problem.
2. Consider the two-dimensional system
```
where the control variable u is scalar and satisfies \u(t)\ < 1. In polar
```
coordinates, the equations of motion become
where a is the angle measured clockwise from the positive X2-axis. The
```
target set A is |a| > a, where 0 < a < TT. (For an interpretation of this
```
system see Example 8.3, later in Chapter 8, with V2 — 0, or Lewin and
```
Olsder (1979).) The decision maker wants the state (£1,2:2) to stay away
```
from A for as long as possible. Determine the time-optimal-control. Show
that, for a > 7T/2, there is a barrier.
3. Consider the following discrete-time, discrete state space optimal control
```
problem. The state space consists of the integers (0,1,2,...}. At time £/-
```
the decision maker steers the system positioned at integer i^ > 1, either to
```
{0} or to the integer {i^ 4-1}, where it arrives at time t^+i- The control
```
```
problem terminates when the system is steered to the origin {0}. The
```
pay-off is completely determined by the last move. If the system moves
```
from {1^} to {0}, then the pay-off is 22lfc, to be maximized by the decision
```
maker.
Show that each playable pure strategy leads to a finite value, and that the
behavioral strategy which dictates both alternatives with equal probabil-
```
ity (^) at each stage leads to an infinite expected pay-off (assuming that
```
```
io > 1).
```
4. Consider the discrete-time system whose state evolves according to the
dynamics
```
(i) Show that every STC optimal policy for this system, under the cost
```
```
function (5.18b) and the information pattern that allows the control
```
at stage k to depend on the current and past values of the state and
```
past values of control, has to be in the form u^ = 7fc(xfe,itjt_i).
```
262 T. BA§AR AND G. J. OLSDER
```
(ii) Obtain the STC solution(s) explicitly for the linear-quadratic (LQ)
```
```
problem where the cost is given by (5.20b), and the system dynamics
```
```
(exhibiting lagged dependence on the control) by
```
```
(iii) Does the LQ problem above admit (a) an STC optimal policy under
```
```
the CLPS information pattern; (b) a WTC optimal policy under the
```
CLPS information pattern? In the latter case obtain one such policy,
if there exists one.
5. In Problem 4 above, replace the deterministic linear dynamics by the
stochastic difference equation
where xi, # 1 , . . . , OK are Gaussian, mutually independent, and have zero
mean and positive definite covariances. Further let the cost function be
```
given by (5.62b).
```
```
(i) Does the problem admit an STC optimal control under the CLPS
```
information pattern?
```
(ii) Obtain a WTC optimal policy for this stochastic control problem
```
under the CLPS information pattern.
```
(iii) Can you generate an optimal OL control policy from the solution ob-
```
tained above? Would it be WTC under the OL information pattern?
5.9 Notes
Section 5.2. As already noted in Remark 5.2, the state-model description of a
```
discrete-time (deterministic or stochastic) infinite dynamic game as presented here is
```
```
clearly not the most general extension of Kuhn's finite game model (cf. Chapters 2 and
```
```
3) to the infinite case. One such extension which has not been covered in Remark 5.2 is
```
the infinite-horizon problem wherein the number of stages K tends to infinity. In such
```
games, the so-called stationary strategies (i.e., strategies which do not depend on the
```
```
discrete-time parameter k) become of real interest, and they have hitherto attracted
```
```
considerable attention in the literature (see Shapley, 1953; Sobel, 1971; Maitra and
```
```
Parthasarathy, 1970), but mostly for finite or at most denumerable state and strategy
```
spaces. The general class of discrete-time stochastic dynamic games under the feedback
information structure are also referred to as Markov games in the literature. For two
```
survey articles on this topic, see Parthasarathy and Stern (1977) and Raghavan and
```
```
Filar (1991).
```
```
Section 5.3. Differential games were first introduced by Isaacs (1954-1956), within
```
the framework of two-person zero-sum games, whose work culminated in the publi-
```
cation of his book about ten years later (see Isaacs, 1975). Nonzero-sum differential
```
```
games were later introduced in the works of Starr and Ho (1969a,b), Case (1967) and
```
GENERAL FORMULATION 263
```
Friedman (1971), but under specific information structures—namely, the open-loop
```
and feedback perfect state information patterns. Some representative references on
```
stochastic differential games, on the other hand, are Friedman (1972), Elliott (1976)
```
```
and Ba§ar (1977b). The book by Krasovskii and Subbotin (1988) has an extensive dis-
```
cussions on non-Lipschitzian differential equations. A class of differential games not
covered by Def. 5.5 are those whose state dynamics are governed by partial differential
```
equations. These will not be treated in this book; for some examples of practical games
```
```
which fit into this framework, the reader is referred to Roxin (1977). Yet another class
```
of differential games not to be covered in subsequent chapters are those on which the
```
players are allowed to employ impulsive controls; see Blaquiere (1977) for some results
```
on this topic.
Section 5.4. Mixed and behavioral strategies in infinite dynamic games were first
```
introduced by Aumann (1964) who also provided a rigorous extension of Kuhn's exten-
```
sive form description of finite games to infinite games, and proved in this context that
if an JV-person nonzero-sum infinite dynamic game is of "perfect recall" for one player,
then that player can restrict himself to behavioral strategies. For a further discussion
of mixed strategies in differential games and for an elaboration on the relation of this
```
concept with that of "relaxed controls" used in control theory (cf. Warga, 1972), the
```
```
reader is referred to Wilson (1977), who also proves the existence of mixed-strategy
```
saddle points in suitably structured zero-sum differential games of fixed duration and
with open-loop information structure. Some other references which deal with exis-
```
tence of mixed-strategy equilibrium solutions in differential games are Elliott (1976),
```
```
Pachter and Yavin (1979), Levine (1979) and Kumar and Shiau (1981).
```
Section 5.5. Some classic texts on the theory of optimal control where the reader
```
can refer to for a more extensive coverage and references are Bryson and Ho (1975),
```
```
Hermes and LaSalle (1969) and Fleming and Rishel (1975).
```
Section 5.6. The concept of "representations of a strategy" was introduced by
```
Ba§arin a series of papers, within the context of infinite dynamic games (see, e.g.,
```
```
Ba§ar, 1974, 1976a, 1977b), and it was shown to be closely related to existence of
```
nonunique Nash equilibria under the CLPS information pattern, as it will be eluci-
dated in Chapter 6. The subsection on time consistency is based on the material in
```
Ba§ar (1989b), which the reader should refer to for extensive discussion on the moti-
```
vation behind these notions, and some specific results. Also, Chapters 6 and 7 contain
further discussion of time consistency in the context of Nash and Stackelberg equi-
```
libria. The issue of time consistency (or inconsistency) has pervaded the economics
```
literature during the past two decades, following the stimulating paper by Kydland
```
and Prescott (1977).
```
```
Section 5.7. Viscosity solutions in the theory of optimal control (and also in the
```
```
theory of zero-sum differential games) became well known with the publication of
```
```
the book by Lions (1982). The theory presented in this section is based mainly on
```
```
Crandall, Evans and Lions (1984). Another reference in the same vein is Crandall
```
```
and Lions (1983). Viscosity solutions for minimum-time problems have been reported
```
```
in Bardi (1989) and Staicu (1989). In all these references it was assumed that the
```
```
value function V is continuous in its arguments, a property not often satisfied (see
```
264 T. BA§AR AND G. J. OLSDER
```
Example 5.2 and also Chapter 8). Recently progress has been made on problems
```
```
with discontinuous value functions, notably for time-optimal control problems; see
```
```
Ishii (1989), Bardi and Soravia (1991) and Bardi and Staicu (1991). The notion of
```
```
discontinuous viscosity solutions was introduced by Ishii (1987). For a more recent
```
rigorous treatment of the topic of viscosity solutions in the context of optimal control,
```
see Fleming and Soner (1993).
```
Chapter 6
Nash and Saddle-Point
Equilibria of Infinite
Dynamic Games
6.1 Introduction
This chapter discusses properties and derivation of Nash and saddle-point equi-
libria in infinite dynamic games of prescribed fixed duration. The analysis is first
confined to dynamic games defined in discrete time, and with a finite number of
stages, and then extended to differential games. Some results for infinite-horizon
formulations are also presented, primarily for affine-quadratic structures.
Utilization of the two standard techniques of optimal control theory, viz.
the minimum principle and dynamic programming, leads to the so-called open-
loop and feedback Nash equilibrium solutions, respectively. These two differ-
```
ent Nash equilibria and their derivation and properties (such as existence and
```
```
uniqueness) are discussed in Section 6.2, and the results are also specialized to
```
affine-quadratic games, as well as to two-person zero-sum games.
When the underlying information structure for at least one player is dy-
```
namic and involves memory, a plethora of (so-called informationally nonunique)
```
Nash equilibria with different sets of cost values exists, whose derivation entails
some rather intricate analysis—not totally based on standard techniques of op-
timal control theory. This derivation, as well as several important features of
Nash equilibria under closed-loop perfect state information pattern, are dis-
cussed in Section 6.3, first within the context of a scalar three-person two-stage
```
game (cf. subsection 6.3.1) and then for general dynamic games in discrete time
```
```
(cf. subsection 6.3.2).
```
Section 6.4 is devoted to derivation of necessary and sufficient conditions for
Nash equilibria in stochastic nonzero-sum dynamic games with deterministic
information patterns. Such a stochastic formulation eliminates informational
265
266 T. BA§AR AND G. J. OLSDER
nommiqueness, thereby making the question of existence of unique Nash equi-
librium under closed-loop perfect state information pattern a meaningful one.
Section 6.5 presents the counterparts of the results of Section 6.2 in the
continuous time, that is, for differential games with fixed duration, and the
```
next one (Section 6.6) discusses some important applications of this theory to
```
```
worst-case controller design (so-called #°°-optimal control), first in continuous
```
and then in discrete time. Finally, Section 6.7 presents the counterparts of the
results of Section 6.4 in the continuous time.
6.2 Open-Loop and Feedback Nash and Saddle-
Point Equilibria for Dynamic Games in
Discrete Time
Within the context of Def. 5.1, consider the class of TV-person discrete-time
```
deterministic infinite dynamic games of prescribed fixed duration (K stages)
```
which are described by the state equation
where Xk 6 X = Rn and x\ is specified a priori. Furthermore, the control sets
```
Uk are taken as measurable subsets of Rmj (Uk C Rmj; k e K,i € N), and a
```
stage-additive cost functional
is given for each player i € N.
In this section, we discuss derivation of Nash equilibria for this class of
```
nonzero-sum games when the information structure of the game is either (i)
```
```
open-loop or (ii) memoryless perfect state for all the players. In the former
```
case, any permissible strategy for Pi at stage k € K is a constant function and
therefore can be considered to be an element of Uk, i.e., FJ. = £/£, k e K, i € N.
Under the latter information structure, however, any permissible strategy for
```
Pi at stage A; £ K is a measurable mapping 7^ : X —> Ul, i € N, k € K. With
```
```
Tlk (k G K, i G N) taken as the appropriate strategy space in each case, we recall
```
```
(from Def. 3.12) that an Af-tuple of strategies {7** 6 P;i G N} constitutes a
```
Nash equilibrium solution if, and only if, the following inequalities are satisfied
```
for all {7* e P j i e N } :
```
NASH AND SADDLE-POINT EQUILIBRIA 267
```
Here, 7* = (7^,..., 7^}—the aggregate strategy of Pi—and the notation 7* G
```
```
P stands for 7^ G F^Vfe G K. Moreover, Jz (71 ,... ,7W) is equivalent to
```
```
L^u1 ,... ,UN ) with 4 replaced by 7J.(-) (i G N,/c G K).
```
Under the open-loop information structure, we refer to the Nash solution as
"open-loop Nash equilibrium solution", which we discuss in the first subsection
to follow. For the memoryless perfect state information, the Nash solution
will be referred to as "closed-loop no-memory Nash equilibrium solution", and
under a further restriction, which is the counterpart of Def. 3.22 in the present
framework. It will be called "feedback Nash equilibrium solution". Both types
of equilibria will be discussed in subsection 6.2.2.
6.2.1 Open-loop Nash equilibria
```
One method of obtaining the open-loop Nash equilibrium solution(s) of the class
```
of discrete-time games formulated above is to view them as static infinite games
and directly apply the analysis of Chapter 4. Toward this end we first note that,
```
by backward recursive substitution of (6.1) into (6.2), it is possible to express Ll
```
```
solely as functions of {uj; j G N} and the initial state x\ whose value is known
```
```
a priori, where uj is defined as the aggregate control vector (u\_ ,uJ2 , • • • ^u3K)'.
```
```
This then implies that, to every given set of functions {fki9k',k G K}, there
```
corresponds a unique function Ll : X x f/1 x • • • x UN —>• R, which is the
cost functional of Pi, i G N. Here, U^ denotes the aggregate control set of
Pj>, compatible with the requirement that if u-7 G f/-7 then ujk G C/j!, V/c G
K, and the foregoing construction leads to a normal form description of the
original game, which is no different from the class of infinite games treated in
Chapter 4. Therefore, to obtain the open-loop Nash equilibria, we simply have
```
to minimize LI(XI, w 1 , . . . ,u*~1, -, ui + 1 ,..., UN) over Ul, for each i G N, and
```
```
then determine the intersection point(s) of the resulting reaction curves. In
```
```
particular, if Ll(x\, u 1 , . . . , UN) is continuous on U1 x • • • x UN, strictly convex
```
in w*, and further if Ul are closed, bounded and convex, an open-loop Nash
```
equilibrium (in pure strategies) exists (cf. Thm. 4.3).
```
Such an approach can sometimes lead to quite unwieldy expressions, espe-
cially if the number of stages in the game is large. An alternative derivation
which partly removes this difficulty is the one that utilizes techniques of optimal
control theory, by making explicit use of the stage-additive nature of the cost
```
functionals (6.2) and the specific structure of the extensive form description
```
```
of the game, as provided by the state equation (6.1). There is in fact a close
```
relationship between derivation of open-loop Nash equilibria and the problem
```
of solving (jointly) N optimal control problems, which can readily be observed
```
```
from inequalities (6.3) since each one of them (together with (6.1) and (6.2))
```
describes an optimal control problem whose structure is not affected by the re-
maining players' control vectors. Exploring this relationship a little further, we
arrive at the following result.
Theorem 6.1 For an N-person discrete-time infinite dynamic game, let
268 T. BA§AR AND G. J. OLSDER
```
(i) /fc(-,u£,... ,wj^) be continuously differentiable on Rn, (A; £ K),
```
```
(ii) g*k(-, u\,... , uj^, •) be continuously differentiable on Rn x Rn, (k e K,
```
```
t e N ) ,
```
```
(Hi) /fc(-, - , . . . , - ) &e convex on Rn x Rmi x • • • Rm", (jfc e K).
```
```
T/ien, «/{7**(#i) = u1*; i € N} provides an open-loop Nash equilibrium solution
```
```
and {Xk+1;k € K} is the corresponding state trajectory, there exists a finite
```
```
sequence of n-dimensional (costate) vectors {pl2,... ,pl^+i} for each i € N such
```
that the following relations are satisfied:
where
Every such Nash equilibrium solution is weakly time consistent.
```
Proof. Consider the zth inequality of (6.3), which says that 7**(xi) =
```
```
ul* minimizes Ll(u1*,..., w*"1*, w*, ul+1*,..., UN*} over Ul subject to the state
```
equation
```
This is a standard optimal control problem for Pi since w-7'* (j G K,j ^ i)
```
are open-loop controls and hence do not depend on u\ The result, then,
follows directly from the minimum principle for discrete-time control systems
```
(cf. Thm. 5.5). The "weak time consistency" of the solution is a direct conse-
```
quence of Def. 5.13, where the "sol" operation is the Nash solution.
```
Theorem 6.1 thus provides a set of necessary conditions (solvability of a set of
```
```
coupled two-point boundary value problems) for the open-loop Nash solution to
```
```
satisfy; in other words, it produces candidate equilibrium solutions. In principle,
```
one has to determine all solutions of this set of equations and further investigate
```
which of these candidate solutions satisfy the original set of inequalities (6.3).
```
```
If some further restrictions are imposed on /& and glk (i G N, k G K) so that
```
where Ak, Bk, Qlk+l Rl£ are matrices of appropriate dimensions, Qlk+i is sym-
metric, Rl£ > 0, Cfc e Rn is a fixed vector sequence, and k £ K, i 6 N.
An affine-quadratic game is of the linear-quadratic type if ck ~ 0.
Theorem 6.2 For an N-person affine-quadratic dynamic game with Q\+i >
```
0 (i 6 N,fc € K), let Afc, Mlk+l (k e K,i € N) be matrices of appropriate
```
dimensions, defined by
```
If the matrices Afc (k € K), thus recursively defined, are invertible, the game
```
admits a unique open-loop Nash equilibrium solution given by
49The stagewise cost functions g\ can also be taken to depend on Xfc instead of ifc+i, as in
```
the proof of Prop. 5.1; but we prefer here the present structure (without any loss of generality)
```
for convenience in the analysis to follow.
NASH AND SADDLE-POINT EQUILIBRIA 269
```
the resulting cost functional U (defined earlier, in this subsection) is convex in
```
ul for all ui e U^, j ^ i, j € K, the latter phase of verification can clearly be
```
eliminated, since then every solution set of (6.4a)-(6.4c) constitutes an open-
```
loop Nash equilibrium solution. A specific class of problems for which this
can be done, and the conditions involved expressed explicitly in terms of the
parameters of the game, is the class of so-called "affine-quadratic" games which
we first formally introduce below.
Definition 6.1 An N-person discrete-time infinite dynamic game is of the
```
affine-quadratic type if Ulk = R™ (i € N, fe € K), and 49
```
```
where {x^+l;k € K} is the associated state trajectory determined from
```
and ££, r/k are defined by
```
which is precisely (6.7b) for k = K. Substitution of this relation into (iv) then
```
```
leads to (6.7a) for k = K.
```
270 T. BA§AR AND G. J. OLSDER
with mj*. recursively generated by
```
Proof. Since Qk+i > 0, and R% > 0, Lt(xi,u1,..., UN) is a strictly convex
```
```
function of u* for all uj € RmjK (j ^ t, j e N) and for all xi € Rn. Therefore,
```
```
every solution set of (6.4a)-(6.4c) provides an open-loop Nash solution. Hence,
```
```
the proof will be completed if we can show that (6.7a) is the only candidate
```
solution. First note that
and since Q\+i > 0, R% > 0, minimization of this "Hamiltonian" over ulk G Rmi
yields the unique relation
where
```
Furthermore, the costate (difference) equation in (6.4c) reads
```
```
Let us start with k = K in which case (i) becomes
```
and if both sides are first premultiplied by B1K and then summed over i € N we
```
obtain, by also making use of (ii) and (6.6a),
```
which further yields the unique relation
This then closes the induction argument, and thereby completes the proof of
the theorem.
Remark 6.1 An alternative derivation for the open-loop Nash equilibrium so-
```
lution of the affine-quadratic game is (as discussed earlier in this subsection
```
```
in a general context) to convert it into a standard static quadratic game and
```
```
then to make use of the available results on such games (cf. Prop. 4.6). By
```
backward recursive substitution of the state vector from the state equation into
the quadratic cost functionals, it is possible to bring the cost functional of Pi
```
into the structural form as given by (4.23), which further is strictly convex in
```
```
ul — (u\ , . . . , ulK}' because of assumptions Qlk > 0, R% > 0, (k € K).50 Conse-
```
quently, each player has a unique reaction curve, and the condition for existence
of a unique Nash solution becomes equivalent to the condition for unique in-
```
tersection of these reaction curves (cf. Prop. 4.6).51 The existence condition of
```
```
Thm. 6.2, i.e., nonsingularity of A^, (k € K), is precisely that condition, but
```
```
expressed in a different (more convenient, recursive) form. As in Prop. 4.6, it is
```
of course possible for an affine-quadratic dynamic game to have multiple Nash
equilibria, which would happen if A^ is singular for some /c, which corresponds
```
to R being singular in (4.26a), and r being in its range space. For yet another
```
derivation of the result of Thm. 6.2, by making use of Prop. 5.1, the reader is
referred to Problem 1, Section 6.8.
```
50The condition Qlk+l > 0 is clearly sufficient (along with Rl£ > 0) to make Ll strictly
```
```
convex in u1, but is by no means necessary. It can be replaced by weaker conditions (which
```
```
ensure convexity) under which the statements of Thm. 6.2 and this remark are still valid. One
```
such condition, that is in fact tight, is given in Lemma 6.1 later, which should be interpreted
for the present context.51
For a derivation of open-loop Nash solution in two-person linear-quadratic games, and
```
along these lines, the reader is referred to Ba§ar(1976a) and Olsder (1977a).
```
NASH AND SADDLE-POINT EQUILIBRIA 271
```
We now prove by induction that the unique solution set of (i)-(iii) is given
```
```
by (6.7a)-(6.7b) and p[ - Afc[M^+1x^+1 + m£+1] (i e N, k € K). Let us assume
```
```
that this is true for k — I + 1 (already verified for / = K — 1) and prove its
```
validity for k = I.
First, using the solution p\+1 = ^/+i[-M/+2x/+2 + ml-\-2\ m W w*th k •= I,
we obtain, after several algebraic manipulations,
Again, premultiplying this expression by B\, and summing it over i e N leads
```
to, also in view of (ii), (6.6a) and (6.7d),
```
```
which is (6.7b). If this relation is used in (v), we obtain the unique control
```
```
vectors (6.7a), for k = /, and if it is further used in (iii) we obtain, in view of
```
```
(6.7e),
```
where J = J1 = —J2. Directly applying Thm. 6.1 in the present context, we
```
first have from (6.4a)-(6.4c) (in view of (6.8a)) the relation p], = —p\ = pk,
```
```
(k € K), and therefore from (6.4d), Hfc = -Hi = Hk, (k € K). Hence, we have
```
arrived at the following conclusion.
Theorem 6.3 For a two-person zero-sum discrete-time infinite dynamic game,
let
```
(i) fk(-,Ufr,ufy be continuously differentiate on Rn, (A: € K),
```
```
(ii) 9k('i w Lu fe'') be continuously differentiate on Rn x Rn, (k € K),
```
```
(Hi) f k ( - , •, •) be convex on Rn x U1 x U2, (k e K).
```
```
Then, if'{71*(xi) = ul*;i = 1,2} provides an open-loop saddle-point solution,
```
```
and (zjt+i; k € K} is the corresponding state trajectory, there exists a finite se-
```
```
quence of n-dimensional (costate) vectors {p2,... ,PK+I} such that the following
```
relations are satisfied:
272 T. BA§AR AND G. J. OLSDER
Zero-sum dynamic games
We now turn our attention to another special class of nonzero-sum dynamic
```
games—the two-person zero-sum games—in which case (assuming, in accor-
```
dance with our earlier convention, that PI is the minimizer and P2 is the
```
maximizer),
```
```
and the "Nash" inequalities (6.3) reduce to the saddle-point inequality
```
where
which PI wishes to minimize and P2 attempts to maximize. It should be
```
noted that we have taken the weighting matrices for the controls in (6.10b) as
```
unity, without any loss of generality, since otherwise they can be absorbed in
```
BjJ; and B\ provided, of course, that R^1 > 0 and R2.2 > 0 which was our a
```
```
priori assumption in Def. 6.1. Let us also assume that Qk+i > 0 (A: € K)
```
```
which essentially makes L(ul,u2) strictly convex in u1. In order to formulate a
```
```
meaningful problem, we also have to require L(w1 ,u2 ) to be (strictly) concave
```
in w2, since otherwise P2 can make the value of L unboundedly large. The
```
following lemma now provides a necessary and sufficient condition for (6.10b)
```
to be strictly concave in u2.
Lemma 6.1 For the affine-quadratic two-person zero-sum dynamic game in-
```
troduced above, the objective functional L(ul,u2} is strictly concave in u2 (for
```
```
all u1 e RKmi) if, and only if,
```
NASH AND SADDLE-POINT EQUILIBRIA 273
Proof. The result follows from Thm. 6.1, as discussed earlier.
As a specific application of this theorem, we now consider the class of affine-
```
quadratic two-person zero-sum dynamic games (cf. Def. 6.1) described by the
```
state equation
and the objective functional
where Sk is given by
```
Proof. Since L(ul,u2) is a quadratic function of u2, the requirement of strict
```
concavity is equivalent to existence of a unique solution to the optimal control
problem
```
subject to (6.10a) and for each u1 G 'RKmi. Furthermore, since the Hessian
```
```
matrix (matrix of second partials) of L with respect of u2 is independent of w1
```
as well as of c, we can instead consider the optimal control problemwith rrik generated by the difference equation
274 T. BA§AR AND G. J. OLSDER
subject to
that is, we can take u], = 0,Cfc = 0 without any loss of generality. Then,
the result follows from the dynamic programming technique outlined in sub-
```
section 5.5.1, and in particular from (5.19), which admits the unique solution
```
```
V(k,x) = —x'SkX + x'QkX, if, and only if, (G.lla) holds.
```
We are now in a position to present the open-loop saddle-point solution of
the affine-quadratic two-person zero-sum dynamic game.
Theorem 6.4 For the two-person affine-quadratic zero-sum dynamic game de-
```
scribed by (6.10a)-(6.10b) and with Qk+i > 0(fc € K), let condition (6.1 la) be
```
```
satisfied and Ak, Mk (k € K) be matrices of appropriate dimensions, defined
```
through
Then,
```
(i) the matrices A& (A; G K), thus recursively defined, are invertible,
```
```
(ii) the game admits a unique open-loop saddle-point solution given by
```
```
where {x'k+1; k e K} is the corresponding state trajectory determined from
```
and £fc,f7fc are given by
```
which verifies (6.13c). When this is used in (iv), it leads to (6.13a)-(6.13b), also
```
```
in view of (6.13d)-(6.13f), thus completing the proof of the theorem.
```
Remark 6.2 The statement of Thm. 6.4 is valid even if the "nonnegative def-
initeness" condition on Qk+i does not hold, provided that some other appro-
priate conditions are imposed on the game parameters to ensure that L defined
```
by (6.10b) is strictly convex in ul. One such set of conditions, which are in
```
fact tight, can be obtained directly from Lemma 6.1, by simply replacing Qk by
-Qk, and B2 by B1, that is,
NASH AND SADDLE-POINT EQUILIBRIA 275
Proof. Let us first note that, with the open-loop information structure and
```
under condition (6.11a) (cf. Lemma 6.1), the game is a static strictly convex-
```
concave quadratic zero-sum game which admits a unique saddle-point solution
by Corollary 4.5. Second, it follows from Thm. 6.3 that this unique saddle-point
```
solution should satisfy relations (6.9a)-(6.9d), which can be rewritten, for the
```
afnne-quadratic game, as follows:
An inductive argument, as in the proof of Thm. 6.2, shows that this set of
equations admits the solution:
where the corresponding value of the state vector, x*k, satisfies
Since there exists a unique saddle-point solution, there necessarily exists a
unique relationship between x£ and £j£+1, implying that the matrix Afc should
be invertible for each k e K. Hence,
where S^ is generated by
276 T. BA§AR AND G. J. OLSDER
6.2.2 Closed-loop no-memory and feedback Nash
equilibria
We now turn our attention to a discussion and derivation of Nash equilibria un-
der the memoryless perfect state information pattern which provides the play-
```
ers with only the current value of the state at every stage of the game (and
```
```
of course also the initial value of the state, which is known a priori). For this
```
class of dynamic games we first obtain the following counterpart of Thm. 6.1,
which provides a set of necessary conditions for any closed-loop no-memory
Nash equilibrium solution to satisfy:
Theorem 6.5 For an N-person discrete-time infinite dynamic game, let
```
(i) fk(-) be continuously differentmble on Rn x U% x • • • x U^, (k 6 K),
```
```
(ii) <7fc(-) be continuously differentiate on Rn x U£ x • • • x U^ x Rn, (k e K, i G
```
```
N).
```
```
Then, if (7^(0:^,0:1) = ul£; k e K, i e N} provides a closed-loop no-memory
```
```
Nash equilibrium solution such that 7£*(-,£i) is continuously differentiate on
```
```
Rn for all k € K, i € N, fk (xk, u1, ... , 7^(0:^,3:1), ul+1, ... , UN) is convex in
```
```
(xfc, it1 ,..., uz-1, ul+1,... ,UN) for every i G N, k € K, and if {x^,+1;k e K} is
```
the corresponding state trajectory, there exists a finite sequence of n-dimensional
```
(costate) vectors {p^,... ,p^+1}, for each i € N such that the following relations
```
are satisfied:
where
NASH AND SADDLE-POINT EQUILIBRIA 277
and a similar convention applies to
Proof. The proof is similar to that of Thm. 6.1, but here we also have to take
```
into account the possible dependence of uj£" on Xk (k e K,i G N). Accordingly,
```
```
the zth inequality of (6.3) now says that (7^* (xk,x\};k G K} minimizes the
```
function
over F* subject to the state equation constraint
Then, the result follows directly from Thm. 5.5 by taking /^ as fk and <?& as
g\ which are continuously differentiable in their relevant arguments because of
the hypothesis that the equilibrium strategies are continuously differentiable in
their arguments.
```
If the set of relations (6.15a)-(6.15d) is compared with (6.4a)-(6.4d), it will
```
be seen that they are identical, except for the costate equations—the latter
```
equation (6.15c) having two additional terms which are due to the dynamic na-
```
ture of the information structure, allowing the players to utilize current values
of the state. Furthermore, it is important to notice that every solution of the
```
set (6.4a)-(6.4c) also satisfies relations (6.15a)-(6.15c), since every such solu-
```
tion is associated with static information, thereby causing the last two terms
```
in (6.15c) to drop out. But, the "open-loop" solution is not the only one that
```
```
the set (6.15a)-(6.15c) admits; surely it could have other solutions which ex-
```
plicitly depend on the current value of the state, thus leading to nonunique
```
Nash equilibria, all of which are weakly time consistent (a la Def. 5.13). This
```
phenomenon of multiplicity of Nash equilibria is closely related to the "informa-
tional nonuniqueness" feature of Nash equilibria under dynamic information, as
```
introduced in Section 3.5 (see, in particular, Prop. 3.10 and Remark 3.15), whose
```
counterpart in infinite games will be thoroughly discussed in the next section.
Here, we deal with a more restrictive class of Nash equilibrium solutions un-
der memoryless perfect state information pattern—the so-called feedback Nash
```
equilibrium—which is devoid of any "informational nonuniqueness" (see also
```
```
Def. 3.22) and is also strongly time consistent (cf. Def. 5.14).
```
278 T. BA§AR AND G. J. OLSDER
Definition 6.2 For an N-person K-stage discrete-time infinite dynamic game
with memoryless perfect state information pattern?2 let Jl denote the cost func-
```
tional of Pi (i G N) defined on F1 x • • • x TN. An N-tuple of strategies
```
```
{7** € Fl; i € N} constitutes a feedback Nash equilibrium solution if it sat-
```
```
isfies the set of K N-tuple inequalities (3.28) for all jlk € r{, i 6 N, k e K.
```
Proposition 6.1 Under the memoryless perfect state information pattern, ev-
ery feedback Nash equilibrium solution of an N-person discrete-time infinite
```
dynamic game is a closed-loop no-memory Nash equilibrium solution (but not
```
```
vice versa).
```
Proof. This result is the counterpart of Prop. 3.9 in the present framework
and therefore its proof parallels that of Prop. 3.9. The result basically follows
by showing that, for each i € N, the collection of all the ith inequalities of the
```
K N-tup\es imply the ith inequality of (6.3).
```
Definition of the feedback Nash equilibrium solution directly leads to a recur-
sive derivation which involves solutions of static Af-person nonzero-sum games
at every stage of the dynamic game. Again, a direct consequence of Def. 6.2
is that the feedback equilibrium solution depends only on Xk at stage &, and
dependence on x\ is only at stage k = I.53 By utilizing these properties, we
readily arrive at the following theorem.
Theorem 6.6 For an N-person discrete-time infinite dynamic game, the set of
```
strategies {jl*(xk}', k 6 K, i € N} provides a feedback Nash equilibrium solution
```
```
if, and only if, there exist functions Vl(k, -):Rn —> R, k 6 K,i £ N, such that
```
the following recursive relations are satisfied:
52The statement of this definition remains valid if the information pattern is instead "closed-
loop perfect state".53
This statement is valid also under the "closed-loop perfect state" information pattern.
Note that the feedback equilibrium solution retains its equilibrium property also under the
feedback information pattern.
where
Every such equilibrium solution is strongly time consistent, and the correspond-
```
ing Nash equilibrium cost for Pi is Vl(l,xi).
```
```
Proof. Let us start with the first set of N inequalities of (3.28). Since they
```
have to hold true for all 7^ € Fjr., i 6 N, k < K — 1, this necessarily implies
```
and the Nash solution has to be valid for all XK-I G XK-I (where XK-I is the
```
```
counterpart of XK at stage k = K — 1). Here again, we observe that the Nash
```
```
equilibrium controls can only be functions of X K - I , and (6.16) with k = K — 1
```
```
provides a set of necessary and sufficient conditions for {iytKi_1(xK-i)'-i € N}
```
to solve this static Nash game. The theorem then follows from a standard
induction argument. Note that the "strong time consistency" property of the
feedback Nash equilibrium, and the expression for the corresponding cost for
each player, are direct consequences of the recursive nature of the construction
of the solution.
The following corollary, which is the counterpart of Thm. 6.2 in the case of
feedback Nash equilibrium, now follows as a special case of Thm. 6.6.
```
PRELIMINARY NOTATION FOR COROLLARY 6.1. Let Pi (i e N,fc e K) be
```
matrices of appropriate dimensions, satisfying the set of linear matrix equations
and
NASH AND SADDLE-POINT EQUILIBRIA 279
that they have to hold true for all values of state xk which are reachable by
utilization of some combination of these strategies. Let us denote that subset
```
of Rn by XK- Then, the first set of inequalities of (3.28) becomes equivalent
```
to the problem of seeking Nash equilibria of an JV-person static game with cost
functionals
```
which should be valid for all XK G XK- This is precisely what (6.16) says for k =
```
```
K, with a set of associated Nash equilibrium controls denoted by {7}^(#/<:);* G
```
```
N} since they depend explicitly on XK G XK, but not on the past values of
```
```
the state (including the initial state xi). Now, with these strategies substituted
```
```
into (i), a similar argument (as above) leads to the conclusion that the second
```
```
set of inequalities of (3.28) defines a static Nash game with cost functionals
```
where
```
where Zlk (i € N) are obtained recursively from
```
280 T. BA§AR AND G. J. OLSDER
```
Furthermore, let a]. € Rmi (i € N, k <E K) be vectors satisfying the set of linear
```
```
equations:
```
```
Corollary 6.1 An N-person affine-quadratic dynamic game (cf. Def. 6.1) with
```
```
Qk+i > 0 (z e N, A; e K) and R% > 0 (i, j € N,j ^ i, k € K) admits a unique
```
```
feedback Nash equilibrium solution if, and only if, (6.17a) and (6.17d) admit
```
```
unique solution sets {P^*;i e N, k € K} and {a^]i € N, A; € K}; respectively,
```
in which case the equilibrium strategies are given by
```
Proof. Starting with k = K in the recursive equation (6.16), we first note
```
```
that the functional to be minimized (for each i e N) is strictly convex, since
```
RtK+BlKQlK+1BlK > 0. Then, the first-order necessary conditions for minimiza-
```
tion are also sufficient and therefore we have (by differentiation) the unique set
```
of equations
which readily leads to the conclusion that any set of Nash equilibrium strategies
```
at stage k = K has to be affine in XK- Therefore, by substituting 7)^ =
```
```
—P1KXK — &IK (* ^ N) into the foregoing equation, and by requiring it to be
```
```
where Q. (i 6 N) are obtained recursively from
```
and
```
Finally, let nlk 6 R (i € N, k e K) be generated by
```
and the corresponding feedback Nash equilibrium cost for each player is
NASH AND SADDLE-POINT EQUILIBRIA 281
```
satisfied for all possible x^, we arrive at (6.17a) and (6.17d) for k = K. Further
```
```
substitution of this solution into (6.16) for k = K leads to Vl(K, x] = ^x'(ZlK —
```
```
QlK}x + C^x + UK] that is, Vl(K, •) has a quadratic structure at stage k = K.
```
```
Now, if this expression is substituted into (6.16) with A; = K — 1, and the
```
```
outlined procedure is carried out for k — K — 1, and this so (recursively) for all
```
k < K — 1, one arrives at the conclusions that
```
(i) Vl(k,x} = ^x'(Zf, ~Qlk)x + £f, x + Hfc is the unique solution of the recursive
```
```
equation (6.16) under the hypothesis of the corollary and by noting that
```
```
Z\. >0(i e N , f c € K ) , and
```
```
(ii) the minimization operation in (6.16) leads to the unique solution (6.18a)
```
```
under the condition of unique solvability of (6.17a) and (6.17d).
```
```
The expression for the cost, (6.18b), follows directly from the expression de-
```
```
rived for the "cost-to-go" Vl(k,x}. This, then, completes verification of Corol-
```
lary 6.1.
Remark 6.3 The result of Corollary 6.1 as well as the verification given above
extends readily to more general affine-quadratic dynamic games where the cost
functions of the players contain additional terms that are linear in x/t, that is,
```
with gl in (6.5b) replaced by
```
```
where llk+i (k € K) is a known sequence of n-dimensional vectors for each
```
i € N. Then, the statement of Corollary 6.1 remains intact, with only the
```
equation (6.17e) that generates Q now reading
```
and the cost-to-go functions admitting the compatibly modified form
Remark 6.4 The "nonnegative-definiteness" requirements imposed on Qk+i
```
and Rl£ (i,j 6 N, j ^ i] k G K) are sufficient for the functionals to be minimized
```
```
in (6.16) to be strictly convex, but they are by no means necessary. A set of
```
```
less stringent (but more indirect) conditions would be
```
under which the statement of Corollary 6.1 still remains valid. Furthermore, it
```
follows from the proof of Corollary 6.1 that, if (6.17a) admits more than one set
```
of solutions, every such set constitutes a feedback Nash equilibrium solution,
which is also strongly time consistent.
Every such saddle-point solution is strongly time consistent, and the unique
```
saddle-point value of the game is V(\,x{).
```
```
Proof. The recursive equation (6.19) follows from (6.16) by taking N = 2,
```
```
g\ = —g\ = Qk (k € K), since then V1 = — V2 = V and existence of a saddle
```
point is equivalent to interchangeability of the min-max operations.
For the further special case of an affine-quadratic zero-sum game, the so-
```
lution of (6.19) as well as the feedback saddle-point solution can be explicitly
```
determined in a simple structural form, but after a rather lengthy derivation.
We accomplish this in two steps: first we obtain directly the special case of
```
Corollary 6.1 for the affine-quadratic zero-sum game (see Corollary 6.3 below),
```
and second we simplify these expressions further so as to bring them into a form
```
compatible with the results of Thm. 6.4 (see Thm. 6.7 in the sequel).
```
Corollary 6.3 For the two-person affine-quadratic zero-sum dynamic game de-
```
scribed by (6.10a)-(6.10b), the unique solutions of (6.17a) and (6.17d) are given
```
by
282 T. BA§AR AND G. J. OLSDER
Remark 6.5 It is possible to give a precise condition for the unique solvability
```
of the sets of equations (6.17a) and (6.17d) for PJ. and a{ (i € N,fc € K),
```
```
respectively. The said condition (which is the same for both) is the invertibility
```
of matrices $£, k € K, which are composed of block matrices, with the iith block
given as R% + B^Z^B^ and the ijth block as B^Zl+1B3k, where i,j e N,
J ^ i-
Zero-sum dynamic games
We now consider the general class of two-person discrete-time zero-sum dynamic
games and determine the "cost-to-go" equation associated with the feedback
saddle-point solution, as a special case of Thm. 6.6.
Corollary 6.2 For a two-person discrete-time zero-sum dynamic game, the set
```
of strategies {7^*(x/fc); k G K, i = 1,2} provides a feedback saddle-point solution
```
```
if, and only if, there exist functions V(k, •) : Rn —> R, k e K, such that the
```
following recursive relation is satisfied:
```
54Verification of (6.22b) requires some algebraic manipulations.
```
NASH AND SADDLE-POINT EQUILIBRIA 283
where
and Zk — — Z^. — Z^ and Cfc — ~Cfc — Cfc satisfy the recursive equations
and
Furthermore, the set of conditions of Remark 6.4 is equivalent to
Proof. By letting N = 2, Qlk+l = -Q2k+l = Qfc+1, Rlkl = -Rf = /, Rf =
—R]^ = I in Corollary 6.1, we first observe that Zk = — Zk and the equations
```
for Zk and (^ are the same as (6.22a) and (6.22b),54 respectively, assuming of
```
course that P^'s and aj>.'s in the these equations are correspondingly the same—
```
a property which we now verify. Toward this end, start with (6.17a) when i = 2,
```
```
solve for Pk terms of Pk (this solution is unique under (6.23b)), substitute this
```
```
into (6.17a) when i = 1 and solve for P^1 from this linear matrix equation.
```
```
The result is (6.20a), assuming this time that the matrix [/ + K^Zk+iBl] is
```
invertible. Furthermore, by following the same procedure with indices 1 and
```
2 interchanged, we arrive at (6.20b), on account of the invertibility of [/ -
```
```
KkZk+iBk}. Repeating the same steps for a], and o^, this time by working with
```
```
(6.17d), we arrive at (6.20c)-(6.20d), again under the same matrix invertibility
```
284 T. BA§AR AND G. J. OLSDER
```
conditions. Therefore, for this special case, PJ. and P% given by (6.20a)-(6.20b),
```
```
and ajj. and a\ given by (6.20c)-(6.20d), are indeed the unique solutions of
```
```
(6.17a) and (6.17d), respectively, provided that the two matrices in question
```
are invertible. A direct manipulation on these matrices actually establishes
```
nonsingularity under conditions (6.23a)-(6.23b); however, we choose here to
```
accomplish this by employing an indirect method which is more illuminating.
```
First note that (6.23a) and (6.23b) correspond to the existence conditions
```
given in Remark 6.4, and therefore they make the functionals to be minimized
```
in (6.16) strictly convex in the relevant control variables. But, for the zero-sum
```
```
game, (6.16) is equivalent to (6.19), arid consequently (6.23a) and (6.23b) make
```
```
the kernel in (6.19) strictly convex in u], and strictly concave in u\ (k G K).
```
```
Since every strictly convex-concave quadratic static game admits a (unique)
```
```
saddle point (cf. Corollary 4.5), the sets (6.17a) and (6.17d) have to admit
```
```
unique solutions for the specific problem (zero-sum game) under consideration.
```
```
Hence, the required inverses in (6.21a) and (6.21b) should exist. This, then,
```
completes the proof of Corollary 6.3.
```
To further simplify (6.20a)-(6.22b), we now make use of the following matrix
```
identity.
```
Lemma 6.2 Let Z = Z' and B be matrices of dimensions (n x n) and (n x m],
```
respectively, and with the further property that B'ZB does not have any unity
eigenvalues. Then,
Proof. First note that the matrix inverse on the RHS of this identity exists,
under the hypothesis of the lemma, since nonzero eigenvalues of B'ZB and
```
ZBB' are the same (see Marcus and Mine, 1964, p. 24). Then, the result
```
follows by multiplying both sides of the identity by In — ZBB' from the right
and by straightforward manipulations.
```
Application of this identity to (6.21a) and (6.21b) readily yields (by identi-
```
fying B\ with B and Zk+i with Z in the former, and B\ with B and — Z^+i
```
with Z in the latter)55
```
```
Furthermore, if these are substituted into (6.20a)-(6.20d), some extensive, but
```
straightforward, matrix manipulations which involve repeated application of
```
(6.24) lead to the expressions
```
55Here we suppress the dimensions of the identity matrices since they are clear from the
context.
```
Now, finally, if (6.25a)-(6.25b) are substituted into (6.22a), we arrive at the
```
```
conclusion that Zk satisfies the same equation as M&, given by (6.12b), and
```
```
hence Fk = AjT1^, and furthermore Cfc generated by (6.22b) is identical with
```
```
rrik generated by (6.13f). Therefore, we have the following.
```
Theorem 6.7 The two-person affine-quadratic zero-sum dynamic game described
```
by (6.10a)-(6.10b) admits a unique feedback saddle-point solution if, and only
```
if,
```
and Mfc+i, A^ (k G K) are as defined in Thm. 6.4- The corresponding unique
```
```
state trajectory {x*k+l;k G K} satisfies the difference equation
```
and Ffc, given in Corollary 6.3, is then expressed as
NASH AND SADDLE-POINT EQUILIBRIA 285
```
where 77^ (k G K) is as defined in Thm. 6.4, with rrik+i replaced by the equivalent
```
vector Oc+i- The corresponding saddle-point value is
Furthermore, ifQk+i > 0 Vfc G K, then M^+i > 0 Vfc G K, and hence condition
```
(6.26a) becomes superfluous.
```
in which case the unique equilibrium strategies, which are also strongly time
consistent, are given by
where
286 T. BA§AR AND G. J. OLSDER
Proof. This result, with the exception of the last statement, follows from
Corollary 6.1, Corollary 6.3 and the discussion given prior to the statement of
the theorem. To prove the last statement, take Ck = 0, without any loss of
generality, and note that with Qk+i > 0 the lower value of any stage-truncated
```
(from below) version of the game is rionnegative (choose, e.g., u\ = 0 ) , and
```
hence the feedback saddle-point value of each such truncated game, which is
^xkMkXk, is nonnegative for any arbitrary x^ G Rn. Hence, Mk > 0 for all
k € K.56
```
Remark 6.6 As in the case of Nash equilibria (cf. Remark 6.3), the result
```
above extends naturally to the more general affine-quadratic zero-sum dynamic
games which have in the cost function an additional linear term in x, to be
denoted xfk+llk+i- Then, the only modification will have to be made in the
equation for £&, which will now read
Otherwise, the statement of Thm. 6.7 remains intact.
Remark 6.7 A comparison of Thms. 6.4 and 6.7 now readily reveals an im-
```
portant property of the saddle-point solution in such games; namely, whenever
```
they both exist, the unique open-loop saddle-point solution and the unique feed-
back saddle-point solution generate the same state trajectory in affine-quadratic
```
zero-sum games. Furthermore, the open-loop values (cf. Section 5.6) of the
```
```
feedback saddle-point strategies (6.27a)-(6.27b) are correspondingly the same
```
```
as the open-loop saddle-point strategies (6.13a)-(6.13b). These two features of
```
the saddle-point solution under different information structures are in fact char-
acteristic of not only affine-quadratic games, but of the most general zero-sum
dynamic games treated in this section, as it will be verified in the next section
```
(see, in particular, Thm. 6.9). In nonzero-sum dynamic games, however, the
```
Nash equilibrium solutions obtained under different information structures do
not exhibit such a feature, as it will also be clear from the discussion of the next
```
section (see, in particular, subsection 6.3.1).
```
Even though the two saddle-point solutions of Thms. 6.4 and 6.7 generate
the same state trajectory, the existence conditions involved are not equivalent—
```
a result that follows from a comparison of (6.1 la) and (6.14b) with (6.26b) and
```
```
(6.26a). This point is further elaborated on in the following proposition for the
```
```
case Qk+i > 0, under which (6.14b) and (6.26a) are automatically satisfied,
```
```
which therefore leaves only (6.1 la) and (6.26b) for comparison.
```
Proposition 6.2 For the affine-quadratic two-person zero-sum dynamic game
```
described by (6.10a)-(6.10b) and with Qk+i > 0 VA; G K, condition (6.11 a)
```
```
implies (6.26b), but not vice versa. In other words, every affine-quadratic two-
```
```
person zero-sum dynamic game (with nonnegative cost on the state) that admits
```
```
56It is possible to verify this property of M& also by direct matrix manipulations; see
```
Prop. 6.2 later for a hint in this direction.
NASH AND SADDLE-POINT EQUILIBRIA 287
a unique open-loop saddle point also admits a unique feedback saddle point, but
existence of the latter does not necessarily imply existence of a saddle point in
the open-loop strategies.
```
Proof. The proof of the implication from (G.lla) to (6.26b) is by induction
```
```
(on fc), where we have to show that for arbitrary k € K,
```
To save from indices, let Sk+i = S, Mk+i = M, and likewise for Bl and B2,
and start with the relation S > M > 0, where the nonnegative definiteness of
M is a property that was already proven in Thm. 6.7. Consider the perturbed
matrices
both of which are positive definite for each e > 0. The following sequence of
```
implications (=£>) and equivalences (•«•) now follows from standard properties of
```
```
matrices:57
```
Since the last inequality holds Ve > 0, and both sides are well defined as e j 0,
```
we finally have (using the continuous dependence of eigenvalues of the matrices
```
```
above on the parameter e)
```
and pre- and post-multiplying both sides by A' and A and then adding Q yields
the desired result 5^ > M^. This completes the proof by induction, since for
```
k = K + 1 both matrices equal QK+I-
```
```
To verify that (6.26b) does not necessarily imply (G.lla), we simply produce
```
```
a counter example. Consider the scalar two-stage game (i.e., K = 2) with
```
```
Qk+i = Blk = Ak = 1, Bl = l/>/2, (k = 1,2). Both (G.lla) and (6.26b) are
```
```
satisfied for k = 2. For k = 1, however, S^ = 3, MI — 5/3, and while (6.26b) is
```
```
satisfied, the condition (G.lla) fails to hold true.
```
```
Remark 6.8 The condition (6.26b) of Thm. 6.7 is quite tight for affine-quadratic
```
```
dynamic games (with nonnegative cost on the state) under the CLPS infor-
```
```
mation structure, in the sense that if the matrix in (6.26b) has any negative
```
eigenvalues, then the upper value of the game becomes unbounded. This is
57For two positive definite matrices V and W, the inequality V > W means that the
matrix difference V — W is nonnegative definite, which also implies that the difference of
their inverses, V~l — W~l, is nonpositive definite. For a proof of this last inequality, see, for
```
example, Bellman (1970).
```
288 T. BA§AR AND G. J. OLSDER
easy to see when PI uses only the current value of the state, because then ex-
```
istence of a negative eigenvalue for the matrix in (6.26b) at some stage k would
```
imply that the recursive derivation of the feedback saddle-point equilibrium so-
lution encounters at stage k a static game that is not concave in the maximizing
```
variable—making the upper value of that game unbounded (as in Remark 4.7).
```
This argument can be extended also to the general CLPS information structure,
implying that even if the minimizer is allowed to use also the past values of the
```
state the upper value would still be unbounded when the matrix in (6.26b) has
```
at least one negative eigenvalue.58 The lower value of the game, however, could
still be bounded, which means that if the minimizer is also allowed to have ac-
```
cess to the maximizer's actions (controls), then condition (6.26b) can be further
```
```
relaxed; this relaxed version of (6.26b) can in fact be obtained quite readily by
```
```
allowing (in the recursive derivation of the feedback saddle-point equilibrium)
```
```
the minimizer (PI) to choose u\. dependent not only on x^ but also on v%,
```
```
and then requiring the resulting minimum value of the stagewise cost (in the
```
```
truncated game) to be strictly concave in u^.
```
```
If the information structure is CLPS, and the matrix in (6.26b) is nonnega-
```
```
tive (but not positive) definite, then whether the upper value is bounded or not
```
depends on the specific structures of the cost and system matrices, which we do
not further discuss here.
6.2.3 Linear-quadratic games with an infinite number of
stages
We now consider stationary dynamic games with an infinite number of stages,
and restrict attention to linear-quadratic structures, which means that in the
formulation of Def. 6.1, all matrices are time invariant, c = 0, and K = oo
or K —> oo. Feedback Nash equilibria of such games can be obtained in two
different ways: as the limit of the feedback Nash solution of any time-truncated
```
version (with, say, K stages) as the number of stages (K) goes to infinity,
```
or from the outset as the Nash equilibrium of an infinite-horizon game under
CLNM information pattern. Of course, a natural procedure to follow would be
a combination of these two methods: find the limit of the finite-horizon solution
```
as K —» oo, and then verify that this limiting solution (if it exists) provides a
```
Nash equilibrium solution for the infinite-horizon game. Toward this end, first
note that a candidate solution can easily be obtained from Corollary 6.1, by
simply taking c = 0 and dropping the time indices from the various matrices.
```
Let Z*k ' (i € N) denote the solution of (6.17b) when there are K stages
```
```
(where we show explicit dependence on K, as in the one-player case discussed
```
```
in subsection 5.5.1, Prop. 5.2), and let Zl (i € N) be its limit as K —> oo for
```
```
fixed A; (assuming that this limit exists, and is independent of k). Likewise, let
```
```
Pi (i e N) denote the limit of P'k(K} (i € N) as K -> oo. Then, these limiting
```
```
58See Ba§ar(1991b) or Ba§arand Bernhard (1995) for details of this argument.
```
NASH AND SADDLE-POINT EQUILIBRIA 289
matrices necessarily satisfy the following two algebraic matrix equations
Conditions that will guarantee that such an JV-tuple of stationary policies are
in Nash equilibrium can be obtained readily from Prop. 5.2, by simply holding
```
all but one (say, zth) players' strategies fixed at (6.29) and requiring that the
```
resulting infinite-horizon optimal control problem be well defined, as explained
in subsection 5.5.1. This result is given below in Prop. 6.3 after introducing
some notation.
```
Let Fi and Qi (i € N) be defined by
```
```
Proposition 6.3 Let there exist two N-tuples of matrices {Zl,Pl, i € N}
```
```
satisfying (6.28a) and (6.28b), and further satisfying the conditions that for
```
```
each i e N the pair (Fi,Bl) is stabilizable and the pair (Fi,Qi) is detectable.
```
Then,
```
(i) the N-tuple of stationary feedback policies (6.29) provides a Nash equi-
```
librium solution for the linear-quadratic nonzero-sum dynamic game of
this subsection, leading to the finite infinite-horizon Nash equilibrium cost
^x'^xi for Pi,
```
(ii) the resulting system dynamics, described by
```
xk+i=Fxk, fe = l , 2 , . . . ,
are stable.
A few remarks regarding the solution presented above are now in order.
First, we have not given any conditions on the parameters of the game that
```
will guarantee the existence of a solution set to (6.28a)-(6.28b); obtaining such
```
```
The corresponding limiting feedback strategy for Pi is then (from (6.18a))
```
where
```
Now, the following lemma, whose proof can be found in (Ba§ar and Bern-
```
```
hard, 1995), provides precise relationships between the limit of the sequence
```
```
generated by (6.12b) and solutions of (6.30d) as well as the value of the un-
```
derlying infinite-horizon game, when Q > 0. Before stating the lemma, let
```
(K)us introduce the notation M^ to denote the unique nonnegative-definite ma-
```
```
trix sequence generated by (6.12b) for a K-stage time-invariant linear-quadratic
```
```
game, with the terminal condition (at k = K+l) taken as Q, and with condition
```
```
(6.30f) satisfied.
```
Lemma 6.3 For the linear-quadratic infinite-horizon zero-sum dynamic game
```
with Q > 0, let the pair (A, Q) be observable (respectively, detectable). Then,
```
```
(i) if there exists a nonnegative-definite solution to the generalized algebraic
```
```
Riccati equation (GARE) (6.30d), M, which also satisfies (6.30e), then
```
M is necessarily positive definite,
```
(ii) if the GARE (6.30d) does not admit a positive (respectively, nonnegative)
```
```
definite solution satisfying (6.30f), then the upper value of the game is
```
unbounded,
290 T. BA§AR AND G. J. OLSDER
conditions seems to be quite a challenging task. Second, even though the set of
```
equations (6.28a)-(6.28b) was obtained by taking a limit on the solution for the
```
time-truncated version of the game, it is quite possible that these equations will
```
admit (other) solutions that are not necessarily related to the solution(s) of the
```
finite-horizon game. But, these would also provide Nash equilibria, as long as
the conditions on stabilizability and detectability are satisfied.
For the special class of linear-quadratic zero-sum games it is possible to ob-
tain stronger results. First, let us rewrite the "stationary" counterparts of the
```
feedback saddle-point policies (6.27a)-(6.27b), along with the "stationary" coun-
```
```
terparts of the relevant matrices (A and M) which were introduced in Thm. 6.4,
```
```
where we again use an "overbar" to denote the limiting values (as K —> oo):
```
```
Furthermore, the counterpart of the concavity condition (6.26b) is
```
```
and for convenience we also introduce the following relaxed version of (6.30e):
```
NASH AND SADDLE-POINT EQUILIBRIA 291
```
(Hi) if M denotes a positive (respectively, nonnegative) definite solution of
```
```
(6.30d) satisfying (6.30e), then for all K > 1,
```
```
An important consequence of this lemma is that if the GARE (6.30d) admits
```
```
multiple positive (respectively, nonnegative) definite solutions satisfying (6.30e),
```
```
there is a minimal such solution (minimal in the sense of matrix partial order-
```
```
ing), denoted M+, to which the sequence {M^ } converges as K —> oo. In view
```
of this observation, and the result of Lemma 6.3, the following theorem can be
```
established. Its proof can be found in Ba§ar(1991b) and Ba§arand Bernhard
```
```
(1995).
```
Theorem 6.8 Consider the infinite-horizon discrete-time linear-quadratic zero-
```
sum dynamic game, with (A, Q] constituting an observable (respectively, de-
```
```
tectable) pair. Then, we have the following.
```
```
(i) The game has equal upper and lower values if, and only if, the GARE
```
```
(6.30d) admits a positive (respectively, nonnegative) definite solution sat-
```
```
isfying (6.30e), and only if it admits a positive (respectively, nonnegative)
```
```
definite solution satisfying (6.30f).
```
```
(ii) If the GARE admits a positive (respectively, nonnegative) definite solution,
```
```
satisfying (6.30e), then it admits a minimal such solution, to be denoted
```
M+. Then, the finite value of the game is ^x^M^xi.
```
(Hi) The upper (minimax) value of the game is finite if, and only if, the upper
```
and lower values are equal.
```
(iv) If M+ > 0 (respectively, > 0) exists, as given above, the controller 71
```
```
given by (6.30a), with M replaced by M+, attains the finite upper value,
```
in the sense that
and a maximizing solution above is the stationary feedback policy given
```
by (6.30b), again with M replaced by M+.
```
```
(v) If M+ > 0 (respectively, > Q), exists, the closed-loop system under the
```
```
policies (6.30a)-(6.30b), with M replaced by M+, is asymptotically stable,
```
that is, the matrix F defined below is Hurwitz:
```
Note that the theorem above does not say that the feedback policies (6.30a)-
```
```
(6.30b), with M replaced by M+, are in saddle-point equilibrium. It only says
```
that Pi's feedback policy as given assures a finite upper value, which is also
292 T. BA§AR AND G. J. OLSDER
equal to the lower value. This finite value of the game may not, however, be
```
assured by the corresponding feedback policy of P2; more will be said on this
```
```
point in the context of continuous-time (differential) games, to be discussed in
```
subsection 6.5.3.
6.3 Informational Properties of Nash Equilibria
in Discrete-Time Dynamic Games
This section is devoted to an elaboration on the occurrence of "information-
ally nonunique" Nash equilibria in discrete-time dynamic games, and to a gen-
eral discussion on the interplay between information patterns and existence-
uniqueness properties of noncooperative equilibria in such games. First, we
consider, in some detail, a scalar three-person dynamic game which admits
uncountably many Nash equilibria, and which features several important prop-
erties of infinite dynamic games. Then, we discuss these properties in a general
context.
6.3.1 A three-person dynamic game illustrating informa-
tional nonuniqueness
Consider a scalar three-person two-stage linear-quadratic dynamic game in which
each player acts only once. The state equation is given by
and the cost functionals are defined as
```
In this formulation, ul is the scalar unconstrained control variable of Pi (i =
```
```
1,2,3), and x\ is the initial state whose value is known to all players. PI and
```
```
P2, who act at stage 2, have also access to the value of x^ (i.e., the underlying
```
```
information pattern is CLPS (or, equivalently, MPS) for both PI and P2),
```
and their permissible strategies are taken as twice continuously differentiable
mappings from R x R into R. A permissible strategy for P3, on the other hand,
is any measurable mapping from R into R. This, then, completes the description
```
of the strategy spaces F1, F2 and F3 (for PI, P2 and P3, respectively), where
```
we suppress the subindices denoting the corresponding stages since each player
acts only once.
```
Now, let J71 G rx ,72 e F2,73 € F3} denote any noncooperative (Nash)
```
equilibrium solution for this three-person nonzero-sum dynamic game. Since Ll
```
is strictly convex in ul (i = 1,2), a set of necessary and sufficient conditions for
```
```
71 and 72 to be in equilibrium (with 73 € F3 fixed) is obtained by differentiation
```
```
of Ll with respect to ul (i = 1,2), thus leading to
```
NASH AND SADDLE-POINT EQUILIBRIA 293
where
```
Solving for 71(x2,£i) and 72(x2,xi) from the foregoing pair of equations, we
```
obtain
which are the side conditions on the equilibrium strategies 71 and 72, and which
depend on the equilibrium strategy 73 of P3. Besides these side conditions, the
Nash equilibrium strategies of PI and P2 have no other natural constraints im-
posed on them. To put it in other words, every Nash equilibrium strategy for PI
```
will be a closed-loop representation (cf. Def. 5.12) of the open-loop value (6.33a),
```
and every Nash equilibrium strategy for P2 will be a closed-loop representation
```
of (6.33b).
```
To complete the solution of the problem, we now proceed to stage 1. Since
```
{7\72?73} constitutes an equilibrium triple, with {ul — 71 (x2,xi),u2 = 72(x2,
```
```
xi)} substituted into L3, the resulting cost functional of P3 (denoted as L3)
```
```
should attain a minimum at u3 = 73(xi); and since 71 and 72 are twice contin-
```
uously differentiable in their arguments, this requirement can be expressed in
terms of the relations
where we have suppressed the arguments of the strategies. Now, by utilizing
```
the side conditions (6.33a)-(6.33b) in the above set of relations, we arrive at a
```
simpler set of relations which are, respectively,
```
59Here, we could of course also have nonstrict inequality (i.e., >) in which case we also
```
have to look at higher-order derivatives of L3. We avoid this by restricting our analysis at
```
the outset only to those equilibrium triples {"y1,72,73} which lead to an L3 that is locally
```
strictly convex at the solution point.
These are the relations which should be satisfied by an equilibrium triple, in
```
addition to (6.33a) and (6.33b). The following proposition summarizes the
```
result.
```
Proposition 6.4 Any triple J71* G T1^2* e T2,73* € T3} that satisfies
```
```
(6.33a)-(6.33b) and (6.34a)-(6.34b), and also possesses the additional feature
```
```
that (6.34a) with 71 = 71* and 72 = 72* admits a unique solution 73 = 73*;
```
constitutes a Nash equilibrium solution for the nonzero-sum dynamic game de-
```
scribed by (6.31)-(6.32).
```
Proof. This result follows from the derivation outlined prior to the statement
```
of the proposition. Uniqueness of the solution of (6.34a) for each pair J71*,72*}
```
is imposed in order to ensure that the resulting 73* is indeed a globally mini-
mizing solution for L3. Q
We now claim that there exists an uncountable number of triplets that satisfy
the requirements of Prop. 6.4. To justify this claim, and to obtain a set of explicit
solutions, we consider the class of 7* and 72 described as
where p and q are free parameters. These structural forms for 71 and 72 clearly
```
satisfy the side conditions (6.33a) and (6.33b), respectively. With these choices,
```
```
(6.34a) can be solved uniquely (for each p, q) to give
```
```
with the existence condition (6.34b) reading
```
294 T. BA§AR AND G. J. OLSDER
where x% is again defined as
The scalar x? is then given by
Hence, we have the following.
Proposition 6.5 The set of strategies
NASH AND SADDLE-POINT EQUILIBRIA 295
```
constitutes a Nash equilibrium solution for the dynamic game described by (6.31)-
```
```
(6.32), for all values of the parameters p and q satisfying (6.35) and with
```
p ^ —11/6. The corresponding equilibrium costs of the players are
Several remarks and observations are in order here, concerning the Nash
equilibrium solutions presented above.
```
(1) The nonzero-sum dynamic game of this subsection admits uncountably
```
many Nash equilibrium solutions, each one leading to a different equilib-
rium cost triple.
```
(2) Within the class of linear strategies, Prop. 6.5 provides the complete
```
solution to the problem, which is parameterized by p and q.
```
(3) The equilibrium strategy of P3, as well as the equilibrium cost values
```
```
of all three players, depend only on p (not on g), whereas the existence
```
```
condition (6.35) involves both p and q. There is indeed an explanation for
```
```
this: the equilibrium strategies of PI and P2 are in fact representations
```
```
of the open-loop values (6.33a)-(6.33b) on appropriate trajectories. By
```
```
choosing a specific representation of (6.33a), PI influences the cost func-
```
tional of P3 and thereby the optimization problem faced by him. Hence,
```
for each different representation of (6.33a), P3 ends up, in general, with a
```
different solution to his minimization problem, which directly contributes
to nonuniqueness of Nash equilibria. For P2, on the other hand, even
though he may act analogously—i.e., choose different representations of
```
(6.33b)—these different representations do not lead to different minimiz-
```
```
ing solutions for P3 (but instead affect only the existence of a minimizing
```
```
solution) since L2 = —I/3, i.e., P2 and P3 have completely conflicting
```
```
goals (see Thm. 6.9, later in this section, and also the next remark for
```
```
further clarification). Consequently, 73*(xi) is independent of g, but the
```
```
existence condition explicitly depends upon q. (This is true also for non-
```
```
linear representations, as it can be seen from (6.34a) and (6.34b).)
```
```
(4) If PI has access to only x<i (and not to £1), then, necessarily, p = 0, and
```
```
both PI and P3 have unique equilibrium strategies which are {"Yl*(x2) =
```
```
-|^2>73*(^i) = -(2/ll)zi}. (This is true also within the class of nonlin-
```
```
ear strategies.) Furthermore, the equilibrium cost values are also unique
```
```
(simply set p = 0 in Jl*,i = 1,2,3, in Prop. 6.5). However, the existence
```
```
condition (6.35) still depends on q, since it now reduces to q2 < 11/9.
```
The reason for this is that P2 still has the freedom of employing differ-
```
ent representations of (6.33b), which affects existence of the equilibrium
```
solution but not the actual equilibrium state trajectory, since P2 and P3
```
are basically playing a zero-sum game (in which case the equilibrium (i.e.,
```
```
saddle-point) solutions are interchangeable). (See Thm. 6.9, later in this
```
```
section; also recall the feature discussed in Remark 6.7 earlier.)
```
296 T. BA§AR AND G. J. OLSDER
```
(5) By setting p = q = 0 in Prop. 6.5, we obtain the unique feedback Nash
```
```
equilibrium solution (cf. Corollary 6.1 and Remark 6.4) of the dynamic
```
```
game under consideration (which exists since p = 0 7^ 11/6, and (6.35) is
```
```
satisfied).
```
```
(6) Among the uncountable number of Nash equilibrium solutions presented
```
in Prop. 6.5, there exists a subsequence of strategies which brings Pi's
Nash cost arbitrarily close to zero which is the lowest possible value L1
can attain. Note, however, that the corresponding cost for P3 approaches
```
(zi)2 which is unfavorable to him.
```
Before concluding this subsection, it is worthy to note that the linear equilibrium
solutions presented in Prop. 6.5 are not the only ones that the dynamic game
```
under consideration admits, since (6.34a) will also admit nonlinear solutions. To
```
obtain an explicit nonlinear equilibrium solution, we may start with a nonlinear
```
representation of (6.33a), for instance,
```
```
substitute it into (6.34a) and solve for a corresponding j3(xi), checking at
```
```
the same time satisfaction of the second-order condition (6.34b). For such a
```
```
derivation (of nonlinear Nash solutions in a linear-quadratic game) the reader
```
```
is referred to Ba§ar(1974). See also Problem 5, Section 6.8.
```
6.3.2 General results on informationally nonunique
equilibrium solutions
Section 3.5 has already displayed existence of "informationally nonunique" Nash
equilibria in finite multi-act nonzero-sum dynamic games, which was mainly due
to the fact that an increase in information to one or more players leaves the Nash
equilibrium obtained under the original information pattern unchanged, but it
```
also creates new equilibria (cf. Prop. 3.10). In infinite games, the underlying
```
reason for occurrence of informationally nonunique Nash equilibria is essentially
```
the same (though much more intricate), and a counterpart of Prop. 3.10 can
```
be verified. Toward this end we first introduce the notion of "informational
inferior" in such dynamic games.
Definition 6.3 Let I and II be two N-person K-stage infinite dynamic games
which admit precisely the same extensive form description except the underlying
```
information structure (and, of course, also the strategy spaces whose descrip-
```
```
tions depend on the information structure). Let rft (respectively, r/^) denote the
```
```
information pattern of Pi in the game I (respectively, II), and let the inclusion
```
relation r\\ C ryfj imply that whatever Pi knows at each stage of game I he also
knows at the corresponding stages of game II, but not necessarily vice versa.
```
Then, I is informationally inferior to II if r}\ C 77^ for all i G N, with strict
```
inclusion for at least one i.
h
Proposition 6.6 Let I and II be two N-person K-stage infinite dynamic games
as introduced in Def. 6.3, so that I is informationally inferior to II. Furthermore,
let the strategy spaces of the players in the two games be compatible with the
```
given information patterns and the constraints (if any) imposed on the controls,
```
```
so that //I C r?!! implies T\ C F|I; i G N. Then,
```
```
(i) any Nash equilibrium solution for I is also a Nash equilibrium solution for
```
II,
```
(ii) if J 7 1 , . . . , 7^} is a Nash equilibrium solution for II such that 7* G F| for
```
all i E N, then it is also a Nash equilibrium solution for I.
```
Proof. Let {7**;^ G N} constitute a Nash equilibrium solution for I. Then,
```
by definition,
```
therefore, PI minimizes J1^^2*,... ,7^*) over Fj, with the corresponding so-
```
lution being 71* e T\.
```
Now consider minimization of the same expression over FJJ Q Fj) which
```
reflects an increase in deterministic information concerning the values of state.
But, since we have a deterministic optimization problem, the minimum value
```
of J1(71,72*,... ,7^*) does not change with an increase in information (see
```
```
Section 5.6). Hence,
```
and furthermore, since 71* £ F^, we have the inequality
Since PI was an arbitrary player in this discussion, it follows in general that
```
which verifies (i) of Prop. 6.6. Proof of (ii) is along similar lines.
```
```
Since there corresponds at least one informationally inferior game (viz. a
```
```
game with an open-loop information structure) to every multi-stage game with
```
CLPS information, the foregoing result clearly provides one set of reasons for
existence of "informationally nonunique" Nash equilibria in infinite dynamic
```
games (as Prop. 3.10 did for finite dynamic games). However, this is not yet the
```
whole story, as it does not explain occurrence of uncountably many equilibria in
such games. What is really responsible for this is the existence of uncountably
```
many representations of a strategy under dynamic information (cf. Section 5.6).
```
To elucidate somewhat further, consider the scalar three-person dynamic game
of subsection 6.3.1. We have already seen that, for each fixed equilibrium strat-
egy 73 of P3, the equilibrium strategies of PI and P2 have unique open-loop
298 T. BA§AR AND G. J. OLSDER
```
values given by (6.33a) and (6.33b), respectively, but they are otherwise free. We
```
also know from Section 5.6 that there exist infinitely many closed-loop represen-
```
tations of such open-loop policies; and since each one has a different structure,
```
this leads to infinitely many equilibrium strategies for P3, and consequently to
a plethora of Nash equilibria.
```
The foregoing discussion is valid on a much broader scale (not only for the
```
```
specific three-person game treated in subsection 6.3.1), it provides a general
```
guideline for derivation of informationally nonunique Nash equilibria, thus lead-
ing to the algorithm given below after introducing the concept of "stagewise
equilibrium".
Definition 6.4 For an N-person K-stage infinite dynamic game, a set of strate-
```
gies {7**;i € N} satisfying the following inequalities for all 7^ e rik, i G N,
```
```
k € K, is a stagewise (Nash) equilibrium solution:
```
```
Remark 6.9 Every Nash equilibrium solution (cf. the set of inequalities (6.3))
```
is a stagewise Nash equilibrium solution, but not vice versa.
An algorithm to obtain informationally nonunique Nash equilibria in
infinite dynamic games
First determine the entire class of stagewise equilibrium solutions.
```
(1) Starting at the last stage of the game (k — K), fix the TV-tuples of strategies
```
at every other stage k < K, and solve basically an TV-person static game
```
at stage K (which is defined by (6.36) with k = K) to determine the open-
```
loop values of the corresponding stagewise equilibrium strategies at k = K
as functions of the strategies applied previously. Furthermore, determine
the equivalence class of representations of these TV-tuples of open-loop
values, again as functions of the previous strategies and so that they are
compatible with the underlying information structure of the problem.
```
(2) Now consider inequalities (6.36) for k = K — 1 and adopt a specific member
```
of the equivalence class determined at step 1 as a strategy TV-tuple applied
at stage K\ furthermore fix the TV-tuples of strategies at every stage k <
K — l, and solve basically an TV-person static game to determine the open-
loop values of the corresponding stagewise equilibrium strategies at stage
K — 1 as functions of the strategies applied previously. Repeat this for
every member of the equivalence class determined at step 1. Each will,
in general, result in a different set of open-loop values. Now, for each set
of open-loop values thus determined, find the corresponding equivalence
class of representations which are also compatible with the underlying
NASH AND SADDLE-POINT EQUILIBRIA 299
information structure and which depend on the strategies applied at earlier
stages.
```
(K) Now, finally, consider the set of inequalities (6.36) for k = 1, and adopt
```
```
specific (and compatible) members of the equivalence classes determined
```
```
at steps 1,..., K — 1 as strategies applied at stages K,..., 2; solve the
```
resulting TV-person static game to determine the corresponding stagewise
equilibrium strategies at stage 1, which will in general also require solution
```
of some implicit equations. Repeat this for every compatible (K — l)-tuple
```
of members of the equivalence classes constructed at steps 1 , . . . , K — 1.
This then completes the construction of the entire class of stagewise equilibrium
solutions of the given dynamic game. Finally, one has to check to see which of
these stagewise equilibrium solutions also constitute Nash equilibria, by referring
```
to the set of inequalities (6.3).
```
Remark 6.10 In obtaining a set of informationally nonunique equilibrium so-
lutions for the scalar example of subsection 6.3.1, we have actually followed the
```
steps of the foregoing algorithm; but there we did not have to check the last
```
condition since every stagewise equilibrium solution was also a Nash equilibrium
solution because every player acted only once.
The foregoing algorithm leads to an uncountable number of information-
ally nonunique Nash equilibria in deterministic multi-stage infinite nonzero-sum
games wherein at least one player has dynamic information. Furthermore, these
informationally nonunique Nash equilibria are, in general, not interchangeable—
thereby leading to infinitely many different equilibrium cost TV-tuples. The
reason why we use the words "in general" in the preceding sentence is be-
cause there exist some extreme cases of nonzero-sum games which do not ex-
hibit such features. One such class is the so-called team problems for which
```
Ll = L2 = • • • = LN, i.e., there is a common objective function. The Nash
```
equilibrium solution corresponds in this case to a person-by-person optimal so-
```
lution and it becomes a team solution under some further restrictions (see the
```
```
discussion given in Section 4.6). Deterministic team problems are no different
```
from optimal control problems discussed in Section 5.5, and in particular the
discussion of Section 5.6 leads to the conclusion that informational nonunique-
ness of equilibria does not create any major difficulty in team problems, since
all these equilibrium solutions are different representations of the same TV-tuple
of strategies which is associated with the global minimum of a single objec-
tive functional. Hence, for the special case of team problems, informational
nonuniqueness does not lead to different equilibrium cost TV-tuples.
Let us now consider the other extreme case—the class of two-person zero-
sum infinite dynamic games—in which case Ll = —L2. For such dynamic
```
games, informational nonuniqueness does not lead to different (saddle-point)
```
300 T. BA§AR AND G. J. OLSDER
```
equilibrium cost pairs either, since multiple saddle points are interchangeable;
```
however, as we have observed in subsection 6.3.1, existence of a saddle point
will depend on the specific pair of representations adopted. Therefore, contrary
to team solutions, every representation of a pair of saddle-point strategies is not
necessarily a saddle-point strategy pair. The next theorem makes this statement
precise, and it also provides a strengthened version of Prop. 6.6 for two-person
zero-sum dynamic games. Before giving the theorem, it will be convenient first
```
to introduce the notion of a strongly unique saddle point (say, {l1*,"?2*}) on a
```
```
given product strategy space F1 x F2, as one where the pair J71*, 72*} is unique
```
as a saddle-point solution, and in addition 71* G F1 is the unique response to
72* and 72* e F2 is the unique response to 71*. Then, we have the following
theorem.
Theorem 6.9 Let I be a two-person zero-sum K-stage dynamic game with
closed-loop perfect state information pattern.
```
(i) If I admits a strongly unique feedback saddle-point solution, and also an
```
open-loop saddle-point solution, then the latter is unique.
```
(ii) If I admits a strongly unique feedback saddle-point solution J7lf,72f} with
```
```
a corresponding state trajectory {xfk+1;k £ K}7 and {71*,72*} denotes
```
some other closed-loop saddle-point solution with corresponding state tra-
```
jectory {z£+1; k G K}, we have
```
```
(Hi) If I admits a strongly unique open-loop saddle-point solution {7lo,72°}
```
```
with a corresponding state trajectory {x%+1;k £ K}, and if J71*,72*}
```
denotes some other closed-loop saddle-point solution with corresponding
```
state trajectory {xk+1;k € K} we have
```
```
Proof, (i) Let J7lf, 72f} be the strongly unique feedback saddle-point so-
```
```
lution and {7lo,72°} be any open-loop saddle-point solution. It follows from
```
```
Prop. 6.6 that {7lf,72f} and {7lo,72°} are also closed-loop saddle-point solu-
```
tions. Furthermore, because of the ordered interchangeability property of saddle
```
points, {7lo,72f} and {7lf,72°} are also closed-loop saddle-point strategy pairs,
```
```
and an analogous pair of inequalities for J7lf, 72°}, where F* denotes the closed-
```
```
loop strategy space of Pi. Now, let us consider the RHS inequality of (i), which
```
is equivalent to
I.e.,
NASH AND SADDLE-POINT EQUILIBRIA 301
This is a standard optimum control problem in discrete time with a cost func-
tional
and a state equation
```
Globally optimal solution of this problem over F1 can be obtained (if it exists)
```
```
via dynamic programming and as a feedback strategy. But by hypothesis (since
```
```
{7lf,72f} is a strongly unique saddle-point pair), there exists a unique feedback
```
```
strategy (namely, 7lf) that solves this problem. Since every closed-loop strategy
```
```
admits a unique open-loop representation (cf. Section 5.6), it follows that there
```
exists a unique open-loop solution to that optimum control problem, which
```
is given by {^(xi) = jkf(xfk}, k e K}, where {x^.+1; k £ K} denotes the
```
```
state trajectory corresponding to {7lf,72f}- Therefore, 7l0 is unique whenever
```
```
7lf is. It can likewise be shown, by making use of the LHS inequality of (i),
```
```
that 72o(xi) = 72f(£fc), and hence 72° is unique whenever 72f is unique. This
```
```
then completes verification of part (i). We note that a converse statement
```
```
cannot be made here (i.e., strong uniqueness of open-loop saddle point may not
```
```
imply strong uniqueness of feedback saddle point), because there could be cost-
```
irrelevant states on which the feedback solution can be chosen to be nonunique.
```
(ii) By Prop. 6.6 and the ordered interchangeability property of saddle
```
```
points, J71*,72f} and {7lf,72*} are also saddle-point strategy pairs; therefore
```
This equality defines an optimum control problem in discrete time, and as
```
in the proof of part (i), every globally optimal solution of this optimization
```
```
problem can be obtained via dynamic programming; and by hypothesis, 7lf
```
is the unique feedback strategy that renders the global minimum. Hence,
every solution in F1 has the same open-loop representation as 7l f , that is,
```
{7fc*(£fc, • • • ,£2,£i) = 7fcf (^l)5 ^ £ K}. Similarly, this time by starting with
```
```
(7lf,72*},7fc*(aJ , . > X 2 , x i ) = TJfCzJ.), k € K. Therefore, the pairs of strate-
```
```
gies J7lf,72f} and {71*,72*} have the same open-loop representations, and this
```
necessarily implies that the corresponding state trajectories are the same.
```
(iii) The idea of the proof here is the same as that of part (ii). By Prop. 6.6
```
```
and the ordered interchangeability property of multiple saddle points, J71*, 72°}
```
```
and {710,72*} are also saddle-point strategy pairs; the former of these implies
```
that
302 T. BA§AR AND G. J. OLSDER
which defines an optimization problem whose open-loop solution is known to
```
be unique (namely, 7l0). Therefore, every other solution of this optimiza-
```
tion problem in F1 has to be a closed-loop representation of 7l0 on the tra-
```
jectory {x%+1;k e K}, i.e., 7£*(z£,. ..,#2,2:1) = 7fc°(#i), k € K. Similarly,
```
```
7fc*(xfc> • • • -ix2ixi} = 7fc°(xi)» & £ K. Furthermore, because of this equivalence
```
between the open-loop representations, we have x£+1 = 2?£+1, k G K.
```
Remark 6.11 The "uniqueness" assumptions of parts (ii) and (iii) of Thm. 6.9
```
serve to simplify the statement of the theorem, but do not bring in any loss of
generality of conceptual nature. "Uniqueness" can be dispensed with provided
that the optimal responses to each strategy in the set of saddle points is also a
saddle-point strategy. To make this qualification more precise, and to indicate
```
the modification to be incorporated in (ii) and (iii) of Thm. 6.9 in case of
```
nonunique open-loop or feedback saddle points, suppose that there exist two
```
open-loop saddle-point solutions, say |7lo,72°} and {7loo,72o°}. Then, because
```
```
of the ordered interchangeability property, {7lo,72o°} and {7loo,72°} will also
```
be saddle-point solutions. Now suppose that 72° and 72o° are the only two
optimal responses to 7*° as well as to 7lo°, and likewise jl° and 7lo° are the
```
only two optimal responses to 72° and 72o°. Then, if J71*,72*} denotes some
```
closed-loop saddle-point solution, the open-loop representation of this pair of
strategies will be equivalent to one of the four open-loop saddle-point strategy
pairs. The saddle-point value is, of course, the same for all these different
saddle-point solutions.
Theorem 6.9, together with Remark 6.11, provides a complete characteriza-
tion of the saddle-point solutions of two-person zero-sum discrete-time dynamic
```
games, if their open-loop and/or feedback saddle-point solutions are known; ev-
```
ery other saddle point can be obtained as their representation under the given
information pattern. Furthermore, whenever they both are strongly unique, the
open-loop representation of the feedback saddle-point solution is necessarily the
open-loop saddle-point solution. This does not imply, however, that if a feed-
```
back saddle point exists, an open-loop saddle point necessarily has to exist (and
```
```
vice versa); we have already seen in subsection 6.2.2 (cf. Prop. 6.2) that the
```
former statement is not true in affine-quadratic zero-sum games.
In conclusion, a nonzero-sum multi-stage infinite game with dynamic infor-
```
mation will, in general, admit uncountably many (informationally nonunique)
```
Nash equilibria which are not interchangeable, unless it is a team problem or a
```
zero-sum game. In Ba§arand Selbuz (1976) it has actually been shown within
```
the context of a class of dynamic two-stage nonzero-sum games that this is in
fact a strict property, i.e., unless the nonzero-sum game can be converted into
an equivalent team problem or a zero-sum game, it admits uncountably many
Nash equilibria.
One possible way of removing informational nonuniqueness in the nonco-
operative equilibria of nonzero-sum dynamic games is to further restrict the
equilibrium solution concept by requiring it to be a feedback Nash equilibrium
```
(cf. Def. 6.2) which has been discussed thoroughly in subsection 6.2.2. However,
```
NASH AND SADDLE-POINT EQUILIBRIA 303
```
it should be noted that this restriction (which is compatible with a delayed com-
```
```
mitment mode of play or strong time consistency) makes sense only if all players
```
```
have access to the current value of the state (e.g., the CLPS, MPS or FB infor-
```
```
mation pattern). If at least one of the players has access to delayed or open-loop
```
information, the feedback equilibrium solution cannot be defined, and one then
has to resort to some other method for eliminating informational nonunique-
ness. One such alternative is to formulate the nonzero-sum game problem in a
stochastic framework, which is the subject of the next section.
6.4 Stochastic Nonzero-Sum Games with
Deterministic Information Patterns
We have already seen in the previous sections that one way of eliminating "infor-
mational nonuniqueness" in the Nash solution of dynamic nonzero-sum games
is to further restrict the equilibrium concept by requiring satisfaction of a "feed-
```
back" property (cf. Def. 6.2) which is, however, valid only under the CLPS and
```
MPS information patterns. Yet another alternative to eliminate informational
nonuniqueness is to formulate the dynamic game in a stochastic framework, in
```
accordance with Def. 5.4; for, under an appropriate stochastic formulation, ev-
```
```
ery strategy has a unique representation (cf. Prop. 5.5) and the statement of
```
Prop. 6.6 is no longer valid. This section is, therefore, devoted to an investiga-
tion of existence, uniqueness and derivation of Nash equilibrium in stochastic
dynamic games with deterministic information patterns.
The class of Ar-person /f-stage discrete-time stochastic infinite dynamic
games to be treated in this section will be as described in Def. 5.4, but with the
```
state equation (5.11) replaced by
```
```
where {x\, 0\,... ,#K} is a set of statistically independent Gaussian random
```
```
vectors with values in Rn, and cov (Ok) > 0, Vfc G K. Furthermore, the cost
```
functional of the players are taken to be stage-additive, i.e.,
Under the CLPS information pattern, the following theorem now provides a set
of necessary and sufficient conditions for any Nash equilibrium solution of such
a stochastic dynamic game to satisfy the following
Theorem 6.10 Every Nash equilibrium solution of an N-person K-stage sto-
```
chastic infinite dynamic game described by (6.37)-(6.38) and with the CLPS
```
```
information pattern (for all players) comprises only feedback strategies; and
```
```
for an N-tuple {7^*(xfc),fc € K; i £ N} to constitute such a solution, it is
```
and E0k denotes the expectation operation with respect to the statistics of 9k.
Any solution obtained as above is strongly time consistent, and the corresponding
```
expected Nash equilibrium cost for Pi is Vl(l,xi).
```
Proof. The first step is to verify the statement of the theorem for stagewise
```
equilibrium solution (cf. Def. 6.4); hence, start with inequalities (6.36) at stage
```
k — K, which read
where
and
Because of the stage-additive nature of Ll, these inequalities equivalently de-
scribe an TV-person static game defined by
where
304 T. BA§AR AND G. J. OLSDER
necessary and sufficient that the following recursive relations are satisfied:
```
Now, since OK is statistically independent of {0K 1,xi}, (i) can equivalently be
```
written as
NASH AND SADDLE-POINT EQUILIBRIA 305
```
which implies that the minimizing ulK will be a function of x*K; furthermore
```
```
(in contrast with the deterministic version) x*K cannot be expressed in terms
```
```
of x*K_1,...,£i since there is a noise term in (ii) which directly contributes to
```
```
additional errors in case of such a substitution (mainly because every strategy
```
```
has a unique representation when the dynamics are given by (ii)). Therefore,
```
under the CLPS information pattern, any stagewise Nash equilibrium strategy
for Pi at stage K will only be a function of XK, i-e., a feedback strategy ulK =
```
^K(XK], which further implies that the choice of such a strategy is independent
```
```
of all the optimal (or otherwise) strategies employed by the players at previous
```
```
stages. Finally note that, at stage k = K, the minimizing solutions of (iii)
```
```
coincide with those of (6.39) since the minimization problems are equivalent.
```
```
Let us now consider inequalities (6.36) at stage k = K — 1, i.e.,
```
and these can further be written as follows because of the stage-additive nature
```
of U and since 7)^ (i € N) have already been determined at stage K as feedback
```
```
strategies (independent of all the previous strategies):
```
```
In writing down (iv), we have also made use of the statistical independence
```
property of 8K~2, #1, OK-I and OK- Through a reasoning similar to the one
```
employed at stage k = K, we readily conclude that any 7x_i(-) that satisfies
```
```
(iv) will have to be a feedback strategy, independent of all the past strategies
```
```
employed and the past values of the state] therefore ul K_l = /JIK_I(XK-I) (i €
```
```
N). This is precisely what (6.39) says for k = K — I .
```
Proceeding in this manner, the theorem can readily be verified for stage-
```
wise equilibrium; that is, every set of strategies {^l*',i 6 N} satisfying (6.39)
```
constitutes a stagewise equilibrium solution for the stochastic game under con-
sideration, and conversely every stagewise equilibrium solution of the stochastic
```
dynamic game is comprised of feedback strategies which satisfy (6.39).
```
To complete the last phase of verification of the theorem, we first observe
that every stagewise equilibrium solution determined in the foregoing derivation
```
is also a feedback Nash equilibrium solution (satisfying (3.28)) since the con-
```
struction of stagewise equilibrium strategies at stage k = / did not depend on
306 T. BA§AR AND G. J. OLSDER
```
the strategies employed at stages k < I (note the italicized statements above).
```
Now, since every feedback Nash equilibrium under CLPS information pattern
```
is also a Nash equilibrium (this is a trivial extension of Prop. 6.1 to stochas-
```
```
tic dynamic games), and furthermore since every Nash equilibrium solution is
```
```
a stagewise equilibrium solution (see Remark 6.9), it readily follows that the
```
statement of Thm. 6.10 is valid also for the Nash equilibrium solution. The
```
strong time consistency of the solution (cf. Def. 5.14) follows readily from the
```
```
constructive nature of the derivation given above. Also, since Vl(k,x) is the
```
```
expected cost-to-go function for Pi, Vl(\,x\) indeed follows as the expected
```
Nash equilibrium cost for him.
Remark 6.12 First, since every strategy admits a unique representation under
```
the state dynamics description (6.37) (with cov (Ok) > 0, k € K), the solution
```
presented in Thm. 6.10 clearly does not exhibit any "informational nonunique-
ness". Therefore if nonunique Nash equilibria exist in a stochastic dynamic
game with CLPS information pattern, this is only due to the structure of fk
and gk and not a consequence of the dynamic nature of the information pattern
as in the deterministic case. Second, the Nash equilibrium solution presented in
```
Thm. 6.10 will, in general, depend on the statistical properties (i.e., the mean
```
```
and covariance) of the Gaussian noise term Ok (k G K). For the special case
```
of affine-quadratic stochastic dynamic games, however, no such dependence on
the covariance exists as it is shown in the sequel.
As an immediate application of Thm. 6.10, let us consider the class of affine-
```
quadratic stochastic dynamic games wherein fk and glk are as defined by (6.5a)-
```
```
(6.5b). Furthermore, assume that E[0k] = OVA: e K. Because of this latter
```
assumption, and since fk is affine and gk is quadratic, the minimization problem
```
in (6.39) becomes independent of the covariance of 6k, as far as the optimum
```
```
strategies are concerned, and a comparison with (6.16) readily leads to the
```
conclusion that they should admit the same solution, which is affine in the
current value of the state. Therefore, we arrive at the following corollary which
we state here without any further verification.
Corollary 6.4 An N-person affine-quadratic stochastic dynamic game as for-
```
mulated above, and with Qk+i > 0 (i € N, k e K), R^ > 0 (i,j € N, j ^ i, k €
```
```
K) and CLPS information pattern, admits a unique Nash equilibrium solution if,
```
```
and only if, (6.17a) and (6.lid) admit unique solution sets {Pk*', i € N, k 6 K}
```
```
and {alk',i G N, A; € K} in which case the equilibrium strategies are given by
```
that is, the unique solution coincides with the unique strongly time-consistent
feedback Nash equilibrium solution of the deterministic version of the game
```
(cf. Corollary 6.1).
```
We therefore observe that an appropriate stochastic formulation in affine-
quadratic dynamic games with CLPS information pattern serves to remove in-
formational nonuniqueness of equilibria, since it leads to a unique equilibrium
NASH AND SADDLE-POINT EQUILIBRIA 307
which is one of the uncountably many equilibria of the original determinis-
```
tic game. This particular equilibrium solution (which is also a strongly time-
```
```
consistent feedback equilibrium solution in this case) may be viewed as the
```
unique robust solution of the deterministic game, which is insensitive to zero-
mean random perturbations in the state equation. As an illustration, let us
consider the three-person dynamic game example of subsection 6.3.1, and, out
of the uncountably many Nash equilibrium solutions obtained there, let us at-
```
tempt to select the one which features such a property. The new (stochastic)
```
```
game is now the one which has zero-mean random perturbations in (6.31). More
```
specifically, the state equation is
```
where £[62] — E[9\] = 0; the variables 02,0i, x\ are Gaussian, statistically inde-
```
```
pendent, and £"[(02)2] > 0, £"[(0i)2] > 0. The unique Nash equilibrium solution
```
```
of this stochastic game is {^{x?} = —(2/3)z2, 72*(^2) — (l/3)x2, 73*(#i) =
```
```
— (2/ll):ri} which is also the feedback Nash equilibrium solution of the associ-
```
ated deterministic game.
If the underlying information pattern of a stochastic dynamic game is not
```
CLPS (or more precisely, if every player does not have access to the current
```
```
value of the state), the statement of Thm. 6.10 is not valid. It is, in fact,
```
not possible to obtain a general theorem which provides the Nash equilibrium
solution under all different types of deterministic information patterns, and
therefore one has to solve each such game separately. One common salient
feature of all these solutions is that they are all free of any "informational
```
nonuniqueness", and any nonuniqueness in equilibria (if it exists) is only due to
```
the structure of the cost functionals and state equation. In particular, for affine-
quadratic stochastic dynamic games, it can be shown that the Nash equilibrium
solution will in general be unique under every deterministic information pattern.
One such class of problems, viz. TV-person linear-quadratic stochastic dynamic
games wherein some players have CLPS information and others OL information,
```
have been considered in Ba§ar(1975) and a unique Nash equilibrium solution
```
has been obtained in explicit form. This solution has the property that for the
players with CLPS information the equilibrium strategies are dependent linearly
not only on the current value of the state, but also on the initial state. The
following example now illustrates the nature of this solution and also serves to
provide a comparative study of Nash equilibria under four different information
patterns.
Example 6.1 Consider the stochastic version of the three-person nonzero-sum
game of subsection 6.3.1. Under four different information structures, namely,
```
when (i) PI and P2 have access to CLPS information and P3 has OL informa-
```
```
tion (this case is also covered by Corollary 6.4), (ii) PI has CLPS information
```
```
and P2, P3 have OL information, (iii) P2 has CLPS information and PI, P3
```
```
have OL information, (iv) all three players have OL information. In these
```
308 T. BAS.AR AND G. J. OLSDER
stochastic games the state dynamics are described by
```
where {#1, #2, 2:1} is a set of statistically independent Gaussian random variables
```
```
with E[0k] = 0, E[(dk}2] > 0> k = 1,2.60 The cost functionals are again as given
```
```
by (6.32), but now their expected values determine the costs incurred to the
```
players.
```
(i) Under the first information pattern (CLPS for PI and P2, OL for P3),
```
```
the unique solution also follows from Corollary 6.4 and is given by (as
```
```
discussed right after Corollary 6.4)
```
The corresponding expected values of the cost functionals are
60This Gaussian distribution may be repletced by any probability distribution that assigns
```
positive probability mass to every open subset of Rn (see also Remark 5.11), without affecting
```
the results to be obtained in the sequel.
```
(ii) Under the second information pattern (CLPS for PI, OL for P2 and P3),
```
```
a direct derivation or utilization of the results of Ba§ar (1975) leads to the
```
unique Nash equilibrium solution
Note that this unique solution of the stochastic game under considera-
tion corresponds to one of the uncountably many equilibrium solutions
presented in Prop. 6.5 for its deterministic version—simply set p = 1/6,
```
q = 1/3. The corresponding expected values of the cost functionals are
```
```
(iii) Under the third information pattern (CLPS for P2, OL for PI and P3),
```
direct derivation leads to a finite solution set
A comparison of the expected equilibrium cost values of the players at-
tained under the four different information patterns now reveals some
interesting features of the Nash equilibrium solution.
```
(1) Comparing (6.43a) with (6.45a), we observe that when the other play-
```
ers' information patterns are OL, an increase in information to PI
```
(that is, going from OL to CLPS) could improve his performance (if
```
```
E[(9i)2} is sufficiently large) or be detrimental (if £[(6>i)2] is relatively
```
```
small).
```
```
(2) When P2 has CLPS information, a comparison of (6.42a) and (6.44a)
```
leads to a similar trend in the effect of an increase of Pi's information
to his equilibrium performance.
```
(3) When PI has CLPS information, an increase in information to P2
```
```
leads to degradation in J2* (simply compare (6.42b) with (6.43b)).
```
```
(4) When PI has OL information, an increase in information to P2 leads
```
```
to improvement in his performance (compare (6.44b) with (6.45b)).
```
These results clearly indicate that, in nonzero-sum games and under the
Nash equilibrium solution concept, an increase in information to one player
does not necessarily lead to improvement in his equilibrium performance,
and at times it could even be detrimental.
D
which can also be obtained from Prop. 6.5 by letting p = 2/3, q = —1/3.
The corresponding expected cost values are
```
(iv) Under the fourth information pattern (OL for all three players), the unique
```
Nash equilibrium solution is
NASH AND SADDLE-POINT EQUILIBRIA 309
which also corresponds to one of the solution sets of Prop. 6.5, this time
obtained by setting p = 2/3, q = 2/3. The corresponding expected values
of the cost functionals are
and cost functionals
Here, [0, T] denotes the fixed prescribed duration of the game, XQ is the initial
```
state known by all players, x(t) G Rn and u^t) e Si C Rmi (i G N) V* G [0,T].
```
```
Furthermore, / and 7* G F* (i G N) satisfy the conditions of Thm. 5.1 so
```
```
that, for a given information structure (as one of (i), (ii) or (iv) of Def. 5.6),
```
the state equation admits a unique solution for every corresponding ./V-tuple
```
{7* G P; i G N}. Nash equilibrium solution under a given information structure
```
```
is again defined as one satisfying the set of inequalities (6.3) where Jl denotes
```
```
the cost functional of Pi for the differential game in strategic (normal) form.
```
6.5.1 Open-loop Nash equilibria
The results to be described in this subsection are counterparts of those given
```
in Section 6.2.1; and in order to display this relationship explicitly, we shall
```
present them here in the same order as their counterparts in subsection 6.2.1.
We therefore first have the counterpart of Thm. 6.1 in the continuous time.
Theorem 6.11 For an N-person differential game of prescribed fixed duration
[0,T|, ^t
```
(i) f(t, -, u 1 , . . . , UN) be continuously differentiate on Rn, Vi G [0,T],
```
```
(ii) gl(t, - j U 1 , . . . ,UN) and ql(-} be continuously differentiate on Rn, W G
```
[ 0 , T ] , i G N .
```
Then, if {^(tjXo) = ul*(t}; i G N} provides an open-loop Nash equilibrium
```
```
solution, and {x*(t), 0 < t < T} is the corresponding state trajectory, there
```
```
exist N costate functions pl(-} : [0, T] —> Rn,i G N, such that the following
```
relations are satisfied:
310 T. BA§AR AND G. J. OLSDER
6.5 Open-Loop and Feedback Nash and Saddle-
Point Equilibria of Differential Games
This section presents results which are counterparts of those given in Section 6.2,
```
for differential games that fit the framework of Def. 5.5; that is, we consider JV-
```
person quantitative dynamic games defined in continuous time, and described
by the state equation
where
Every such Nash equilibrium solution is weakly time consistent.
Proof. The proof follows the same lines as in the proof of Thm. 6.1, but now
```
the minimum principle for continuous-time control systems (i.e., Thm. 5.4) is
```
used instead of Thm. 5.5.
Remark 6.13 One class of differential games for which the necessity condition
```
of Thm. 6.11 is satisfied is that with weakly coupled players (a la Remark 4.1),
```
```
that is one with the following state equation and cost functions (taking N=2,
```
```
without any loss of generality):
```
NASH AND SADDLE-POINT EQUILIBRIA 311
```
where e is a sufficiently small scalar. Under some appropriate convexity (on g^'s)
```
```
and differentiability conditions, it can be shown (see Srikant and Ba§ar(1992))
```
```
that there exists an CQ > 0 such that for all e e (—eo,eo), the differential game
```
admits a unique open-loop Nash equilibrium solution that is stable with respect
```
to Gauss-Seidel or Jacobi iterations (see Def. 4.6 for terminology). This solution
```
can be obtained by expanding the state and control corresponding to the open-
loop Nash equilibrium solution in power series in terms of e,
```
substituting these into (6.47a)-(6.47c), along with a similar expansion for pl(i],
```
and solving for the different terms x^ and u\ , k — 0,1,..., iteratively. It
```
turns out that u\'(i = I , 2 ) are the (open-loop) optimal controls associated
```
with the decoupled optimal control problems:
and cost functionals
k
312 T. BA§AR AND G. J. OLSDER
```
and x^ is the corresponding state trajectory, with x^ = (x\ , x2 )'• For
```
k > 1, u[ and u2 are obtained by solving some appropriate linear-quadratic
```
optimal control problems (see Srikant and Ba§ar (1991)). Hence this approach
```
decomposes the original two-player differential game into two nonlinear opti-
```
mal control problems (the zeroth-order problems) and a sequence of iteratively
```
constructed linear-quadratic control problems. Halting this iteration at the kth
step yields an ^-approximate open-loop Nash equilibrium solution.
As indicated earlier, Thm. 6.11 provides a set of necessary conditions for the
open-loop Nash equilibrium solution to satisfy, and therefore it can be used to
generate candidate solutions. For the special class of "affine-quadratic" differ-
ential games, however, a unique candidate solution can be obtained in explicit
terms, which can further be shown to be an open-loop Nash equilibrium solution
under certain convexity restrictions on the cost functionals of the players.
Definition 6.5 An N-person differential game of fixed prescribed duration is
```
of the affine-quadratic type if Sl = Rmi (i G N) and
```
```
where A(-), Bl(-}, Q l ( - ) , R^(-) are matrices of appropriate dimensions, c(-)
```
is an n-dimensional vector, all defined on [0,T], and with continuous entries
```
(i,j € N). Furthermore Qlf,Ql(-) are symmetric, and R™(-) > 0 (i 6 N).
```
An affine-quadratic game is of the linear-quadratic type if c = 0.
```
Theorem 6.12 For an N-person affine-quadratic differential game with Ql(-} >
```
```
0, Qy- > 0 (i € N), let there exist a solution set {Ml;i 6 N} to the coupled ma-
```
trix Riccati differential equations
Then, the differential game admits an open-loop Nash equilibrium solution given
by
```
where (m^-), i e N} solve uniquely the set of linear differential equations:
```
0 0
k
```
)
```
Proof. For the affine-quadratic differential game, and in view of the addi-
```
tional restrictions Ql(-) > 0, Q^ > 0, Ll(ul,... ,UN) is a strictly convex func-
```
```
tion of uz(-) for all permissible control functions u^(-) (j ^ i, j e N) and for all
```
XQ e Rn. This then implies that Thm. 6.11 is also a sufficiency result and every
solution set of the first-order conditions provides an open-loop Nash solution.
Hence, we now show that the solution given in Thm. 6.12 indeed satisfies the
first-order conditions.
First note that the Hamiltonian is
```
whose minimization with respect to ul(t) G Rmi yields the unique relation
```
NASH AND SADDLE-POINT EQUILIBRIA 313
```
andx*(-} denotes the (Nash) equilibrium state trajectory, generated by
```
Furthermore, the costate equations are
and the optimal state trajectory is given by
This set of differential equations constitutes a two-point boundary value prob-
```
lem. Now, substituting pl — Mlx* + ml (i € N) into the costate equations, we
```
```
arrive at the conclusion that if Ml (i e N) and ml (i € N) satisfy (6.49) and
```
```
(6.51), respectively, this indeed solves the two-point boundary value problem,
```
along with x*. The expressions for the open-loop Nash strategies follow read-
```
ily from (i) by substituting pl = Mlx* + raz, and likewise the associated state
```
```
trajectory (6.52) follows from (ii).
```
Remark 6.14 It is possible to obtain the result of Thm. 6.12 in a somewhat
```
different way, by directly utilizing Prop. 5.2; see Problem 10 in section 6.8.
```
314 T. BA§AR AND G. J. OLSDER
Theorem 6.12 provides a set of sufficiency conditions for the Nash equilibrium
solution to exist. It will now be shown that a Nash equilibrium may exist even
```
if (6.49) does not admit a solution. For simplicity of presentation assume now
```
```
that c(t] = 0. The differential equations for p%(t] and x*(t) can be written
```
jointly as
```
where 5* = Bi(B*i)"lBr', i = 1,2,..., N. The boundary conditions of (6.53)
```
```
are z(0) = x0, p*(T) = Q}x(T), i = 1,2,...,AT. Symbolically (6.53) will be
```
written as
```
Prom (6.55) we immediately get that a unique open-loop Nash equilibrium so-
```
lution exists if and only if the equation
and
with
```
is uniquely solvable in z(0) for every XQ. The matrix G consists of (N 4- 1) x
```
```
(N+l) blocks. The same block structure will be applied to eGt and these blocks
```
```
will be indicated by Wij(t), i,j = l,2,...,N + l. If we define
```
```
then the solvability of (6.57) is equivalent to the invertibility of H(T).
```
The following example now shows that an open-loop Nash equilibrium so-
```
lution can exist even if the coupled set of Riccati differential equations (6.49)
```
does not have a solution.
```
)
```
is not invertible.
The conclusion therefore is that an open-loop Nash equilibrium does not
exist and the corresponding set of Riccati differential equations does not have
a solution. Now we modify the problem statement in one respect, viz. instead
```
of T — 0.1 now take T — 0.11. Numerical calculation then shows that H(T) is
```
now invertible. However, the solution of the set of Riccati differential equations,
```
when integrated backward and starting at T — 0.11, blows up (i.e., becomes oo)
```
```
at t — 0.01 (note that the solution of the Riccati equations is shift-invariant).
```
Thus it has been shown that for T = 0.11 an open-loop Nash equilibrium solu-
tion exists despite the fact that the Riccati differential equations do not have a
solution on the interval [0,0.11].
NASH AND SADDLE-POINT EQUILIBRIA 315
```
Example 6.2 Consider the two-person (N = 2) linear-quadratic game with
```
Choose T = 0.1. Then numerical calculation leads to
Pick
and
where
```
Here the notation (H/i^jfc refers to the j'fcth element of the 2 x 2 matrix Wn(T}.
```
Clearly both Q\ and Q2, are positive definite, whereas
316 T. BA§AR AND G. J. OLSDER
What the preceding example has shown is that for a linear-quadratic game
defined on a fixed interval [0, T], existence of a Nash equilibrium does not imply
```
existence of a solution to the set of Riccati equations (6.49). An equivalence
```
between these two conditions exists, however, if T is a variable, as shown in
```
(Engwerda, 1998). What can be shown is that the following three statements
```
are equivalent, where T/ is a positive real number.
```
For each T € (0, T/] an open-loop Nash equilibrium solution exists for the
```
linear-quadratic game defined on [0,T].
```
H(T) is invertible for all T e [0, T/].
```
```
The set of Riccati differential equations (6.49) has a solution on [0, Tf\.
```
Open-loop saddle points of zero-sum differential games
For the class of two-person zero-sum differential games, we rewrite the cost
```
functional (to be minimized by PI and maximized by P2) as
```
and obtain the following set of necessary conditions for the saddle-point strate-
gies to satisfy.
Theorem 6.13 For a two-person zero-sum differential game of prescribed fixed
duration [0,T], let
```
(i) f ( t , - j U ^ u 2 ) be continuously differentiate on Rn, \/t € [0, T],
```
```
(ii) g(t,-,ul,u*} and q(-) be continuously differentiate on Rn,V£ £ [0,T].
```
```
Then, if {jl*(t,xo) — ul*(t); i — 1,2} provides an open-loop saddle-point
```
```
solution, and {x*(t},0 < t < T} is the corresponding state trajectory,
```
```
there exists a costate function p(-) : [0, T] —> Rn such that the following
```
relations are satisfied:
where
le
NASH AND SADDLE-POINT EQUILIBRIA 317
```
we now first impose additional restrictions on the parameters so that L(ul,u2}
```
is strictly concave in w2.
Lemma 6.4 For the affine-quadratic two-person zero-sum differential game in-
```
troduced above, the objective functional L(ul,u2) is strictly concave in u2 for all
```
```
permissible ul(-) if, and only if, there exists a unique bounded symmetric solu-
```
```
tion S(-) to the matrix Riccati equation
```
subject to
The lemma then follows, in view of this observation, from Remark 5.8.
Theorem 6.14 For the two-person affine-quadratic zero-sum differential game
```
described by (6.60a)-(6.60b) and with Qf > 0, Q(-} > 0, let there exist a unique
```
```
bounded symmetric solution to the matrix Riccati differential equation (6.61) on
```
[0,T]. Then,
```
(i) there exists a unique bounded symmetric solution M(-) to the (generalized)
```
matrix Riccati differential equation
on the interval [0,T].
Proof. A reasoning similar to the one employed in the proof of Lemma 6.1
leads to the observation that the requirement of strict concavity is equivalent to
the condition of existence of a unique solution to the optimal control problem
Proof. This result follows directly from Thm. 6.11 by taking N = 2, gl ~
```
-g2 — g,ql = -q2 = q, and by noting that p1(-) = -p2(-) = p(-) and Hl(-) =
```
```
-H2(.)^H(-}. D
```
For the special case of two-person zero-sum affine-quadratic differential games
described by the state equation
and the objective functional
2
318 T. BA§AR AND G. J. OLSDER
```
on [0,T], and again on [0,7"] a unique bounded solution ra(-) to the linear
```
vector differential equation
Proof. The steps involved in the proof of this theorem are similar to those
used in the proof of Thm. 6.4: we first establish existence of a unique saddle-
point solution to the affine-quadratic differential game under consideration, and
```
we then verify (i) and (ii) above in view of this property. For the former step we
```
shall proceed rather casually, since a rigorous verification requires some prior
knowledge of functional analysis which is beyond the scope of this book. The
```
interested reader can, however, refer to Balakrishnan (1976) for a rigorous expo-
```
```
sition of the underlying mathematics, or to Bensoussan (1971) and Lukes and
```
```
Russell (1971) for a framework (Hilbert space setting) wherein similar math-
```
ematical techniques are used at a rigorous level but in a somewhat different
context.61
```
Let C/1, the space of permissible control functions u*(')> be a complete (Ba-
```
```
nach) space equipped with an inner product
```
```
Then, since L(ul,u2} is quadratic in x, u1^2, and x is linear in u1 and u2, the
```
```
cost L(ul,u2} can be written as
```
61 For an alternative rigorous verification of the result of this theorem, see Ba§arand Bern-
```
hard (1995).
```
```
(ii) the game admits a unique open-loop saddle-point solution given by
```
```
where x*(-) denotes the associated state trajectory, generated by
```
where LI : U1 -> U1, L2 : U2 -> U2, LJ2 : U2 -> U1 are bounded linear
```
operators, ri € C/1, r^ € U2 and r3 e R. Moreover, since Q(-) > 0, Qf > 0,
```
```
LI is a strongly positive operator (written as LI > 0), and furthermore under
```
```
Since r*i(-) and r2(-) introduced earlier are affine functions of XQ, the unique
```
```
solution it1* of (ii) is necessarily an affine function of XQ, and so is u2*. Therefore,
```
```
the dependence of p(-) on XQ in (iii) should be affine. This readily implies that
```
```
we can write, without any loss of generality, p(i) = M(t)x*(t) + m(t), where
```
```
M(-) is some bounded matrix-valued function of dimension (n x n) and m(-) is
```
some bounded vector-valued function of dimension n, both with continuously
```
differentiate elements. Now, substitution of this relation into (iii), and requiring
```
```
(iii) to hold true for all XQ G Rn, leads to (6.62a)-(6.62b), which in turn yields
```
```
(6.63).
```
Remark 6.15 The statement of Thm. 6.14 is valid even if the "nonnegative-
```
definiteness" condition on Q(-) does not hold, provided that some other appro-
```
priate conditions are imposed on the game parameters to ensure that L defined
```
by (6.60b) is strictly convex in u1. One such condition, which is in fact tight,
```
```
can be obtained directly from Lemma 6.4, by simply replacing Qf by — Q/, Q(-)
```
```
by -Q(-) and B2 by B1, which is the existence of a unique bounded symmetric
```
```
solution S l ( - } to the matrix Riccati differential equation
```
on the interval [0, T].
NASH AND SADDLE-POINT EQUILIBRIA 319
```
the condition of unique solvability of (6.61) (cf. Lemma 6.4) L2 > 0. Now, if
```
```
L(u1,iu2) admits a unique saddle point in open-loop policies, it is necessary and
```
```
sufficient that it satisfy the following two relations (obtained by differentiation
```
```
of (i))
```
where L^2 denotes the adjoint of Li2. Solving for u2 € U2 from the second one
```
(uniquely), and substituting it into the first one, we obtain the single relation
```
which admits a unique solution, since LI +Li2L2 1LJ2 > 0 and is thereby invert-
```
ible. (This latter feature follows since LI > 0 and L^LJ1!^ is a nonnegative
```
```
self-adjoint compact operator; see Balakrishnan (1976), for details.) We have
```
thus established existence of a unique open-loop saddle-point strategy for PI
under the condition of Lemma 6.4. This readily implies existence of a unique
open-loop saddle-point strategy for P2 since L2 is invertible.
Thus completing the proof of existence of a unique saddle point, we now
```
turn to verification of (6.62a)-(6.62b) and (6.63). At the outset, let us first
```
note that the unique saddle-point solution should necessarily satisfy relations
```
(6.59a)-(6.59d) which can be rewritten for the affine-quadratic game as
```
320 T. BA§AR AND G. J. OLSDER
6.5.2 Closed-loop no-memory and feedback Nash
equilibria
Under the memoryless perfect state information pattern, the following theo-
rem provides a set of necessary conditions for any closed-loop no-memory Nash
equilibrium solution to satisfy.
Theorem 6.15 For an N-person differential game of prescribed fixed duration
[0,T], let
```
(i) f(t, •) be continuously differentiate on Rn x 51 x • x SN ,Vt e [0,T],
```
```
(ii) gl(t, •) and <?*(•) be continuously differentiate on Rn x 51 x • • • x SN and
```
Rn, respectively, Vi <E [0,T], i e N.
```
Then, if {yl*(t,x,XQ);i € N} provides a closed-loop no-memory Nash equilib-
```
```
rium solution such that 7**(t,-,:ro) is continuously differentiate on Rn (V£ £
```
```
[0, T],i € N), and {x*(t),0 <t<T} is the corresponding state trajectory, there
```
```
exist N costate functions pl(-} : [0,T] —> Rn ,z € N, such that the following
```
relations are satisfied:
where Hl is as defined in Thm. 6.11.
```
Proof. Let us consider the ith inequality of (6.3), which fixes all players'
```
```
strategies (except the ith one) at 7J = ji* (j ^ i,j e N) and constitutes an
```
```
optimal control problem for Pi. Therefore, relations (6.66a)-(6.66c) directly
```
```
follow from Thm. 5.4 (the minimum principle for continuous-time systems), as
```
```
do relations (6.47a)-(6.47c) of Thm. 6.11. We should note, however, that the
```
```
partial derivative (with respect to x} in the costate equations (6.66c) receives
```
contribution also from dependence of the remaining N — 1 players' strategies
on the current value of x—a feature which was clearly absent in the costate
equations of Thm. 6.11.
```
The set of relations (6.66a)-(6.66c) in general admits an uncountable number
```
of solutions, which correspond to "informationally nonunique" Nash equilibrium
solutions of the differential game under memoryless perfect state information
pattern. All of these are weakly time consistent, one of which is the open-loop
NASH AND SADDLE-POINT EQUILIBRIA 321
solution given in Thm. 6.11. The analysis of subsection 6.3.1 can in fact be
duplicated in the continuous time to produce explicitly a plethora of informa-
```
tionally nonunique Nash equilibria under dynamic information; and the reader
```
```
is referred to Ba§ar(1977b) for one such explicit derivation.
```
In order to eliminate informational nonuniqueness in the derivation of Nash
```
equilibria under dynamic information (specifically under the MPS and CLPS
```
```
patterns) we refine the Nash solution concept further, by requiring it to satisfy
```
conditions similar to those of Def. 6.2, but now in the continuous time. Such
a consideration leads to the concept of a "feedback Nash equilibrium solution"
which is introduced below.
Definition 6.6 For an N-person differential game of prescribed fixed dura-
```
tion [0, T] and with memoryless perfect state (MPS) or closed-loop perfect state
```
```
(CLPS) information pattern, an N-tuple of strategies {7** G P;« G N}62 con-
```
```
stitutes a feedback Nash equilibrium solution if there exist functionals Vr t (-,-)
```
defined on [0, T] x Rn and satisfying the following relations for each i G N:
```
and rjs, stands for either the data set {X(S),XQ} or {x(a},a < s}, depending on
```
whether the information pattern is MPS or CLPS.
One salient feature of the concept of feedback Nash equilibrium solution
```
introduced above is that if an Af-tuple {7**;^ G N} provides a feedback Nash
```
```
equilibrium solution (FNES) to an TV-person differential game with duration
```
[0,T], its restriction to the time interval [t,T] provides an FNES to the same
differential game defined on the shorter time interval [<,T], with the initial state
```
taken as x(t), and this being so for all 0 < t < T; hence, an FNES is strongly
```
time consistent. An immediate consequence of this observation is that, under
either MPS or CLPS information pattern, feedback Nash equilibrium strategies
62Here Tl is chosen to be compatible with the associated information pattern.
where, on the iterva [t,T],
322 T. BA§AR AND G. J. OLSDER
will depend only on the time variable and the current value of the state, but not
```
on memory (including the initial state XQ).
```
The following proposition now verifies that an FNES is indeed a Nash equi-
librium solution.
```
Proposition 6.7 Under the memoryless (respectively, closed-loop) perfect state
```
information pattern, every feedback Nash equilibrium solution of an N-person
```
differential game is a closed-loop no-memory (respectively, closed-loop) Nash
```
```
equilibrium solution (but not vice versa).
```
```
Proof. Note that Vl(t,x] is the value function (cf. subsection 5.5.2) asso-
```
ciated with the optimal control problem of minimizing over 7* e P the func-
```
tion Jl(i1*,..., 7*"1*, 7l, 7*"1"1* , . . . , 7^*) (see in particular, equation (5.24a)).
```
```
Therefore (6.67), together with the given terminal boundary condition, implies
```
```
satisfaction of the zth inequality of (6.3). Since i € N was arbitrary, and the
```
foregoing argument is valid under both the MPS and CLPS information pat-
terns, the result stands proven. D
```
If the value functions V1 (i e N) are continuously differentiate in both ar-
```
guments, then N partial differential equations, related to the Hamilton-Jacobi-
```
Bellman equation of optimal control (cf. subsection 5.5.2), replace (6.67).
```
Theorem 6.16 For an N-person differential game of prescribed fixed duration
[0, T], and under either MPS or CLPS information pattern, an N-tuple of strate-
```
gies {7** € Tl;i 6 N} provides a feedback Nash equilibrium solution if there
```
exist functions V1 : [0, T] x Rn —» R, i e N, satisfying the partial differential
equations
where
Every such equilibrium solution is strongly time consistent, and the correspond-
```
ing Nash equilibrium cost for Pi is VI(O,XQ).
```
Proof. In view of the argument employed in the proof of Prop. 6.7, this
```
result follows by basically tracing the standard technique of obtaining (5.25)
```
```
from (5.24a).
```
NASH AND SADDLE-POINT EQUILIBRIA 323
```
For the class of TV-person affine-quadratic differential games (cf. Def. 6.5),
```
```
it is possible to obtain an explicit solution for (6.69), which is quadratic in x.
```
This also readily leads to a set of feedback Nash equilibrium strategies which
are expressible in closed-form.
```
Corollary 6.5 For an N-person affine-quadratic differential game with Ql(-} >
```
```
0, Qlj > 0, Rli(-) > 0 (i, j € N,i ^ j } , let there exist a set of matrix valued
```
```
functions Zl(-} > 0, i € N7 satisfying the following N coupled matrix Riccati
```
differential equations:
where
Then, under either the MPS or CLPS information pattern, the differential game
admits a feedback Nash equilibrium solution, affine in the current value of the
state, given by
```
where (? (i € N) are obtained as the unique solution of the coupled linear dif-
```
ferential equations
with
The corresponding values of the cost functional are
```
where nl(-} (i G N) are obtained from
```
Proof. Simply note that, under the condition of solvability of the set of
```
matrix Riccati differential equations (6.70a), the partial differential equation
```
324 T. BA§AR AND G. J. OLSDER
```
(6.69) admits a solution in the form F'foz) = ^Z'tyx + x'?(t) + n*(t) (i €
```
```
N) with the corresponding minimizing controls being given by (6.71). The
```
```
"nonnegative definiteness" requirement imposed on Zl(-) is a consequence of
```
```
the fact that Vl(t, x) > 0 Vx e Rn, t € [0, T], this latter feature being due to the
```
```
eigenvalue restrictions imposed a priori on Q*(-)> Q/ and Rli(-}. Finally, the
```
corresponding "Nash" values for the cost functionals follow from the definition
```
of V(t,x) (see equation (6.67)).
```
Remark 6.16 The foregoing corollary provides only one set of feedback Nash
equilibrium strategies for the affine-quadratic game under consideration, and
it does not attribute any uniqueness feature to this solution set. However,
```
in view of the discrete-time counterpart of this result (cf. Corollary 6.1), one
```
```
would expect the solution to be unique under the condition that (6.70a) admits
```
```
a unique solution set; but, in order to verify this, one has to show that it i
```
```
not possible to come up with other (possibly nonlinear) solutions that satisfy
```
```
(6.67), and hitherto this has remained an unresolved problem. What has been
```
accomplished, though, is verification of uniqueness of feedback Nash equilibrium
```
when the players are restricted at the outset to affine memoryless strategies (cf.
```
```
Lukes, 1971).
```
Remark 6.17 As in the case of Remark 6.3, the result above extends readily
to more general affine-quadratic dynamic games where the cost functions of the
players contain additional terms that are linear in x, that is, with gl and ql in
Def. 6.5 replaced, respectively, by
```
where /*(•) is a known n-dimensional vector-valued function, continuous on
```
[0,T], and l^ is a fixed n-dimensional vector, for each i e N. Then, the state-
ment of Corollary 6.5 as well as its derivation remains intact, with only the
```
differential equation (6.72a) that generates £*(•) now reading
```
Remark 6.18 For general nonlinear-nonquadratic differential games wherein
the players are weakly coupled through the system equation as well as the cost
functions, the features observed in Remark 6.13 for the open-loop Nash solution
can also be derived for the feedback Nash solution, but now we have to use the
sufficiency result of Thm. 6.16. Again confining ourselves to the two-player case,
we take the system equation and the cost functions, as well as the expansion of
NASH AND SADDLE-POINT EQUILIBRIA 325
the state, to be in the same structural form as in Remark 6.13, and only replace
```
the expansion for ul(t; e) by a similar expansion for the feedback strategy:
```
Invoking a similar expansion on V1,
```
it can be shown (see Srikant and Ba§ar, 1991) that V^ (i = 1,2), the ze-
```
```
roth order terms, satisfy decoupled Hamilton-Jacobi-Bellman equations (asso-
```
```
ciated with the optimal control problems obtained by setting e = 0), and the
```
higher-order terms, V^ , k > 1, involve simple cost evaluations subject to some
state equation constraints. Furthermore, the higher-order feedback strategies,
7j , k > 1, are obtained from linear equations. More explicitly, for the zeroth
order we have
and 7- is obtained from the linear equation
```
Note that (6.74a) is the Hamilton-Jacobi-Bellman equation associated with
```
the optimal control problem obtained by setting e = 0 in the the differential
game with weakly coupled players, and hence 7. ' is the feedback representation
```
and to first-order V^ satisfies(1)
```
0
326 T. BA§AR AND G. J. OLSDER
of the open-loop policy wj obtained in Remark 6.13, on a common zeroth-order
trajectory x\ '. Hence, to zeroth order, there exists a complete equivalence be-
tween open-loop and feedback Nash equilibrium solutions in weakly coupled
```
differential games—as in the case of optimal control (see, for example, Bensous-
```
```
san, 1988). For higher-order terms, however, no such correspondence exists in
```
nonzero-sum differential games, which is one explanation for the nonequivalence
of the equilibrium trajectories under different information structures—a feature
we have observed throughout this chapter.
Feedback saddle-point solution of zero-sum differential games
We now first specialize the result of Thm. 6.16 to the class of two-person zero-
sum differential games of prescribed fixed duration, and obtain a single partial
```
differential equation to replace (6.69).
```
Corollary 6.6 For a two-person zero-sum differential game of prescribed fixed
duration [0, T], and under either MPS or CLPS information pattern, a pair of
```
strategies {7** 6 P;i = 1,2} provides a feedback saddle-point solution if there
```
exists a function V : [0,T] x Rn —> R satisfying the partial differential equation
Every such saddle-point solution is strongly time consistent, and the unique
```
saddle-point value of the game is V(O,XQ).
```
Proof. This result follows as a special case of Thm. 6.16 by taking N = 2,
```
g l ( - ) = -<?2(-) = #(•) and g1(-) = — q2(-) = q, in which case V1 = -V2 — V and
```
existence of a saddle point is equivalent to interchangeability of the min-max
operations.
```
Remark 6.19 The partial differential equation (6.75) was first obtained by
```
```
Isaacs in the early 1950s (see Isaacs, 1954-1957; see also Isaacs, 1975) and is
```
therefore generally referred to as Isaacs equation, and also as Hamilton-Jacobi-
Isaacs equation. Furthermore, the requirement for interchangeability of the
```
"min" and "max" operations in (6.75) is also known as the "Isaacs condition".
```
For more details on this, the reader is referred to Chapter 8.
```
We now obtain the solution of the Isaacs equation (6.75) for the case of a two-
```
```
person zero-sum affme-quadratic differential game described by (6.60a)-(6.60b).
```
```
Since the boundary condition is quadratic in x (i.e., V(T, x) = q(x) = ^x'Q/x),
```
```
we try out a solution in the form V(t,x) = ^x'Z(t)x + x'((t) +n(t), where Z(-)
```
```
//, furthermore, Q(-) > 0 and Qf > 0, then the solution to (6.76a), Z(t], is
```
nonnegative definite for all t € [0, T].
```
subject to the initial condition x(0) = XQ, and the saddle-point value of the game
```
is
```
where £(•) is obtained as the unique solution of (6.76b). The state trajectory
```
associated with this unique pair of feedback saddle-point strategies is given by
the solution of
The following theorem now even strengthens this result.
Theorem 6.17 Let there exist a unique symmetric bounded solution to the gen-
```
eralized matrix Riccati differential equation (6.76a) on the interval [0,T]. Then,
```
```
the two-person affine-quadratic zero-sum differential game described by (6.60a)-
```
```
(6.60b), and under the MPS or CLPS information pattern, admits a unique
```
feedback saddle-point solution given by
```
Since the kernel on the RHS is strictly convex (respectively, strictly concave) in
```
```
ul (respectively, u2), and is further separable, it admits a unique saddle-point
```
solution given by
If these unique values for u1 and u2 are substituted back into the RHS of the
```
Isaacs equation, we finally obtain the following differential equations for Z(-},
```
```
£(•) and n(-), by also taking into account the end-point conditions, the start-
```
```
ing assumption that Z is symmetric (which is no loss of generality) and the
```
```
requirement that equation (i) should be satisfied for all x G Rn:
```
NASH AND SADDLE-POINT EQUILIBRIA 327
```
is symmetric, with Z(T) = Qf, and C(T) = 0, n(T) = 0. Substitution of this
```
```
general quadratic structure in (6.75) yields the following equation:
```
328 T. BA§AR AND G. J. OLSDER
Proof. With the exception of the "uniqueness" of the feedback saddle-point
```
solution, and the nonnegative definiteness of Z ( - ) , the result follows as a special
```
case of Corollary 6.6, as explained prior to the statement of the theorem. To ver-
ify uniqueness of the solution, one approach is to go back to the Isaacs equation
and investigate existence and uniqueness properties of solutions of this partial
differential equation. This, however, requires some advance knowledge on the
theory of partial differential equations, which is beyond the scope of our coverage
in this book. In its stead, we shall choose here an alternative approach which
makes direct use of the affine-quadratic structure of the differential game. This
approach will provide us with the uniqueness result, as well as the equilibrium
```
strategies (6.77).
```
Toward this end, let us first note that, under the condition of unique solv-
```
ability of (6.76a), the linear differential equations (6.76b) and (6.76c) admit
```
```
unique solutions. Furthermore, x(T)'Qfx(T) can be written as
```
```
and using (6.76a)-(6.76c), the integrand can equivalently be written as
```
where we have suppressed the ^-dependencies of various terms. If we now sub-
```
stitute for x(-) its expression from (6.60a), we obtain (after canceling out some
```
```
common terms)
```
```
If this identity is used in (6.60b), the result is the following equivalent expression
```
```
for L(u\u2):
```
NASH AND SADDLE-POINT EQUILIBRIA 329
```
Since the first term above is independent of ul and w2, L(u1, u2) is separable in
```
```
u1 and u2, and it clearly admits the saddle-point solution given by (6.77). The
```
```
saddle-point value of L(ul, u2) also readily follows as ix[)Z(0)xo+XoC(0)+m(0).
```
```
To prove that £(•).>. 0 whenever Q(-) > 0 and Q/ > 0, follow the line of
```
```
reasoning used in verifying the last statement of Thm. 6.7 (for the discrete-time
```
```
counterpart). With c(-) = 0, the value function is V(t,x) = ^x'Z(t}x, which is
```
necessarily nonnegative since the maximizer can assure such a lower bound by
```
simply choosing w2 (-) = 0.
```
```
Remark 6.20 As in the case of Nash equilibria (cf. Remark 6.17), the result
```
above extends naturally to the more general affine-quadratic zero-sum dynamic
games which have in the cost function additional linear terms in x, to be denoted
```
x(T)'lf for the terminal cost term, and x(t)'l(t} for the integrand. Then, the
```
```
only modification will have to be made in equation (6.76b), for (,(•), which will
```
now read:
Otherwise, the statement of Thm. 6.17 remains intact.
Remark 6.21 A comparison of Thms. 6.14 and 6.17 reveals one common fea-
ture of the open-loop and feedback saddle-point solutions in affine-quadratic
```
differential games with Q(-) > 0, Qf > 0, which is akin to the one observed
```
```
for games described in discrete time (cf. Remark 6.7). The two matrix Riccati
```
```
differential equations (6.62a) and (6.76a) are identical, and so are the two lin-
```
```
ear differential equations for m and C ((6.62b) and (6.76b), respectively) and by
```
this token the state trajectories generated by the open-loop saddle-point solution
```
and feedback saddle-point solution are identical; and furthermore the open-loop
```
values of the feedback saddle-point strategies are correspondingly the same as
the open-loop saddle-point strategies. A comparison of the existence conditions
of Thms. 6.14 and 6.17, on the other hand, readily leads to the conclusion that
existence of an open-loop saddle-point solution necessarily implies existence of a
```
feedback saddle-point solution (since then (6.62a) (equivalently (6.76a)) admits
```
```
a unique bounded symmetric solution), but not vice versa. This latter feature
```
is a consequence of the following observation: for the class of affine-quadratic
differential games satisfying the restriction
```
the matrix Riccati differential equation (6.76a) admits a unique nonnegative
```
```
definite solution, whenever Qf > 0, Q(-) > 0 (since then (6.76a) becomes com-
```
```
parable with (5.35a)); however, (6.79) is not sufficient for (6.61) to admit a
```
bounded solution on [0,T]. This situation is now exemplified in the sequel.
Example 6.3 Consider a scalar linear-quadratic two-person zero-sum differen-
tial game described by
330 T. BA§AR AND G. J. OLSDER
```
For these parametric values, the Riccati differential equation (6.76a) can be
```
written as
which admits the unique solution
Hence, the differential game under consideration admits the unique feedback
saddle-point solution
Let us now investigate the existence of an open-loop saddle-point solution for
```
the same scalar differential game. The Riccati differential equation (6.61) can
```
be written, for this example, as
which admits a bounded solution only if T < 1, in which case
For T > 1, the Riccati differential equation does not admit a solution, and it
```
displays (in retrograde time) finite escape at t = T — 1; i.e., it has a conjugate
```
point at t = T — I . In our case, since T — 2 > 1, the existence condition of
Thm. 6.14 is thus not satisfied, and therefore an open-loop saddle-point solution
does not exist. This implies, in view of Lemma 6.4, that under the open-loop
```
information structure L(ul, u2) is not concave in u2, and hence the upper value
```
```
of the game is unbounded (since it is scalar and quadratic).
```
Even though feedback saddle-point equilibrium requires less stringent ex-
istence conditions than open-loop saddle-point equilibrium in afRne-quadratic
```
differential games (as also exemplified in the preceding example), this feature
```
```
does not necessarily imply (at the outset) that the least stringent conditions
```
for existence of saddle-point equilibrium under closed-loop information struc-
ture are required by the feedback saddle-point solution. This also brings up the
question of whether the affine-quadratic differential game still admits a saddle-
point solution in spite of existence of a conjugate point to the matrix Riccati
```
equation (6.76a) on the interval [0, T]. In other words, can a saddle point survive
```
a conjugate point? Intuitively, this could happen if at a conjugate point £*, while
Z goes off to infinity the corresponding state trajectory x* goes to zero at least
at the same rate, so that the control actions dictated by the feedback strategies
Clearly, being linear, these equations admit unique solutions regardless of the
length of the time interval, and hence the conjugate point of Z is the first instant
```
(in retrograde time) when X becomes singular. Let us adopt the convention that
```
```
at points where X is singular, X~l in (6.80) is replaced by its pseudo-inverse
```
```
X^. This then extends the solution of (6.76a) uniquely beyond a conjugate
```
```
point. The following result, which is from (Bernhard, 1979), now provides the
```
necessity part of Thm. 6.17.
Theorem 6.18 Consider the affine-quadratic zero-sum differential game de-
```
scribed by (6.60a)-(6.60b), with Q(-) > 0, Qf > 0, and under the CLPS infor-
```
```
mation structure. The game admits a saddle point (with finite value) if, and
```
only if, the following four conditions are satisfied:
```
(i) XQ e Range (X(0)),
```
```
(ii) rank of X ( t ) is piecewise constant,
```
```
(in) Range (B2(t}} C Range (X(t)), \/t e [0,T], except, perhaps, at isolated
```
points,
```
(iv) Z(i] > 0 Vt G [0,T], under the extended definition of a solution provided
```
```
by (6.80) and (6.81a)-(6.81b).
```
```
Then, the pair of feedback strategies (6.77) provides a saddle-point solution,
```
```
and the saddle-point value is given by (6.78b). If the conditions above are not
```
satisfied, then the upper value of the game becomes unbounded.
```
The following example (again due to Bernhard (1979)) illustrates the fore-
```
going result, and demonstrates the occurrence of an "even" conjugate point.
NASH AND SADDLE-POINT EQUILIBRIA 331
```
remain (square-) integrable, and that Z does not change sign in any small open
```
neighborhood oft*. To permit for such behavior, the allowable strategies can no
longer be continuously differentiable, or even continuous, but their realizations
will have to be square-integrable for every square-integrable open-loop policy
```
of the opponent player. Bernhard (1979) has shown, by working in this more
```
general class of strategies, that a saddle point may survive a conjugate point, if
the conjugate point is of an "even" type.
To give a brief description of this result, let us first note that in the absence
```
of a conjugate point, the solution of (6.76a) can be decomposed as
```
where X and Y are n x n matrices satisfying the linear matrix differential
equations
332 T. BA§AR AND G. J. OLSDER
Example 6.4 Consider the time-varying scalar linear-quadratic differential game
described by
```
The Riccati equation (6.76a) in this case is
```
```
which has a conjugate point at t = I. Using the construction (6.80), however,
```
we can obtain an extended solution as
```
This shows that the conjugate point t* = 1 is of the even type (i.e., Z(t]
```
```
does not change sign in an open neighborhood of t*). Using this solution, the
```
```
strategies (6.77) can be written as
```
with the corresponding state trajectory satisfying
```
Note that, for t < 1, x*(t) = (1 — t)2, and hence open-loop representations of
```
both 71 and 72* remain bounded as t t 1, and so does the value function.
Note also that all four conditions of Thm. 6.18 are satisfied in this case, with
```
X(t} = (l-t}2.
```
Another perspective on the relationship between existence of conjugate points
and saddle points in zero-sum affine-quadratic games can be obtained by study-
ing the dependence of the solution on a scalar parameter. Of particular impor-
```
tance (as we shall later see in Section 6.6) is the case when the parameter is
```
chosen as the weighting on the control of the maximizer. Consider, therefore,
```
the modified cost (in place of (6.60b))
```
```
where A is a scalar positive parameter, Q(-) > 0 and Q/ > 0. Let the state dy-
```
```
namics be again described by (6.60a). Then, the counterpart of the generalized
```
```
Riccati differential equation (6.76a) is
```
NASH AND SADDLE-POINT EQUILIBRIA 333
```
whose solution over the interval [0,T] we will denote by Z\(-}. Note that the
```
```
limiting case A = oc yields the Riccati equation (5.35a) associated with the one-
```
```
player game (with the maximizer being absent), which admits a nonnegative
```
definite solution. Hence, by continuity, for every fixed T there exists some range
```
of values of A for which (6.83) does not have any conjugate point on [0, T]. What
```
```
can in fact be shown (see Ba§arand Bernhard (1995)) is that there exists some
```
```
threshold value of A, say A*, such that (i) for all A > A*, (6.83) has no conjugate
```
```
points (and hence the game has a finite value under CLPS information), (ii) for
```
```
all A < A*, (6.83) has a conjugate point, and (iii) for all A < A*, the upper value
```
```
of the differential game is unbounded (regardless of the underlying information
```
```
structure). We summarize this result in the following theorem.
```
Theorem 6.19 Consider the parameterized affine-quadratic zero-sum differen-
```
tial game described by (6.60a) and (6.82), with Q(-} > 0, Qf > 0. There exists
```
a critical value A* > 0 for A such that
```
(i) For each A > A*, (6.83) admits a unique nonnegative definite solution
```
```
Z\(-) (which is positive definite if either Qf > 0 or Q(-) > 0), and the
```
```
game has a finite saddle-point value, given by (6.78b) with Z replaced by
```
```
Zx, and B2B2' in (6.76b) and (6.76c) replaced by ±B2B2'.
```
```
(ii) For A < A*, (6.83) has a conjugate point.
```
```
(iii) For A < A*, the upper value of the game is unbounded, even if the min-
```
```
imizer has access to full information (on the control action of the maxi-
```
```
mizer).
```
```
(iv) For A > A*, the unique feedback saddle-point strategies of the players are
```
```
where CA(') is given by (6.76b) with Z\ replacing Z and j^B2B2 replacing
```
B2B2'.
```
Proof. A proof of this result for the linear case (with c — 0) can be found
```
```
in Chapter 9 of Ba§ar and Bernhard (1995), which readily extends to the affine
```
case in view of Thm. 6.17.
6.5.3 Linear-quadratic differential games on an
infinite time horizon
In this subsection, we will study the limiting cases of the Nash equilibrium
solution of the linear-quadratic differential game as the time horizon becomes
unbounded—first for the open-loop case and then for the closed-loop feedback
case.
Define
```
i.e., Qmv is the set of all (7-invariant subspaces of R3n. It is well known that this
```
```
set consists of a finite number of (different) elements if and only if all eigenvalues
```
```
of G have geometric multiplicity one (otherwise there is an infinite number of
```
```
different elements). Next define the subset £sub of £inv such that
```
where S refers to "image of. Note that the elements in <?sub are the images
```
of suitably chosen matrices of size 3n x n. In Engwerda (1998) the following
```
theorem has been proved.
```
Theorem 6.20 The set of coupled algebraic Riccati equations (6.84) and (6.85)
```
```
have a real solution (M1, M2) if, and only if, Ml = XiX-1, i = 1,2 for
```
matrices X, X\ and X2 such that
```
Moreover, if the control functions ul*(t} = —(Ru) 1Bl M*<fr(£,0)xo are used inty
```
the system equation x = Ax -f Y^J=I BJuJ, the eigenvalues of the closed-loop
matrix A — ^2j=i S^M^ coincide with the eigenvalues of — G\K, which is the
restriction of —G to the subspace /C.
334 T. BA§AR AND G. J. OLSDER
The open-loop case
Here, we will study the limiting case of Thm. 6.12 as the time horizon becomes
unbounded. The starting point will be Def. 6.5, with the assumptions that N =
2 and all matrices involved are time-invariant, and Q\ — Q^ = 0. Moreover, we
will restrict ourselves to the linear-quadratic case, i.e., c = 0.
```
The so-called algebraic Riccati equations (compare with (6.49))
```
```
where, as before, Sl — Bl(Ru) 1BT , i = 1,2, play an important role here.
```
```
Under certain technical conditions solutions of (6.49), with N = 2, will approach
```
```
the solutions of (6.84) and (6.85) as T —> oo. Since the dependence of the
```
```
solutions of (6.49) on the final time T will be emphasized, these solutions will be
```
```
written as Mz(t, T). We will elaborate on the relationship between the solutions
```
of these equations and the matrix G:
NASH AND SADDLE-POINT EQUILIBRIA 335
```
It follows that every element of C/sub defines one solution of (6.84) and (6.85). If
```
the geometric multiplicity of all eigenvalues of G equals one, then the set Qsub
```
has a finite number of different elements and consequently (6.84) and (6.85)
```
```
have at most a finite number of real solutions. Equations (6.84) and (6.85) do
```
not have a real solution if and only if £sub is empty. An immediate consequence
of Thm. 6.20 is the following corollary.
```
Corollary 6.7 The algebraic Riccati equations (6.84) ana (6.85) have a set
```
```
of stabilizing solutions (i.e., the closed-loop matrix A — Y^J=I ^M^ has all its
```
```
eigenvalues in the left-half plane) if, and only if a G-invariant subspace K. £ £/sub
```
```
exists such that 3£A > 0 for all A € a(G\^} (the symbol a refers to the spectrum
```
```
of its argument).
```
We now introduce a useful concept, which will lead to a technical condition
needed in the main result.
Definition 6.7 The 3n x 3n matrix G is called dichotomically separable if sub-
spaces V\ and V<2, with dimVi = n and dimV2 = 2n, exist such that GVi €
```
Vi, i = l,2, V i 0 V 2 = R 3 n a n d 3 f c A > K / z / o r a / / A e < 7 ( G | V l ) , ^ e a(G|Va).
```
It can be shown that dichotomic separability implies that C?sub is nonempty, but
the reverse implication is not necessarily true.
```
Theorem 6.21 Assume that H(T] is invertible for all finite T, G is dichotom-
```
ically separable and
```
Then, MZ(0,T) —> XiX 1, i = l,2ifT—>oo, where X and Xi are defined as
```
```
Vi = span(X' X'2 X'2}'.
```
Corollary 6.7 and Thm. 6.21 immediately lead to the following corollary.
Corollary 6.8 // the time horizon of the linear quadratic differential game
```
tends to infinity, then the unique open-loop Nash equilibrium converges to ul*(t] =
```
```
— (Rll}~1B'1' Ml<&(Q,t}xQ, i = 1,2, which leads to a stable system, if the follow-
```
ing conditions are satisfied:
```
The conditions of Thm. 6.21 are fulfilled;
```
```
ft\>Oforall\£a(G\Vl).
```
For the cost function of each player to remain bounded, we assume that Ql >
0, i = 1, 2, and under the equilibrium solution the state of the system converges
to zero as t tends to infinity.
336 T. BA§AR AND G. J. OLSDER
```
Theorem 6.22 Let M*, i = 1,2, be solutions of (6.84) and (6.85) satisfy-
```
ing the additional constraint that the eigenvalues of the resulting system ma-
```
trix A — J2i=i SlMl are all in the left-half plane. Then the controls ul(t] =
```
```
—(Rll}~lBl Mt$(Q,t)xQ, i = 1,2, form an open-loop Nash equilibrium solution.
```
```
Proof. Let Ll(ul, u2; T) denote the cost function of Pl, on the time interval
```
[0, T]. It can then be shown that the optimization problem
```
where u2*(£) = -(R22}~1B2''MV-4-^1-^^^, admits a solution given
```
```
by u1*^) = -(Ru}-lBl>Mle(A-§lMl-§2M^tXQ. A similar reasoning holds for
```
the optimization problem
```
leading to the conclusion that (u1*,ii2*) forms an open-loop Nash equilibrium
```
solution.
The following example now demonstrates the somewhat surprising fact that
an infinite-horizon differential game may admit a Nash equilibrium solution in
```
open-loop policies, while its finite-horizon version (with a particular terminal
```
```
state weighting) may not.
```
Example 6.5 Reconsider Example 6.2 and replace the ^4-matrix by
```
The eigenvalues of the related (7-matrix are {^, 1.8810,0.1883, i,-1.7966}.
```
Further numerical calculation shows that the algebraic Riccati equations have
three stabilizing solutions. Hence, according to Thm. 6.22, the infinite-horizon
problem has at least three open-loop Nash equilibrium solutions.
On the other hand, the matrices Q\, i = 1,2, can suitably be chosen such
```
that the matrix G(t] fails to be invertible for certain i, e.g., t = 0.1 as in
```
Example 6.2. Thus the infinite-horizon problem can have solutions whereas the
finite-horizon problem fails to have a solution.
One might conjecture that if the finite-horizon game has a unique solution
for all finite T, then this solution converges to a solution of the infinite-horizon
```
game. This is not true, as shown in Engwerda (1998), which presents an example
```
in which the limiting solution does lead to a solution of the coupled algebraic
Riccati equations, but fails to be a stabilizing solution.
The closed-loop feedback case
```
We now study the limiting cases of Corollary 6.5 and Thm. 6.17 (as well as
```
```
Thm. 6.19) as the time horizon becomes unbounded (that is, T —* oo), when
```
NASH AND SADDLE-POINT EQUILIBRIA 337
```
c = 0, Qf — 0, and all matrices are time-invariant. This will constitute the
```
counterparts of the discrete-time results of subsection 6.2.3 in the continuous
time.
First we consider the convex-quadratic Nash differential game covered by
```
Corollary 6.5, and note that any asymptotically convergent (as T —> oo) limit
```
```
of the solution of (6.70a) will have to satisfy the coupled algebraic Riccati equa-
```
```
tions:
```
```
where F is given by (6.70b), with all matrices replaced by their time-invariant
```
```
counterparts.63 The limiting stationary feedback (Nash) strategies are (from
```
```
(6.71))
```
Furthermore, introduce for each i 6 N the matrices
Then the following result is the natural counterpart of Prop. 6.3 in the contin-
uous time.
```
Proposition 6.8 Let there exist an N-tuple of matrices {Zl,i 6 N} satisfy-
```
```
ing (6.87a), and further satisfying the conditions that for each i € N the pair
```
```
(Fi,Bl) is stabilizable and the pair (Fi,Qi) is detectable.64 Then,
```
```
(i) the N-tuple of stationary feedback policies (6.87b) provides a Nash equi-
```
librium solution for the linear-quadratic nonzero-sum differential game
of this subsection, leading to the infinite-horizon Nash equilibrium cost
^XQ'ZIXQ for Pi,
```
(ii) the resulting system dynamics, described by
```
are asymptotically stable.
Again, as in the discrete-time case, there could exist a Nash equilibrium for
```
the infinite-horizon game (as covered by Prop. 6.8) which is not obtainable as
```
the limit of any Nash equilibrium of the time-truncated finite-horizon version.
```
Furthermore, we do not have any direct conditions (on the system parameters)
```
that would guarantee satisfaction of the indirect conditions of Prop. 6.8 for any
63Note that here we are using the same notation as in the discrete-time case, but this should
not create any confusion.64
See subsection 5.5.2 for the terminology.
338 T. BA§AR AND G. J. OLSDER
sufficiently general class of nonzero-sum differential games. For zero-sum games,
however, more explicit results can be obtained as will be discussed next.
```
Consider the class of zero-sum differential games described by (6.60a)-(6.60b),
```
with c = 0, Qf = 0, all matrices time-invariant, Q > 0, and T = oo. As in
subsection 6.2.3, we would expect the feedback saddle-point solution for this
game to be in the form
```
where Z is the limiting solution of (6.76a) which (if it exists) should satisfy the
```
```
continuous-time generalized algebraic Riccati equation (GARE)
```
```
Denote the solution of the generalized Riccati differential equation (6.76a) by
```
```
Z(£;T), where dependence on the terminal time is explicitly recognized. Now,
```
```
the first question that would be of interest is: If Z(t; T) is bounded for all
```
```
T > 0 (i.e., (6.76a) does not have any conjugate points on [0,T] for any T) and
```
```
limj'_00 Z(t,T) = Z exists, does this necessarily imply that the pair (6.88a)-
```
```
(6.88b) is in saddle-point equilibrium? The answer is "no", as the following
```
```
(counter) example, taken from (Mageirou, 1976) clearly illustrates.
```
Example 6.6 Consider the scalar state dynamics
and the objective function
```
The generalized Riccati differential equation (6.76a) associated with this game
```
is
which admits the unique solution
This function has a well-defined limit, as T —» oo, which is ^-independent:
```
Hence the policies (6.88a)-(6.88b) are
```
NASH AND SADDLE-POINT EQUILIBRIA 339
Note that the closed-loop system under these policies is
which is asymptotically stable. Also, the system only under 7* is
which is bounded input-bounded state stable.
```
Now it is easy to see (using Prop. 5.4) that if ul = 71 (-)5 the policy for P2
```
```
that maximizes L is j2. However, when u2 — 72 (-)> 71 is not a minimizing
```
solution for PI . The underlying minimization problem is
Since the open-loop system is unstable, and there is a negative cost on state in
```
the cost function, the cost can be driven to — oo (pick, for example, u1 =0).
```
Even though there is no continuity in the minimizer's policy at T — oo,
```
nevertheless the value of the game (even in the absence of a saddle point) is
```
continuous. It can be shown that given an e > 0, there exists a t£ > 0 such that
by choosing the time-varying feedback policy
P2 can assure that
where
```
which is the limit of the value of the finite-horizon game (as T —> oo).65 Since
```
e > 0 can be chosen arbitrarily small, this shows that the infinite-horizon game
of this example indeed has a value.
What has been demonstrated in the last part of the example above actually
turns out to be the case for the general linear-quadratic differential game, un-
der some general conditions, as stated in the following theorem, which is the
counterpart of Thm. 6.8, as well as of Lemma 6.3.
subject to
65 if. above is chosen such that
340 T. BA§AR AND G. J. OLSDER
Theorem 6.23 Consider the infinite-horizon time-invariant linear-quadratic dif-
```
ferential game, where Qf — 0, Q > 0 and the pair (A, Q) is observable. Then,
```
```
(i) for each fixed t, the solution to (6.76a), Z(t;T), is nondecreasing in T; that
```
```
is, if (6.76a) has no conjugate point in a given interval [0,T], Z(t;T'} —
```
```
Z(t- T"} >0,T>T'>T"> 0;
```
```
(ii) every nonnegative definite solution of (6.89) is in fact positive definite, and
```
if there exists a positive definite solution there is a minimal such solution,
```
denoted Z+. This matrix has the property that Z+ — Z(t;T) > 0 for all
```
```
r > 0 ;
```
```
(Hi) the game has equal upper and lower values if, and only if, the GARE
```
```
(6.89) admits a positive definite solution, in which case the common value
```
```
is J°°* = x'0Z+x0;
```
```
(iv) the upper value is finite if, and only if, the upper and lower values are
```
```
equal;
```
```
(v) if Z+ > 0 exists, as given above, the controller^1 given by (6.88a), with
```
Z replaced by Z+, attains the finite upper value, in the sense that
```
and the maximizing feedback solution above is given by (6.88b), again with
```
```
Z = Z+;
```
```
(vi) the minimal positive definite solution of (6.89), Z+, is feedback stabilizing,
```
i.e., the closed-loop system
has no eigenvalues in the right-half plane. If Q > 0, then Z+ is the only
```
feedback stabilizing solution of (6.89).
```
```
Proof. See Mageirou (1976) or Ba§arand Bernhard (1995).
```
The result of Thm. 6.23 above is not the strongest possible, since the given
```
policies (6.88a)-(6.88b) are not strictly feedback stabilizing (i.e., the closed-loop
```
feedback matrix could have some eigenvalues on the imaginary axis of the com-
```
plex plane) and furthermore the policies are not necessarily in saddle-point
```
```
equilibrium (as also observed in Example 6.6). In view of this one would like to
```
```
obtain conditions under which the policies (6.88a)-(6.88b) would be in saddle-
```
point equilibrium and/or they would be strictly feedback stabilizing.
```
If we assume that there exists a solution to GARE (6.89) that is strictly feed-
```
```
back stabilizing, then it can be shown (see Jacobson (1977)) that the strategy
```
```
pair (6.88a)-(6.88b) is in saddle-point equilibrium in the restricted class of feed-
```
```
back policies that are strictly feedback stabilizing and under which x(t] —> 0
```
as t —> oo for all XQ 6 Rn. This result follows essentially from a completion
NASH AND SADDLE-POINT EQUILIBRIA 341
```
of squares argument, using the GARE (6.89). An alternative condition, again
```
under the assumption of existence of a strictly feedback stabilizing solution to
```
(6.89), is to require that under 72, the minimization problem faced by PI is well
```
```
defined; this is equivalent to requiring (using some results of Willems (1971))
```
```
that an associated ARE has a negative definite solution (see Mageirou (1976)).
```
But, this is a rather complicated condition, and admits no clean interpretation
in terms of the parameters of the game.
Yet another set of conditions can be obtained, by parameterizing the dif-
ferential game, as in the finite-horizon case discussed earlier. Consider the
```
infinite-horizon version of the cost function (6.82), where A is again a scalar
```
```
parameter (to be varied). For A = oo, the underlying problem is essentially a
```
single player game, which is the same as the linear regulator problem discussed
```
in subsection 5.5.2. We know from Prop. 5.4 that if (A, B) is stabilizable and
```
```
(A, Q) is observable (respectively, detectable), then the associated ARE admits
```
```
a positive (respectively, nonnegative) definite solution, which is further strictly
```
feedback stabilizing. Since eigenvalues of the feedback matrix in the game case
are continuous functions of A, the result will also hold for some range of finite
values for A. This result is now made precise under also a weaker condition of
detectability in the following theorem, which is the infinite-horizon counterpart
```
of Thm. 6.19. Its proof can be found in Ba§arand Bernhard (1995).
```
Theorem 6.24 For the parameterized infinite-horizon linear-quadratic differ-
```
ential game formulated above, let (A, B) be stabilizable and (A, Q) be observable
```
```
(respectively, detectable). Then, there exists a threshold value A^ > 0 for A
```
such that the following three properties hold.
```
(i) For every A > A^, the GARE
```
```
admits a minimal positive (respectively, nonnegative) definite solution, Z\.
```
```
(ii) For every A > A^7 the game has a finite upper value, which is achieved
```
by the pair of strategies
Furthermore, the closed-loop matrix
```
is Hurwitz (has all its eigenvalues in the open left-half plane).
```
```
(Hi) For A < A^, the upper value of the game is unbounded.
```
342 T. BA§AR AND G. J. OLSDER
6.6 Applications in Robust Controller Designs:
H°°-Optimal Control
The results presented in the previous section on zero-sum differential games, and
especially on linear-quadratic games, as well as their counterparts in Section 6.3,
have important applications in a special class of worst-case controller design
```
problems, known as "/f°°-optimal control" problems (see Zames (1981); Fran-
```
```
cis (1987); Doyle et al. (1989); Ba§arand Bernhard (1995); Stoorvogel (1992),
```
```
Green and Limebeer (1995) to cite just a few representative papers and books
```
```
from this voluminous literature). In these design problems, the (linear) plant
```
```
has two types of inputs (controlled and disturbance) and two types of outputs
```
```
(regulated and measured), and the objective is to obtain a controller (that uses
```
```
the measured output) such that the effect of the disturbance on the regulated
```
output is minimized. Even though both frequency and time domain formula-
tions are possible, the latter is preferable since it allows for transient analysis
and can handle time varying as well as nonlinear plants.
In this section we first present a general formulation of this worst-case design
problem, and then show how the theory of the previous section can directly be
```
used to construct minimax (jF7°°-optimal) controllers. The presentation will be
```
brief, and be restricted to the continuous-time problem only. Further details,
and counterparts of these results in the discrete-time, can be found in the book
```
by Ba§arand Bernhard (1995).
```
```
To introduce the H°°-optimal control problem (in the continuous-time), let
```
us adopt the plant dynamics
```
to replace the linear state equation (6.60a). Here u is the controlled input, w is
```
the disturbance, and A, B, D are matrices which could have time-varying entries
if the time horizon is not infinite. Let the regulated and measured outputs be
given, respectively, by
where H, G, C, E are again appropriate dimensional matrices, with possibly
time-variant entries for the finite-horizon problem. The controlled input is cho-
sen as a function of the present and past values of the measured output y, a
relationship that we will write as
```
or simply as u = n(y). Here // is the (causal) control policy. If in (6.92c) C = I
```
and E = 0, then we have the CLPS information pattern, where the control has
access to all components of the state.
Now for any given JJL € M, where M is chosen so that some measurability and
```
smoothness conditions are satisfied, substitution of u into (6.92a) and (6.92c)
```
NASH AND SADDLE-POINT EQUILIBRIA 343
leads to a unique mapping from w to z, obtained from,
Let us denote this mapping by £M, i.e.,
```
Then, the /f°°-optimal control problem is to minimize the induced (operator)
```
norm of C/M, that is, to solve the optimization problem
where || • \\z and \\-\\w denote Hilbert space norms on the regulated output and
disturbance spaces, respectively, where the latter space is denoted by "H. More
specifically, for the finite-horizon problem, defined on the interval [0,T],
```
and for the infinite-horizon problem T = oo, and Qf — 0. Note that (6.94)
```
```
defines the upper value of a differential game where the minimizer (PI) is the
```
```
controller, and the maximizer (P2) is the disturbance. Let the square of this
```
upper value be denoted by A, and assume for the moment that there exists a
```
control policy, //*, that achieves it. Then, (6.94) can equivalently be expressed
```
```
as (where we suppress the subindices on different norms)
```
and
```
(ii) there exists no other p G M (say, /i), and a corresponding A = A < A, such
```
that
```
Now, introducing the parameterized (in A > 0) family of cost functions
```
```
(i) and (ii) above become equivalent to the problem of finding the "smallest"
```
value of A > 0 under which the upper value of the associated game with objective
```
function L\(n, w) is zero, and finding the corresponding controller that achieves
```
this upper value.
344 T. BA§AR AND G. J. OLSDER
We now relate this problem to the ones covered by Thms. 6.17, 6.19, 6.23
and 6.24, when the information pattern is CLPS. Toward this end let us first
```
make the simplifying assumptions that H'G = 0, G'G = /, H'(T}QfG(T] = 0
```
```
and G'(T)QfG(T) = 0. Then, Lx can be written as (6.82), with
```
In the finite horizon, we then have precisely the linear-quadratic game covered
in Thm. 6.19, with Bl replaced by B, B2 replaced by D, and XQ = 0. We know
from Thm. 6.19, together with Thm. 6.18, that the upper and lower values are
equal whenever the upper value is bounded, and that this finite value is zero
```
(since XQ = 0). Furthermore, where this breaks down (that is, when the upper
```
```
value becomes unbounded) is at the largest value of A for which the generalized
```
```
Riccati differential equation (6.83), that is,
```
```
has a conjugate point. This value of A (which we had denoted A*) is precisely the
```
```
A introduced earlier as the square of (6.94). Hence the following result follows
```
as a direct corollary to Thm. 6.19.
Corollary 6.9 For the finite-horizon H°°-optimal control problem formulated
above, and with CLPS information, let A* be defined as in Thm. 6.19, in con-
```
nection with the Riccati differential equation (6.98). Then,
```
```
(i)
```
```
(ii) for alll
```
delivers a performance level of at least V% that is, for all w £ H,
For the infinite-horizon problem, the counterpart of Corollary 6.9 is the
following corollary to Thm. 6.24.
Corollary 6.10 For the infinite-horizon H°°-optimal control problem, and with
```
CLPS information, let (A, B) be stabilizable and (A, Q) be observable (respec-
```
```
tively, detectable). Let A* be the smallest positive scalar with the property that
```
for all A > A*, the GARE
```
admits a minimal positive (respectively, nonnegative) definite solution Z\. Then,
```
the control policy
NASH AND SADDLE-POINT EQUILIBRIA 345
```
(ii) for all A > A*7 the feedback control policy
```
delivers a performance level of at least v\, and under it the linear system
is bounded-input bounded-state stable.
```
The following scalar example (taken from Ba§arand Bernhard (1995)) illus-
```
trates the two corollaries above.
Example 6.7 Consider the scalar plant with dynamics
and //^-performance index:
The associated GRDE is
whose unique solution takes two different forms, depending on whether A > 1
or not. For the former case,
Note that the given solution family is continuous at A = 1. Now, for fixed
```
T, the GRDE has a conjugate point in the interval [0,T] if A < A*(T), and
```
```
for all A < A*(T) the upper value of the related game is unbounded. Hence,
```
```
the optimum H°°-performance level (that is, the upper value of the game with
```
```
performance index (i)) is ^/X*(T). For A > A*(T), the controller that guarantees
```
this level of performance is
and for the latter,
which exists provided that
346 T. BA§AR AND G. J. OLSDER
```
Note that as A J, A*(T), Z\(0) | co, and hence the "optimum" controller is a
```
```
high gain controller, which may not be desirable. For any A > A*(T), however,
```
```
the control gain is finite, and one can achieve e-optimal performance (that is,
```
```
one that is within an e-neighborhood of A*(T)) using finite gain controllers of
```
```
the type (iii).
```
For the infinite-horizon case, we simply let T | oo, leading to A* = 1, and
the unique stationary feedback controller
Again note that the solution ceases to exist as A j A* = 1, and hence an
"optimal" controller does not exist, but an e-optimal controller does.
We now consider the imperfect state measurements case, as originally for-
mulated. For technical reasons we will treat the initial state XQ as also unknown
and as part of the disturbance, and attach weight to it in the cost function.
Furthermore, we take the system and measurement disturbances to be indepen-
```
dent (which is tantamount to taking ED' — 0), and take without any loss of
```
generality EE' = I and let Ew = v, which is viewed as a separate disturbance
```
(in addition to w and :TO). Then, the associated parameterized game is one with
```
state dynamics
measurement equation
```
and objective function (for finite horizon)
```
For the infinite horizon, we simply let T = oo, Q/ = 0. In view of our earlier
discussion in this section, we seek the smallest value of A, say A*, such that for
```
all A > A*, and with uj = {w, v, XQ},
```
```
where M. is defined here as the class of all (measurable, smooth) control policies
```
that depend causally on y.
The theory we have developed in this chapter does not directly apply to
this problem, since the information structure is neither CLPS nor OL. Similar
game theoretic techniques can be used, however, to obtain a complete, very
NASH AND SADDLE-POINT EQUILIBRIA 347
appealing solution to the problem. The result is given below in Thm. 6.25 for
the finite horizon, and Thm. 6.26 for the infinite-horizon case. Verifications of
these results will be sketchy, since the details are lengthy and fairly technical.
```
A complete treatment can be found in Ba§arand Bernhard (1995).
```
```
Before presenting Thm. 6.25, we introduce a second GARE, dual to (6.98),
```
again defined on [0,Tj:
We further define
whenever it exists, and introduce two different equations
and
Theorem 6.25 Let A be the set of all X > 0 such that
```
(a) the GRDE (6.98) does not have a conjugate point on [0, T],
```
```
(b) the GRDE (6.102) does not have a conjugate point on [0,T],
```
and
```
(c) the matrix XI — S(£)Z(t) has only positive eigenvalues for all t € [0, T], or
```
equivalently
```
where p(-} is the spectral radius.
```
For each A € A, let a controller n\ G M. be defined by
```
where Z\ is the solution of (6.98), and x\ is generated by (6.102). Then,
```
```
(i) inf{A > 0 : A € A} — A*, where A* is the smallest positive scalar A such
```
```
that (6.101) holds;
```
```
(ii) for each A > A*, the controller given by (6.106a) delivers a performance
```
```
level of at least \/A;
```
```
(iii) for each A > A*; the controller (6.106a) can equivalently be written as
```
348 T. BA§AR AND G. J. OLSDER
```
where G\ is given by (6.103) and x is generated by (6.104b).
```
Proof. We will sketch here a proof of sufficiency, using completion of squares.
```
First note that since Q$l > 0, the solution of (6.102) is positive definite, thus
```
admitting an inverse. Define the function
and add the identically zero function
```
to L\ given by (G.lOOc). The derivative V involves the derivatives of G, S"1
```
```
and e = x — X, which can be shown to be given by (through elementary but
```
```
extensive manipulations)
```
If all these relationships are used in L\, it simplifies to
where || • || denotes the appropriate Hilbert space norm. Now, invoking condition
```
(c) of the theorem we conclude that
```
```
and hence the first two terms in (o) are nonpositive. Furthermore, since the
```
```
control given by (6.106b) makes the third (nonnegative) term in (o) zero, we
```
have the bound
```
Its maximum is actually equal to zero, which can be seen by picking in (o),
```
```
This completes the proof of (ii). To prove (iii), we start with the transformation
```
```
and differentiate the RHS with respect to t. This leads (after some algebra) to
```
```
the conclusion that x satisfies (6.104a) whenever x satisfies (6.104b).
```
NASH AND SADDLE-POINT EQUILIBRIA 349
```
Remark 6.22 The proof given above for sufficiency (that is, parts (ii) and
```
```
(iii)) is an indirect one, and follows the arguments of Uchida and Fujita (1989).
```
```
Ba§ar and Bernhard (1995) provide a more constructive proof, for all three parts
```
```
of the theorem. Other proofs can be found in Limebeer et al. (1989), Khar-
```
```
gonekar (1991) and Khargonekar, Nagpal and Poolla (1991) to cite just a few.
```
An important observation to be made in the context of Thm. 6.25 is the
```
duality between the two GRDEs (6.98) and (6.102). This duality enables us
```
```
to deduce the behavior of the minimax controller fi\ given by (6.106a) for the
```
time-invariant problem, as T —> oo, by making direct use of the results of
subsection 6.5.3, as well as that of Corollary 6.10. This leads to the following
counterpart of Corollary 6.10.
Theorem 6.26 For the infinite-horizon H°°-optimal control problem with im-
```
perfect state measurements, as defined above, let (A, B] be stabilizable, (A, Q)
```
```
be detectable, (A, C) be detectable, and (A, D) be stabilizable. Let A00 be the
```
```
set of all X > 0 such that (6.99) admits a minimal nonnegative definite solution
```
```
(denoted Z^), the following dual GARE admits a minimal nonnegative definite
```
```
solution (denoted S^j:
```
and the following spectral radius condition is satisfied:
Then.
```
(i) inf{A > 0 : A € A00} = A^; where A^ is the smallest positive scalar A
```
```
such that the infinite-horizon version of (6.101) holds;
```
```
(ii) for each X > A^; the controller
```
where
delivers a performance level of at least
We close this section by revisiting Example 6.7 for the imperfect state mea-
surements case.
Example 6.8 Consider Example 6.7 now with a measurement equation
350 T. BA§AR AND G. J. OLSDER
and the //^-performance index compatibly replaced by
where we have abused the notation and have taken Dw = w. Note that since
```
XQ = 0, we have taken in (G.lOOc) QQI = 0. In view of this, the second GRDE
```
```
(6.102) reads
```
```
which is the dual of (ii) in Example 6.7 (simply the time is reversed), and hence
```
```
requires the same conjugate-point condition (A < 4T2/(4T2 -f 7r2)) as the Z
```
```
equation. Using the notation of Example 6.7, for A > A*(T) we have
```
```
and hence the spectral radius condition (6.105) is
```
```
which is more restrictive than the other two conditions (which are identical):
```
```
A < A*(T). Hence, in this case the spectral radius condition is binding.
```
For the infinite-horizon case, again from duality existence of Z+ and E+
requires the same condition, which is A > 1. The spectral radius condition is
Hence A^, = 2, and again the spectral radius condition is binding.
6.7 Stochastic Differential Games with
Deterministic Information Patterns
As it has been discussed in subsection 6.5.2, one possible way of eliminating
"informational nonuniqueness" of Nash equilibria in nonzero-sum differential
games is to require the equilibrium solution also to satisfy a feedback property
```
(cf. Def. 6.2), which necessarily leads to strongly time consistent solutions; but
```
this is a valid approach only if the information structure of the problem per-
mits the players to have access to the current value of the state. Yet another
approach to eliminate informational nonuniqueness is to formulate the problem
in a stochastic framework, and this has been done in Section 6.4 for dynamic
games defined in discrete time.
```
A stochastic formulation for quantitative differential games (i.e., dynamic
```
```
games defined in continuous time) of prescribed duration involves (cf. Sc-
```
```
tion 5.3) a stochastic differential equation
```
NASH AND SADDLE-POINT EQUILIBRIA 351
```
which describes the evolution of the state (and replaces (6.37)), and N cost
```
functionals
```
If P (i G N) denotes the strategy-space of Pi (compatible with the determin-
```
```
istic information pattern of the game), his expected cost functional, when the
```
differential game is brought to normal form, is given by
```
with 7J € Ti(j 6 N) and E[-] denoting the expectation operation taken with re-
```
```
spect to the statistics of the standard Wiener process {wt, t > 0}. Furthermore,
```
F and a are assumed to satisfy the requirements of Thm. 5.2, and in particular
<j is assumed to be nonsingular.
This section presents counterparts of some of the results of Section 6.4 for
```
stochastic nonzero-sum differential games of the type described above; the anal-
```
```
ysis, however, will be rather informal since the underlying mathematics (on
```
```
stochastic differential equations, stochastic calculus, etc.) is beyond the scope
```
of this volume. Appropriate references will be given throughout for the inter-
ested reader who requires a more rigorous exposition of our treatment here.
First, by way of introduction, let us consider the one-person version of the
TV-person stochastic differential game, which is a stochastic optimal control
problem of the type described in Chapter 5, for which a sufficiency condition
for existence of an optimal control has been given in Prop. 5.7. This condition
involves the existence of a solution to a particular partial differential equation
```
(specifically, (5.71)), which corresponds to the Hamilton-Jacobi-Bellman equa-
```
```
tion (5.25) of dynamic programming, but now we have the extra second term
```
which is due to the stochastic nature of the problem. We should note that if
the control was allowed to depend also on the past values of the state, the op-
timal control would still be a feedback strategy since the underlying system is
```
Markovian (i.e., F and a depend only on xt and not on {xs, s < t} at time t).
```
We now observe an immediate application of Prop. 5.7 to the N-person
nonzero-sum stochastic differential game of this section.
Theorem 6.27 For an N-person nonzero-sum stochastic differential game of
```
prescribed fixed duration [0,T], as described by (6.110)-(6.111), and under FB,
```
```
MPS or CLPS information pattern, an N-tuple of feedback strategies {7** G
```
```
P;x e N} provides a Nash equilibrium solution if there exist suitably smooth
```
functions Wi : [0, T] x Rn —> R, i G N, satisfying the coupled semilinear
parabolic partial differential equations
352 T. BA§AR AND G. J. OLSDER
where aki is the kjth element of the symmetric matrix era',
Proof. This result follows readily from the definition of Nash equilibrium
and from Prop. 5.7, since by fixing all players' strategies, except the zth ones, at
```
their equilibrium choices (which are known to be feedback by hypothesis), we
```
arrive at a stochastic optimal control problem of the type covered by Prop. 5.7
```
and whose optimal solution (if it exists) is a feedback strategy.
```
```
Theorem 6.27 actually involves two sets of conditions: (i) existence of minima
```
```
to the RHS of (6.113), and (ii) existence of "sufficiently smooth" solutions to
```
```
the set of partial differential equations (6.113). A sufficient condition for the
```
former is the existence of functions ul° : [0, T] x R" x Rn —> 5* which satisfy
the inequalities
```
If such functions ulo(t,x,p1} (i 6 N) can be found, then we say that the Nash
```
```
condition holds for the associated differential game.66 In fact, Uchida (1978)
```
```
has shown, by utilizing some of the results of Davis and Varaiya (1973) and
```
```
Davis (1973) obtained for stochastic optimal control problems, that under the
```
```
Nash condition and certain technical restrictions on F, cr, 5% gl and ql (i G N)
```
there indeed exists a set of sufficiently smooth solutions to the partial differential
```
equations (6.113). Therefore, the Nash condition is sufficient for a suitably
```
well-defined class of stochastic differential games to admit a Nash equilibrium
```
solution;67 and a set of conditions sufficient for the Nash condition to hold are
```
```
given in Uchida (1979).
```
For the special case of zero-sum stochastic differential games, i.e., when Ll =
```
-L2 = L and N = 2, every solution set of (6.113) is given by W1 = -W2 = W,
```
66Note that such a condition could also have a counterpart in Thm. 6.16, for the determin-
istic problem.67
```
Uchida (1978) actually proves this result for a more general class of problems for which
```
the state equation is not necessarily Markovian, in which case a more general version replaces
```
(6.113).
```
```
Furthermore, the Nash condition (6.114a) can be re-expressed for the stochastic
```
zero-sum differential game as existence of functions ul°: [0,T] x Rn x Rn —» 5*,
i — 1,2, which satisfy
where
```
Because of the similarity with the deterministic problem, condition (6.116a) is
```
```
known as the Isaacs condition. Now, it can actually be shown (see Elliott, 1976)
```
that, if the Isaacs condition holds and if S1 and S2 are compact, there exists a
```
suitably smooth solution to the semilinear partial differential equation (6.115)
```
and consequently a saddle-point solution to the stochastic zero-sum differential
game under consideration. Therefore, we have the following.68
Corollary 6.11 For the two-person zero-sum stochastic differential game de-
```
scribed as an appropriate special case of (6.110)-(6.111), and satisfying certain
```
technical restrictions, let S1 and S2 be compact and let the Isaacs condition
```
(6.116a) hold. Then, there exists a saddle-point solution in feedback strategies,
```
which is
```
where W(t,x) satisfies (6.115) and ul°(t,x,p] (i e N) are determined from
```
```
(6.116a). This solution is strongly time consistent.
```
For the special case of affine-quadratic stochastic differential games, both
Thm. 6.27 and Corollary 6.11 permit explicit expressions for the equilibrium
```
strategies. Toward this end, let F, gl and ql (i £ N) be as given in Def. 6.5
```
```
(with F replacing /), and let cr(t,rrt) = a(t] which is nonsingular. Furthermore,
```
```
assume that Si = Rm' (i e N).69 Then, ul° from (6.114a) is uniquely given by
```
```
68Elliott (1976) has proven this result for a non-Markovian problem in which case (6.115) is
```
```
replaced by a more complex relation; here we provide a version of his result which is applicable
```
to Markovian systems.69
In order to apply Corollary 6.11 directly, we in fact have to restrict ourselves to an
```
appropriate (sufficiently large) compact subset of RmJ.
```
NASH AND SADDLE-POINT EQUILIBRIA 353
where W satisfies
which admits the unique solution set
where Z% i G N, satisfy the N coupled matrix Riccati differential equations
```
(6.70a), C and ml satisfy (6.72a) and (6.73b), respectively, and £* satisfies the
```
differential equation
Now, since 7** is given by
it readily follows that the Nash equilibrium solution of the stochastic affine-
quadratic differential game coincides with that of the deterministic game, as
provided in Corollary 6.5. Therefore, we have the following.
```
Corollary 6.12 The set of Nash equilibrium strategies (6.71) for the determin-
```
istic affine-quadratic differential game with MPS or CLPS information pattern
also provides a Nash equilibrium solution for its stochastic counterpart, with a
not depending on the state Xt.
A similar analysis and reasoning also leads to the following.
Corollary 6.13 Under the conditions of Thm. 6.17, the set of saddle-point
```
strategies (6.77) for the deterministic a ffine-quadratic two-person zero-sum dif-
```
ferential game with MPS or CLPS information pattern also provides a saddle-
point solution for its stochastic counterpart, provided that a does not depend on
the state xt.
Remark 6.23 Under appropriate convexity restrictions, which hold for the
affine-quadratic nonzero-sum game treated here, existence of a solution set to
```
(6.113) is also sufficient for existence of a Nash equilibrium (see Varaiya, 1976);
```
354 T. BA§AR AND G. J. OLSDER
```
in view of which (6.113) can be written as
```
NASH AND SADDLE-POINT EQUILIBRIA 355
therefore the solution referred to in Corollary 6.12 is the unique Nash equi-
librium solution for the affine-quadratic stochastic differential game within the
class of feedback strategies. It should be noted that we do not rule out existence
of some other Nash equilibrium solution that depends also on the past values of
```
the state (though we strongly suspect that this should not be the case), since
```
Thm. 6.27 provides a sufficiency condition only within the class of feedback
strategies. The reader should note, however, that we cannot have information-
```
ally nonunique equilibrium solutions here, since {wt,t > 0} is an independent-
```
increment process and a is nonsingular. For the zero-sum differential game, on
the other hand, an analogous uniqueness feature may be attributed to the result
```
of Corollary 6.13, by making use of a sufficiency result of Elliott (1976), which
```
is obtained for compact S1 and S2 but can easily be generalized to noncompact
action spaces for the affine-quadratic problem.
6.8 Problems
1. An alternative derivation of the result of Thm. 6.2 makes use of Prop. 5.1.
```
First show that the iih inequality of (6.3) dictates an optimization problem
```
to Pi which is similar to the one covered by Prop. 5.1, with Ck in Prop. 5.1
```
replaced by c^ + X^6N Blujk* which is only a function of x\ (and hencei^i
```
```
can be considered as a constant). Then the open-loop Nash equilibrium
```
solution is given as the solution of the set of relations
```
where x*k+l (k G K) denotes the corresponding state trajectory. Now
```
finally prove that the preceding set of relations admits a unique solution
as given in Thm. 6.2, under precisely the same existence condition.
2. Consider the general formulation of an N-person nonzero-sum dynamic
```
game, as described by (6.1) and (6.2), where the players have access to
```
the state with a delay of one time step, i.e., the information available to
each player at stage k is
We wish to obtain a strongly time consistent Nash equilibrium solution
for this class of dynamic games.
356 T. BA§AR AND G. J. OLSDER
```
(i) Obtain the counterpart of Thm. 6.6 under this information structure.
```
```
(ii) Specialize the result above to affine-quadratic dynamic games, so as
```
to obtain the counterpart of Corollary 6.1 for this one-step delay
CLPS pattern.
3. For the class of affine-quadratic zero-sum dynamic games covered by
Thm. 6.7, modify the information structure so that now PI is allowed
to have access to the current and past actions of P2, i.e., Pi's informa-
tion structure is
Develop the counterpart of the result of Thm. 6.7 for this class of games.
In particular, show that
```
(i) there exists a saddle point under conditions that are less stringent
```
```
(for PI) than the conditions of Thm. 6.7;
```
```
(ii) the saddle-point policies of the players that are also strongly time
```
```
consistent are affine functions of the current information (which is
```
```
{xk,u%} for PI, and Xk for P2, at stage fc);
```
```
(iii) the corresponding state trajectory is the same as the one generated
```
by the feedback saddle-point solution of Thm. 6.7, whenever the lat-
ter exists.
4. Consider the infinite-horizon two-player linear-quadratic dynamic game
```
whose two-dimensional state Xk — (x^xfy' is described by
```
where e > 0 is a scalar parameter. Let the cost functions be given by
```
(i) Find the largest value of e > 0 (say, e*), such that for all e < e*, the
```
game admits a stationary feedback Nash equilibrium solution.
```
(ii) Obtain the feedback Nash equilibrium solution when e = e*/2, by
```
```
directly using (6.28a)-(6.28c), and also by iterating on the solution
```
of the finite-horizon game.
5. Derive a set of nonlinear Nash equilibrium solutions for the dynamic game
of subsection 6.3.1. Specifically, obtain a counterpart of the result of
Prop. 6.5 by starting with the representations
NASH AND SADDLE-POINT EQUILIBRIA 357
and by mimicking the analysis that precedes Prop. 6.5. How does the
```
underlying existence condition compare with (6.35)?
```
6. Consider the scalar two-person three-stage stochastic dynamic game de-
scribed by the state equation
and cost functionals
Here 9 denotes a Gaussian random variable with mean zero and variance
1, and both players have access to CLPS information.
Show that this dynamic game admits an uncountable number of Nash
equilibria, and obtain one such set which is linear in the state information
available to both players. Will there be additional Nash equilibria if 9 has
instead a two-point distribution with mean zero and variance 1?
7. For the stochastic dynamic game of the previous problem, prove that the
Nash equilibrium solution is unique if the underlying information pat-
tern is, instead, one-step delay CLPS pattern. Obtain the corresponding
solution, and identify it as a member of the solution set determined in
Problem 6.
8. Consider the class of two-person K-st&ge linear-quadratic stochastic dy-
```
namic games with cost functional (6.10b) and state equation
```
```
where {9k;k € K} denotes a set of independent Gaussian vectors with
```
```
mean zero and covariance In. Obtain the saddle-point solution when (i)
```
```
both players have access to one-step delayed CLPS information, (ii) PI
```
has access to CLPS information and P2 has access to one-step delayed
```
CLPS information, and (iii) PI has access to CLPS information whereas
```
```
P2 has only access to the value of x\ (i.e., OL information). How do the
```
existence conditions compare in the three cases, among themselves and
```
with (6.26a)-(6.26b)? (Hint: Make use of Thm. 6.9 in order to simplify
```
```
the derivation of the saddle-point strategies.)
```
9. Execute the procedure outlined in Remark 6.13 (for two-person games with
```
weakly coupled players) when the state dynamics in (6.48a) are linear and
```
```
the cost functions in (6.48b) are quadratic, and obtain expressions for the
```
first three terms in the expansions for the open-loop Nash strategies of the
358 T. BA§AR AND G. J. OLSDER
10. Show that Thm. 6.12 can also be obtained by directly utilizing Prop. 5.2
in a manner which is parallel to the one outlined in Problem 1 for the
discrete-time counterpart.
11. This problem addresses a generalization of Problem 1 in Section 5.8. As-
```
sume that we now have two companies, PI and P2, with Pi having Xi(t)
```
customers at time t and making a total profit of
```
up to time t, where cl is a constant. The function ul(t), restricted by
```
```
ul(t] > 0, fl(t) > 0, represents the money put into advertisement by Pi,
```
which helps to increase the number of customers attracted to that firm,
according to
The total number of customers shared between the two companies is a con-
stant, so that we may write, without any loss of generality, the constraint
relation
where T denotes the end of the time period when each firm wishes to
```
maximize its total profit (i.e., /r(T)).
```
Obtain the open-loop Nash equilibrium solution of this two-person nonzero-
```
sum differential game wherein the decision variable for Pi is ul. (For
```
```
details see Olsder, 1976).
```
12. Consider a variation on the previous problem so that now the state
equation is described by
and the decision variables satisfy the constraints
```
The objective for Pi is still maximization of /*(T).
```
```
(i) Obtain the open-loop Nash equilibrium solution.
```
```
(ii) Obtain the feedback Nash equilibrium solution when both players
```
have access to the current values of x\ and X2 •
NASH AND SADDLE-POINT EQUILIBRIA 359
13. Obtain the complete set of informationally nonunique linear Nash equi-
librium solutions of the scalar linear-quadratic two-person nonzero-sum
differential game described by the state dynamics
and cost functionals
and with the underlying information structure being MPS for both players.
```
Which of these would still constitute a Nash equilibrium when (i) P2
```
```
has, instead, access to OL information, (ii) both players have access to
```
```
OL information? (Note that, in this case, OL information is in fact "no
```
```
information" since the initial state is a known number.)
```
14. Prove that the statement of Thm. 6.9 is also valid for two-person zero-sum
differential games with prescribed fixed duration.
15. Repeat Problem 9 for the CLNM information structure and using the feed-
back Nash equilibrium solution. Here Remark 6.18 replaces Remark 6.13,
and Corollary 6.5 replaces Thm. 6.12.
16. Generalize the result of Thm. 6.17 to the case when the cost function
```
has, under the integral, the additional term u1 ( t ) ' K ( t ) u 2 ( t ) , where K(-)
```
is a matrix of appropriate dimensions and with continuous entries. Is it
possible for the differential game to have a bounded lower value even if
the conditions for existence of a saddle point fail?
17. This problem uses the notation and terminology introduced in Section 6.6.
We wish to design optimal controllers for the following two-dimensional
system, under different information patterns, using the H°° criterion:
Here, x\ and x2 denote the two states, whose initial values are completely
unknown, u is the scalar control variable, y is the measured output, z is
the regulated output, w is the scalar system disturbance and v is the mea-
surement disturbance. With this system, we associate the performance
index
where T is the terminal time.
360 T. BA§AR AND G. J. OLSDER
```
(i) Let T = oo, and the controller have access to perfect state infor-
```
```
mation. Denote the optimum (minimax) performance level for this
```
problem by A*. Obtain the value of A*, and a controller that will
ensure a performance level no worse than A = A* -f 0.01.
```
(ii) Repeat the above now under the original imperfect state information.
```
```
(iii) Now take the time horizon to be finite, and T = 1. Furthermore take
```
```
the initial state to be known to be zero. Obtain (approximately) the
```
A* for this problem under perfect state measurements by numerically
solving the corresponding generalized Riccati differential equation for
different values of A.
18. The following two-dimensional system description depends on a parameter
```
c:
```
Given the cost function:
```
obtain the /f°°-optimal performance level (A*(e)), under perfect state mea-
```
surements, for c — 1,0.1,0.01,0.001,0.0001, to the nearest four decimal
places.
```
Solution: 2.4332, 2.7843, 2.9724, 2.9972, 2.9997.
```
19. Repeat Problem 18 for the following system and cost function:
```
Solution: 1.3306, 1.3393, 1.4212, 1.4237, 1.4240.
```
20. Consider the following two-dimensional system which again depends on a
parameter c.
We now have a measurement equation
where v is the measurement noise. Given the cost function,
```
obtain the /f^-optimal performance level (A*(e)), under imperfect state
```
measurements, for e — 1,0.1,0.01,0.001,0.0001, to the nearest four dec-
```
imal places, and determine a controller (for each case) that assures a
```
```
performance no worse than A*(e) + 0.01.
```
NASH AND SADDLE-POINT EQUILIBRIA 361
Partial Solution: A* = 55.1993, 17.3221, 9.9331, 9.1197, 9.0308.
21. Consider the differential game of Problem 13, but with the state equation
replaced by
where a is a random variable taking the values +1 and —1 with equal
probability 1/2. Under the CLPS information pattern for both players,
does this differential game, which also incorporates a chance move, admit
a unique Nash equilibrium solution? In case the reply is "yes", obtain
```
the unique solution; in case it is "no", obtain the complete set of Nash
```
equilibria that are linear in the current value of the state.
22. Obtain the saddle-point solution of the class of two-person zero-sum
stochastic differential games described by the state equation
```
and cost functional (6.60b), where PI has access to CLPS information and
```
```
P2 has OL information. Here, a(-} is a nonsingular matrix, {wt,t > 0}
```
is the standard Wiener process, and the initial state vector XQ is a known
```
quantity. (For details see Ba§ar (1977b)).
```
6.9 Notes
Section 6.2. Results pertaining to Nash equilibria in infinite dynamic games first
```
appeared in the works of Case (1967, 1969) and Starr and Ho (1969a, b) but in the
```
```
continuous time (i.e., for differential games). Here we present counterparts of these
```
```
results in discrete time, and with special attention paid to affine-quadratic (nonzero-
```
```
sum and zero-sum) games, and existence conditions for equilibria under open-loop and
```
memoryless perfect state information patterns. A possible extension of these results
would be to the class of game problems in which the players also have the option of
whether they should make a perfect state measurement or not, if it is costly to do
```
so; for a discussion of such problems see Olsder (1977b). For applications of some
```
```
of these results in macroeconomics, see Pindyck (1977), Kydland (1975) and Ba§ar,
```
```
Turnovsky and d'Orey (1986). The last reference also studies, within the context of a
```
two-country macroeconomic model, the computation and stabilizability of stationary
feedback Nash equilibria using the solutions of a sequence of time-truncated dynamic
games.
Section 6.3. Existence of uncountably many informationally nonunique Nash equi-
```
libria in dynamic games under closed-loop information was first displayed in Ba§ar (1974),
```
```
Ba§ar (1976a) for discrete-time games, and in Ba§ar (1977b) for differential games. The
```
underlying idea in this section therefore follows these references. The algorithm of sub-
```
section 6.3.2 is from Ba§ar (1977d), and Thm. 6.9 was first presented in Ba§ar (1977a);
```
some further discussion on the relation between existence of a saddle point and the un-
```
derlying information pattern in zero-sum dynamic games can be found in Ba§ar (1976b)
```
362 T. BA§AR AND G. J. OLSDER
```
and Witsenhausen (1971b), where the latter reference also discusses the impact of in-
```
formation on the upper and lower values of a game when a saddle point does not
exist.
Section 6.4. Elimination of informational nonuniqueness in Nash equilibria through
```
a stochastic formulation was first discussed in Ba§ar (1976a), and further elaborated
```
```
on in Ba§ar (1975, 1979b, 1989b); this section follows basically the lines of those ref-
```
erences. The fact that an increase in information could be detrimental for a player
```
under Nash equilibria was first pointed out in Basarand Ho (1974), but within the
```
```
context of stochastic games with noisy (imperfect) observation. For some results on
```
the equilibrium solution of linear-quadratic stochastic games with noisy observation,
```
see Ba§ar(1978), Bas,arand Mintz (1972, 1973).
```
Section 6.5. Derivation of open-loop and feedback Nash equilibria in nonzero-sum
```
differential games was first discussed in Case (1967, 1969) and Starr and Ho (1969a, b),
```
```
where the latter two references also display the differences (in value) between these two
```
types of equilibria. Derivation of open-loop saddle-point equilibria, however, predates
```
this development, and was first discussed in Berkovitz (1964) and Ho et al. (1965),
```
with the latter devoted to a particular class of linear-quadratic differential games of the
pursuit-evasion type, whose results were later put into rigorous framework in Schmi-
```
tendorf (1970); see also Halanay (1968) which extends the results of Ho et al. (1965) to
```
differential games with delays in the state equation. For an excellent exposition on the
status of nonzero-sum and zero-sum differential game theory in the early 1970s, see
```
the survey paper by Ho (1970). Some selective references which display the advances
```
on Nash and saddle-point equilibria of deterministic differential games since the early
```
1970s are (in addition to those already cited in the text) Bensoussan (1974), Ho, Luh
```
```
and Olsder (1980), Leitmann (1974, 1977), Scalzo (1974), Vidyasagar (1976) and re-
```
```
lated papers in the edited volumes Blaquiere (1973), Hagedorn et al. (1977), Ho and
```
```
Mitter (1976), Kuhn and Szego (1971), Ba§ar (1986b), Basarand Bernhard (1989),
```
```
Hamalainen and Ehtamo (1991), Ba§arand Haurie (1994). For applications of this
```
```
theory to problems in economics, see Case (1971, 1979), Clemhout et al. (1973),
```
```
Pohjola (1986), Jorgensen (1986), Dockner and Feichtinger (1986) and Leitmann and
```
```
Wan, Jr. (1979), where the last one discusses a worst-case approach toward stabiliza-
```
tion of uncertain systems, that utilizes the zero-sum differential game theory. Tolwin-
```
ski (1978b) provides a rather complete account of the results on existence of open-loop
```
```
Nash equilibria in nonzero-sum differential games, and Tolwinski (1978a) discusses a
```
possible computational scheme for the numerical evaluation of the Nash equilibrium
```
solution; see also Varaiya (1967,1970), Rosen (1965) and Friedman (1971) for existence
```
results concerning noncooperative equilibria. For the special class of linear-quadratic
```
games, Papavassilopoulos and Cruz, Jr. (1979b) and Papavassilopoulos et al. (1979)
```
```
discuss existence of a unique solution to the coupled set of Riccati equations (6.70a),
```
```
which plays an important role in the characterization of feedback Nash equilibria;
```
```
for computational algorithms for some specially structured games, see Ba§ar (1991a),
```
```
Srikant and Ba§ar (1991), Gajic, Petkovski and Shen (1990). Papavassilopoulos and
```
```
Olsder (1984) demonstrate that an infinite-horizon linear-quadratic differential game
```
might have multiple Nash equilibria even though every time-truncated version of it has
```
a unique feedback Nash equilibrium. Eisele (1982) was one of the first to point out
```
that linear-quadratic differential games might admit multiple open-loop Nash equi-
```
libria, whose results were later extended by Engwerda (1998) who showed that an
```
NASH AND SADDLE-POINT EQUILIBRIA 363
open-loop Nash equilibrium solution may exist for a linear quadratic game even if the
corresponding differential Riccati equations may not have a solution. The latter ref-
erence also studied the limiting behavior of finite time horizon open-loop solutions of
linear-quadratic games as the final time approaches infinite. For a discussion of non-
cooperative equilibria in differential games with state dynamics described by partial
```
differential equations, see Roxin (1977).
```
Section 6.6. The theory briefly presented here has extensions to other types of
```
information structures, such as sampled-data (both perfect state and imperfect state),
```
```
and delayed measurements, as well as to nonlinear systems; see Bas.arand Bern-
```
```
hard (1995). It is also possible to study the robustness (or sensitivity) of the H°°-
```
optimal controllers to unmodeled fast plant dynamics, within the framework of singu-
```
lar perturbations; see Pan and Ba§ar(1993, 1994a,b). These papers also develop, as
```
a byproduct, a complete theory for the saddle-point equilibria of singularly perturbed
linear-quadratic differential games under perfect and imperfect state measurements. It
is also possible to combine Nash and saddle-point equilibria to develop a counterpart
of H°°-optimal control theory for multiple controller systems under multiple worst-case
```
design criteria (Ba§ar 1992).
```
Section 6.7. For some explicit results in the case of stochastic quadratic differen-
```
tial games with mixed information patterns, see Ba§ar (1977b,c, 1979a, 1980a). Exten-
```
sions of these results to stochastic differential games with noisy observation meet with
formidable difficulties, some of which have been resolved in the case of linear-quadratic
```
zero-sum games (see Willman (1969), Bagchi and Olsder (1981) and Ba§ar (1981c),
```
where the latter reference deals with the case of identical noisy measurements for
```
the players). For a distributed algorithm for the computation of Nash equilibria in
```
```
linear-quadratic stochastic differential games, see Ba^arand Li (1989).
```
This page intentionally left blank
Chapter 7
Stackelberg Equilibria of
Infinite Dynamic Games
7.1 Introduction
This chapter is devoted to derivation of the Stackelberg solution in infinite
dynamic games with fixed prescribed duration. The chapter starts with a treat-
ment of dynamic games defined in discrete time and with the number of players
restricted to two. Continuous-time counterparts of most of these results and
possible extensions to many-player games are treated in the latter part.
The next two sections, i.e., Sections 7.2 and 7.3, deal, respectively, with
the Stackelberg solution under open-loop information and the feedback Stack-
elberg solution under CLPS information. These solutions are obtained using
two standard techniques of optimal control theory, viz. the minimum principle
```
and dynamic programming, respectively. Derivation of the (global) Stackelberg
```
solution under the CLPS information pattern, however, requires a much more
subtle analysis since all standard techniques and approaches of optimal control
theory fail to provide the solution. Therefore, Section 7.4 is devoted exclusively
to this topic and to elucidation of an indirect approach toward derivation of
the closed-loop Stackelberg solution. This indirect approach is first introduced
```
within the context of two scalar examples (cf. subsections 7.4.1 and 7.4.2), and
```
then its application is extended to the class of two-person linear-quadratic dy-
namic games in subsection 7.4.3. There is a clear relationship with the theory
of "incentives", and this is the subject of subsection 7.4.4.
Section 7.5 discusses derivation of the Stackelberg solution in stochastic dy-
namic games with deterministic information patterns, and primarily for the
```
open-loop information (cf. subsection 7.5.1) and for the CLPS information struc-
```
```
ture under the "feedback Stackelberg" solution (cf. subsection 7.5.2). Derivation
```
```
of the (global) Stackelberg solution under the CLPS information pattern is a
```
challenging topic, which is briefly discussed also in subsection 7.5.1. Stochas-
tic incentive problems are discussed in subsection 7.5.2, as the counterpart of
365
366 T. BA§AR AND G. J. OLSDER
subsection 7.4.4 in the stochastic case.
Section 7.6 treats continuous-time games, first presenting the Stackelberg
```
solution under open-loop information for the leader (subsection 7.6.1), and then
```
```
deriving the feedback Stackelberg solution (defined as a natural extension of
```
```
the discrete-time FB Stackelberg solution concept) under CLPS information
```
```
(subsection 7.6.2).
```
7.2 Open-Loop Stackelberg Solution of
Two-Person Dynamic Games in
Discrete Time
In this section we discuss the Stackelberg solution for the class of two-person
deterministic discrete-time infinite dynamic games of prescribed fixed duration
```
(cf. Def. 5.1) when the underlying information structure is open-loop (for both
```
```
players) and PI acts as the leader. Hence, abiding by our standard terminology
```
and notation, the state evolves according to
```
where Xk £ X = Rn, x\ is specified a priori, and ulk € UJ. C Rm% i = 1,2; and
```
the stage-additive cost functional for Pi is introduced as
Because of the open-loop information structure, the game is already in normal
form, and therefore Defs 3.26, 3.27 and 3.28 readily introduce the Stackelberg
```
equilibrium solution with PI acting as the leader (since finiteness of the game
```
```
was not essential in those definitions).
```
One method of obtaining the Stackelberg solution for this class of games is
to make use of the theory of Section 4.5, since these games can be viewed as
```
static ones under the open-loop information structure; such an approach also
```
readily leads to conditions for existence of a Stackelberg solution. Toward this
```
end, we first note that, by recursive substitution of (7.1) into (7.2), the cost
```
```
functional Jl(ul, u2} can be made to depend only on the control vectors ul, u2
```
```
(which are of dimensions m\K and m^K, respectively) and the initial state xi,
```
which is known a priori. Now, if
```
(i) Jl is continuous on
```
```
(ii) «72(w1, •) is strictly convex on U2 for all
```
```
(Hi) U1 is a closed and bounded (thereby compact) subset of
```
```
then it follows from Corollary 4.4 (more generally from Thm. 4.8) that a Stack-
```
elberg equilibrium solution exists for the open-loop infinite game. A brute-force
```
HIERARCHICAL (STACKELBERG) EQUILIBRIA 367
```
method to obtain the corresponding solution would be first to determine the
```
unique reaction curve of the follower by minimizing J2(ul, u2) over u2 € U2 for
```
every fixed u1 £ C/1, which is a meaningful optimization problem because of as-
```
sumptions (i)-(iii) above. Denoting this unique mapping by T2 : U1 —> U2,
```
```
the optimization problem faced by PI (the leader) is then minimization of
```
```
J1(u1,T2u1) over [71, thus yielding a Stackelberg strategy for the leader in
```
this open-loop game.
Despite its straightforwardness, such a derivation is not practical, especially
if the number of stages in the game is large—which directly contributes to the
dimensions of the control vectors u1, u2. An alternative derivation, which paral-
lels that of Thm. 6.1 in the case of open-loop Nash equilibria, utilizes techniques
of optimal control theory. Toward this end, we first determine the unique opti-
mal response of the follower to every announced strategy 71 € F1 of the leader.
Since the underlying information pattern is open-loop, the optimization problem
```
faced by the follower is (for each fixed u1 G U1)
```
subject to
This is a standard optimal control problem whose solution exists and is unique
```
under conditions (i)-(iii), and which can further be obtained by utilizing Thm. 5.5.
```
```
Lemma 7.1 In addition to conditions (i)-(iii) assume that70
```
```
(iv) /fc(-, u£, it2,) is continuously differentiable on Rn7 and /&(•,«£, •) is convex
```
on
```
(v) <^(-, wj^u2., •) is continuously differentiable on
```
Then, to any announced strategy u1 = 71 G F1 of the leader, there exists a
```
unique optimal response of the follower (to be denoted by^l(x\] — u2) satisfying
```
the following relations:
```
70Except for very special cases, conditions (iv) and (v) given below, and conditions (vi),
```
```
(vii), (ix) and (x) given later, are required for satisfaction of condition (i) (i.e., they are, in
```
```
general, implicit in condition (i)); but we nevertheless rewrite them here since they will be
```
needed explicitly in some of the results obtained in the sequel.
368 T. BA§AR AND G. J. OLSDER
where
```
Here, {pi,.. • ,PK+I} is a sequence of n-dimensional costate vectors associated
```
with this optimal control problem.
If we further assume
```
(vi) fk(xk,ul, •) is continuously differentiate on U% (k £ K),
```
```
(vii) g%(xk+i, uj., -,Xk) is continuously differentiable on U% (k e K),
```
```
(viii) u2 in Lemma 7.1 is an inner-point solution for every u1 € U1,
```
```
then (7.3b) can equivalently be written as
```
Now, to obtain the Stackelberg strategy of the leader, we have to minimize
```
L1(u1,u2), in view of the unique optimal response of the follower to be deter-
```
```
mined from (7.4) in conjunction with (7.3a) and (7.3c). Therefore, PI is faced
```
with the optimal control problem
subject to
Note that, in this optimal control problem the constraint equations involve
```
boundary conditions at both end points, and hence Thm. 5.5 is not applicable;
```
```
but a standard result of nonlinear programming (cf. Canon et al., 1970) can be
```
utilized to yield the following conclusion.
```
Theorem 7.1 In addition to conditions (i)-(viii) stated in this section, assume
```
that
```
(ix) fk(xk,-,ul), 9k(xk+i-:'iu'kixk} are continuously differentiable on U£,
```
A r e K ,
```
where H% is defined by (7.3d) and
```
```
HIERARCHICAL (STACKELBERG) EQUILIBRIA 369
```
```
(x) g\ ( • , - , - , • ) is continuously differentiable on
```
```
(xi) /jt(-,w^, •) zs £iwce continuously differentiable on
```
is iwnce continuously differentiable on
```
Then, if J7^*(xi) — u£* G ^ , f c G K} denotes an open-loop Stackelberg equi-
```
librium strategy for the leader in the dynamic game formulated in this section,
```
there exist finite vector sequences { A i , . . . , AX}, {/-ti, • • • , UK}, {^i; • • • ? ^/c} that
```
satisfy the following relations:
where
```
H% is defined by (7.3d), F^ by (7.5e), and U^ denotes the interior ofU^. Fur-
```
```
thermore, {u%*,k € K.} is the corresponding unique open-loop Stackelberg strat-
```
```
egy of the follower (P2), and {x£+1,fc € K} is the state trajectory associated
```
with the Stackelberg solution.
```
Proof. As discussed prior to the statement of the theorem, the leader (PI)
```
```
is faced with the optimal control problem of minimizing L1(u1, it2) over Ul and
```
```
subject to (7.5b)-(7.5d). Since the problem is formulated in discrete time, it
```
is equivalent to a finite dimensional nonlinear programming problem the "La-
grangian" of which is
```
where Afc, /^, i/fc (k € K) denote appropriate Lagrange-multiplier vectors. Now,
```
```
if {ul*,k G K} is a minimizing solution and {x^+1,p^.+l,u^*;k € K} are the
```
370 T. BA§AR AND G. J. OLSDER
```
corresponding values of the other variables so that the constraints (7.5b)-(7.5d)
```
```
are satisfied, then it is necessary that (see, e.g., Canon et al., 1970, p. 51)
```
```
wherefrom (7.6b)-(7.6e) follow.
```
```
Remark 7.1 If the follower (P2) has, instead, access to closed-loop perfect
```
```
state (CLPS) information, his optimal response (cf. Lemma 7.1) will be any
```
```
closed-loop representation of the open-loop policy {u%,k € K}; however, since
```
```
the constraints (7.5b)-(7.5d) are basically open-loop relations, these different
```
representations do not lead to different optimization problems for the leader
```
(PI). Therefore, the solution presented in Thm. 7.1 also constitutes a Stackel-
```
berg equilibrium for the case in which the follower has access to CLPS informa-
```
tion (with the leader still having OL information). Under this latter information
```
pattern, we may still talk about a unique open-loop Stackelberg strategy for the
```
leader; whereas for the follower the corresponding optimal response strategy
```
will definitely not be unique, since any closed-loop representation of u2* given
in Thm. 7.1 will constitute an optimal strategy for P2.
```
Remark 7.2 The open-loop Stackelberg (OLS) solution is not weakly time con-
```
```
sistent (a la Def. 5.13), since the OLS solution of any subgame does not necessar-
```
ily coincide with the OLS solution of the full game, not even on the equilibrium
trajectory.
Linear-quadratic games
We now specialize the result of Thm. 7.1 to the class of linear-quadratic dynamic
```
games (cf. Def. 6.1) wherein
```
We further assume that Q\+l > 0, R%* > 0, and Ulk = Rmi, Vfc e K, i =
1,2. Under these assumptions, it readily follows that the optimal response of
the follower to any announced control sequence of the leader is unique and
affine, and moreover the minimum of L1 under the constraint imposed by this
```
affine response curve is attained uniquely by an open-loop control sequence (for
```
```
PI) that is linear in x\.71 This unique solution can be obtained explicitly by
```
```
utilizing Thm. 7.1 and in terms of the current (open-loop) value of the state;
```
it should be noted that Thm. 7.1 provides in this case both necessary and
```
71This follows since Ll(ul,u2) is quadratic and strictly convex on Rmi xRm2. It should also
```
be noted that the assumptions made on Qik+-i and /?£2 are sufficient, but not necessary, for
```
the open-loop Stackelberg solution to exist and be unique; they are made here for convenience
```
in the derivation to follow.
```
HIERARCHICAL (STACKELBERG) EQUILIBRIA 371
```
sufficient conditions because of strict convexity.72 The result is given below in
Corollary 7.1, which is obtained, however, under a condition of invertibility of
a matrix related to the parameters of the game.
Corollary 7.1 The two-person linear-quadratic dynamic game described by
```
(7.7a)-(7.7b) under the parametric restrictions Qj,.+1 > 0; R^2 > 0 (k G K,i —
```
```
1, 2) admits a unique open-loop Stackelberg solution with PI acting as the leader,
```
which is given, under the condition of invertibility of the matrix appearing in
```
the braces in (7.9c), by
```
where
and Pk, Afe, Mk are appropriately dimensioned matrices generated by the dif-
ference equations
with
Furthermore, the unique state trajectory associated with this pair of strategies
satisfies
```
72 Note that condition (iii) in the general formulation is also met here since we already know
```
that a unique solution exists with Ui taken as Rmi and therefore it can be replaced by an
```
appropriate compact subset of Rm* (i = 1,2).
```
372 T. BA§AR AND G. J. OLSDER
Proof. Existence of a unique solution to the linear-quadratic open-loop
Stackelberg game has already been verified in the discussion prior to the state-
ment of the corollary. We now apply Thm. 7.1 to obtain the corresponding equi-
```
librium strategies of the players, in the structural form (7.8a)-(7.8b). Toward
```
```
this end, we first rewrite (7.6b)-(7.6g), under the "linear-quadratic" assumption
```
```
(7.7a)-(7.7b), respectively, as follows:
```
```
The coefficient of i/k in (ii) is clearly positive definite; therefore, solving for v^
```
```
from (ii) and substituting this in (i), (iii) and (iv), we obtain, respectively,
```
```
Now, letting p*k+1 = (Pk+i - Q2k+l)x*k+1, xk = (Afe+i -Qi+iK+n M = Mfcxfc>
```
```
and substituting (vii) and (v) into the state equation, we obtain the relation
```
```
assuming that the required inverse in the expression for $fc (defined by (7.9c))
```
```
exists. In determining the expression for <!>&, as given by (7.9c), we have also
```
utilized the matrix identity of Lemma 6.2 in simplifying some of the terms
```
involved. Now, finally, by further making use of (x) in (vii), (v), (vi), (viii) and
```
```
(ix), we arrive at (7.9a), (7.9b), (7.10a), (7.10b) and (7.10c), respectively. D
```
```
HIERARCHICAL (STACKELBERG) EQUILIBRIA 373
```
```
Remark 7.3 The coupled matrix difference equations (7.10a)-(7.10c) consti-
```
tute a two-point boundary value problem, since the starting conditions are
```
specified at both end points. If the problem is initially formulated as a (high di-
```
```
mensional) static quadratic game problem, however, such a two-point boundary
```
```
value problem does not show up; but then one is faced with the task of invert-
```
```
ing high dimensional matrices (i.e., the difficulty again shows up in a different
```
```
form).
```
```
Remark 7.4 Corollary 7.1 extends naturally (without any conceptual diffi-
```
```
culties) to affine-quadratic games where the state equation has an additional
```
control-independent driving term and the cost functions have cross terms be-
tween the current values of the state vector and the players' control vectors. For
this more general case, the follower's optimal response will again be an affine
```
function of the leader's control, and the leader's open-loop Stackelberg control
```
will be affine in the initial state xi, obtained from the solution of a generalized
quadratic optimization problem.
7.3 Feedback Stackelberg Solution Under CLPS
Information Pattern
The feedback Stackelberg solution concept introduced in Def. 3.29 for finite
games is equally valid for infinite dynamic games defined in discrete time and
with additive cost functionals, provided that the underlying information struc-
```
ture allows the players to have access to the current value of the state (such
```
```
as in the case of CLPS or MPS information pattern). Furthermore, in view of
```
```
Prop. 3.15 (which is also valid for infinite games defined in discrete time), it is
```
```
possible to obtain the solution recursively (in retrograde time), by employing a
```
dynamic programming argument and by solving static Stackelberg games at ev-
ery stage. Toward this end, we consider, in this section, the class of two-person
```
dynamic games described by the state equation (7.1), and the cost functionals
```
```
Ll(ul, u2) given by (7.2). Under the delayed-commitment mode of play, we re-
```
```
strict attention to feedback strategies, i.e., uk = 7£(xfc), k £ K, i — 1,2, and
```
seek to obtain the feedback Stackelberg solution that is valid for all possible
initial states xi £ Rn. To complete the description of the dynamic Stackelberg
```
game, let Tlk denote the class of all permissible strategies of Pi at stage k (i.e.,
```
```
measurable mappings from Rn into Uk). Then, the cost functional for Pi, when
```
```
the game is in normal form, is (as opposed to (7.2))
```
Because of the additive nature of this cost functional, the following theorem
```
now readily follows from Def. 3.29 and Prop. 3.15 (interpreted appropriately for
```
```
the infinite dynamic game under consideration), and it provides a sufficient con-
```
dition for a pair of strategies to constitute a feedback Stackelberg solution. Such
374 T. BA§AR AND G. J. OLSDER
a solution is clearly also a strongly time consistent one, using the terminology
introduced in Def. 5.14 with the sol operator taken as the feedback Stackelberg
equilibrium solution.
Theorem 7.2 For the two-person discrete-time infinite dynamic game formu-
```
lated in this section, a pair of strategies {jl* € F1^2* £ F2} constitutes a
```
feedback Stackelberg solution with PI 05 the leader if
```
where -R2.^) is a singleton set defined by
```
and Glk is recursively defined by
Furthermore, every such solution is strongly time consistent.
```
Remark 7.5 If the optimal response set R%(-) of P2 (the follower) is not a
```
```
singleton, then (in accordance with Def. 3.29) we have to take the maximum
```
```
of the LHS of (7.11) over all these nonunique response functions. However, if
```
```
G\(ik,-,Xk) is strictly convex on Uk (k e K), the underlying assumption of
```
"unique follower response" is readily met, and the theorem provides an easily
implementable algorithm for computation of feedback Stackelberg strategies.
Linear-quadratic games
For the special case of linear-quadratic games with strictly convex cost function-
als, the feedback Stackelberg solution of Thm. 7.2 can be determined in closed
```
form. Toward this end, let /& and glk be as defined by (7.7a) and (7.7b), respec-
```
```
tively, with Q{+1 > 0, Rij > 0 Vfc e K, i, j = 1,2, i ^ j, and take Ulk = Rm'.
```
```
Then, solving recursively for Gk, in view of (7.11) and (7.12a), we obtain the
```
quadratic structure
```
HIERARCHICAL (STACKELBERG) EQUILIBRIA 375
```
where Llk satisfies the backward recurrence relation
```
It should be noted that L{+1 > 0, Vfc e K, i = 1,2 (since Q{+1 > 0, R% > 0),
```
```
and therefore the required inverses in (7.15a) and (7.15b) exist. By the same
```
```
token, (5^(7^,-,Xfc) is strictly convex on Rm2 and hence R\ is a singleton for
```
```
every k G K; furthermore, the minimization problem (7.11) admits a unique so-
```
```
lution, again because of the strict convexity (on Rmi). The implication, then, is
```
the following corollary to Thm. 7.2, which summarizes the feedback Stackelberg
solution of the linear-quadratic game.
Corollary 7.2 The two-person linear-quadratic discrete-time dynamic game,
```
as defined by (1.1 a) and (7.7b) and with strictly convex cost functional and
```
FB, CLPS or MPS information pattern, admits a unique feedback Stackelberg
```
solution with PI as the leader (under the delayed-commitment mode of play),
```
which is strongly time consistent and linear in the current value of the state:
```
Here S^ and S% are defined by (7.15a) and (7.15b), respectively, via the recursive
```
```
relations (7.14).
```
Remark 7.6 The counterpart of Remark 7.4 holds here, in the sense that
Corollary 7.2 extends naturally to affine-quadratic games where the state equa-
tion has an additional control-independent driving term and the cost functions
have cross terms between the current values of the state vector and the players'
control vectors. In this case, the unique feedback Stackelberg strategies of the
players will be in the form:
```
where S^. and 8% are some matrix sequences generated as in (7.15a) and (7.15b),
```
and s£ and s| are state-independent vector sequences, again generated in ret-
rograde time. The precise expressions for these can readily be obtained by
```
observing that in this more general case the counterpart of Glk (cost-to-go for
```
```
Pi) will also contain a state-independent term, a term that is linear in x^+i,
```
```
and some (quadratic) cross terms between xjfc, u\ and u\.
```
376 T. BA§AR AND G. J. OLSDER
```
7.4 (Global) Stackelberg Solution Under CLPS
```
Information Pattern
```
We consider, in this section, the derivation of (global) Stackelberg solutions
```
```
when the leader has access to dynamic information (e.g., CLPS information).
```
Such decision problems cannot be solved by utilizing standard techniques of
```
optimal control theory (as in Sections 7.2 and 7.3), because the reaction set
```
of the follower cannot, in general, be determined in closed form, for all possi-
ble strategies of the leader, and hence the optimization problem faced by the
```
leader on this reaction set becomes quite an implausible one (at least, using the
```
```
standard available techniques of optimization). In finite games (cf. Section 3.6)
```
this difficulty can definitely be circumvented by converting the original game in
```
extensive form into normal form (which is basically a static (matrix) game) and
```
then by performing only a finite number of comparisons. In infinite dynamic
games, however, such an approach is apt to fail because of the "infinite" nature
```
of the strategy space of the leader (i.e., he has uncountably many alternatives).
```
In the sequel we first consider a specific example to elucidate the difficulties
to be encountered in a brute-force derivation of the Stackelberg solution when
```
the leader has access to dynamic information, and introduce (within the context
```
of that example which involves two stages—each player acting only once, first
```
the follower and then the leader) an indirect method to obtain the solution
```
```
(see subsection 7.4.1). Then, we extend this indirect approach to a modified
```
version of the example of subsection 7.4.1, in which the follower also acts at
```
the second stage (see subsection 7.4.2) and we obtain in subsection 7.4.3 closed-
```
form expressions for linear Stackelberg strategies in general linear-quadratic
dynamic games, by utilizing the methods introduced in subsections 7.4.1 and
7.4.2. Finally, in subsection 7.4.4 we provide a brief discussion on the closely
related topic of incentives.
```
7.4.1 An illustrative example (Example 7.1)
```
Example 7.1 Consider the two-stage scalar dynamic game described by the
state equations
and cost functionals
```
(Note that we have suppressed the subindices on the controls, designating the
```
```
stages, since every player acts only once in this game.) The information struc-
```
```
ture of the problem is CLPS, i.e., PI (the leader), acting at stage 2, has access
```
```
to both xi and a?2, while P2 (the follower) has only access to the value of x\;
```
therefore, a permissible strategy 7* for PI is a measurable mapping from R x R
```
HIERARCHICAL (STACKELBERG) EQUILIBRIA 377
```
into R, and a permissible strategy 72 for P2 is a measurable mapping from R
```
into R. At this point, we impose no further restrictions (such as continuity,
```
```
differentiability) on these mappings, and denote the strategy spaces associated
```
with them by F1 and F2, respectively.
Now, to any announced strategy 71 G F1 of the leader, the follower's optimal
```
reaction (for fixed xi) will be the solution of the minimization problem
```
```
which determines R (71). For the sake of simplicity in the discussion to follow,
```
let us now confine our analysis to only those strategies in F1 which lead to a
```
singleton R2(71). Denote the class of such strategies for the leader by F1, and
```
note that the follower's response is now a mapping 72 : R x F1 —> R, which we
```
symbolically write as 72[xi; 71]. There is, in general, no simple characterization
```
```
of 72 since the minimization problem (7.18) depends "structurally" on the choice
```
of 71 G f1. Now, the optimization problem faced by the leader is
where
This is a constrained optimization problem which is, however, not of the stan-
dard type, since the constraint is in the form of the minimum of a functional.
If we further restrict the permissible strategies of the leader to be twice con-
```
tinuously differentiable in the first argument (in which case the underlying F1
```
```
becomes a smaller set), the first- and second-order conditions associated with
```
```
the minimization problem (7.18) can be obtained, which implicitly determine
```
```
72[xi;71]. The problem faced by the leader then becomes one of calculus of
```
variations, with equality and inequality constraints, which is quite intractable
even for the "simple" problem formulated in this subsection. Besides, even if
```
the solution of this constrained calculus of variations problem is obtained (say,
```
```
through numerical techniques), we cannot be sure that it constitutes a Stack-
```
```
elberg strategy for the leader in the game problem under consideration (i.e., it
```
```
might not be the best choice for the leader) since the a priori assumption of
```
twice differentiability might be overly restrictive. We shall, in fact, see in the
sequel that, depending on the value of /5, the Stackelberg strategy of the leader
could be nondifferentiable.
These considerations then force us to abandon the direct approach toward
the solution of the Stackelberg game under CLPS information pattern, and to
seek indirect methods. One such indirect method involves determination of a
```
(tight) lower bound on the (Stackelberg) cost of the leader, and then finding an
```
appropriate strategy for the leader that realizes that bound in view of possible
rational responses of the follower.
378 T. BA§AR AND G. J. OLSDER
In an attempt to explore this indirect method somewhat further, let us first
observe that the lowest possible cost the leader can expect to attain is
```
which is the one obtained when the follower cooperates with him to (globally)
```
minimize his cost functional. This quantity clearly provides a lower bound to
```
the leader's Stackelberg cost; but it is not as yet known whether it can be
```
realized,73 and this is what we investigate in the sequel.
```
First, solving the optimization problem (7.19) in view of (7.16) and (7.17a),
```
```
we obtain (by utilizing dynamic programming or directly Prop. 5.1) the pair of
```
feedback strategies
as providing the unique globally minimizing solution within the class of feedback
strategies. The corresponding optimum state trajectory, in this two-stage team
problem, is described by
and the minimum cost is
```
The superscripts in (7.20a)-(7.20d) stand for "team", since (7.19) actually de-
```
scribes a two-member team problem with the common objective being the min-
```
imization of (7.17a), and (7.20a)-(7.20b) is the unique team-optimal solution
```
in feedback strategies. We should note, however, that if PI is also allowed to
```
know (and utilize) the value of the initial state Zi, (7.20a) is no longer a unique
```
team-optimal strategy for PI, but any representation of it on the state trajec-
```
tory (7.20c) also constitutes an optimal strategy (cf. Section 5.6). Denoting the
```
class of all such strategies by F1 , we now have the more general result that any
pair
```
constitutes a team-optimal solution to (7.19). For each such pair, the state
```
```
trajectory and the corresponding cost value are still as given by (7.20c) and
```
```
(7.20d), respectively.
```
```
Now, if (7.19), or equivalently (7.20d), is going to be realized as the Stackel-
```
berg cost of the leader, there should be an element of F1 , say 71*, which forces
73Note that the follower in fact attempts to minimize his own cost functional which is
different from J1.
```
HIERARCHICAL (STACKELBERG) EQUILIBRIA 379
```
the follower to play 72 even though he is minimizing his own cost functional
```
(which is J2—the counterpart of L2 in the extensive form description). This,
```
inevitably, asks for a relation of the form
```
and this being so uniquely, i.e., the minimum of J2(71*,-) on F2 should be
```
attained uniquely at 72 .74 Intuitively, an appropriate strategy for the leader
```
is the one that dictates (7.20a) if the follower plays (7.20b), and penalizes the
```
follower rather heavily otherwise. Since the leader acts, in this game, after
```
the follower does, and since the state equation is scalar and linear, he (the
```
```
leader) is in a position to implement such a strategy (by utilizing solely the
```
```
state information available to him). In particular, the strategy
```
where k is any positive number, is one such candidate. Such a strategy is clearly
in F1 . Moreover, one may readily check validity of the inequality
```
for k > O,75 which implies that the strategy (7.21) does indeed force the follower
```
```
to the team strategy (7.20b), since otherwise he incurs a cost which is higher
```
```
than the one obtained under the strategy (7.20a). The conclusion, therefore, is
```
```
that the strategy (7.21) constitutes, for k > 0, a Stackelberg strategy for the
```
```
leader, with the unique optimal response of the follower being (7.20b).
```
To recapitulate:
```
(1) For the dynamic game described by (7.16), (7.17a) and (7.17b) and un-
```
der the CLPS information pattern, a Stackelberg solution exists for all
```
nonnegative values of (3.
```
```
(2) The Stackelberg cost of the leader (PI) is equal to the global minimum
```
```
value of his cost function (obtained by cooperative action of the follower),
```
```
even though the follower (P2) seeks to minimize his own (different) cost
```
functional and may not know the cost functional of PI.
```
(3) The leader's Stackelberg strategy is a representation of his optimal feedback
```
```
strategy in the related team problem described by (7.19), and it necessarily
```
```
involves memory. (A feedback strategy for PI cannot be a Stackelberg
```
```
strategy.)
```
74In case of nonunique solutions, we have to take the supremum of J1 over all those solutions,
which should equal to J1 .75
This inequality, in fact, holds for k > — 1 + -y/8/13, but we consider only positive values
of k for the sake of simplicity in presentation.
380 T. BA§AR AND G. J. OLSDER
```
(4) The Stackelberg strategy of PI is nonunique (parameterized, in this case, by
```
```
a positive scalar k), but the optimal response of P2 to all those strategies
```
```
of PI is unique (and independent of k); it is the strategy that minimizes
```
the leader's cost function, even though the follower's objective is quite
different.
```
(5) The Stackelberg strategy of the leader, as given by (7.21), is not even
```
```
continuous; this, however, does not rule out the possibility for existence
```
```
of continuous (and differentiable) Stackelberg strategies.
```
```
(6) The strategy given by (7.21) may be viewed as a threat strategy on part
```
of the leader, since he essentially threatens the follower to make his cost
```
worse if he does not play the optimal team strategy (7.20b).
```
One natural question that arises at this point is whether there exists a Stackel-
```
berg strategy for the leader which is structurally more appealing (such as contin-
```
```
uous, differentiable, etc.). In our investigation in this direction, we necessarily
```
```
have to consider only the strategies in F1 , since the quantity (7.19) constitutes
```
a tight lower bound on the cost of the leader. In particular, we now consider
only those strategies in F1 which are linear in the available information, which
can be written as
```
where p is a scalar parameter. To determine the value of p for which (7.22) con-
```
```
stitutes a Stackelberg strategy for the leader, we have to substitute (7.22) into
```
```
J2, minimize the resulting expression over 72 £ F2 by also utilizing (7.16), and
```
```
compare the argument of this minimization problem with the strategy (7.20b).
```
Such an analysis readily leads to the unique value
```
provided, of course, that (3^0. Hence, we have the following.
```
Proposition 7.1 Provided that J3 / 0, the linear strategy
constitutes a Stackelberg strategy for the leader in the dynamic game described
```
by (7.16), (7.17a) and (7.17b), and the unique optimal response of the follower
```
```
is as given by (7.20b). The corresponding state trajectory is described by (7.20c)
```
```
and the leader's Stackelberg cost is equal to the team cost (7.20d).
```
```
Therefore, with the exception of the singular case (3 = 0, the leader can force
```
```
the follower to the team strategy (7.20b) by announcing a linear (continuously
```
```
differentiable) strategy, which is definitely more appealing than (7.21) which
```
```
HIERARCHICAL (STACKELBERG) EQUILIBRIA 381
```
```
was discontinuous on the equilibrium trajectory. For the case (3 = 0, how-
```
ever, a continuously differentiable Stackelberg strategy does not exist for the
```
leader (see Ba§ar (1979b), pp. 23-25) for a verification), and he has to employ a
```
```
nondifferentiable strategy such as (7.21).
```
Remark 7.7 The linear strategy presented in Prop. 7.1 and the discontinu-
```
ous strategies given by (7.21) do not constitute the entire class of Stackelberg
```
```
strategies of the leader in the dynamic game of this subsection; there exist other,
```
```
nonlinear (continuous and discontinuous) strategies for the leader which force
```
```
the follower to the optimal team strategy (7.20b). Each such strategy can be
```
determined by basically following the analysis that led to Prop. 7.1 under a dif-
```
ferent nonlinear representation of (7.20a) (instead of the linear representation
```
```
(7.22).
```
Remark 7.8 The results of this example can also be visualized graphically for
each fixed x\ and /3. Let us take x\ = I and leave j3 as a parameter, in which
```
case the iso-cost curves of P2 look as depicted in Fig. 7.1,and the (open-loop)
```
```
team solution for PI can be written as (from (7.20a)-(7.20d))
```
Figure 7.1: Graphical illustration of Example 7.1.
Since there is a one-to-one correspondence between x% and u2, any permis-
```
sible strategy of PI can also be written as a function of u2, i.e., ul = 71(u2).
```
Now, if we can find a strategy 71* with the property that its graph has only the
```
point (ul,u2} in common with the set
```
382 T. BA§AR AND G. J. OLSDER
```
then the follower (P2) will definitely choose u2 . Thus 71* constitutes a Stack-
```
elberg strategy for PI, leading to the global minimum of his cost function.
```
It easily follows from Fig. 7.1 that, for (3 > 0, there exists a unique linear 71*
```
```
which satisfies the above-mentioned requirement; it is the tangent at (ul , u2 )
```
```
to the corresponding iso-cost curve of P2. For (3 — 0 this tangent line turns
```
```
out to be vertical (parallel to the w1-axis), and hence cannot be described by a
```
```
linear strategy; however, a Stackelberg strategy exists in the class of continuous
```
```
strategies for the leader. (In Fig. 7.1 we have drawn the tangent lines for (3 = 2
```
and /3 = 0, and for the latter case the dashed curve provides a continuous
```
nonlinear Stackelberg strategy for the leader.)
```
Remark 7.9 A common property of the Stackelberg solutions obtained for
```
the dynamic game described by (7.16), (7.17a) and (7.17b) and under CLPS
```
information, is that they are also Nash equilibrium solutions. To see this, we
first note the obvious inequality
for such a Stackelberg equilibrium solution pair. Furthermore, the inequality
```
also holds, since the Stackelberg solution is also team-optimal (under J1) in
```
this case. These two inequalities then imply that every such pair also consti-
tutes a Nash equilibrium solution. We already know that the dynamic game
under consideration admits an uncountable number of Nash equilibrium solu-
```
tions (cf. Section 6.3); the above discussion then shows that, since the leader
```
can announce his strategy ahead of time in a Stackelberg game, he can choose
```
those particular Nash equilibrium strategies (and only those) which lead to an
```
equilibrium cost that is equal to the global minimum of his cost functional.
```
7.4.2 A second example (Example 7.2): Follower acts twice
```
in the game
Example 7.2 The indirect method introduced in the foregoing subsection for
the derivation of the Stackelberg solution of dynamic games under CLPS infor-
```
mation basically involves two steps: (1) determination of a tight lower bound
```
on the cost function of the leader, which coincides with the global minimum
```
value of a particular team problem; (2) adoption of a particular representation
```
of the optimal team strategy of the leader in this team problem, which forces the
```
follower to minimize (in effect) the cost function of the team problem while he
```
is in fact minimizing his own cost function. Even though this general approach
```
is applicable in a much more general context (i.e., applicable to dynamic games
```
```
with several stages and more complex dynamics), the related team problem is
```
```
not always the one determined solely by the cost function of the leader (as in
```
```
the previous subsection), in particular if the follower also acts at the last stage
```
```
HIERARCHICAL (STACKELBERG) EQUILIBRIA 383
```
of the game. The following scalar dynamic game now exemplifies derivation of
the related team problem in such a situation.
As a modified version of the scalar dynamic game of subsection 7.4.1, con-
sider the one with dynamics
and cost functionals
```
The information structure of the problem is CLPS, and PI (the leader) acting
```
```
at stage 2 has a single control variable it1, while P2 (the follower) acting at both
```
stages 1 and 2 has control variables it2 and u2,, respectively.
```
Let us introduce the strategy spaces F1 (for PI), F2 (for P2 at stage 1)
```
```
and F2 (for P2 at stage 2) in a way compatible with the governing information
```
structure. Then, to any announced strategy 71 € F1 of the leader, the follower's
optimal reaction at stage 2 will be
which is obtained by minimization of J2 over 7! € F?>. Hence, regardless of
what strategy 71 G F1 the leader announces, and regardless of what strategy
```
the follower employs at stage 1, his (the follower's) optimal strategy at stage 2
```
is a unique linear function of x% and 71, as given above. Substituting this
```
structural form into J1 and J2, derived from (7.24a) and (7.24b), respectively,
```
we obtain the reduced cost functionals
and therefore the lowest possible cost value the leader can achieve is
```
which is the quantity that replaces (7.19) for this game.
```
The original Stackelberg game has thus been converted into one wherein the
```
follower does not act at the last stage (his only strategy now being 72), and
```
```
the related team problem is described by (7.25), whose optimal team solution
```
in feedback strategies is
384 T. BA§ AR AND G. J. OLSDER
with the corresponding unique team trajectory being
The associated minimum team cost is
which forms a lower bound on the leader's cost in this game, and is definitely
higher than the quantity
Now, paralleling the arguments of subsection 7.4.1, we obtain the following rep-
```
resentation of (7.26a), on the trajectory (7.26c), to be a Stackelberg strategy for
```
```
the leader, forcing the follower at the first stage to the team strategy (7.26b):76
```
```
where k > — 1 + ^(96/113). The optimal response of the follower to any such
```
strategy is
```
leading to (7.26d) as the Stackelberg cost of the leader.
```
To obtain the counterpart of Prop. 7.1 for the dynamic game of this sub-
```
section, we again start with a general linear representation of (7.26a), and by
```
following similar arguments we arrive at the following proposition.
```
Proposition 7.2 Provided that (3 ^ 0; the linear strategy
```
constitutes a Stackelberg strategy for the leader in the dynamic game described
```
by (7.23), (7.24a) and (7.24b), and the unique optimal response of the follower
```
```
is given by (7.28a), (7.28b). The corresponding state trajectory is described by
```
```
(7.26c). and the leader's Stackelberg cost is as given by (7.26d).
```
```
Remark 7.10 It can again be shown that, for (3 = 0, no continuously differen-
```
```
tiate Stackelberg solution exists for the leader; therefore, he has to announce
```
```
a strategy which is discontinuous in the derivative, such as (7.27), or the one
```
76The reader is asked to fill in the details of this derivation.
```
HIERARCHICAL (STACKELBERG) EQUILIBRIA 385
```
analogous to that discussed in Remark 7.7. Furthermore, the reader should
note that, for the dynamic game under consideration, there does not exist a
```
Stackelberg solution which also constitutes a Nash equilibrium solution; i.e., a
```
counterpart of Remark 7.8 does not apply here. The main reason for this is the
fact that the follower also acts at the last stage of the game.
7.4.3 Linear Stackelberg solution of linear-quadratic
dynamic games
We now utilize and extend the indirect methods introduced in subsections 7.4.1
and 7.4.2 to solve for the Stackelberg solution of two-person linear-quadratic
dynamic games within the class of linear-memory strategies, so as to obtain the
counterpart of Props 7.1 and 7.2 in this more general context. The class of
dynamic games under consideration is described by the state dynamics
```
and cost functionals (for PI and P2, respectively)
```
```
with Qlk+l > 0, R™ > 0, Q2k+l > 0, R2kl > 0, VA: 6 K = ( 1 , . . . , K], dim(x) = n,
```
```
dim(ulk) — mi, i — 1,2. The information structure of the problem is CLPS,
```
```
under which the closed-loop strategy spaces for PI (the leader) and P2 (the
```
```
follower) are denoted by V\ and F^,, respectively, at stage k G K. Furthermore,
```
```
let J1 and J2 denote the cost functionals derived from (7.30a) and (7.30b),
```
```
respectively, for the normal (strategic) form of the game, under these strategy
```
spaces.
Since B^K does not necessarily vanish in this formulation, the follower also
acts at the last stage of the game, and this fact has to be taken into account in
the derivation of a tight lower bound on the leader's cost functional. Proceeding
as in subsection 7.4.2, we first note that, to any announced strategy X-tuple
```
{7^ 6 r£; k G K} by the leader, there corresponds a "robust" optimal reaction
```
```
by the follower at stage K, which is determined by minimizing J2(71,72) over
```
TK ^ ^K' witn the remaining strategies held fixed. This quadratic strictly
convex minimization problem readily leads to the relation
```
where T\K — {^1,^25 • • • , X K } , and 7^[x^; 7]^] stands for 7^(77^)—the strategy
```
of P2 at stage K—and displays the explicit dependence of the choice of 7^ on
7^. It should be noted, however, that the structure of this optimal response of
386 T. BA§AR AND G. J. OLSDER
the follower does not depend on the choice of 7^: regardless of what 7^- and the
```
previously applied strategies are, 7^- depends linearly on XK and 7^(77^)- The
```
```
unique relation (7.31) can therefore be used in (7.30a) and (7.30b) without any
```
loss of generality, in order to obtain an equivalent Stackelberg game wherein the
follower does not act at the last stage. The cost functionals of this new dynamic
game in extensive form are
where
Let us further introduce the notation
Then, a lower bound on the leader's Stackelberg cost is, clearly, the quantity
which is the minimum cost of a team problem in which both PI and P2 strive to
minimize the single objective functional J1. This team problem admits a unique
optimal solution within the class of feedback strategies, which is given in the
following lemma whose proof readily follows from Prop. 5.1 by an appropriate
decomposition.
```
Lemma 7.2 In feedback strategies, the joint optimization (team) problem de-
```
```
fined by (7.33) admits a unique solution given by
```
```
HIERARCHICAL (STACKELBERG) EQUILIBRIA 387
```
and with minimum cost
where
```
M(.) is defined recursively by
```
where
The optimum team trajectory is then described by
In view of the analysis of subsection 7.4.2, we now attempt to find a representa-
```
tion of the team strategy {7^ ; k € K} on the optimum team trajectory (7.34),
```
which provides a Stackelberg strategy for the leader. The following proposition
justifies this indirect approach for the problem under consideration.
```
Proposition 7.3 Let {7^*;^ € K} be a representation of the team strategy
```
```
{7^ ;fc 6 K} on the optimum team trajectory (7.34), such that every solution
```
of the minimization problem
```
is a representation of {7^ ; k E K — {-ft'}} on the same team trajectory. Then,
```
```
{ll*',k £ K} provides a Stackelberg strategy for the leader.
```
388 T. BA§AR AND G. J. OLSDER
Proof. We have already shown, prior to the statement of Lemma 7.2, that
```
the quantity (7.33) provides a lower bound on the cost function of the leader, for
```
the dynamic game under consideration. But, under the hypothesis of Prop. 7.3,
this lower bound is tight, and is realized if the leader announces the strategy
```
{7^*; k G K} which, thereby, manifests itself as a Stackelberg strategy for the
```
leader.
To obtain some explicit results, we now confine our attention to linear one-
```
step memory strategies for the leader (more precisely, to those representations
```
```
of {7^ ; k e K} which are linear in the current and most recent past values of
```
```
the state), viz.
```
```
where (P^; fc€K — {!}}isa matrix sequence which is yet to be determined.
```
This matrix sequence is independent of x^ and £fc_i, but it may, in general,
depend on the initial state x\ which is known a priori. Our objective for the
```
remaining portion of this subsection may now be succinctly stated as (i) deter-
```
mination of conditions under which a Stackelberg strategy for the leader exists
```
in the structural form (7.35) and also satisfies the hypothesis of Prop. 7.3, (ii)
```
```
derivation of the corresponding strategy of the leader (more precisely, the matri-
```
```
ces Pk, k € K — {!}) recursively, whenever those conditions are fulfilled. Toward
```
```
this end, we substitute (7.35) into J2(71,72), minimize the resulting functional
```
```
over 72 € F2 and compare the minimizing solutions with {7^; k e K} and
```
their representations on the optimum team trajectory. Such an analysis leads
to Thm. 7.3 given in the sequel.
Preliminary notation for Theorem 7.3
```
S(.): an (7712 x m2)-matrix defined recursively by
```
```
Afc: an (mi x n)-matrix defined recursively by (as a function of {Pjt+i,..., PK})
```
Condition 7.1 For a given x\ e Rn, let there exist at least one matrix sequence
```
{PK, PK-I, • • • i PZ] that satisfies recursively the vector equation
```
```
where A^ is related to {PK, • • • , Pk+i} through (7.36), and x*k is a known linear
```
```
function of x\, as determined through (7.34).
```
```
Theorem 7.3 Let Condition 1.1 be satisfied and let {P^(xi),... ,P2*(zi)} de-
```
note one such sequence. Then, there exists a Stackelberg solution for the dy-
```
namic game described by (7.29)-(7.30b) and under the CLPS information, which
```
389
is given by
```
where Llk (i — 1,2; k E K) were defined in Lemma 7.2. The corresponding
```
Stackelberg costs of the leader and the follower are given, respectively, by
```
Proof. Substituting u^ = 7fc*(-)> k e K, as given by (7.38a), into I/2 defined
```
```
by (7.32b), we obtain a functional which is quadratic and strictly convex in
```
```
{u^k € K — {-ftT}}- Let us denote this functional by L(u\,... ,v?K_l], which
```
is, in fact, what the follower will minimize in order to determine his optimal
response. Strict convexity of L readily implies that the optimal response of the
```
follower to the announced strategy K-tuple (7.38a) of the leader is unique in
```
open-loop policies, and that every other optimal response in f2 is a representa-
tion of this unique open-loop strategy on the associated state trajectory. Con-
sequently, in view of Prop. 7.3, verification of the theorem amounts to showing
```
that the minimum of L(u\,..., u2K_l) is attained by the open-loop represen-
```
```
tation of the feedback strategy (K — l)-tuple (7.38b) on the state trajectory
```
```
described by (7.34); in other words, we should verify that the set of controls
```
```
minimizes L(u\,..., u2K_1).
```
Now, since L is quadratic and strictly convex, it is both necessary and suffi-
```
cient that {u^*} be a stage-by-stage optimal solution; i.e., the set of inequalities
```
```
should be satisfied for all wjr G Rm2 and all k = 1 , . . . , K — 1, where {u2*}-k
```
denotes the entire sequence u 2 * , . . . ,uj£_l with only u2,* missing. Because of
```
the additive nature of the cost function L, (i) can equivalently be written as
```
for all u2, e Rm2 and all k = 1 , . . . , K - 1, where, for fixed k,
```
HIERARCHICAL (STACKELBERG) EQUILIBRIA390 T. BA§AR AND G. J. OLSDER
```
with
```
It is important to note that, at the RHS of (ii), Lk is not only a function of
```
u% but also of Xfc, which we take at its equilibrium value x\. because of the
```
RHS of (i) (i.e., the sequence { u f , . . . , u\_l} that determines Xk is already at
```
```
equilibrium while treating (ii)). It is for the same reason that yk, on the last
```
```
row of (iv), is taken to be equal to x*k.
```
```
Now, if the relation in the last line of (iv) is used iteratively in the remaining
```
```
relations of (iv), we can express y j , j > k 4- 2, in terms of yk+i and x*k, and if
```
```
this is used in (v), fj,j, j > k +I, can be expressed in terms of the same variables.
```
The resulting expressions are
where
Utilizing these expressions, we can next show that
Toward this end, we first obtain
where
and
```
HIERARCHICAL (STACKELBERG) EQUILIBRIA 391
```
```
Some further manipulations prove that (mi) can equivalently be written as
```
```
where A£+1 satisfies the recursive equation (7.36) with Pfc+i replaced by P%+\-
```
```
But, by Condition 7.1 [(7.37)], the foregoing expression identically vanishes;
```
```
this proves (vi), and in turn establishes the desired result that u%* minimizes
```
```
L at stage k and is in equilibrium with {u2*}_fc. (Note that, since L is a
```
```
strictly convex functional, (vi) is a sufficient condition for minimization.) Since
```
k was an arbitrary integer, the proof of the main part of the theorem follows.
```
Expression (7.39a) is readily obtained from Lemma 7.2 under the stipulation
```
that the optimum team trajectory is realized, since then J1* is identical with
```
J1 . Expression (7.39b), on the other hand, follows from substitution of the
```
team solution given in Lemma 7.2 into J2.
Remark 7.11 Condition 7.1, and thereby the Stackelberg solution presented
```
in Thm. 7.3, depends, in general, on the value of the initial state x\; that is,
```
the Stackelberg strategy of the leader at stage k is not only a function of Xk
and £fc-i, but also of x\. This latter dependence may be removed under a
```
more stringent condition (than Condition 7.1) on the parameters of the game,
```
```
in which case the vector equation (7.37) is replaced by the matrix equation
```
Such a restriction is, in fact, inevitable if we seek a solution to the infinite horizon
```
problem (i.e., with K —> oo), in which case our attention is solely confined to
```
```
stationary controls (see Ba§arand Selbuz (1979b), for an illustrative example).
```
Remark 7.12 The analyses of subsections 7.4.1 and 7.4.2 have already dis-
played the validity of the possibility that a linear-quadratic dynamic game might
not admit a Stackelberg solution in linear strategies. Therefore, depending on
the parameters of the dynamic game, Condition 7.1 might not be fulfilled. In
```
such a case, we have to adopt, instead of (7.35), a parameterized nonlinear rep-
```
```
resentation of {7^ ; k e K} as a candidate Stackelberg strategy of the leader,
```
and then determine those parameter values in view of Prop. 7.3. For a deriva-
tion of such a nonlinear Stackelberg solution, the reader is referred to Tolwin-
```
ski (1981b).
```
Another important point worth noting is that, for the linear-quadratic game
```
of this section, and for general parameter values, J{ given in Lemma 7.2 is not
```
always the Stackelberg cost of the leader, i.e., there may not exist a strategy
```
71* in F1 (linear or otherwise) under which the follower is forced to choose a
```
representation of 72 on the optimal team trajectory. This would be the case if,
for example, there exist certain control variables of the follower, at intermediate
stages, which do not affect the state variable, and thereby cannot be "detected"
by the leader. In such a case, the leader cannot influence the cost function
392 T. BA§AR AND G. J. OLSDER
of the follower through these control variables and hence cannot enforce the
```
team solution. This then necessitates derivation of a new team cost (which
```
```
is realizable as the Stackelberg cost of the leader), by taking into account the
```
```
robust optimal responses of the follower in terms of these controls (as we did at
```
```
the final stage). The resulting Stackelberg cost will be higher than Jl .
```
```
Another such case occurs if the leader can detect (through his state obser-
```
```
vation) a linear combination of the control variables of the follower, but does
```
```
not observe them separately. (This would be the case if, for example, the di-
```
```
mension of the state is lower than that of the control vector of the follower.)
```
```
Here again, we have to determine a new minimum team cost (different from
```
```
J1 ) to be realized as the Stackelberg cost of the leader, by taking into account
```
```
the freedom allotted to the follower in the nondetectable (by the leader) region
```
of his control space. We do not pursue this point any further here, and refer the
```
reader to Tolwinski (1981b) for such a derivation, and also to Ba§ar (1980a) and
```
```
Ba§ar (1982) for indirect derivations of the Stackelberg cost value of the leader
```
in general dynamic games when the leader has access to closed-loop imperfect
```
state information (cf. Def. 5.2). See also Problem 5 in Section 7.7.
```
```
7.4.4 Incentives (deterministic)
```
```
The idea of declaring a reward (or punishment) for a decision maker PI ac-
```
cording to his particular choice of action in order to induce a certain "desired"
```
behavior on the part of another decision maker P2 is known as an incentive (or
```
```
in case of the punishment, as a threat). Mathematical formulation and analysis
```
of such decision problems bear strong connections with the theory of Stackelberg
games presented in the previous subsections, which is what we will be discussing
next, for deterministic scenarios. Counterparts of these results in the stochastic
```
case will be presented later in Section 7.5 (particularly, subsection 7.5.3).
```
Following the convention of the previous subsections, we call, in the above
scenario, PI the leader and P2 the follower. Then, the action outcome desired
by the leader is:
The incentive problem can now be stated as: Find a 71 € F1, where F1 is an
admissible subclass of all mappings from 52 into S1, such that
```
Note that (7.42a) and (7.42b) require choosing a set of mi scalar functions which
```
together map 52 into S1 so as to satisfy m\ + m^ equations. If this set of mi
functions has mi +m2 or more parameters, then we might in general accomplish
this by choosing the parameters appropriately.
Incentive problems do arise in real life decision making. Think of PI as a
government and of P2 as a citizen. The income tax which P2 has to pay is a
```
HIERARCHICAL (STACKELBERG) EQUILIBRIA 393
```
```
fraction (say k) of his income (before taxation) u2. The amount of money that
```
the government receives is ul = ku2. It is up to P2 how hard to work and
```
thus how much money to earn. The incentive here is u1 — 71(u2) = ku2. The
```
government will choose k so as to achieve certain goals, but it cannot choose
its own income, u1, directly. In reality, the 71-functions will often be nonlinear,
but that does not take away the incentive phenomenon.
```
Example 7.3 Consider L1 = (u1)2 + (u2)2 and!2 = (u1-!)2^2-!)2, where
```
the u* are scalars. By inspection, u1 = u2 = 0. Consider the choice u1 = ku2:
with k approaching oo if necessary, as a possible incentive mechanism for PI.
The idea is that any choice of u2 ^ 0 will make L2 approach oo if k —> oo and
thus force P2 to choose u2 arbitrarily close to u2 in his own interest. However,
by substituting u1 = ku2 into L2, it is easily shown that the minimizing action
```
is u2 = (k + l)/(k2 + 1) and consequently u1 ~ (k2 + k } / ( k 2 + 1). Thus u2
```
```
approaches u2 = 0 and u1 approaches 1 (which is different from u1 ) as k
```
```
approaches oo and hence (7.42b) is violated. Consequently "infinite threat"
```
as just described is not generally feasible. Besides, such a threat may not be
credible in practice.
Let us now consider an incentive 71 of the form
```
where g is a function which satisfies g(u2 , u2 ) = 0 . With this restriction on
```
```
g, equation (7.42b) is automatically satisfied. Let us try the linear function
```
```
g — k(u2 — u2 ). Equation (7.42a) reduces to (A; + 1)/(A;2 + 1) = 0 and hence
```
k must be equal to —1. Graphically, the incentive u1 — ku2 = —u2 is a line
```
through the team solution (u1 ,u2 ) and has only this point in common with the
```
```
set of points (ul,u2} defined by L2(ul,u2) < L2(ul ,u2 ). By announcing the
```
```
71-function, PI ensures that the solution (w1 ,^2 ) will lie on the line u1 = — u2
```
```
in the (ul,u2) plane, independent of the action of P2. Being rational, P2 will
```
```
choose that point on this line which minimizes his cost function; such a choice is
```
```
u2 = 0. The relationship of this with the derivation of the Stackelberg solution
```
discussed in subsection 7.4.1 and particularly with the depiction of Fig. 7.1
should be clear now. From this graphical interpretation, it should also be clear
```
that any (nonlinear, continuous or discontinuous) incentive policy u1 = jl(u2)
```
which passes through the point characterized by the team solution and has only
this point in common with the set just described, will lead to the team solution
for Pi. If we restrict ourselves to linear incentives, then the solution is unique
in this example.
This example exhibits yet another feature. Note that with
In other words, by this choice of incentive, the objectives of both players are
```
identical (apart from a constant), thus fulfilling the old adage "if you wish other
```
people to behave in your own interest, then make them see things your way".
394 T. BA§AR AND G. J. OLSDER
In general making the cost functions identical by the choice of an appropriate
71-function will be too strong a requirement. A weaker form, however, which
also leads to the team solution, is
```
where it is assumed that 71 is of the form as described by (7.43).
```
```
Definition 7.1 The incentive problem, as defined in this subsection, is (lin-
```
```
early) incentive controllable, if there exists a (linear) ^-function such that
```
```
(7.42a) and (7.42b) are satisfied.
```
Of course, not all problems are incentive controllable. What can PI achieve
in problems that are not incentive controllable? The method to be employed
to answer this question will be described in the context of an example, given
below. It should then be clear how the method would apply to more elaborate
problems.
```
Example 7.4 Consider L1 = (u1 - 4)2 + (u2 - 4)2 and L2 = (u1)2 4- (u2 - I)2,
```
```
where the u1 are scalars; u1 € Sl = [0,3] and u2 £ S2 = [0,6]. The team solution
```
```
in this case is: u1 = 3, u2 = 4 and Ll(ul , u2 ) = 1. This is depicted in Fig. 7.2,
```
```
where some contours of L1 have been drawn; here the horizontal axis stands for
```
```
u2 (which is the independent variable) and the vertical one corresponds to u1.
```
The worst possible outcome for P2, even if he minimizes his cost function with
respect to his own decision variable, is
```
This occurs for u2 = I,!/1 = 3 (point A in the figure), and L2(3,1) = 9.
```
Whatever choice PI makes for 71, the cost for P2 will never be higher than 9.
```
If PI chooses u1 = 71(w2) = 3 on the interval [0,6], then the outcome becomes
```
```
u2 = I,!*1 = 3, (point A), and the costs for PI and P2 become 10 and 9,
```
respectively. This, however, is not optimal for PI. He should instead consider
minui L1 subject to L2 < 9. This latter region has been shaded in the figure.
```
The solution of this minimization problem is u1 = 12/5, u2 = 14/5 (point B
```
```
in the figure). Now, any 71-curve, in the rectangle 0 < u1 < 3,0 < u2 < 6,
```
which has only the points A and B with the shaded region in common, would
```
lead to a nonunique choice for P2; he might either choose u2 = 1 or u2 = 14/5.
```
Both choices lead to L2 = 9. The costs for PI are, respectively, 10 and 4 for
these choices. Therefore, PI will choose a 71 function as just described, with
```
one exception; it will have a little "dip" in the shaded area near point B, such
```
```
that the choice of P2 will be unique again. (A possible choice is: 71(u2) = 3 for
```
```
0 < u2 < 14/5 - e, where £ > 0 and jl(u2) = 12/5 for 14/5 - e < u2 < 6.) The
```
outcome will now be a point ul,u2 near point B, just within the shaded area.
```
PI can keep his costs arbitrarily close to 4 (but not equal to 4). D
```
```
HIERARCHICAL (STACKELBERG) EQUILIBRIA 395
```
Figure 7.2: Graphical illustration of Example 7.4.
Extensions of the foregoing analysis are possible in different directions, such
```
as the multi-stage problems (as in the case of the example of subsection 7.4.2)
```
```
or problems with multiple hierarchies; see the Notes section 7.7 for selected
```
references that cover these extensions. Another possibility is the many-followers
case which we now briefly discuss. If there are two or more followers in the
```
decision problem, the relationship (that is, the solution concept to be adopted)
```
between the followers must be specified.77 We shall illustrate a few of these,
barring the formation of coalitions between followers. Let PI be the leader,
```
Pi,i — 2 , . . . , M , be the M — I followers. An incentive policy (equivalently,
```
```
incentive mechanism) for the leader is a mapping 7* : S2 x • • • x Sm —•> Sl.
```
Suppose that ul , i = 1,..., M, is the M-tuple of actions desired by the leader.
An incentive mechanism 71 is said to induce a dominant strategy solution if
arg min
with arbitrary
```
For an illustration of a three decision maker problem, let u1 — (u\, u\], and
```
```
Then, the incentive mechanism u\ — 7J(w2) — 2u2 will induce u\ — 0 regardless
```
```
of the value of u3, and similarly u\ = ^\(u3) = 2w3 will induce u\ = 0 for all
```
values of u2, and hence the concatenation of 7* and 72 constitutes a dominant
```
strategy. Such a policy is the most desirable one (for the leader), since it effec-
```
tively decouples the followers from each other, and the leader can control each
one's cost function separately. However, such a solution is generally difficult
to realize, since the cost functions of the followers may not have the required
```
structure. An alternative is the Nash equilibrium concept (among the followers),
```
77Recall the discussion in Section 3.6 in the context of finite games.
396 T. BA§AR AND G. J. OLSDER
where we only require
arg min
with
```
i.e., each follower will behave desirably conditioned on the fact (expectation)
```
that the others will do the same. A particular subcase of this equilibrium occurs
if the incentive mechanism can be chosen so that under it the cost functions of
all followers become identical. The followers then face a team problem which has
```
only one "reasonable" solution; see Problem 10 in Section 7.7 for an illustration
```
of such a situation. The same problem also deals with the case of two followers
facing a zero-sum game after implementation of the correct incentive mechanism.
The saddle-point solution of this zero-sum game leads to the team solution of
the leader who thus practices the adage "divide and rule".
7.5 Stochastic Dynamic Games with Determin-
istic Information Patterns
This section is devoted to a brief discussion on possible extensions of the results
```
of the previous sections to stochastic dynamic games (cf. Def. 5.4) wherein the
```
state evolves according to
```
where {#1,... ,&K} is a set of statistically independent Gaussian vectors with
```
```
values in Rn and with cov (Ok) > 0, k G K. In a general context, the cost
```
functional of Pi, for the game in extensive form, is again taken to be stage-
additive, viz.
and, abiding by our standard convention, PI is taken to act as the leader and
P2 as the follower. In the discussion to follow, we shall consider three different
```
information structures; namely, (A) open-loop for both players, (B) open-loop
```
```
for the leader and closed-loop perfect state (CLPS) for the follower, (C) CLPS
```
```
for both players. In addition to derivation of the (global) Stackelberg solution
```
```
(cf. subsection 7.5.1), we shall also discuss derivation of the feedback Stackelberg
```
```
solution under the third information pattern listed above (cf. subsection 7.5.2),
```
```
and stochastic incentive problems (as the counterpart of the material presented
```
```
in subsection 7.4.4).
```
```
7.5.1 (Global) Stackelberg solution
```
A. Open-loop information for both players
If the underlying deterministic information structure is open-loop for both play-
```
ers (in which case the controls depend on the initial state x\ which is assumed to
```
```
HIERARCHICAL (STACKELBERG) EQUILIBRIA 397
```
be known a priori], then the Stackelberg solution can be obtained by basically
converting the game into equivalent static normal form and utilizing the first
method outlined in Section 7.2 for the deterministic open-loop Stackelberg solu-
```
tion. Toward this end, we first recursively substitute (7.46) into (7.47), and take
```
```
expected values of Ll over the statistics of the random variables {9\,..., OK}, to
```
```
obtain a cost functional (Jl (u1, u2)) which depends only on the control vectors,
```
```
u1, u2 (and also on the initial state vector xi which is already known by the
```
```
players). Since such static games have already been treated in Section 4.5, we
```
do not discuss them here any further, with the exception of one special case
```
(see the next paragraph and Prop. 7.4). It should be noted that the Stackelberg
```
solution will, in general, depend on the statistical moments of the random dis-
turbances in the state equation—unless the system equation is linear and the
cost functionals are quadratic.
```
For the special case of linear-quadratic dynamic games (i.e., when //- in (7.46)
```
```
is given as in (7.7a), and glk in (7.47) is structured as in (7.7b), and under the
```
```
open-loop information structure, the cost functional J^u^u2 ) can be written
```
as
where
and £* is independent of the controls and xi, and depends only on the covariances
```
of ( # i , . . . , QK}- This result is, of course, valid as long as the set {$1, ...,##•} is
```
statistically independent of xi, which was one of our underlying assumptions at
the beginning. Hence, for the linear-quadratic stochastic game, the stochastic
```
contribution completely separates out; and, furthermore, if E[9k] = 0 VA; G K,
```
```
the open-loop Stackelberg solution matches (completely) with the one given in
```
Corollary 7.1 for the deterministic problem. This result is now summarized
below in Prop. 7.4.
Proposition 7.4 Consider the two-person linear-quadratic stochastic dynamic
```
game described by (7.46)-(7.47) together with the structural assumptions (7.7a)-
```
```
(7.7b), and under the parametric restrictions Qlk+i > 0, R]2 > 0, E[dk] = 0 (k €
```
```
K; i — 1,2). It admits a unique open-loop Stackelberg solution with PI acting
```
as the leader, which is given, under the condition of invertibility of the matrix
```
appearing in the braces in (7.9c), by (7.8a)-(7.8b).
```
B. OL information for PI and CLPS information for P2
For the deterministic Stackelberg game, we have already seen in Section 7.2 that
an additional state information for the follower does not lead to any difference
in the open-loop Stackelberg strategy of the leader, provided, of course, that
```
he'(the leader) still has only open-loop information (cf. Remark 7.1). The only
```
398 T. BA§AR AND G. J. OLSDER
difference between the OL-OL and OL-CLPS Stackelberg solutions then lies in
the optimum response strategy of the follower, which, in the latter case, is any
closed-loop representation of the open-loop response strategy on the equilibrium
```
trajectory associated with the OL Stackelberg solution; hence, the optimum
```
strategy of the follower under the OL-CLPS information pattern is definitely not
unique, but it can, nevertheless, be obtained from the OL Stackelberg solution.
For the stochastic Stackelberg game, however, the issue is more subtle, espe-
cially if the state dynamics is not linear and the cost functional is not quadratic.
In general, the OL-OL Stackelberg solution does not coincide with the OL-CLPS
Stackelberg solution, and the latter has to be obtained independently of the for-
mer. The steps involved in this derivation are as follows:
```
(1) For each fixed {w^ € £/£; k e K}, minimize
```
```
subject to (7.46) and over the permissible class of strategies, F2. Any solution
```
72 € F2 of this stochastic control problem satisfies the dynamic programming
```
equation (see Section 5.6, equation (5.61b))
```
where
and E0k denotes the expectation operation with respect to the statistics of Ok.
```
(2) Now, minimize the cost functional
```
over the permissible class of strategies, F1, and subject to the constraints im-
```
posed by the dynamic programming equation (7.48) and the state equation
```
```
(7.46) with u^ replaced by 7jt°(r?j(;).78 The solution of this optimization prob-
```
lem constitutes the Stackelberg strategy of the leader in the stochastic dynamic
game under consideration.79
```
78Note that 7^°(-) is, in fact, dependent on 71, but this dependence cannot be written
```
```
explicitly (in closed-form), unless the cost functional of P2 and the state equation have a
```
```
specific simple structure (such as quadratic and linear, respectively).79
```
```
The underlying assumption here is that step (i) provides a unique solution {^°;k € K}
```
for every
```
HIERARCHICAL (STACKELBERG) EQUILIBRIA 399
```
For the class of linear-quadratic games, these two steps can readily be carried
out, to lead to a unique Stackelberg solution in closed-form. In this case, the
leader's equilibrium strategy, in fact, coincides with its counterpart in the OL-
```
OL game (presented in Prop. 7.4)—a property that can be verified without
```
actually going explicitly through the two steps of the foregoing procedure. This
result is given below in Prop. 7.5, whose proof, as provided here, is an indirect
one.
Proposition 7.5 Consider the two-person linear-quadratic stochastic Stackel-
berg game of Prop. 7.4, but under the OL-CLPS information pattern. Provided
that the condition of matrix invertibility of Prop. 7.4 is satisfied, this stochastic
game admits a unique Stackelberg solution with PI as the leader, which is given
for k e K by
where
```
and K^. is defined by (7.9a), and x*k by (7. We).
```
```
Proof. For each fixed control strategy {u^ — 7^(xi); k e K} of the leader,
```
```
the dynamic programming equation (7.48) can explicitly be solved to lead to
```
the unique solution
```
where P% and Sfc+i are defined by (7.51a) and (7.51b), respectively, and s~k is
```
defined by
```
(This result follows from Prop. 5.4 by taking E[dk] — B^u^..)
```
```
Now, at step (2) of the derivation, (i) and (ii) above will have to be used,
```
```
together with the state equation, as constraints in the minimization of (7.49)
```
```
over F1. By recursive substitution of (ii) and the state equation into (i), we
```
```
obtain the following equivalent expression for (i), to be utilized as a constraint
```
in the optimization problem faced by the leader:
```
where Tfc(£), Mfc, Nk(l] are the coefficient matrices associated with this repre-
```
```
sentation, whose exact expressions will not be needed in the sequel. Now, (iii)
```
400 T. BA§AR AND G. J. OLSDER
being a linear relation, and Ll being a quadratic expression in
```
K}, it follows that, with u1 = (u\ , . . . , u^)', J1(71,72°) can be written as
```
where T1, T2, T3 are deterministic matrices, and Jm is independent of u1 and
```
is determined solely by the covariance matrices of {#&; k € K}—this decompo-
```
sition being due to the fact that the leader's information is open-loop and hence
```
his controls are independent of the random variables {Ok',k G K} which were
```
taken to have zero mean. Since the Stackelberg strategy of the leader is deter-
```
mined as the global minimum of J1(71,72°), it readily follows that it should be
```
```
independent of the covariance of {#&; k G K}. Now, if there were no noise in
```
```
the state equation, the follower's response (though not unique) would still be
```
```
expressible in the form (i), and therefore the leader's Stackelberg strategy would
```
```
still be determined as the control that minimizes [u1 Tlul + ul T2x\ + x'yT^x\}.
```
Hence the leader's Stackelberg strategy in this stochastic game is the same as the
```
one in its deterministic counterpart—thus leading to (7.50a) (see Corollary 7.1
```
```
and Remark 7.1). The follower's unique optimal response (7.50b) then follows
```
```
from (i) by substituting (7.50a) into (ii). Note that this strategy is, in fact,
```
```
a particular closed-loop representation of (7.8b) on the open-loop equilibrium
```
```
trajectory of the deterministic problem (which is described by (7.10e)).
```
C. CLPS information for both players
When the leader has access to dynamic information, derivation of the Stack-
elberg solution in stochastic dynamic games meets with insurmountable diffi-
```
culties.80 First, any direct approach readily fails at the outset (as in its de-
```
```
terministic counterpart, discussed in Section 7.4) since the optimal response of
```
```
the follower to any announced strategy of the leader (from a general strategy
```
```
space) cannot be expressed analytically (in terms of that strategy), even for
```
linear-quadratic games. Second, the indirect method of Section 7.4, which is
developed for deterministic dynamic games, cannot be extended to stochastic
```
games, since, in the latter case, every strategy has a unique representation (see
```
```
Section 5.6) as opposed to the existence of infinitely many closed-loop represen-
```
tations of a given strategy in a deterministic system. Consequently, derivation
of the closed-loop Stackelberg solution of stochastic dynamic games remains,
today, as a challenge for the researchers.
If, however, we make some structural assumptions on the possible strategies
of the leader—which is tantamount to seeking sub-optimal solutions—then the
problem may become tractable. In particular, if, under the stipulated structural
assumptions, the class of permissible strategies of the leader can be described by
a finite number of parameters, and if the follower's optimal response can be de-
termined analytically as a function of these parameters, then the original game
80Consistent with the CLPS information pattern, we are assuming here that the leader
does not have direct access to the follower's control. If this is not the case, and the leader's
information exhibits redundancy, then the problem might be more tractable. This case will
be discussed later in subsection 7.5.3 in the context of stochastic incentive problems.
```
HIERARCHICAL (STACKELBERG) EQUILIBRIA 401
```
may be viewed as a static one in which the leader selects his strategy from a
```
Euclidean space of appropriate dimension; such a static Stackelberg game is, in
```
general, solvable—but more often numerically than analytically. The following
example now illustrates this approach and demonstrates that even for simple
stochastic dynamic games, and under the crudest type of structural assump-
```
tions, the corresponding (sub-optimal) Stackelberg solution cannot be obtained
```
analytically, but only through some numerical minimization techniques.
Example 7.5 Consider the two-stage scalar stochastic dynamic game described
by the state equations
and cost functionals
Here, 9\ and 9-2 are taken as independent random variables with mean zero and
```
variances a\ and (72, respectively. The leader (PI) acts at stage 2 and has access
```
```
to both xi and X2, while the follower (P2) acts at stage 1 and has only access
```
to x\.
```
If 7* 6 P denotes a general strategy of Pi (i — 1, 2), the expected (average)
```
cost functional of P2 can be written as
which has to be minimized over 72 € F2, to determine the optimal response of P2
to 71 € F1. Barring the stochastic aspects, this is similar to the problem treated
in subsection 7.4.1, where the difficulties involved in working with a general 71
```
have been delineated. We therefore now restrict our investigation (also in line
```
```
with the discussion preceding the example) to a subclass of strategies in F1
```
which are affine in x%, that is, to strategies of the form81
```
where a and (3 are free parameters which are yet to be determined. They will, in
```
general, be dependent on x\ which is, though, known a priori by both players.
```
Under the structural restriction (7.53), J2 admits a unique minimum, thus
```
```
leading to the optimal response strategy (for the follower)
```
81 The apparent linear structure of the second term below is adopted for the sake of conve-
```
nience in the analysis to follow; it could also have been taken as a single function (3.
```
402 T. BA§AR AND G. J. OLSDER
which explicitly depends on the parameters a and /? that characterize the
```
leader's strategy. To determine their optimal values, we now substitute (7.53)-
```
```
(7.54) into J1(71,72), together with the corresponding values of £3 and x^ from
```
```
(7.52), to obtain the function F given below, which has to be minimized over
```
```
a € R, (3 € R for fixed x\:
```
Let us now note that
```
(i) F is jointly continuous in (a,/?), F(a,(3) > 0 V(a,/3) € R x R, and
```
```
F(ot,(3} —> oo as |a|, \(3\ —> oo. Therefore, we can restrict our search for a
```
minimum on R2 to a closed and bounded subset of R2, and consequently
```
there exists (by the Weierstrass theorem82) at least one pair (a*,/?*) that
```
```
minimizes F for any given pair (xi,ai).
```
```
(ii) The optimum pair (a*,/3*) depends on (XI,(TI), but not on o'2, and it
```
```
cannot be expressed analytically as a function of (xi, a\). Hence, for each
```
```
fixed (XI,<TI), F(a,/3) has to be minimized numerically, by utilizing one
```
```
of the available optimization algorithms (Luenberger, 1973).
```
```
(iii) With (a*,/?*) determined as above, the linear-in-a:2 strategy 71*(x2,xi) =
```
```
a*(xi)x2 -f P*(XI)XI is only a sub-optimal Stackelberg strategy for the
```
leader, since he may possibly achieve a better performance by announcing
a strategy outside the "linear-in-o^" class.
7.5.2 Feedback Stackelberg solution
In this subsection, we extend the results of Section 7.3 to stochastic dynamic
```
games described by (7.46)-(7.47), and, in particular, we obtain the counter-
```
```
part of Thm. 7.2 in the present context (see Thm. 7.4 below). In this analy-
```
```
sis, we do not restrict our attention at the outset to feedback strategies (as it
```
```
was done in Section 7.3), but rather start with general closed-loop strategies
```
```
ulk = ^(x^i < k), k 6 K, i — 1,2. The conclusion, however, is that the feed-
```
back Stackelberg solution can be realized only in feedback strategies. To see
```
this, we start (in view of Def. 3.29) at the last stage k = K and solve basically
```
a static Stackelberg game between PI and P2, with their strategies denoted by
7Jf and 7^-, respectively. Because of the additive nature of the cost functional
```
of each player, and since cov (OK] > 0, OK is Gaussian and statistically in-
```
dependent of the remaining random vectors, every Stackelberg solution at this
stage will depend only on XK and not on the past values of the state vector.
Proceeding to the next stage k = K — 1, after substitution of the Stackelberg
solution at stage k — K into the state equation, we conclude, by the same rea-
soning, that the Stackelberg strategies at stage k = K — 1 are only functions
```
of XK-I- An inductive argument (as in the proof of Thm. 6.10) then verifies
```
82See Section 5 of Appendix A.
```
HIERARCHICAL (STACKELBERG) EQUILIBRIA 403
```
the "feedback" property of the feedback Stackelberg strategies, and it also si-
multaneously leads to the equations that yield the corresponding solution for
the stochastic dynamic game. Theorem 7.4, below, now provides the complete
solution under the assumption of singleton reaction sets for the follower.
Theorem 7.4 Every feedback Stackelberg solution of a two-person K-stage sto-
```
chastic infinite dynamic game described by (7.46)-(7.47) and with the CLPS
```
```
information pattern (for both players) comprises only feedback strategies, and is
```
```
strongly time consistent. A pair of strategies {7** e F1^2* £ F2} constitutes a
```
feedback Stackelberg solution with PI as the leader if
```
where R%.(-} is a singleton set defined by
```
and G\. is recursively defined by
Proof. This result follows from a direct application of Def. 3.29 and Prop. 3.15
```
(interpreted appropriately for the infinite stochastic dynamic game under con-
```
```
sideration) , by employing the techniques and lines of thought used in the proof
```
of Thm. 6.10 for the feedback Nash solution.
```
Remark 7.3 has a natural counterpart here for the stochastic game; and
```
for the special case of the linear-quadratic stochastic dynamic game in which
E[9k] = 0 Vfc € K, it may readily be verified that the feedback Stackelberg
solution of Thm. 7.4 is precisely the one given in Corollary 7.2.
7.5.3 Stochastic incentive problems
We have seen in subsection 7.5.1 that for stochastic dynamic games, and un-
der CLPS information, it is generally very difficult, if not impossible, to obtain
the global Stackelberg solution—with the indirect method developed in the de-
```
terministic case (cf. Section 7.4) not applicable here because of uniqueness of
```
strategy representations. What if, however, the leader has also access to the
follower's past control actions, in addition to the state information based on
404 T. BA§AR AND G. J. OLSDER
which these actions were obtained? In this enlarged information structure, a
given strategy of the leader will have multiple representations, thus opening
```
the possibility of enforcement of a team solution (to the leader's advantage)
```
```
by selecting an appropriate representation of the team-optimal strategy (of the
```
```
leader). To illustrate this line of thought now, let us revisit Example 7.5, but
```
with the following enlarged information structure.
Example 7.6 Consider the two-stage scalar stochastic dynamic game of Ex-
ample 7.5, but with the enlarged information structure that allows PI to have
access to u2, in addition to xi and x%. Note that since u2 depends only on #1,
this enlarged information structure carries the same statistical information as
```
the earlier one for each fixed (pure) policy of the follower; however, as we will see
```
shortly, the informational redundancy that it generates will bring a substantial
advantage to the leader.
In the spirit of the analysis of Example 7.1 for the deterministic case, let
us first determine the best performance the leader would achieve if the follower
```
were cooperating with him (in the minimization of the leader's expected cost
```
```
function). The associated team problem is
```
where
```
Here, the cost shows dependence on u2 — ^2(x\) not only directly, but also
```
```
through £2 as given by (7.52), which has to be taken into account in the min-
```
imization. Furthermore, the strategy spaces F1 and F2 are taken as in Exam-
ple 7.1, since the additional knowledge of u2 for PI does not help in further
reducing the minimum team cost J*. Now, this team problem is in fact a stan-
dard LQ stochastic control problem of the type covered by Prop. 5.4, and its
solution can readily be obtained as:
which is the unique minimizing pair in F1 x F2. It is not, however, unique
```
in the enlarged strategy space for the leader, as (for example) the following
```
parameterized strategy also constitutes an optimal solution, along with 72 given
above, for every a e R:
```
This in fact characterizes the complete class of linear (in x%,xi, u2) optimal
```
strategies, but of course there are also nonlinear ones—all leading to the same
```
(minimum) expected value for the leader. By a slight generalization of the termi-
```
nology introduced in Def. 5.11, we will refer to all these "minimum expected cost
```
HIERARCHICAL (STACKELBERG) EQUILIBRIA 405
```
achieving" strategies as representations of 7* under the team-optimal solution
```
(71 ' 72 )• This is a rich family of strategies, among which we seek one with the
```
additional property that if the follower instead minimizes his own expected cost
function, then the strategy in F2 that achieves this minimum is still 72 . The
```
corresponding strategy (representation) for the leader would then clearly con-
```
stitute a global Stackelberg solution, leading to the best possible performance
for him.
```
Let us now conduct the search in the family of linear representations (7.56),
```
which leads to the quadratic optimization problem:
where
Since x\ is independent of 9\ and $2, which have zero mean, this problem is
equivalent to the following deterministic optimization problem:83
where we have written v for u2, to simplify the notation. Now, a simple opti-
mization shows that for the value of a — 8/27, this optimization problem admits
```
the unique solution v = (5/14)xi = 72 ' ( x i ) , and hence the policy pair
```
provides a global Stackelberg solution. This is in fact the unique such solution
in the linear class.
Stochastic decision problems of the type above, where the leader is allowed
to have access to past actions of the follower are known as stochastic incentive
problems, which are the stochastic counterparts of those briefly discussed in
subsection 7.4.4. In stochastic incentive problems, the information structure
```
may not always be nested (for the leader), as in the example above, where the
```
```
leader has access to all the information that the follower has access to (plus
```
```
more). If, for instance, in Example 7.6 the leader has only access to £2 and w2,
```
then we have a problem with a nonnested information structure, to which the
methodology presented above does not apply, since the dynamic information for
the leader no longer exhibits redundancy. Discussion of such problems, where
the follower possesses private information not known to the leader, is beyond
```
the scope of our coverage here; the interested reader can consult with Ho, Luh
```
```
and Olsder (1982) and Ba§ar(1984, 1989a)). For stochastic incentive problems
```
with nested information, however, the methodology used in Example 7.6 can be
```
83This equivalence holds as far as its optimum solution goes (which is what we seek), but
```
not for the corresponding minimum values.
406 T. BA§AR AND G. J. OLSDER
developed into a general procedure as briefly discussed below for a special class
of such problems.
Consider a two-person stochastic incentive problem with the cost functions
```
Ll(ul,u2;Q) and L2(ul ,u2;Q], for PI (leader) and P2 (follower), respectively,
```
where 9 is some random vector with a known distribution function. Let yl =
```
hl(Q] be the measurement of PI on i9, and y2 = h2(0) be P2's measurement,
```
```
with the property that what P2 knows is also known by PI (but not necessarily
```
```
vice versa).8* Let F* be the set of all measurable policies of the form ul = 7t(y*),
```
```
i = 1,2, and F1 be the set of all measurable policies of the form u1 = 71(y1, u2).
```
Introduce the pair of policies
assuming that the underlying team problem admits a minimizing solution.
Then, a representation of the leader's strategy 71 under the pair above is an
element of F1, say 71, with the property
The following result now readily follows:
Proposition 7.6 For the stochastic incentive decision problem with nested in-
```
formation as formulated above, the pair (Iy1,'j2 ) constitutes a global Stackelberg
```
solution, leading to the best possible outcome for the leader. Equivalently, if
```
a strategy 71 € F1 exists satisfying (7.57), the stochastic decision problem is
```
incentive controllable.86
```
Remark 7.13 For the special class of linear-quadratic-Gaussian (LQG) prob-
```
```
lems, where the decision variables (u1, u2) belong to finite dimensional Euclidean
```
spaces, 9 is a Gaussian random vector, /i1 and h2 are linear, and Ll is jointly
```
quadratic in the triple (ul,u2,Q] and strictly convex in (ul,u2) for each 0, the
```
team-optimal policies 71 and 72 exist, are unique and linear in y1 and y2,
```
respectively (see any standard book on stochastic control, such as (Bertsekas,
```
```
1987)). If, furthermore, L2 is also a quadratic function, then except for some
```
isolated cases one can restrict the search to linear representations of 71 :
where P is a matrix of appropriate dimensions. Now, invoking the condition
```
(7.57) one can obtain an equation for P, whose solution (when used in (7.58))
```
leads to a linear incentive policy. This then makes the decision problem linear
```
incentive controllable; for details see (Ba§ar, 1979d).
```
84 In mathematical terms, this requirement can be stated as the sigma-field generated by y1
including the sigma-field generated by y2.85
The equality should hold for almost all values of 9, under its assumed distribution function.86
```
The terminology we have used here is the natural counterpart (in the stochastic case) of
```
the one introduced in Def. 7.1 for deterministic incentive problems.
```
HIERARCHICAL (STACKELBERG) EQUILIBRIA 407
```
```
The development above does not cover (even in the LQG framework) the
```
most general class of dynamic nested stochastic incentive problems, because
the measurements of the decision makers have been taken to be static—not
depending on the past actions. If the leader's measurement at stage k depends
```
on the past actions of the follower (uf, t < &), then the approach discussed above
```
can easily be adjusted to apply to such multi-stage problems too. If, however,
the follower also has access to the leader's past control actions, then because of
```
the nestedness of the information structure for the leader (which does not allow
```
```
for the follower to have access to all measurements of the leader) the associated
```
dynamic team problem becomes what is called a nondassical stochastic control
problem, for which no general theory exists. Issues such as learning, inference,
and filtering become of relevance then, whose treatment requires background in
stochastic processes, information theory and control, much beyond the level of
our coverage here.
For static stochastic incentive problems where there is one leader and several
followers, with the followers playing according to the Nash concept, a natural
```
counterpart of (7.45) exists, which is:
```
Since the Nash equilibrium is defined here over strategy spaces, it admits natural
```
(conceptual) extensions to the multi-stage case, where the informations of the
```
```
players (leader as well as the followers) might be intertwined in more complicated
```
ways.
7.6 Stackelberg Solution of Differential Games
In this section we discuss the continuous-time counterparts of some of the results
presented in the previous sections, and in particular the open-loop and feedback
Stackelberg solutions.
7.6.1 The open-loop information structure
As counterparts of the results of Section 7.2 in the continuous time, we present
here the open-loop Stackelberg solution of deterministic two-person differential
```
games of prescribed fixed duration (cf. Def. 5.5), when PI acts as a leader. The
```
class of games under consideration are described by the state equation
and cost functionals
408 T. BA§AR AND G. J. OLSDER
where [0, T] denotes the fixed prescribed duration of the game, XQ is the initial
```
state known by both players, x(t) e Rn and u*(t) e 5* C Rmi (i = 1,2),
```
Vt E [0, T]. The underlying information structure is open-loop for both players,
```
so that the controls w1(-) and u2(-) depend only on the time variable t and
```
```
the initial state XQ. We allow only for controls (or synonymously, in this case,
```
```
strategies) that are continuous in the time variable; and this determines the
```
```
strategy sets (F^F2) which are in this case equivalent to the control function
```
```
sets (C/1,?/2). We further assume that / satisfies the conditions of Thm. 5.1
```
so that a unique continuously differentiable trajectory exists as a solution to
```
(7.59a) for every permissible control pair (u1,^2).
```
To determine the set of relations to be satisfied by an open-loop Stackel-
```
berg solution, we first obtain the optimal reaction of the follower (P2) to every
```
```
announced control ul of the leader by minimizing L2(ul,u2) over u2 E u2.
```
Lemma 7.3 In addition to the conditions of Thm. 5.1, let
```
i) f(tj -jU1,!*2) be continuously differentiable on Rn
```
```
ii) g2(t, -,ul,u2) and q2(-} be continuously differentiable on Rn Vt 6 [0,T].
```
Then, if u1 E U1 is a fixed control of PI, andu2 E U2 denotes a corresponding
```
optimal response ofP2, there exists a function p(-) : [0,T] —* Rn such that the
```
following relations are satisfied:
where
If, furthermore,
```
Hi) H2(t,p, x,u*, •) is continuously differentiable and strictly convex on S2
```
which is taken as an open set, the second relation above is replaced by
Proof. This result follows directly from Thm. 5.4.
To proceed further, we now assume that there exists a unique u2° E U2
under which the set of relations of Lemma 7.3 is satisfied for a given u1 E t/1,
```
a sufficient condition for which is strict convexity of L2(ul, •) on U2, with x(-)
```
```
substituted from (7.59a). Then, to determine his Stackelberg strategy, the leader
```
will be faced with the optimal control problem:
```
HIERARCHICAL (STACKELBERG) EQUILIBRIA 409
```
such that
This optimization problem is not of the type covered by Thm. 5.4 since its differ-
ential constraints involve specified boundary conditions at both ends. The prob-
lem, however, is still tractable, and can be solved by utilizing some other stan-
```
dard results of optimal control theory, particularly form (Bryson and Ho, 1975,
```
p. 65). Toward this end, we introduce the "Hamiltonian"
```
where AI(-) : [0,T] —> Rn is the costate function corresponding to (7.61b),
```
```
A2(-) : [0,T] —>• Rn is the costate function corresponding to (7.61c) and A3(-) :
```
[0,T] —> Rm2 is the Lagrange multiplier function associated with the equality
```
constraint (7.61d). Under suitable differentiability conditions (to be made pre-
```
```
cise in Thm. 7.5 to follow), AI(-) and A2 (-) satisfy the set of differential equations
```
and with 51 taken as an open set, the Stackelberg open-loop control of PI
satisfies the relations
These results are now summarized in Thm. 7.5 below.
Theorem 7.5 For the class of two-person differential games under consider-
```
ation in this subsection, assume, in addition to i), ii) and Hi) of Lemma 7.3,
```
that
```
iv) /(£, -,ul,u2) is twice continuously differentiate on
```
```
v) 9*(t,f,ru>l,u2} and q2(-) are twice continuously differentiate on
```
```
vi) gl(t, •, w1,^2) and q l ( - ) are twice continuously differentiable on
```
410 T. BA§AR AND G. J. OLSDER
```
vii) / ( £ , £ , - , u ) , gl(t,x, -,u ), i — 1,2, are continuously differentiable on Rmi
```
we[o,r],
vmj S1 is on open set
```
Then, if {^(t^xo) = ul*(t};i = 1,2} provides an open-loop Stackelberg so-
```
```
lution with PI as the leader, and {x*(t),0 < t < T} denotes the correspond-
```
```
ing state trajectory, there exist continuously differentiable functions p(-), AI(-),
```
```
^2('); [0»T] ~* Rn> and a continuous function \3(-): [0, T] —> R7712, such that
```
the following relations are satisfied:
```
where H2 is defined in Lemma 7.3, and Hl is defined by (7.62).
```
```
Proof. Since the optimization problem faced by the leader is (7.61a)-(7.61d),
```
the theorem follows, in view of Lemma 7.3, from a standard optimal control
result that can, for instance, be found on p. 65 of the text by Bryson and
```
Ho (1975).
```
As a specific application of this theorem, we now consider the special class
```
of linear-quadratic differential games (cf. Def. 6.5), wherein we also assume
```
```
R12(-) > 0, £*(•) > 0, i = 1,2. The Hamiltonians Hl and H2 can be written as
```
```
and thus the set of relations (7.62) reduce to (with the arguments suppressed)
```
```
HIERARCHICAL (STACKELBERG) EQUILIBRIA 411
```
Hence, Thm. 7.5 says, for this special case, that every open-loop Stackelberg so-
```
lution of the linear-quadratic differential game is determined by the set (7.64a)-
```
```
(7.64g). Because of the specific structure of the problem, we can actually obtain
```
a stronger result, which is that the linear quadratic differential game admits a
```
unique solution, and this, in turn, implies that (7.64a)-(7.64g) admits a unique
```
solution set. To establish this result, let us first observe that87 there exist,
for the linear-quadratic problem, bounded linear operators L\ : U1 —> U1,
T2 •L,l .
with
i — 1,2, so that
where
denotes the inner product of two vectors in Ul which is assumed to be structured
```
as a complete vector space. Now, by hypothesis, L2 (w1 ,-) is quadratic and
```
```
strictly convex on C/2, and Ll(-, •) is quadratic and strictly convex on Ul x f/2,
```
```
and therefore it follows from Prop. 4.7, together with Remark 4.8 (interpreted in
```
```
infinite dimensional spaces), that the linear-quadratic Stackelberg game under
```
consideration admits a unique solution given by
where
This solution, written in operator form, should clearly correspond to the one
```
given by (7.64a)-(7.64g), and this can also be verified directly (see Simaan and
```
```
Cruz (1973a)).
```
Finally, let us note that, since the solutions of the coupled set of differential
```
equations (7.64a)-(7.64d) depend linearly on XQ, and since x*(t) can be recovered
```
87The analysis to follow is in the same spirit as the proof of Thm. 6.14, and requires some
prior knowledge of functional analysis. Our treatment here is rather informal and without
```
details; for a more complete version the reader is referred to Simaan and Cruz (1973a). Fur-
```
thermore, for the terminology and notation used here, the reader is referred to the proof of
Thm. 6.14.
0 and elements
412 T. BA§AR AND G. J. OLSDER
```
from .r*(s) by linear invertible transformation for all t, s > 0, the set (7.64a)-
```
```
(7.64d) can be replaced by a set of matrix differential equations independent
```
```
of XQ, which are obtained by letting \i(t) = AI(£)Z*(£), \2(t) — A2(£)x*(£),
```
```
p(t) = P(t)x*(t), where AI, A2 and P are the corresponding matrices. This set
```
of coupled matrix differential equations is
The following theorem now summarizes the result.
```
Theorem 7.6 The two-person linear-quadratic differential game (cf. Def.
```
```
6.5) characterized by the additional parametric restrictions -R12(-) > 0, Ql > 0,
```
Q\ > 0, i = 1,2, admits a unique open-loop Stackelberg solution with PI acting
as the leader, which is given by
```
where P and AI are uniquely determined from (7.65), and £*(•) satisfies the
```
differential equation
Proof. It follows from the preceding discussion.
```
Remark 7.14 All results of this subsection (with the exception of uniqueness
```
```
of follower response strategy) are valid if the follower is allowed to have access to
```
CLPS information, and the discussion of Remark 7.1 has a natural counterpart
here.
7.6.2 The CLPS information pattern
```
In discrete-time dynamic games (see Sections 7.3, 7.4), the CLPS information
```
```
(for the leader) has led to two types of equilibria-—the global Stackelberg and
```
feedback Stackelberg solutions. In the continuous time, we have direct counter-
parts of these, which we briefly discuss below.
Global Stackelberg solution
For the global Stackelberg solution, a direct approach is again quite unwieldy,
since the optimum response of the follower to an arbitrary CLPS strategy of
```
HIERARCHICAL (STACKELBERG) EQUILIBRIA 413
```
the leader cannot be obtained using standard methods of optimal control—
the major difficulty stemming from the fact that a general control strategy for
the leader will also incorporate memory, and may even be discontinuous. To
circumvent this difficulty two alternatives exist.
One alternative is to make some structural assumption on the permissible
```
strategies of the leader (such as, taking 71 to be a general CLNM policy, or
```
one that is parameterized in terms of its dependence on the current and past
```
values of the state), which then would make it possible to write down a set of
```
```
necessary (and/or sufficient) conditions for the follower's reaction function to
```
satisfy. Then, the leader will have to solve a dynamic optimization problem
over an infinite dimensional strategy space subject to these infinite dimensional
dynamic constraints. A theory for such nonclassical control problems can be
```
found in Papavassilopoulos and Cruz (1979a), which is also applied there to
```
Stackelberg problems. If, on the other hand, the strategy set of the leader is
```
parameterized (say, by a finite dimensional vector), then the leader will have
```
to solve an optimization problem on the corresponding parameter space, which
might be more feasible88 but definitely sub-optimal.
The second alternative approach to the global Stackelberg solution is the
counterpart of the methodology discussed in Section 7.4, which is to find an
equivalent team problem the global solution of which forms a tight lower bound
on the leader's cost, and then to obtain a particular representation of the team
```
solution (for the leader) on the optimum team trajectory that achieves this
```
```
bound. This methodology has been applied in (Ba§arand Olsder, 1980b) to
```
linear-quadratic differential games, and conditions have been obtained for a
```
finite dimensional linear representation (i.e., a linear dynamic compensator) to
```
provide a global Stackelberg solution. Counterparts of these results when the
```
leader has access to sampled state information has been presented in (Ba§ar,
```
```
1981b); here representation of the leader's team strategy will have to meet
```
the additional side condition that it can depend only on the current and past
sampled values of the state. Details of these derivations can be found in the
references cited.
Feedback Stackelberg solution
For the feedback Stackelberg solution, we have to extend the definition from
discrete to continuous time. Let us recall that in discrete time a feedback
Stackelberg solution is one that retains the Stackelberg property at every stage—
with the leader having only stagewise advantage over the follower.
The continuous-time problem can be viewed as the limit of the discrete-time
game as the number of stages becomes unbounded in a finite interval, which
means that two consecutive decision points get arbitrarily close to each other.
Hence, in a continuous-time dynamic game stagewise advantage of the leader
```
(on the follower) turns into instantaneous advantage. Formally, the feedback
```
```
88This has been demonstrated by Medanic (1977) for linear-quadratic games by taking the
```
initial state uniformly distributed on the unit sphere.
414 T. BA§AR AND G. J. OLSDER
Stackelberg solution in a differential game can be obtained as the limit of the
feedback Stackelberg solutions of a sequence of discrete-time dynamic games,
each one obtained by time-discretization of the original differential game, with
```
the k -f- 1st game in the sequence corresponding to a finer discretization (sam-
```
```
pling) than the kth one. One possible construction would be to divide the
```
finite interval [0, T] into k uniform subintervals, and to assume that over the ^th
```
subinterval \jfc(t — 1), ^), where 0 < t < k, the players' policies depend only
```
```
on the values of state prior to and including t = ^(£ — 1), and that the leader
```
can enforce his policy on the follower over each such interval—thus defining the
kth game in the sequence. If necessary, one can take a subsequence of these
games, corresponding to k = 1,2,4,8,..., so that the information set of each
time-discretized game is strictly included in the information set of the next such
game. An appropriate terminology to use for these games is "sampled-state",
as the only time-discretization is in the information set, which comprises the
state vector.
The feedback Stackelberg solution of each state-sampled game can be ob-
tained by again using a dynamic programming-type argument, by solving a
sequence of open-loop Stackelberg games of the type covered in Section 7.2,
```
with the one on the time interval [^, j(£ 4- 1)) having the initial state x ( j t y -
```
```
The Stackelberg costs associated with this open-loop game (which will be ex-
```
```
pressed in terms of x (^), will constitute the terminal state cost part of the
```
cost functions of the respective players in the open-loop game defined on the
```
next subinterval (in retrograde time) [|r(^ — 1), ^)-
```
A complete analysis of the convergence of these solutions as k —> oo is beyond
```
the scope of our coverage here; but, if a limit exits, then the limiting solution
```
should involve solutions of a sequence of open-loop Stackelberg games, each one
defined on an infinitesimally small subinterval, which means that we now have
to obtain Stackelberg solutions based on incremental costs at each time t. If
```
Vl(t,x) denotes the feedback Stackelberg cost-to-go of Pi at time £, at state x,
```
```
then the counterpart of the pair of Nash equilibrium PDEs (6.52) are in this
```
```
case (for the game described by (7.59a)-(7.59b))
```
where "sol" stands for the static Stackelberg solution with PI as the leader.
By analogy with the discrete-time case, we will call any set of policies obtained
```
from (7.66) the continuous-time feedback Stackelberg solution, which is clearly
```
```
strongly time consistent (by definition).
```
```
To bring (7.66) to a more explicit form, let us introduce the instantaneous
```
```
HIERARCHICAL (STACKELBERG) EQUILIBRIA 415
```
reaction function of P2 by
```
Substituting this into the RHS of the first line of (7.66) and minimizing the
```
resulting expression over ul G 51 yields the instantaneous Stackelberg solution
for the leader:
with the corresponding one for the follower being
```
Using these, (7.66) can equivalently be written as
```
which is a pair of coupled PDEs, whose solution yields the feedback Stackelberg
cost-to-go pair. Hence, we have the following.
```
Proposition 7.7 For the differential game described by (7.59a)-(7.59b), and
```
```
under the CLPS information pattern, the pair of policies (7.68a)-(7.68b) con-
```
```
stitutes a feedback Stackelberg solution, where V l ( t ^ x } , satisfying (7.69), is the
```
```
corresponding cost-to-go function for Pi (i = 1, 2).
```
Since the asymmetry in the roles of the players in a continuous-time feedback
Stackelberg solution is only incremental, one may be led to the conclusion that
```
the feedback Stackelberg solution should coincide with (or be very close to)
```
```
the feedback Nash solution (cf. Thm. 6.16). This, however, is not necessarily
```
the case as the feedback Nash solution corresponds to choosing the static sol
```
operator in (7.66) as Nash equilibrium, whereas in the present case it is the
```
Stackelberg equilibrium, and the two are not generally the same. To illustrate
this point, as well as the possibility that they may sometimes be equivalent, we
consider in the sequel a class of linear-quadratic games with a coupling term
between the controls of the players:
416 T. BA§AR AND G. J. OLSDER
```
The PDEs (7.69) then become
```
where
For the feedback Nash equilibrium solution, however, the relevant set of PDEs
```
is in the same form as (7.72a)-(7.72c) but with KI, K2, L\, L2, BI and B2,
```
respectively, replaced by the "hat'ted" quantities
Then, the instantaneous optimum response function of the follower is
```
leading to the following structure for (7.68a)-(7.68b):
```
which imply that the two sets of PDEs become identical, thus admitting the
same set of solutions. If the cross terms are not absent, however, the two sets
of PDEs are intrinsically different and admit different sets of solutions. Hence,
even in linear-quadratic games with generalized quadratic cost functionals, the
feedback Stackelberg and Nash solutions may be different.
We now conclude this subsection by reporting a result on the existence and
structure of the feedback Stackelberg solution of the linear-quadratic differential
```
game described by (7.70a)-(7.70c).
```
```
Proposition 7.8 If T is sufficiently small and the matrix inverse in (7.71a)
```
```
exists, the linear-quadratic differential game described by (7.70a)-(7.70c), and
```
with PI as the leader, admits a feedback Stackelberg solution given by
```
where (Pi(£),P2(t)} are symmetric solutions of the set of coupled matrix Riccati
```
differential equations
```
Proof. This result follows by substituting V1 = (l/2)x'PiX into the pair of
```
```
PDEs (7.72a)-(7.72b) and observing that (7.75a)-(7.75b) imply satisfaction of
```
```
(7.72a)-(7.72b) by such quadratic cost-to-go functions. Existence of a (unique)
```
```
solution to (7.75a)-(7.75b) when T is sufficiently small follows from a standard
```
property of ordinary differential equations with continuous right-hand sides.
Remark 7.15 Proposition 5.1 has a natural counterpart in the context of feed-
back Nash equilibria, simply with Ki, Li and Bi replaced by their corresponding
"hat'ted" versions introduced earlier. The solution to the resulting set of Ric-
```
cati equations will not be the same as the solution to (7.75a)-(7.75b), unless
```
```
RZI = 0, R\2 = 0. For the latter case, these equations are equivalent to the
```
```
two-player version of (6.17a) with R12 = R21 — 0.
```
```
HIERARCHICAL (STACKELBERG) EQUILIBRIA 417
```
```
Note that if the cross terms in the cost functions are absent (i.e., Ri2 — 0,
```
```
-^21 = 0)) we have the simple relations
```
418 T. BA§AR AND G. J. OLSDER
7.7 Problems
1. Show that, for the special case of two-person zero-sum dynamic games (i.e.,
```
with L1 = -I/2), Thm. 7.1 coincides with Thm. 6.3, and Corollary 7.1
```
coincides with Thm. 6.4.
2. Obtain the open-loop Stackelberg solution of the two-stage dynamic game
```
described by (7.23), (7.24a) and (7.24b) when (i) PI is the leader, (ii)
```
P2 is the leader. For what values of the parameter /3 is the open-loop
```
Stackelberg solution concurrent? (See Section 4.5 for terminology.)
```
3. Obtain the feedback Stackelberg solution of the dynamic game of the pre-
```
vious problem under the CLPS information pattern when (i) PI is the
```
```
leader, (ii) P2 is the leader.
```
4. Consider the scalar K-stage dynamic game described by the state equation
and cost functionals
Obtain the global and feedback Stackelberg solutions of this dynamic game
```
under CLPS information pattern when (i) PI is the leader, (ii) P2 is the
```
leader. Compare the realized values of the cost functions of the players
under these different Stackelberg solutions for the cases when K — 5 and
5. Consider the two-stage dynamic game characterized by the two-dimensional
state equation
and cost functionals
```
Here, u1 is the scalar control variable of PI (the leader) who has access
```
to
```
HIERARCHICAL (STACKELBERG) EQUILIBRIA 419
```
```
and u2 is the two-dimensional control vector of P2 (the follower) who
```
has open-loop information. Show that the global minimum of L1 cannot
constitute a tight lower bound on the leader's Stackelberg cost function
and a tight lower bound can be obtained by first minimizing L2 over
u2 G R2 subject to the constraint
and then minimizing L1 over u1 e R and y 6 R, with u2 determined as
```
above. (For more details on this problem and the method of solution, see
```
```
Ba§ar(1980a, 1982).)
```
6. Consider the scalar dynamic game of subsection 7.4.1 with Xi = 1, (3 — 2,
```
and under the additional restriction — 1 < u1 < 0, i = 1,2. Show (by
```
```
graphical display) that the Stackelberg cost of the leader is higher than
```
the global minimum value of his cost function, and a strategy that leads
```
to this cost value (which is
```
7. Prove Prop. 7.4 by carrying out the two steps preceding it in subsection 7.5.1.
8. Consider the scalar stochastic two-stage dynamic game described by the
state equation
and cost functionals
where Q\ and 6% are statistically independent Gaussian variables with mean
zero and equal variance 1. The underlying information structure is CLPS
for both players.
```
Determine the feedback Stackelberg solution and the linear (global) Stack-
```
elberg solution with PI as the leader. Can the leader improve his perfor-
mance in the latter case by employing nonlinear strategies?
9. Obtain the linear (global) Stackelberg solution of the previous problem when
```
the stochastic terms are correlated so that £[6162] = 1 (in other words,
```
there exists a single Gaussian variable 9 with mean zero and variance 1,
```
so that Ol=e2 = 0}.
```
10. Consider the three-player incentive problem where PI is the leader and
```
Pi, i = 2, 3, are the followers. The controls are u1 = (uj, u\] for the leader
```
420 T. BA§AR AND G. J. OLSDER
```
and ul (scalar quantities) for Pi, i = 2, 3. The cost functions are
```
```
Determine an incentive mechanism u\ — jl(u2, u3), i = 1,2, such that the
```
```
two followers face a team problem; i.e., after substitution of the incentive
```
policy into the cost functions of the followers, these cost functions become
identical. Moreover, the solution of this resulting "team" problem of the
two followers must lead to the team solution of the leader, i.e., u\ = u\ =
u2 — u3 = 0. Next determine another incentive mechanism such that the
followers face a zero-sum game and such that the saddle-point solution of
this game leads to the team solution of the leader.
11. Consider the two-person incentive problem with PI the leader and P2 the
follower. The decision variables are scalars and the cost functions are:
where 9 is a zero-mean Gaussian random variable with unit covariance.
The value of 9 is known to P2 only, i.e., the information is nonnested.
Show that any continuous and deterministic incentive mechanism 71 will
```
not lead to Ll — 0 (depict the contours for the possible L2-functions). If,
```
however, the leader applies the following incentive mechanism in mixed
```
strategies:
```
where N is a constant larger than 1, show that it indeed leads to achieve-
ment of his optimum team cost.
12. Extend the result of Thm. 7.5 to generalized linear-quadratic differential
games, where
```
Here c(-} is a continuous vector-valued function of dimension n, and the
```
entries of all matrices are continuous in t.
What conditions on the weighting matrices in the cost functions will ensure
the existence and uniqueness of open-loop Stackelberg solution?
421
13. Obtain the counterpart of Prop. 7.7 for a three-player differential game with
```
CLPS information pattern (with general state dynamics, and general cost
```
```
functions), where PI is the leader, and P2 and P3 are followers who play
```
according to the feedback Nash equilibrium for every announced strat-
egy of the leader. Furthermore, the equilibrium solution concept adopted
between the two levels of the hierarchy is the feedback Stackelberg.
Under what conditions on the state dynamics and/or the cost functions
```
would the equilibrium strategies of the players (obtained under this two-
```
```
level hierarchy) also constitute a Nash equilibrium solution for the three-
```
```
player differential game (without any decision hierarchy)?
```
7.8 Notes
Sections 7.2 and 7.3. The open-loop Stackelberg solution concept in infinite
dynamic games was first treated in the continuous time in the works of Chen and
```
Cruz (1972) and Simaan and Cruz (1973a,b), where the latter has also introduced
```
the feedback Stackelberg concept in the context of finite and discrete-time infinite
dynamic games. Some other references that discuss the open-loop and feedback
```
Stackelberg solutions in discrete-time infinite dynamic games are Kydland (1975),
```
```
Hamalainen (1976), Ba§ar (1979b) and Cruz (1978), where the latter also discusses
```
derivation of feedback Stackelberg solution of continuous-time dynamic games under
sampled-data state information. For applications in microeconomics, see Okuguchi
```
(1976).
```
Section 7.4. Derivation of the global Stackelberg solution in infinite dynamic games
with CLPS information has remained a challenge for a long time, because of the dif-
ficulties explained in this section prior to subsection 7.4.1 and illustrated in subsec-
tion 7.4.1. The indirect approach presented in this section was first introduced in
```
Ba§arand Selbuz (1979a,b) and later in Tolwinski (1981b) and Ba§ar (1980a); the spe-
```
```
cific examples included in subsections 7.4.1 and 7.4.2 are taken from Ba§ar (1980a, 1982).
```
```
This approach was extended later to more general (infinite dimensional Hilbert and
```
```
Banach) spaces in (Zheng and Ba§ar, 1982). The analysis of subsection 7.4.3 fol-
```
```
lows closely the one of Ba§arand Selbuz (1979a) which also includes an extension to
```
```
N(> 2)-person games with one leader and N — I followers, and with the followers
```
playing according to the Nash equilibrium solution concept among themselves. Exten-
```
sions of this approach to other types of N(> 2)-person dynamic games (with different
```
```
types of hierarchy in decision making) can be found in Ba§ar (1981a). Other repre-
```
```
sentative articles devoted to this topic are Ba§ar (1981b), Olsder (1977a), Ho, Luh
```
```
and Olsder (1982) and Tolwinski (1980, 1981a). Subsection 7.4.4 on incentives con-
```
```
tains material from Ho, Luh and Olsder (1982); see this reference, as well as Ho and
```
```
Olsder (1981) and Zheng, Ba§arand Cruz (1984) for results on multi-stage decision
```
problems, the nonnested case, and problems with multiple hierarchies. The method
presented in Example 7.4 as to what the leader can achieve if his team solution is
beyond reach is due to Tolwinski. The fact that the optimum incentive mechanism is
generally nonunique has opened the possibilities for further refinement, by choosing
the one that is least sensitive to changes in the values of some parameters defining the
```
game. Some results along this direction have been presented in (Cansever and Ba§ar,
```
```
1983).
```
```
HIERARCHICAL (STACKELBERG) EQUILIBRIA422 T. BA§AR AND G. J. OLSDER
```
```
Section 7.5. This section follows the lines of Ba§ar(1979b) and extends some
```
of the results given there. The difficulties illustrated by Example 7.5 were first
```
pointed out in Castanon (1976). For a discussion of the derivation of the Stackel-
```
```
berg solution when players have access to noisy (but redundant) state information, see
```
```
Ba§ar (1979a); this is related to the topic of stochastic incentives as discussed in sub-
```
```
section 7.5.3. Furthermore, Ba§ar(1979c) discusses the feedback Stackelberg solution
```
in linear-quadratic stochastic dynamic games with noisy observation. A general exis-
tence theory for stochastic incentive problems, with nested information, can be found
```
in Ba§ar (1984), which was later extended to problems with multiple levels of hierar-
```
```
chy in Ba§ar(1989a). Cansever and Basar (1985a,b) discuss a further refinement of
```
the notion of optimum schemes in stochastic incentive problems based on insensitivity
to system parameters.
Section 7.6. The open-loop Stackelberg solution to two-player differential games
```
was first studied by Chen and Cruz (1972), and subsequently refined by Simaan and
```
```
Cruz (1973a). Wishart and Olsder (1979) have shown (based on necessary conditions)
```
that the open-loop Stackelberg solution of some simple investment problems could be
discontinuous. Details of the indirect approach discussed in subsection 7.6.2 for the
global Stackelberg solution of differential games with CLPS information pattern can
```
be found in Basar and Olsder (1980b) and Papavassilopoulos and Cruz (1980). The
```
material on the continuous-time feedback Stackelberg solution in this subsection is
```
from Basar and Haurie (1984). This reference, besides developing a theory for the
```
feedback Stackelberg solution in differential games, has also introduced and analyzed
a general framework for differential games where the role of leadership, and the nature
of the solution concept to be adopted by the players change as the game evolves, with
the change being prompted by the past policy choices of the players, as well as the
```
outcome of a chance mechanism (modeled as a jump process). An application of the
```
```
continuous-time feedback Stackelberg solution in economics can be found in (Ba§ar,
```
```
Haurie and Ricci, 1985).
```
Chapter 8
Pursuit-Evasion Games
8.1 Introduction
This chapter deals with two-person deterministic zero-sum differential games
with variable terminal time. We have already discussed zero-sum differential
games in Chapter 6, but as a special case of nonzero-sum games and with fixed
terminal time. The class of problems to be treated in this chapter, however, are
of the pursuit-evasion type for which the duration of the game is not fixed. Actu-
```
ally, it was through this type of problems (i.e., through the study of pursuit and
```
```
evasion between two objects moving according to simple kinematic laws) that
```
the theory of differential games was started in the early 1950s. Extensions to
nonzero-sum dynamic games, as treated in Chapters 6 and 7, were subsequently
considered in the late 1960s.
Section 8.2 discusses the necessary and sufficient conditions for existence of
saddle-point equilibrium strategies. Sufficiency conditions are provided by a
natural two-person extension of the Hamilton-Jacobi-Bellman equation, which
is called the "Isaacs equation", after Isaacs—the acknowledged father of pursuit-
evasion games. A geometric derivation of the Isaacs equation is given, which
utilizes the principle of dynamic programming, and by means of which the con-
```
cept of semipermeable surfaces is introduced; such surfaces play an important
```
role in the remaining sections of the chapter. Subsequently, necessary conditions
are derived, which form the two-person extension of the Pontryagin minimum
```
principle (cf. Section 5.5). These conditions are valid not for feedback strategies
```
but for their open-loop representations, and in order to obtain the feedback
strategies these open-loop solutions have to be synthesised. We also briefly dis-
```
cuss upper and lower value functions (which are important if the so-called Isaacs
```
```
condition does not hold), and their relation with viscosity solutions.
```
In Section 8.3, we treat "capturability", which addresses the question of
whether the pursuer can "catch" the evader or not. The answer to this is
```
completely determined by the kinematics, initial conditions and the target set;
```
the cost function does not play any role here. It is in this section that the
423
424 T. BA§AR AND G. J. OLSDER
homicidal chauffeur game and the two-cars model are introduced.
In Section 8.4, we return to the derivation of saddle-point strategies for
quantitative games. What makes the application of necessary and sufficient
conditions intellectually interesting and challenging is the presence of a large
number of singular surfaces. These surfaces are manifolds in the state space
across which the backward solution process of dynamic programming fails be-
cause of discontinuities in the value function or in its derivatives.
The complexity that arises in the derivation of the solution of a differential
game due to the presence of singular surfaces is best illustrated by the game of
the "lady in the lake," which is the main topic of Section 8.5. In Section 8.6, a
problem in maritime collision avoidance is treated: under what circumstances
can one ship avoid collision with another? The approach taken is one of worst-
case analysis, which quite naturally leads to a differential game formulation.
The solution method, however, is also applicable to other collision avoidance
problems which do not necessarily feature noncooperative decision making.
In Section 8.7 we address the problem of role determination in a differential
```
game wherein the roles of the players (viz. pursuing or evading) are not specified
```
at the outset, but rather are determined as functions of the initial conditions.
Role determination is applied to an aeronautical problem, which involves a
dogfight between two airplanes.
8.2 Necessary and Sufficient Conditions for
Saddle-Point Equilibria
The systems considered in this chapter can all be described by
```
where x(t) € 5° C Rn, u^t) e 5* C Rm% i = 1,2. The function / is continuous
```
in t, u1 and u2, and is continuously differentiate in x. The first player, PI, who
chooses u1, is called the pursuer, which we shall abbreviate as P. The second
player is called the evader and we shall refer to him as E instead of P2. The
final time T of the game is defined by
where A is a closed subset, called the target set, in the product space 5° x R+.
The boundary dA. of A is assumed to be an n-dimensional manifold, i.e., a
hypersurface, in the product space R+ x Rn, characterized by a scalar function
```
l(t,x) = 0. This function is assumed to be continuous in t and continuously
```
differentiable in x, unless stated differently.
Since all differential games in this chapter will be of the zero-sum type, we
have a single objective function
PURSUIT-EVASION GAMES 425
```
where g is continuous in t, ul and w2, and continuously differentiable in x;
```
```
q is continuous in T and continuously differentiable in x(T). We define J as
```
the cost function of the game in normal form, which is determined from L in
```
terms of the strategies 7i (t,x(t)) = ul(t), i = 1,2. Only feedback strategies will
```
```
be considered, and we assume 7l(£,x) to be piecewise continuous in t and x.
```
With the permissible strategy spaces denoted by F1 and F2, let us recall that a
```
strategy pair (71*,72*) is in (saddle-point) equilibrium if
```
for all 7* e F*. The reason for considering feedback strategies, and not strategies
```
of a more general class (such as strategies with memory), lies in the continuous-
```
time counterpart of Thm. 6.9: as long as a saddle point in feedback strategies
exists, it is not necessary to consider saddle points with respect to other classes
of strategies. Actually, as it will be shown in the sections to follow, "solutions"
to various problems are, in general, first obtained in open-loop strategies, which
may then be synthesized to feedback strategies, provided that they both exist.
8.2.1 The Isaacs equation
```
In Corollary 6.6 a sufficiency condition (in the form of the solution of a partial
```
```
differential equation) was obtained for the feedback saddle-point equilibrium of
```
zero-sum differential games with a fixed final time. It will now be shown that
this result essentially holds true also if the final time, instead of being fixed, is
```
determined by the terminal constraint (8.2). The underlying idea here, too, is
```
```
the principle of optimality; wherever the system is at any given arbitrary time,
```
```
from then onwards the pursuer (respectively, evader), minimizes (respectively,
```
```
maximizes) the remaining portion of the cost function under the feedback in-
```
```
formation structure. The function describing the minimax (upper) value of the
```
```
cost function, when started from the position (£,x), is
```
which is called the value function. Under the assumption that such a func-
tion exists and is continuously differentiable in x and t, it satisfies the partial
differential equation
```
which is known as the Isaacs equation. The lower (maximin) value function is
```
```
defined analogously, with (8.5) and (8.6) replaced, respectively, by
```
426 T. BA§AR AND G. J. OLSDER
If the differential game has equal upper and lower values, then we denote this
```
common value by V, which satisfies (8.(3) or equivalently (8.8), with the ordering
```
of the min and max operations being irrelevant here.89 For future reference, let
us rewrite this PDE explicitly:
```
We now give a geometrical derivation of (8.9) in the case of a variable ter-
```
```
minal time (as determined by (8.2)). It is also possible to extend the derivation
```
given in Thm. 6.16 by allowing the terminal time to be variable, but we prefer
the geometric derivation here, since it provides more insight into the problem,
in particular if there is only a terminal pay-off. Accordingly, we assume now
that # = 0, which in fact leads to no loss of generality because of the argument
```
of Remark 5.1 (suitably modified to apply to continuous-time systems).
```
Figure 8.1: Geometrical derivation of the Isaacs equation, where c\ > c> c?..
The value function V, if it exists, is then defined by
```
when the initial position is (t,x). One particular saddle-point trajectory is
```
```
sketched in Fig. 8.1; it is indicated by curve ra and the value corresponding to
```
any point of m is c. We now make the assumption that for initial points in
```
a certain neighborhood below m, V(t,x) < c, and similarly for initial points
```
89If not explicitly stated otherwise, it will be assumed henceforth that the upper and lower
```
values are equal, that is, the max and min operations in (8.5) (or (8.7)) commute. Only in the
```
next subsection, where we deal with viscosity solutions, will these operations not necessarily
commute.
PURSUIT-EVASION GAMES 427
```
above m, V(t,x) > c.90 At point A, on curve ra, P will choose u1 E 5"1 so
```
```
as to make the inner product between (dV/dx,dV/dt}f and (/', 1)' as small as
```
```
possible, i.e., he tries to make the vector (/', 1)' point downward towards points
```
corresponding to lowest possible costs. On the other hand, E wants to choose
u2 6 S2 in such a way so as to maximize this inner product, because then the
system will move in E's favorite direction, i.e., towards points corresponding to
the highest possible costs. Hence u1* and u2* are chosen as the arguments of
Since m is an equilibrium trajectory, the system will move along m if u1* and
u2* are applied, and that results in
```
which is equivalent to (8.6) if g = 0.
```
If E plays optimally, i.e., he chooses u2*, then
for all u1 6 S1, which implies that the outcome of the game will be > c, since
the system will move into the area where V > c. However, P can keep the
```
outcome at c by playing u1*, since then the equality sign will apply in (8.13).
```
```
Thus, without E's cooperation (i.e., without E playing non-optimally), P cannot
```
make the system move from A towards the area where V < c. Analogously, by
interchanging the roles of the players in this discussion, E cannot make the
system move from A in the "upward" direction if P plays optimally. Only
if P plays non-optimally, E will be able to obtain an outcome higher than c.
Because of these features, the n-dimensional manifold in Rn x R+, comprising
```
all initial positions (t, x) with the same value, is called semipermeable, provided
```
that g = 0.
```
In conclusion, if the value function V(t,x] is continuously differentiate,
```
```
then the saddle-point strategies are determined from (8.9). This equation also
```
provides a sufficiency condition for saddle-point strategies as stated in the fol-
lowing theorem, the proof of which is a straightforward two-player extension of
```
Thm. 5.3; compare it also with Corollary 6.6.
```
```
Theorem 8.1 // (i) a continuously differentiable function V(t,x) exists that
```
```
satisfies the Isaacs equation (8.9), (ii) V(T,x) = q(T,x) on the boundary of
```
```
the target set, defined by l(t,x) = 0, and (Hi) either u1*^} = ~fl*(t,x], or
```
```
u2*(t) = ~/2*(t,x), as derived from (8.9), generates trajectories that terminate
```
```
infinite time (whatever •j2, respectively jl, is), then V(t,x} is the value function
```
```
and the pair (71*,72*) constitutes a saddle point.
```
```
90If this assumption does not hold (which is the case of V being constant in an (n + 1)-
```
```
dimensional subset of Rn x R+) the derivation to follow can easily be adjusted.
```
428 T. BA§AR AND G. J. OLSDER
Remark 8.1 The underlying assumption of interchangeability of the min and
max operations in the Isaacs equation is often referred to as the Isaacs condition.
The slightly more general condition
for all n-vectors p, is sometimes also referred to as the Isaacs condition. Re-
gardless of which definition is adopted, the Isaacs condition will hold if both /
and g are separable in u1 and u2, i.e., they can be written as
If the Isaacs condition does not hold, and if only feedback strategies are allowed,
then one has to seek for equilibria in the general class of mixed strategies defined
on feedback strategy spaces. In this chapter, we will not extend our investigation
to mixed strategies, and deal only with the class of differential games for which
```
the Isaacs condition holds (almost) everywhere.
```
Note, however, that, even if separability holds, this does not necessarily mean
that the order of the actions of the players is irrelevant, since the underlying
assumption of V being continuously differentiate may not always be satisfied.
In regions where V is not continuously differentiate, the order in which the
players act may be crucial.
```
In the derivation of saddle-point strategies, equation (8.9) cannot directly
```
be used, since V is not known at the outset. An alternative is to use Thm. 8.2,
given below, which provides a set of necessary conditions for an open-loop rep-
```
resentation (or realization) of the feedback saddle-point solution. In spite of the
```
fact that it does not deal with feedback solutions directly, the theorem is ex-
```
tremely useful in the computation (synthesis) of feedback saddle-point solutions.
```
It can be viewed as the variable terminal-time extension of Thm. 6.13 and the
two-player extension of Thm. 5.4. The proof follows by direct application of
Thm. 5.4, which is also valid under the weaker assumption of / and g being
```
measurable in t (instead of being continuous in t) (see Berkovitz (1974), p. 52).
```
```
Theorem 8.2 Given a two-person zero-sum differential game, described by (8.1)-
```
```
(8.4), suppose that the pair J71*,!2*} provides a saddle-point solution in feed-
```
```
back strategies, with x* (t) denoting the corresponding state trajectory. Further-
```
```
more, let its open-loop representation {ul(t) = jl(t,x*(t)),i = 1,2} also provide
```
```
a saddle-point solution (in open-loop policies). Then there exists a costate June-
```
PURSUIT-EVASION GAMES 429
```
tion p(-) : [0,T] —> Rn such that the following relations are satisfied:
```
where
Remark 8.2 Provided that both open-loop and feedback equilibria exist, con-
ditions of Thm. 8.2 will lead to the desired feedback saddle-point solution. Even
if an open-loop saddle-point solution does not exist, this method can still be
utilized to obtain the feedback saddle-point solution, and, by and large, this
has been the method of approach adopted in the literature for solving pursuit-
evasion games. For an example of a differential game in which this method is
```
utilized, but an open-loop (pure) saddle point does not exist, see the "lady in
```
the lake" game treated in Section 8.5.
```
Also, for certain classes of problems, the value function V(t, x) is not contin-
```
uously differentiable, and therefore the sufficiency conditions of Thm. 8.1 cannot
```
be employed. Sufficiency conditions for certain types of nonsmooth V(£, x] have
```
```
been discussed in Bernhard (1977), but they are beyond the scope of our treat-
```
ment here. D
In the literature on pursuit-evasion differential games, it is common practice
```
to write Vx instead of p' (for the costate vector), and this will also be adopted
```
here. The reader should note, however, that in this context, Vx is only a func-
```
tion of time (but not of x ) , just the way p' is. After synthesis, the partial
```
derivative with respect to x, of the value function corresponding to the feed-
back equilibrium solution, coincides with this function of time, thus justifying
the notation.
We now illustrate the procedure outlined above by means of a simple ex-
ample. As with most other problems to be considered in this chapter, this
example is time-invariant, and consequently the value function and the saddle-
```
point strategies do not explicitly depend on t (cf. Remark 5.5). Therefore the
```
t-dependence has been dropped from the notation.
Example 8.1 Consider the two-dimensional system described by
where the initial value of the state lies in the half plane X2 > 0, it2 satisfies
the constraint u2 < I and no restrictions are imposed on it1. The target set is
430 T. BA§AR AND G. J. OLSDER
```
X2 < 0 and the objective functional is L — xi(T). Note that the game terminates
```
when the rci-axis is reached, and since ±2 < — 1, it will always terminate. In
order to apply Thm. 8.2, we first write the Hamiltonian H for this pursuit-
evasion game as
```
Since the value function does not explicitly depend on time, we have V(xi, x% =
```
```
0) = xi, and hence VXl = 1 along the xi-axis. The final condition for VX2 along
```
```
the ari-axis cannot be obtained directly; it will instead be obtained through the
```
relation minui maxu2 H = 0 at t = T. Carrying out these min-max operations,
```
we get u2 = sgn (VXl) = 1 and hence J2*(x) = 1, and the vector (sinu1,cosw1)
```
```
is parallel to (VXl, VX2). Substitution of these equilibrium strategies into H leads
```
to
```
which yields VX2 = I at t — T. From (ii) it now follows that VXl = I and
```
```
VX2 = 1 for all t. In this example it is even possible to obtain V explicitly:
```
```
V(xi,X2) = xi 4-£2- Hence, the feedback saddle-point strategies are constants:
```
```
71*(o:) = 7T/4, 72*(z) = 1. The corresponding state trajectories are straight
```
```
lines, making an angle of vr/4 radians with the negative X2-axis (see Fig. 8.2).
```
Figure 8.2: Vectogram for Example 8.1.
Figure 8.2 also helps to visualize the geometric derivation of the saddle-point
strategies. Suppose that the system is at point A corresponding to the value 6,
```
say. Emanating from point A, the velocity vector (x\,X2}' has been drawn in
```
```
terms of the vectors (0, -2)', ((2 + v/2)72*,0)/ and (-sh^1*, -COS71*)', which
```
```
add up to the RHS of (i). E, the maximizer, would like to make the angle
```
```
a; (see Fig. 8.2) as large as possible. The best he can do, irrespective of P's
```
decision, is to choose j2* = +1. If E would play j2 ^ I , i.e., 72 < 1, the angle a
will become smaller, and consequently the outcome will become smaller than 6.
```
satisfy, according to (8.14),where
```
PURSUIT-EVASION GAMES 431
```
Similarly, P, the minimizer, would like to have a vector (xi, ±2)' with angle a as
```
```
small as possible, which leads to his optimal strategy 71* = Tr/4; for any other
```
```
71, the vector (xi,^)' will point more to the right, and this is exactly what P
```
wants to avoid.
All conditions of Thm. 8.1 are satisfied and therefore the strategies obtained
indeed constitute a saddle-point solution in feedback strategies.
In addition to the necessary conditions of Thm. 8.2, we now exemplify some
of the intricacies which might occur with respect to termination.
Example 8.2 Consider the one-dimensional system
where the controls w1 and u2 are constrained to — 1 < ul < 0, i = 1,2. The
target set is the half line x < 0. The cost functional is
The costate equation for this problem is
```
and the optimal {ul*,u2*} satisfy
```
```
How do we determine p(T}l The appropriate condition in (8.14) cannot be
```
```
used since that condition, which gives p(T) along the boundary of the target
```
```
set, requires variations in x within £(T, x) — 0; but this latter relation only yields
```
```
x = 0. In its stead, we make use of the definition of the derivative for p(T] and
```
```
also make use of the functional form F(Ax) — Ax — ^(Ax)2 for sufficiently
```
```
small (positive) Ax (which is obtained through some analysis) to arrive at the
```
```
value p(T) = 1. (See Problem 1 in Section 8.8 for another derivation.)
```
```
Integration of the costate equation and substitution into (i) leads to
```
```
For x = 1 we have p = 0 and (i) does not determine the equilibrium strategies.
```
```
For 7** = 0 (i = 1, 2), the system would remain at x = 1 and the game would
```
not terminate in finite time. To exclude the occurrence of such singular cases,
we have to include an additional restriction in the problem statement, which
```
forces P to terminate the game in finite time; this can be achieved by defining
```
```
L = oo at T = oo. Then, at state x = 1, the preceding restriction forces P
```
```
to choose 7l!"(x) = — 1. Now we can integrate further in retrograde time. For
```
```
Now the question is whether the strategy pair {71*,!2*}, defined by (ii) and
```
```
(iii), constitutes a saddle point. To this end, Thm. 8.1 will be consulted. The
```
```
value function is easily determined to be V(x) = x(2 — x)/2, and all conditions
```
```
of Thm. 8.1 hold, except condition (iii). The reason why condition (iii) does not
```
```
hold is because, for initial points x > 1, the strategy pair (71*,72*(-) = 0} does
```
not lead to termination in finite time, and hence, by playing 72 = 0, E would
be much better off. However, in such a situation, P can do much better if he is
allowed to have an informational advantage in the form
```
i.e., at each instant of time, P is informed of E's current decision (open-loop
```
```
value).91 If P chooses his new strategy as
```
the saddle-point equilibrium will be restored again by means of the pair
which follows from direct comparison.
Another way of getting out of the "termination dilemma" is to restrict the
class of permissible strategies to "playable pairs", i.e., to pairs for which termi-
```
nation occurs in finite time (cf. Example 5.1 and Def. 5.7). Such an assumption
```
```
excludes for instance the pair {71 = 0,72 = 0} from consideration.
```
8.2.2 Upper and lower values, and viscosity solutions
```
In the previous subsection it was tacitly assumed (with the exception of (8.6)
```
```
and (8.8)) that the Isaacs condition holds. In situations where it does not, one
```
common approach is to endow one player with an instantaneous informational
advantage over the other player, which we now briefly discuss.92 An assumption
```
throughout this subsection (only) will be that the final time T is fixed. With
```
Tl taken as the set of all open-loop controls for Pi, let us introduce a mapping
V? from T1 to T2 with the property that for each * 6 [0,T] and ul,ul e T1
```
the following holds: if ul(r] = ul(r] for almost all T € [0,t], then ^2[u1](r) =
```
```
p,2[ul}(r) almost everywhere. In more popular terms this says that u2(t) will
```
depend on the past and current values of u1, but not on its future values. It
should be emphasized that the current value of u1 is part of the information for
P2. The set of all such strategies ^2 for P2 will be denoted by A2. Similarly, /z1
changes sign, and therefore, for x > 1, we obtain
```
from (i):
```
432 T. BA§AR AND G. J. OLSDER
91 This is, of course, possible if E has access to only open-loop information, and can therefore
```
only pick open-loop policies. If E has access to feedback information, however, (iv) is not
```
physically realizable and a time delay has to be incorporated.92
```
At the outset, this is different from the caise of minimax ((8.5)) or maximin ((8.7)) values
```
where one of the players is assumed to know not only the current but also the future values
of the other player's controls.
PURSUIT-EVASION GAMES 433
and A1 are defined with the roles of the players reversed. Formally, a mapping
/z1 from F2 to F1 will be called a strategy for PI provided that for each t G [0, T]
```
and u2,u2 € F2 the following holds: If u2(r) — u2(r) for almost all T € [0,t],
```
```
then /u1[u2](r) = //1[u2](r) almost everywhere.
```
Definition 8.1 The value V+ of the zero-sum differential game described by
```
(8.1) and (8.3), if the maximizer has instantaneous informational advantage
```
over the minimizer, is defined as
and the value V~ of the same game, but now with the minimizer having instan-
taneous informational advantage over the maximizer, is defined as
Remark 8.3 Clearly,
Remark 8.4 In the study of the existence of strategies that achieve the values
V+ and V~, the starting point is to partition the time interval of interest, [0, T],
into n subintervals each of length 6 = T/n. The informational advantage of one
player with respect to the other then concerns the entire subinterval to which
the current time belongs. Thus one can define the so-called upper <5-value and
lower <5-value. Under some regularity conditions, it can be shown that in the
limit as n —* oo and hence 6 —> 0, these values become equal to V+ and V~,
respectively. D
Definition 8.2 The upper Hamiltonian and the lower Hamiltonian are, respec-
tively,
and
Remark 8.5 To be very explicit, the maximizing w2-value in the definition of
```
H+ will in general depend on the minimizing u1-value, i.e., u2(u1), whereas in
```
the definition of H~ it will be exactly the other way around. Note also that
H+ > H~ for all values of their arguments.
The following theorem now makes the connection between Defs. 8.1 and
```
8.2, and also relates them to (8.5) and (8.7). The precise regularity conditions
```
under which it is valid, as well as its proof can be found in Barren, Evans and
```
Jensen (1984).
```
434 T. BA§AR AND G. J. OLSDER
Theorem 8.3 Subject to appropriate regularity conditions on the functions f , g
and q, the functions V+ and V~ are uniformly Lipschitz continuous, and hence
differentiable almost everywhere, and they are solutions, respectively, of
and hence are equal to V and V_, respectively.
One can also define viscosity solutions for zero-sum differential games. In
Chapter 5 the starting point to define viscosity solutions for optimal control
```
problems was (5.65) (or (5.66)). In zero-sum differential games the equivalent
```
```
starting point is (8.6) and (8.8), or their time-invariant equivalents if V does not
```
```
directly depend on t. Since (5.65) and (8.6) are similar PDEs, the theory of vis-
```
cosity solutions as presented in Section 5.8 remains equally valid for differential
games, simply with H replaced by H+ or H~. The following theorem, proved
```
in Barren, Evans and Jensen (1984), and in Evans and Souganidis (1984) for
```
problems with fixed terminal time, relates upper and lower value functions to
viscosity solutions.
```
Theorem 8.4 The upper value function V+ is the viscosity solution of (8.6)
```
```
and the lower value function V~ is the viscosity solution of (8.8).
```
8.3 Capturability
Consider the class of pursuit evasion games formulated in Section 8.2, under the
additional assumption of time-invariance, i.e., /, g and t are independent of t,
and q does not depend on T. The target set A is therefore a tube in Rn x R+,
with rays parallel to the time axis. The projection of A onto the Rn-space is
denoted by A.
Before studying the derivation of a saddle-point solution, we must first ad-
dress the more fundamental question of whether the target set can be reached
at all. If it cannot be reached, we simply say that the pursuit-evasion game
is not well defined. Accordingly, we deal, in this section, with the following
qualitative pursuit-evasion game. The evader tries to prevent the state from
reaching A, whereas the pursuer seeks the opposite. Once it is known, for
```
sure, that the target set can be reached, one can return to the original (quan-
```
```
titative) pursuit-evasion game and investigate existence and derivation of the
```
saddle-point solution.
For the differential game of kind introduced above, we define an auxiliary
cost functional as
PURSUIT-EVASION GAMES 435
Figure 8.3: The construction of the barrier.
thus converting it into a differential game of degree. The value function for this
```
differential game can take only two values, V(x) = —I and V(x) = +1, where
```
the former corresponds to initial states that lead to capture, while the latter cor-
responds to initial states from where capture cannot be assured. Consequently,
the value function is discontinuous, and therefore the theory of subsection 8.2.1
is not directly applicable. It can, however, be modified suitably to fit the present
framework, which we now discuss below.
Let us first note that only those points x of <9A for which
are candidates for a terminal position of the game. Here, the n-dimensional
```
vector v is the outward normal of A at or (see Fig. 8.3).
```
```
If strict inequality holds in (8.15), then the state will penetrate A; further-
```
more, the points x for which the equality sign holds may only be grazing or
```
touching points. The set of all points x of <9A that satisfy (8.15) is called the
```
```
usable part (UP) of dA. Let us suppose that the initial points situated in the
```
shaded region of Fig. 8.3, and only those, can be steered to the target set by P,
regardless of how E acts, and investigate whether we can determine the surface
S that separates these initial states from the others which do not necessarily
lead to termination. This surface S is sometimes referred to as a barrier. The
points that are common to S and A are called the boundary of the usable part
```
(BUP), it consists of those points of the UP for which the equality sign holds
```
```
in (8.15).
```
Consider a point A on S, at which a tangent hyperplane relative to S exists.
The outward normal at A, indicated by p, is unique, apart from its magnitude.
The surface S is then determined by
which is only a necessary condition and therefore does not characterize S com-
pletely. This semipermeable surface should in fact have the property that with-
```
out P's cooperation, E cannot make the state cross S (from the shaded area
```
436 T. BA§AR AND G. J. OLSDER
```
V — — 1 to the non-shaded area V = 4-1) and conversely, without E's coopera-
```
```
tion, P cannot make the system cross S (from V — +1 to V = — 1). Hence if a
```
```
"tangential" penetration of S is impossible, (8.16) is also sufficient.
```
```
Points of S at which no tangent hyperplane exists (such as BI and 62 in
```
```
Fig. 8.3) must be considered separately. Such points will be considered later, in
```
Sections 8.6 and 8.7, while discussing intersections of several smooth semiper-
meable surfaces.
```
To construct S, we now substitute the saddle-point solution of (8.16), to be
```
```
denoted by ul* — jl*(x), i — 1,2, into (8.16) to obtain the identity
```
```
Differentiation with respect to x leads to (also in view of the relation (d/du1)(p'f)-
```
```
(d^/dx) = 0, which follows from the discussion included in subsection 5.5.3
```
```
right after (5.42))
```
```
which is evaluated along S. The boundary condition for (8.17) is
```
where v is the outward normal of A at S.
We now introduce various features of the barrier S by means of the following
example, which is known as the "homicidal chauffeur game".
Example 8.3 Consider a pursuer and an evader, both moving in a two-dimensional
```
plane (with coordinates 0:1,0:2) according to
```
where the subscripts p and e stand for pursuer and evader, respectively. The
controls u1 and u2 satisfy the constraints | w * | < l , i = l,2. The scalars v\ and v^
are the constant speeds, and u\ and ^2 are the maximum angular velocities of P
and E, respectively. Since the angular velocities are bounded, three coordinates
```
(xi, x-2 and 9} are needed for each player to describe his position. For u1 = 0
```
```
(respectively, u2 = 0), P (respectively, E) will move in a straight line in the
```
```
(EI, £2) plane. The action u — +1 stands for the sharpest possible turn to the
```
```
right; u = — 1 analogously stands for a left turn. The minimum turn radius for
```
P is given by R\ = MI/VI and for E by R^ — ^2/^2-
For this differential game, yet to be formulated, it is the relative positions
```
of P and E that are important (and relevant), and not the absolute positions
```
```
by means of which the model (8.19) has been described. Model (8.19) is six-
```
```
dimensional; however, by means of the relative coordinates, defined as
```
PURSUIT-EVASION GAMES 437
it can be converted into a three-dimensional model, as to be shown in the sequel.
```
In this relative coordinate system, (#1, £2) denotes E's position with respect to
```
P and the origin coincides with P's position. The rr2-axis is aligned with P's
```
velocity vector, the xi-axis is perpendicular to the X2-axis (see Fig. 8.4), and
```
```
the angle 9 is the relative direction (heading) in which the players move.
```
Figure 8.4: The relative coordinate system.
```
With respect to the relative coordinates (8.20), model (8.19) can be described
```
as
which is sometimes referred to as the two-cars model. We shall consider a special
```
version of (8.21), namely the one in which E is allowed to change his direction
```
```
instantaneously (i.e., u)^ = oo). Then E will have complete control over 9, which
```
we therefore take as E's new control u2. Also, normalizing both P's speed and
minimum turn radius to one, we obtain
with no bounds imposed on u2, while ul still satisfies the constraint \ul\ < 1.
The two normalizations introduced above determine the distance scale and time
```
scale, respectively. Figure 8.4 depicts model (8.22); note that the angle u2 is
```
measured clockwise with respect to the positive X2-axis. This convention of
```
measuring u2 has been initiated by Isaacs (1975), and since then it has been
```
adopted in the literature on pursuit-evasion games. Note that the coordinate
system is affixed to P and moves along with P through the "real" space.
The target set A is defined through the inequality
```
that is, P tries to get within a distance (3 of the evader, whereas the evader
```
tries to avoid this. The game thus formulated is referred to as the homicidal
```
chauffeur game, in which a chauffeur (P) tries to overrun a pedestrian (E). The
```
438 T. BA§AR AND G. J. OLSDER
homicidal chauffeur game is trivial for v? > I — v\, since capture will then never
be possible, provided that E plays the appropriate strategy. Therefore we shall
henceforth assume 0 < v% < 1.
```
The UP (part of the circle described by x^+x2 = 02) is determined by those
```
```
(£1,3:2) for which the following relation holds (note that at the point (£1,0:2),
```
```
the vector (^1,^2)' = (^1,^2)' is an outward normal to A):
```
Hence only those #2 for which x^ > v^fi on the circle x\ + x\ = /32 constitute
```
the UP. Intuitively this is clear: directly in front of P, E cannot escape; if E is
```
```
close to P's side, he can side-step and avoid capture, at least momentarily; if E
```
is behind P, i.e., x-2 < 0, there is no possibility for immediate capture.
```
In order to construct the surface S, we shall start from the end point (x\ =
```
```
/3\/(l — v2}, x-2. = v2/5), on the capture set. Another branch of S can be obtained
```
```
if we start from the other end of the UP, which is (x\ = —/3^/(l — v?>), x2 = vz/3),
```
```
but that one is the mirror image of the first one; i.e., if the first branch is given by
```
```
(xi(t),X2(t)), then the second one will be given by ( — x i ( t ) , X 2 ( t ) ) . Therefore
```
```
we shall carry out the analysis only for the "first" branch. Equation (8.16)
```
determines S and reads in this case
whereby
```
The differential equations for p\ and P2 are, from (8.17),
```
```
The final condition for p, as expressed by (8.18), will be normalized to unit
```
magnitude, since only the direction of the vector p plays a role, i.e.,
```
for some appropriate angle a. In order to solve for p ( t ) , we need to know u1*.
```
```
At t --= T, s = 0 and therefore ul(T] cannot be determined directly. For an
```
indirect derivation we first perform
PURSUIT-EVASION GAMES 439
```
Since p\ > 0 at t — T, we have ds/dt < 0 at t = T, which leads to s(t) > 0
```
for t sufficiently close to T, and therefore u1 = 4-1 just before termination. The
```
solution to (8.24), together with (8.25) and ul = 4-1, now becomes
```
```
For parameter values ({3, v^} for which /32 + v\ > 1, the semipermeable surface
```
S moves inside A and thereby loses its significance.93 In the latter case the
```
proposed construction (which was to split up the state space into two parts,
```
```
V = 4-1 and V = — 1) obviously fails. In the present example we henceforth
```
```
assume that (8.29) holds.
```
```
Let us now explore the trajectories described by (8.28). The following two
```
```
possibilities may be distinguished: (i) xi(t-2) = 0 for some t^ < T and (ii)
```
```
xi(t) > 0 for all t < T, i.e., as long as s(t) > 0. These two cases have been
```
sketched in Fig. 8.5.
```
From (8.28) it follows that the two branches of S intersect (if they do, the
```
```
point of intersection will be on the positive X2-axis) if
```
```
93In fact, it is still a semipermeable surface, but for a different ("dual") game in which E is
```
originally within the set x\ + x| < /32 and tries to remain there, whereas P tries to make the
state cross the circle x\ + x\ — 02.
and the equations determining the branch of S emanating from xi
```
which are valid as long as s(t] > 0. The solution is
```
which leaves the circle x\ + x\ = /32 tangentially. What we have to ensure is
```
that the trajectory (8.28) lies outside A. This is what we tacitly assumed in
```
Fig. 8.3, but which unfortunately does not always hold true as we shall shortly
```
see. If we define r = \/(x\ + x%}, we require f > 0 at t = T (note that f = 0 at
```
```
t = T), and some analysis then leads to the conclusion that this holds if, and
```
only if,
whose verification is left as an exercise to the reader.
```
Case 1 (Fig. 8.5a)
```
For the situation depicted in Fig. 8.5a, let us first assume that E is initially
somewhere within the shaded area. Then he cannot escape from the area, except
440 T. BA§AR AND G. J. OLSDER
Figure 8.5: Possible configurations for the barriers.
```
for moving through the UP (= capture), provided that P plays optimally, which
```
is that P will never let E pass through the two semipermeable surfaces indicated
by Si and 82- If E is initially outside the shaded area, however, he will never
```
be captured (assuming that he plays optimally when in the neighborhood of Si
```
```
or S2).
```
Figure 8.6: A leaking corner.
If E is initially in the shaded area, the intersection point C requires closer
```
attention. Toward this end let us assume that the state is initially on Si; then P
```
```
should try to force it to move in the direction towards the UP (which fortunately
```
```
can be done in this example), and not towards C and pass C eventually. This
```
latter possibility has been depicted in Fig. 8.6, where P tries to keep E in the
shaded region. Suppose that, in Fig. 8.6, the state is initially at point Q. In
order for P to keep the state within or on the boundary of the shaded area,
and for E to try to escape from it, both players should employ their strategies
```
according to (8.16). If these strategies cause the state to move in the direction
```
of C, and eventually pass C, then P cannot keep E within the shaded area,
though he can prevent E from passing either Si or 82- The intersection point
of the two semipermeable surfaces "leaks" and it is therefore called a leaking
corner. As already noted, point C in Fig. 8.5a does not leak.
The next question is: once E is inside the shaded area, can P terminate the
PURSUIT-EVASION GAMES 441
game in finite time? In other words, would it be possible for E to maneuver
within the shaded area in such a way so that he can never be forced to pass
the UP? Even though such a stalemate situation may in general be possible,
in the present example E can always be captured in finite time, which, for the
```
time being, is left to the reader to verify; Example 8.5 in Section 8.4 will in fact
```
provide the solution.
```
Case 2 (Fig. 8.5b)
```
We now turn our attention to the situation depicted in Fig. 8.5b in which case
Figure 8.7: The failure of continuation of the semipermeable surface.
In the figure, the semipermeable surfaces have been drawn up to t\ — T — TT —
la. which corresponds to the first switching time of u1 in retrograde time from
```
t = T. (From (8.26) and (8.27) it follows that s(f) = - sin(t - T + a) + sin a.)
```
For t < ti we have to continue with u1* = —1. Carrying out the corresponding
analysis, it follows that the resulting continuation of Si, designated by Si, would
be as sketched in Fig. 8.7.
The trajectory leaves point C in exactly the opposite direction as it arrived,
```
and the semipermeable surface has a sharp corner (which is also known as a
```
```
cusp). For the semipermeable surface Si, P can keep the state on the shaded
```
```
side of Si, and the same is true for Si (the reader should note the directions of
```
```
p(t) in Fig. 8.7). Clearly, Si and Si together do not constitute a semipermeable
```
surface. Therefore, Si will be disregarded altogether, and we shall consider only
the semipermeable surface of Fig. 8.5b.
It is now claimed that termination is possible from any initial point. If the
```
state is originally at I, say, (see Fig. 8.5b), E can prevent the state from passing
```
```
Si; P, on the other hand, can force the system to move, along the dotted line,
```
and therefore termination will occur. We have to ensure that no little "island"
```
exists anywhere in the (£1,2:2) plane, surrounded by semipermeable surfaces,
```
and not connected to A, and wherein E is safe. Such a possibility is easily
ruled out in this example, on account of the initial hypothesis that P's velocity
442 T. BA§AR AND G. J. OLSDER
is greater than E's, since then P can always force the state to leave such an
island, and therefore such an island cannot exist to begin with.
In conclusion, the foregoing example has displayed various features of the
barriers which separate states from where capture is possible from those from
which it is not possible. If in particular two semipermeable surfaces intersect,
then we have to ensure that the composite surface is also semipermeable, i.e.,
there should not be a leaking corner. Furthermore, in the construction discussed
above, within the context of Example 8.3, we have treated only the case where
the barrier does not initially move inside the target set.
8.4 Singular Surfaces
An assumption almost always made at the outset of every pursuit-evasion game
is that the state space can be split up into a number of mutually disjoint regions,
the value function being continuously differentiable in each of them. The behav-
```
ior and methods of construction of V(t, x) are well understood in such regions.
```
The boundaries of these regions are called singular surfaces, or singular lines if
they involve one-dimensional manifolds, and the value function is not continu-
ously differentiable across them. In this book we in fact adopt a more general
```
definition of a singular surface. A singular surface is a manifold on which (i) the
```
equilibrium strategies are not uniquely determined by the necessary conditions
```
(8.14), or (ii) the value function is not continuously differentiable, or (iii) the
```
value function is discontinuous.
In general, singular surfaces cannot be obtained by routinely integrating the
state and costate equations backward in time. A special singular surface that
manifests itself by backward integration is the switching surface, or equivalently,
a transition surface. In Example 5.2, we have already encountered a switching
line within the context of an optimal control problem, and its appearance in
pursuit-evasion games is quite analogous. The procedure for locating a switching
or transition line is fairly straightforward, since it readily follows from the state
and costate equations.
A singular surface which does not manifest itself by backward integration of
state and costate equations is called a dispersal surface. The following example
now serves to demonstrate what such a surface is and how it can be detected.
Example 8.4 The system is the same as that of Example 8.1:
with \u2\ < I and with no bounds on u1. The initial state lies in the positive x^
```
half plane and the game terminates when x^ (t} = 0. Instead of taking L — %i(T)
```
```
as in Example 8.1, we now take L = x\(T}. In order to obtain the solution, weFigure 8.8: Intersection of candidate optimal trajectories.
```
PURSUIT-EVASION GAMES 443
initially proceed in the standard manner. The Hamiltonian is
where VXl = 0, VX2 = 0. Since V is the value function, we obviously have
VXl — 2xi along the xi-axis. Then, as candidate equilibrium strategies, we
obtain
```
where || stands for "parallel to". Note that the ul and w2 in (ii) are candidates for
```
```
open-loop strategies and therefore the expression w2(t) = sgn (xi(T)) does not
```
```
contradict with causality. Substituting (ii) and VXl = 1x\ into (i) with t = T and
```
```
setting the resulting expression equal to zero, we obtain VX2(x(T}) = 2|xi(T)|.
```
The costate variables VXl and VX2 are completely determined now.
```
For terminal state conditions of (xi > 0,X2 = 0) we find, as candidate
```
equilibrium strategies,
```
and in the case of terminal state conditions (xi < 0, X2 = 0) their counterparts
```
are
The corresponding state trajectories are sketched in Fig. 8.8, wherefrom we ob-
serve that they intersect. Starting from an initial position /i, there are two
possibilities, but only one of them—the trajectory going in the "south-east"
direction—is optimal. Starting from /2, the trajectory in the "south-west" di-
```
rection would be optimal. The separation line between the south-east (SE) and
```
```
south-west (SW) directions is obviously the X2-axis, since it is on this line where
```
```
the two values (one corresponding to a trajectory going SW, the other one to
```
```
SE) of the cost function match. While integrating the optimal paths backward
```
in time, we should stop once the X2-axis is reached, which is called a dispersal
line. More precisely, it is a dispersal line for the evader, since, if the initial
condition is on this line, it is E who chooses one of the two optimal directions
```
along which to proceed; along both of these directions, the system trajectory
```
leaves the dispersal line and both lead to the same outcome.
444 T. BA§AR AND G. J. OLSDER
In the foregoing example we have discovered a dispersal line for the evader.
Of course, dispersal lines for the pursuer also exist. Toward that end, consider
the homicidal chauffeur game introduced in Example 8.3. Suppose that the
```
parameters of the game (/3, v^} are such that termination is always possible and
```
that the cost functional stands for the time elapsed before capture. If E is far
```
behind P (i.e., if, in Fig. 8.4, the state is x\ — 0, x% <C —/?), then P will turn
```
either to the right or to the left as quickly as possible, so as to have E directly
```
in front of him. Therefore, the half line (x\ = 0, x<i < a) for some negative a,
```
is a dispersal line for the pursuer.
By means of the same game we now introduce another type of a singular
```
surface, viz. the so-called universal line or universal surface. Assume that (8.29)
```
```
and (8.31) hold, which means that E can be captured by P, whatever the initial
```
position is. Suppose that E is situated far in front of P, i.e., x^ ^> ft, relatively
close to the X2-axis. Then, the best P can do is to turn towards E, such that,
```
in the relative (#1, x^} space, E's position moves towards the a;2-axis. Once E is
```
on the X2-axis, the remaining phase of the game is trivial, since it is basically a
chase along a straight line. A line, or more generally a surface, to which optimal
trajectories enter from both sides and then stay on, is called a universal line
```
(surface}. It turns out that, in the case when both (8.29) and (8.30) hold, the
```
X2-axis is also a universal line, as shown in the following example.
Example 8.5 Consider the homicidal chauffeur game as introduced in Exam-
ple 8.3, in which the cost for P is the "time to capture", and where the pa-
```
rameters (3 and v-2 are such that (8.29) and (8.30) hold. In order to construct
```
candidates for optimal paths by backward integration from the UP, we now fol-
```
low the analysis of Example 8.3 rather closely, along with the formulas (8.23)
```
```
and (8.24).
```
```
For the present example, equation (8.6) becomes
```
whereby
where VXl and Vx2 satisfy
along optimal trajectories. The final conditions for the costate equations are
```
the derivation of which is left as an exercise to the reader. (Rewrite the prob-
```
```
lem in polar coordinates (r, 9), and utilize the boundary condition VQ — 0 at
```
```
termination.)
```
PURSUIT-EVASION GAMES 445
Exactly as in Example 8.3, it can be shown that u1* = +1 near the end. For
the right half plane, the terminal conditions are
```
where r is the retrograde time T — t. Equation (8.33) displays two facts:
```
```
(i) X2(r) > 0 for r > 0, which means that part of the shaded region in Fig. 8.5a
```
```
is not yet filled with (candidate) optimal trajectories,
```
```
(ii) for r = fl/v^, the state (xi(r),X2(r)) is independent of the arrival angle
```
00-
Figure 8.9: The decision points in the homicidal chauffeur game.
Figure 8.9 depicts these trajectories. Point A\ in this figure has the coordi-
```
nates x\(P / v<z), x<2((31 v-z); it is a so-called decision point] E has various options,
```
all leading to the same pay-off. Point A% is the mirror image of A\ with respect
to the X2-axis. In order to fill the gap with optimal paths, we try a singular arc
for which the switching function is identically zero, i.e.,
where OQ is the parameter determining the final state, and the solution of the
state equation close to termination can be written as
For this to be true, its time derivative must also be identically zero, which yields
```
For (8.34) and (8.35) to be true when not both VXl and VX2 are zero, the
```
determinant of the coefficients must be zero, which results in
446 T. BA§AR AND G. J. OLSDER
```
Figure 8.10: The £2-;axis is a universal line.
```
```
Because of (8.32) and (8.34), xicosu2 - x^ sinu2 = 0, which ultimately leads
```
to #1 = 0 as the only candidate for a singular surface. Along this line, the
switching function is VX1X2 and therefore we must have VXl = 0, leading to
```
{u1* = 0, u2* = 0}. Along the line x\ = 0, both P and E follow the same
```
straight path in real space. Prom the singular arc, paths can leave in retrograde
time at any instant, leading to Fig. 8.10, thus rilling the whole shaded area in
Fig. 8.5b with optimal trajectories. The line xi = 0 is a universal line. It should
```
be noted that both V and dV/dx are continuous across the universal line (see
```
```
Problem 2, Section 8.8).
```
Figure 8.11: Classification of singular surfaces.
There are also other types of singular surfaces. In Fig. 8.11 we have sketched
```
all hitherto known singular surfaces of co-dimension one (i.e., these surfaces are
```
```
manifolds of dimension one less than that of the state space) and across which
```
```
V(x) is continuous. It is believed that no other such singular surface exists, but
```
PURSUIT-EVASION GAMES 447
a verification of this claim is not yet available. The singular surfaces not treated
heretofore in this chapter are the equivocal line, focal line and switching envelope.
```
They will not be treated in this book; for examples of games where equivocal
```
```
lines arise, the reader is referred to Isaacs (1975), Lewin and Olsder (1979)
```
```
and Lewin (1994); the focal line appears for instance in the "obstacle tag"
```
```
example (Breakwell, 1971) and a game with a switching envelope is treated in
```
```
Breakwell (1973). Problem 7 in Section 8.8 is also believed to feature a switching
```
envelope in its solution.
In the cases of the equivocal line and the switching envelope, one of the
```
players has the option of either staying on the singular surface or leaving it;
```
both possibilities lead to the same outcome. The difference between these two
```
surfaces is, that in the switching envelope case the trajectories enter (in forward
```
```
time) tangentially, whereas in the equivocal line case they do not. An analogous
```
difference exists between the universal line and the focal line.
Some of the singular surfaces can only be approximated by strategies of the
```
form u1 = 71(x),u2 = 72(x). In the case of the focal line, for instance, one
```
player, say P, tries to get the state vector off the focal line, but E brings it back
```
to the focal line. Strategies of the form u1 = 71(x),u2 = J2(x, u1), with an
```
instantaneous informational advantage to the evader, such as those described in
Section 8.2 with respect to upper and lower values, would be able to keep the
state precisely on the focal line.
```
Singular surfaces across which V(x) is discontinuous are the barrier and
```
the safe contact. We have already encountered barriers in Example 5.2 in the
context of optimal control, and in Section 8.3 where games of kind were treated.
Their appearance in games of degree will now be delineated within the context
of the homicidal chauffeur game. An example of a safe contact can be found in
```
Chigir (1976).
```
Example 8.6 Consider the homicidal chauffeur game with the cost function
defined as the time-to-go. In Example 8.3 we have described how semiperme-
able surfaces, starting from the end points of the UP of the target, could be
constructed. We now treat the case depicted in Fig. 8.5b, i.e., the case when
```
the parameters 1*2 and (3 are such that (8.31) holds. If E is initially situated
```
```
at point C-2 (see Fig. 8.5b), P cannot force him across Si, but instead he can
```
force him to go around it. Therefore it is plausible—and this can be proven
```
rigorously (see Merz, 1971), that capture from C^ will take considerably more
```
time than capture from C\. Hence, the value function is discontinuous across
Si which, therefore, is a barrier.
This concludes the present section on singular surfaces. In conclusion, the
crucial problem in the construction of the value function is to locate the singular
surfaces, but hitherto this problem has not been solved in a systematic way. On
the other hand, once a particular V has been constructed, which is continuously
differentiable in each of a finite number of mutually disjoint regions of the state
```
space, some conditions (known as "junction conditions") exist to check whether
```
```
the V(x) obtained is really the value function or not. Since these conditions are
```
448 T. BA§AR AND G. J. OLSDER
```
not yet very well understood, we do not treat them here; but for some discussion
```
```
on this topic the reader is referred to Bernhard (1977).
```
8.5 Solution of a Pursuit-Evasion Game: The
Lady in the Lake
In this section we obtain the complete solution of a pursuit-evasion game called
"the lady in the lake" by utilizing the techniques developed in the previous
```
sections. This game features a dispersal surface for the evader; there is also a
```
decision point.
```
A lady (E) is swimming in a circular pond with a maximum constant speed
```
v-2- She can change the direction in which she swims instantaneously. A man
```
(P), who has not mastered swimming, and who wishes to intercept the lady
```
when she reaches the shore, is on the side of the pond and can run along the
perimeter with maximum speed 1. He, also, can change his direction instanta-
neously. Furthermore, it is assumed that both E and P never get tired. E does
```
not want to stay in the lake forever, though; she wishes eventually to come out
```
```
without being caught by the man. (On land, E can run faster than P.) E's goal
```
is to maximize the pay-off, which is the angular distance PE viewed from the
```
center of the pond, at the time E reaches the shore (see Fig. 8.12). P obviously
```
wants to minimize this pay-off. To make the game nontrivial, it is assumed that
v2 < 1.
Figure 8.12: The "lady in the lake".
Even though the problem could be cast in a rectangular coordinate system
fixed in space, a description in the relative coordinates 0 and r turns out to be
```
simpler, where 9 is the angle between P and E (see Fig. 8.12 for the direction)
```
and r is E's distance from the center of the lake. This coordinate system is called
```
relative since it is attached to P; it is not fixed in space, and yet it describes
```
the game completely. The kinematics of the game are
PURSUIT-EVASION GAMES 449
```
where R is the radius of the pond. P's control ul is restricted by |u1(t)| < 1,
```
while E's control w2, which stands for the angle of E's velocity vector with
```
respect to the radius vector CE (see Fig. 8.12), is not restricted in any way. We
```
now seek for the equilibrium strategies in feedback form. The cost function is
```
|0(T)|, where T is denned as T = min{t : r(t) = R}, and -TT < 0(t] < +TT.
```
The Isaacs equation for this problem is
```
94Note that, although the open-loop representation of the feedback saddle-point solution (if
```
```
it exists) may exist, an open-loop saddle-point solution surely does not exist for this pursuit
```
evasion game. Whether a mixed open-loop saddle-point equilibrium exists is an altogether
different question which we do not address here.
whence
The differential equations for the costate variables along the optimal trajectories
are
and the value function at t — T is given by
The optimal control w1*, taken as the open-loop representation of the feed-
```
back equilibrium strategy,94 can now be written as w1*^) = sgn (dV/dO) =
```
```
sgn (0(T)). Since we assumed —TT < 0(t) < +TT, this implies that P will move
```
```
in E's direction along the smallest angle. Substituting u1* and u2* into (8.36),
```
we obtain
```
Two observations can be made at this stage. First, (8.38) is valid as long as
```
```
r ( t ) > Rv2', for r ( t ) < Rv<2 the outlined derivation does not hold. Second, E
```
```
swims along a straight line in real space, tangent to a circle of radius Rv% (see
```
```
Fig. 8.13a).
```
```
This result could also have been obtained geometrically (see Fig. 8.13b). In
```
the relative coordinate system, E's velocity VR is made up of two components:
```
(i) E's own velocity vector in real space, $2, and (ii) a vector #1, obtained by
```
```
rotation around the center C, opposite to P's direction of motion (recall that
```
```
P tries to reduce |0(T)|), and with magnitude r(t)/R. The best policy for E
```
is to make v-2 and the resulting velocity vector VR perpendicular to each other.
```
For any other choice of u2 (i.e., direction of v2 in Fig. 8.13b), the angle between
```
CE and VR will become smaller, or, in other words, E will move faster into P's
direction, which is precisely what E tries to avoid. Therefore, E's equilibrium
strategy is to keep the angle between v-2 and VR at Tr/2 radians, or equivalently,
```
sinu2* = Rv2/r(t).
```
450 T. BA§AR AND G. J. OLSDER
Figure 8.13: E's equilibrium strategy for the "lady in the lake".
```
This way of constructing E's optimal strategy fails for r(t] < Rv% in which
```
```
case E can achieve a larger angular velocity (with respect to the center C] than
```
```
P, and therefore she can always maneuver herself into the position 6(t) = TT, i.e.,
```
to a position diametrically opposite from P. Note that, in order to maintain
```
9(i] ~ TT, E would have to know P's current action. If she does not have access
```
```
to this (which is a more realistic situation), she can stay arbitrarily close to
```
```
9(t) = TT as she moves outward towards the circle with radius Rv<2. In case E
```
```
would know P's current action, she can reach the point (6 = TT, r = Rv^) in finite
```
```
time (see Problem 3 in Section 8.8); if she does not know P's current action, she
```
can get arbitrarily close to this point, also in finite time.95 From the position
```
(6 = TT, r = Rv2), E moves right or left along the tangent. More precisely, E will
```
first swim outward to a position r > Rv% which is sufficiently close to r = Rv?,.
Then P has to make a decision as to whether to run clockwise or anti-clockwise
```
around the pond (P cannot wait since, as E moves outward, his pay-off will
```
```
then become worse). Once E knows which direction has been chosen by P, she
```
will swim "away" from P, along the tangent line just described. In the region
```
r(t) > Rv-2, P's angular velocity is larger, and therefore he will continue to run
```
in the direction he had chosen before.
The outcome of the game is readily calculated to be
which holds true for all initial positions inside the circle of radius Rv^. The
```
lady can escape from the man if |#(T")| > 0, which places a lower bound on E's
```
```
speed: v<2 > 0.21723 From all initial positions inside the pond, E can always
```
first swim to the center and then abide by the strategy just described, resulting
```
in the outcome (8.39). From some initial positions she can do better, namely,
```
the positions in the shaded area in Fig. 8.14 which is bounded by the pond and
95This would inevitably involve some delay in the current state information of E and thereby
some memory in her information. Restriction to only feedback strategies would lead to some
subtle measurability questions.
PURSUIT-EVASION GAMES 451
Figure 8.14: Optimal trajectories in the relative space.
the two equilibrium trajectories which, constructed in retrograde time, end at
```
(r — Rv2,0 = TT). Within the shaded area the optimal strategies are determined
```
```
by (8.37). The backward construction is only valid until \0(t)\ = rr; the line
```
```
segment (9 = TT, Rv% < r < R) obviously forms a dispersal line on which it is
```
P who decides whether to go "to the left" or "to the right". In the non-shaded
```
area of the pond in Fig. 8.14, the value function is constant and equals |#(T)|
```
```
given by (8.39). Properly speaking, this value can only be obtained if there
```
```
is an informational advantage to E as explained above; see also Problem 9 in
```
Section 8.8. If this is not the case and only feedback strategies are allowed,
only an approximation to this value function can be obtained. Therefore, in
such a case, a saddle point will not exist, but an e-saddle-point equilibrium will
```
(cf. Section 4.2, specifically Def. 4.2).
```
The solution obtained for the "lady in the lake" has a very special feature.
If E would accidentally choose a wrong maneuver, she can always return to the
center of the lake and start all over again. The point EQ in the relative coordinate
```
system (see Fig. 8.14), which plays a special role, is sometimes referred to as a
```
decision point.
8.6 An Application in Maritime Collision
Avoidance
In this section, collision avoidance during a two-ship encounter in the open sea
will be treated as a problem in the theory of differential games. Given two ships
close to each other in the open sea, a critical question to be addressed is whether
a collision can be avoided. If, for instance, we assume that the helmsman
```
of ship 1 (PI) has complete information on the state of ship 2, whereas the
```
```
helmsman of ship 2 (P2) is not aware of the presence of the first ship, this lack
```
of information on the part of P2 may lead to a hazardous outcome. It is quite
possible that P2 may perform a maneuver leading to a collision which would
not have occurred if P2 had had full knowledge of Pi's position. Hence, in
such a situation, it would be reasonable to undertake a worst-case analysis, by
```
where r = VA^i+^i)' and the terminal range r(T) is the cost function. This
```
constitutes a game of degree. In this section we shall consider, instead, the
directly related game of kind, characterized by a given number rm, the minimum
```
range or minimum miss distance. If r(T) > rm, no collision takes place, and for
```
```
f ( T ) < rm, a collision will take place.
```
In the three-dimensional state space, the target set is described by the cylin-
der x\ + x\ = r^. The UP, i.e., the set of points on this cylinder at which a
collision can take place, is determined by
452 T. BA§AR AND G. J. OLSDER
```
assuming that one of the ships (PI) tries to avoid and the other one (P2) tries
```
to cause a collision.
One can envisage other situations, such as the case in which both ships are
aware of each other's position and they both try to prevent a collision, i.e., they
cooperate. This case, although not a differential game in the proper sense, can
be solved by similar techniques. Roughly speaking, the minmax operation in
the differential game case must be replaced by the maxmax operation in the
cooperative case. Still another situation arises when one ship holds course and
```
speed (is "standing on") because of the "rules of the road", whereas the other
```
ship must evade.
The dynamic system comprises two ships, PI and P2, maneuvering on a sea
surface which is assumed to be homogeneous, isotopic, unbounded and undis-
turbed. The kinematic equations to be used are identical to those of the two-cars
```
model (see Example 8.3):
```
```
This simple model may not be (too) unreasonable if only short duration maneu-
```
vers are considered, during which the ships cannot change their speeds markedly.
Each ship is characterized by two parameters: the maximum angular velocity
u>i and the constant forward speed Vi. The control of Pi is ul, which is the
```
rudder angle, and which is bounded by \ul(t)\ < I. Extensions of this model are
```
```
possible, such as, for instance, the case of variable forward speed (the engine
```
```
setting will then become a control variable as well), but these would make the
```
```
analysis to follow much more complicated. Some other effects (hydrodynamic
```
```
and inertial) have been ignored in the model (8.40), such as the ^-dependence
```
of Vi.
The cost function is described in terms of the closest point of approach
```
(CPA), the distance corresponding to it is called the miss distance. The terminal
```
condition is given by the CPA at first pass, characterized by
PURSUIT-EVASION GAMES 453
The terminal condition wherefrom a barrier can be constructed is determined
from this relation with the inequality sign replaced by an equality sign. Sub-
stituting Xi = rm sina and x-2 — rmcosa, where a is the bearing, into this
equality, we obtain, at the final time,
```
where w = \/(Vi + v\ — 2viV2COs9(T)}, and e = ±1. For each 0(T) we obtain
```
```
two values for a(T), one corresponding to e = +1 and the other one to e = — 1.
```
```
These two values correspond to a right and left barrier, respectively; P2 can
```
just miss PI, either on the right or on the left side.
From the final conditions thus obtained, we must construct equilibrium tra-
jectories backward in time in order to obtain the barrier which separates the
points from where avoidance is possible from the points from where it is not.
The barrier—actually composed of two parts, each corresponding to a different
```
value of e in (8.41)—is depicted in Fig. 8.15. In this figure the two parts of
```
the barrier intersect, which indicates that the enclosed points are guaranteed
```
collision points for P2 (provided that the intersection of the two parts does not
```
```
leak).
```
Figure 8.15: A schematic picture of the barrier.
We now turn to the actual construction of the barriers. The optimal ul and
u2 are the arguments of the corresponding Isaacs equation, thus leading to
Furthermore, the costate variables satisfy
```
As long as the arguments of the sgn-relations in (8.42) are nonzero, it is simple
```
```
to obtain the solution of (8.40) and (8.43); but, substitution of the final values
```
454 T. BA§AR AND G. J. OLSDER
```
for V X l , VX2, VQ into (8.43) shows that the arguments of the sgn-relations are
```
zero at t = T. The situation can, however, be saved by replacing the arguments
with their retrograde time derivatives, which leads to
where e = ±1. Hence, the equilibrium strategies at the terminal situation
```
are determined for almost all values of 0(T}. For v<2/v\ < 1, we can expect
```
```
a singular control for P2 if 6(T) = ±arccos(i>2/fi) for the last part of the
```
maneuver. Similarly, if v\/vz < 1, PI may have a singular control.
Figure 8.16: The trajectories which make up the right barrier, projected in the
```
(x-2,9] plane.
```
Let us proceed with the case v^ < f i < 1. Closer scrutiny reveals that there
```
are no equilibrium paths leading to the singular point 0(T) = earccos(v2/vi),
```
```
but, in its stead, a dispersal line ends at this point. The other point, 6(T] =
```
```
—earccos(i>2/^i), however, is the end point of a singular line (with u2 = 0),
```
which is a universal line. Integrating backward in time from the final condition,
we first start with the singular control u2 = 0 which may be terminated at any
```
time (switching time) we desire; and going further backward in time we obtain
```
```
a nonsingular u2. In Fig. 8.16, it is shown how the right barrier (e = +1) can be
```
constructed by determining the optimal trajectories backward in time. In the
```
figure, the projections of the trajectories on the (x2,Q) plane have been drawn.
```
```
In order to construct these trajectories, Q(T] can be taken as a parameter, and
```
```
for 9(T} = —€. arccos(v2/vi) we have another parameter, viz. the time instant at
```
```
which the singular control switches to a nonsingular one (there are two options:
```
```
u2 = +1 and u2 — —1) in retrograde time. The dispersal line is found as the
```
intersection of two sets of trajectories obtained through backward integration,
```
namely that set of trajectories which have d(T) smaller than arccos(f2/Vi),
```
PURSUIT-EVASION GAMES 455
```
modulo 2yr, and the set of trajectories with 6(T) greater than arccos^/f i). The
```
left barrier can be constructed analogously. If the two barriers intersect over the
whole range 0 < 9 < 27r, and if the line of intersection does not leak, then the
enclosed points are guaranteed collision points for P2. Also a tacit assumption
which makes this whole construction work is that the two parts which define the
```
dispersal line (for both the right and left barriers) do indeed intersect. If they do
```
```
not intersect, then there would not be a dispersal line; instead, there would be
```
a hole in the surface that encloses the points we described above. Consequently,
this set of points would not be enclosed anymore, and PI might be able to
force escape by steering the state through this hole. As yet, no sufficiency
```
conditions are known which ensure enclosure; hence, for each problem one has
```
to verify numerically that no holes exist. For this reason, the method can be
used conveniently only for systems with state space dimension not greater than
three.
Figure 8.17: Intersection of the semipermeable surfaces with the plane 9 = OQ.
A possible and appealing way of constructing the barriers which enclose the
```
points wherefrom collision is unavoidable, is to consider cuts in the (xi,X2,#)
```
```
space for which 0 is a constant, say 6 = 9$. For each parameter value 0(T]
```
```
and also for the retrograde time parameter when 9(T] — — earccos^/^i) one
```
calculates the equilibrium trajectories until they hit the plane 9 = OQ—this
being performed for both the right and left barriers. For different values of the
parameters, one obtains, in general, different points in the plane 9 — OQ, with
the ultimate picture looking like the one sketched in Fig. 8.17. In this approach
```
existence of a dispersal line cannot be detected; one simply "by-integrates"
```
```
it. The semipermeable surface (line) m in Fig. 8.17 is the result of such a
```
procedure. However, this is not a serious problem, since what one seeks in the
```
plane 9 = 9$ is a region completely surrounded by semipermeable surfaces; the
```
connecting corners must not leak. Hence, in a picture like Fig. 8.17, one has
to check whether point A leaks or not. If it does not. then one can disregard
the semipermeable line m. If it leaks, however, then this is an indication that
```
either the problem is ill-posed (i.e., no enclosed region exists from where collision
```
```
can be guaranteed), or there must be other semipermeable surfaces which will
```
```
define, together with (parts of) the already existing semipermeable surfaces, a
```
456 T. BA§AR AND G. J. OLSDER
```
Figure 8.18: Initial points (£1,2:2), with 9 = Tr/2, for which distance of CPA is
```
rm.
new enclosed region.
```
The next figure, Fig. 8.18, taken from Miloh and Sharma (1976), depicts
```
several enclosed areas for different rm-values. The figure shows the barrier
cross-sections at 9 = vr/2. The numerical data are as follows: v\ = 1, v? — ^,
uj\ — 0/2 = 1- This means that the minimum turn radii of PI and P2 are 1 and
|, respectively. By constructing similar pictures for different sets of parameter
```
values (different speed ratios and different maneuvering capabilities), it is pos-
```
sible to gain insight into collision sensitivity with respect to these parameters.
```
As a final remark, collision was defined here as "two ships (considered as
```
```
points in the plane) approaching each other closer than a given distance". If
```
ships would actually collide this way, then they must have the shape of round
disks, as viewed from above. The construction presented can, however, be
```
applied to more realistic shapes, which is though slightly more complicated;
```
```
such an example will be presented in Section 8.7. Problem 5 (Section 8.8)
```
presents yet another slightly different problem in which the target set is still a
round disk, but it is not centered at the origin.
8.7 Role Determination and an Application in
Aeronautics
Heretofore we have considered the class of pursuit-evasion games wherein the
roles of the players are prescribed at the outset of the problem. These problems
are sometimes called one-target games which model situations such as a missile
chasing an airplane. However, in a "dogfight" that takes place between two
planes or ships which are both armed and capable of destroying their opponents,
it may not be apparent at the outset who pursues whom. Accordingly, we
now introduce two-target games, where either player may be the pursuer or
PURSUIT-EVASION GAMES 457
the evader, depending on their current configuration, and each target set is
determined by the shooting range of the respective player. Each player's task
```
is to destroy his opponent (i.e., maneuver the opponent into his target set) and
```
to avoid destruction by his opponent.
The solution method to two-target games comprises essentially two stages:
```
(i) Given the initial conditions, determine the relative roles of the players; who
```
pursues whom? It is clear that in a deterministic differential game the
roles of the players are determined completely by the initial state and
will not change during the course of the game—provided that only pure
strategies are permissible and that the players act rationally.
```
(ii) With the roles of the players determined, what are the optimal, e.g., time-
```
optimal, strategies for the players? Note that the question of who pursues
whom has nothing to do with the cost function.
Here we shall be concerned with the first stage. The state space consists of three
mutually disjoint regions: two regions corresponding to victory by one or the
other player, and the third region corresponding to a draw, i.e., neither player
is capable of destroying his opponent. The states within the draw region may
correspond to different situations. A particular possibility is a stalemate: both
players play certain strategies such that neither player will be destroyed and,
moreover, a deviation from his equilibrium strategy by one of the players will
lead to his own destruction. Another possibility is that the faster player can
escape—tacitly assuming that this player cannot win the game in finite time.
The region RI of the state space, which corresponds to victory by PI, may
be expected to be bounded by a surface comprising a part of the surface of AI,
the target set for PI, together with a semipermeable surface EI which prevents
the state from leaving RI provided that PI plays optimally in the neighborhood
of EI. The interior of RI does not contain any points of P2's target set A2, and
```
thus, assuming that the game terminates in finite time (i.e., a stalemate cannot
```
```
arise), victory for PI is guaranteed for initial states inside RI (see Fig. 8.19a).
```
That portion of AI, which forms part of the boundary of RI, is permeable only
Figure 8.19: Regions RI and R2 corresponding to victory by PI and P2, re-
spectively.
458 T. BA§AR AND G. J. OLSDER
in the direction away from RI. In a, similar way we can construct R2—the
region corresponding to victory by P2. If RI and R2 do not fill up the whole
state space, then points belonging to neither RI nor R2 belong to RS, the draw
region. Such a situation is illustrated schematically in Fig. 8.19b. Victory by
P2 in region R2 is guaranteed if we assume that no stalemate is possible, so
that the target set A2 is reached in finite time.
```
States belonging to both EI and £2 (such as I in Fig. 8.19b) will terminate
```
on both AI and A2, thereby giving rise to "victory" by both players, or, what
is essentially the same, to a simultaneous confrontation. States belonging to
£2 and not to Si, or belonging to EI and not to £2, give rise to near miss
situations.
We now present an example of a two-target game which has the additional
feature that the two target sets are nonsmooth.
Consider an aerial duel or dogfight, wherein each combatant wishes to de-
stroy his opponent without himself being destroyed. In order to keep the analysis
at a reasonable level, we take the dimension of the state space to be three, and
assume that the two players move in a single horizontal plane. Equations of
motion within this plane are the same as those in the ship collision avoidance
```
problem, and they are given by (8.40). The constraints on the controls are also
```
the same. We assume that v\ > v^ > 0 and 0 < uJi < u^, i-e., PI has the
advantage of having a higher speed, whereas P2 is more maneuverable. Pi has
```
a confrontation range (which is his target set Aj) consisting of a line segment of
```
length £i in the direction of his velocity vector. If PI crosses the confrontation
range of P2, or vice versa, the game ends. For the analysis to follow, we make
the additional assumption 0 < i\ < £-2.
We start with the construction of the barrier £ which is the composition of
EI and £2 introduced earlier. Each portion of E, whether it leads to simultane-
ous confrontation or to near miss, is obtainable by solving a particular "local"
differential game, with a terminal cost function defined only in the immediate
neighborhood of the simultaneous confrontation or near miss. If we adopt the
```
convention that the local game has a positive (respectively, negative) value when
```
```
it ends in favor of P2 (respectively, PI), the equilibrium controls are determined
```
from
```
The costate equations are the same as in (8.43), but in this case the terminal
```
conditions are different as will be seen shortly. In fact, the terminal conditions
will be different for each local game.
```
From (8.44) it follows that, if u1 remains constant for a duration of at least r
```
units of time before the final time T is reached, then, apart from an insignificant
constant multiplier, assumed henceforth to be unity, the costates at time T =
which leads to
PURSUIT-EVASION GAMES 459
T — t satisfy
Since many of the local games will include singular controls, we now inves-
```
tigate their properties. From (8.44) it follows that a singular control for P2 is
```
```
only possible for VQ = 0, which results in VQ = 0. Equation (8.46) now leads to
```
the conclusion that the only possible singular control for P2 is u2 = 0. Similarly,
it can be shown that the only possible singular control for PI is u1 = 0.
Returning to the actual construction of S, we shall first deal with simul-
taneous confrontation and thereafter with the near miss situations. For each
situation, several types may be distinguished, since simultaneous confrontation
and near miss occur in different ways.
Simultaneous confrontation
Consider the situation depicted in Fig. 8.20, where the trajectories of PI and P2
in real space have been sketched, together with the relative coordinate system
at terminal time T. In the figure, PI turns to the right as fast as possible, and
Figure 8.20: A situation of simultaneous confrontation.
```
where 0 is determined by the final conditions. From (8.44) and (8.43) it further
```
follows that
so that, if u2 ^ 0,
```
Then, it follows that (f> = 0 in (8.45), which, together with (8.44) and (8.47),
```
```
indeed leads to u1* = 4-1, u2* — — 1. But why should (8.49) be a requirement?
```
```
Let us consider the case of VXl(T): a small deviation of P2's position in the
```
```
positive xi-direction in the final situation (keeping 0:2(T) = 0, 6(T) = TT) will
```
be to the advantage of P2, since he will be able to win the game from the
new relative position. Hence, after this deviation, we have V > 0 instead of
```
V = 0, and therefore VXl (T) > 0. A similar argument verifies the sign of V#( T).
```
```
Furthermore, since a change in x^(T) does not change the outcome of the game
```
```
(it remains V = 0), we have VX2(T] = 0.
```
Let us now investigate how this simultaneous confrontation affects the bar-
```
rier S in the (xi,X2,Q) space, which separates the initial points where from PI
```
```
can win from those corresponding to P2's victory. Since x^(T) is a parame-
```
```
ter, subject to the constraint (8.48), a whole family of equilibrium trajectories
```
exists, all leading to a simultaneous confrontation of the type described. This
```
family forms a semipermeable surface in the ( x i , X 2 , 9 ) space. For the actual
```
construction we shall consider the intersection of this surface with the planes
```
& = constant, say 0 = 0Q. Equations (8.40), (8.43), (8.44) and (8.49) are in-
```
```
tegrated backwards in time until 9 = OQ is reached and for varying X2(T) we
```
obtain different points in the 0 = 8$ plane, which make up a line segment.
In Fig. 8.21, the solution of a specific example has been sketched, for which
the characteristics of the airplanes are
460 T. BA§AR AND G. J. OLSDER
```
P2 does likewise, but to the left. The final state is given as follows: x\(T] = 0,
```
```
0(T} = TT and #2 (T) is a variable that must satisfy
```
```
The equilibrium strategies are 7l!*(-) = 4-1, 72*(-) = —1- Initial conditions as-
```
sociated with these strategies and terminal conditions can easily be found by
backward integration of the state equations. The value of the game correspond-
```
ing to these initial conditions is neither positive (victory for P2) nor negative
```
```
(victory for PI) and is therefore determined as V = 0. It should be clear from
```
Fig. 8.20 that 71* and 72* are indeed in equilibrium since a deviation from either
strategy will lead to a destruction of the deviating player. This is also confirmed
by application of the necessary conditions. Let us assume for the moment that
the terminal values of the costate variables satisfy the requirements
In the figure, OQ = Tr/3. The line segment which separates the states corre-
sponding to victory by PI from those corresponding to P2's victory, according
to the simultaneous confrontation described above, has number 1 attached to
it. From initial positions slightly above this line, PI will win, and from initial
positions slightly below, P2 will win.
PURSUIT-EVASION GAMES 461
Figure 8.21: Intersection of semipermeable surfaces with the plane 9 = Tr/3.
Different types of simultaneous confrontation exist, each one corresponding
to a different maneuver. These different types are shown in Fig. 8.22. Type 1
has already been discussed extensively. Type 2 is similar, but now P2 turns
```
right as fast as possible; x<z(T] is again the parameter. Types 3 and 4 are
```
different in the sense that destruction is not caused by the confrontation range,
```
but instead by collision; the angle #(T) is the parameter. Type 3 is a collision
```
with PI turning right and P2 turning left. In type 4, both turn right. Type 5
calls for an 5-maneuver by PI to get within the range of P2, who has the longer
confrontation range, before P2, who is turning right, faces directly towards PI,
```
at a distance of x^(T] — l\; the turn angle a of the final bend of PI is the
```
parameter. Type 6 is an extension of the 5-maneuver of PI in type 5, in which
the final bend with a fixed angle a\ is preceded by a straight-line segment
```
(singular arc) whose length functions as a parameter. This singular arc itself
```
is preceded by a left turn. Type 7 is similar to type 6, the only difference
being that now PI starts with a right turn. In addition to these 7 types, there
are 7 other types, indicated by 1', 2 ' , . . . , 7', obtained from the first 7 types by
left-right reflections. For instance, in case 1', PI turns left and P2 turns right.
In all these types, the parameter is subject to certain constraints. We will
```
not treat detailed analyses of all these types here; they are somewhat technical
```
and not enlightening for the results to be obtained. The details can be found
```
in Olsder and Breakwell (1974). Those types which give rise to curves lying in
```
the 9 = 7T/3 plane have been indicated in Fig. 8.21.
A question that comes into mind now is whether these fourteen different
types of simultaneous confrontation constitute the whole picture. In fact, we
do not know for sure whether there are other types or not. The ones that have
462 T. BA§AR AND G. J. OLSDER
Figure 8.22: Different maneuvers, all leading to simultaneous confrontation.
been depicted in Fig. 8.21 have been obtained using intuitive reasoning, and the
analysis indeed showed that they give rise to semipermeable surfaces. However,
together with the semipermeable surfaces corresponding to near miss situations,
```
for which the discussions will follow shortly, they separate (and enclose) the
```
states where from PI can guarantee victory from the other states, i.e., there is
```
no hole in S (cf. Section 8.6). If, however, another simultaneous confrontation
```
```
maneuver (or another yet-unknown near miss maneuver) is discovered, it may
```
lead to either a larger or a smaller RI .
Near miss
Essentially ten different types of near miss situations have been discovered so
far, which are labeled as a, b,... ,j in Fig. 8.23, which depicts the corresponding
trajectories in real space. Another set of ten types exists, obtained by inter-
changing left and right, as in the case of simultaneous confrontation. We now
give a brief rundown of the first ten types.
```
In type a both players move to the right as fast as possible; P2's trajectory
```
in relative space looks as sketched in Fig. 8.24, and its characteristic feature is
that it touches the boundary of Pi's confrontation range, but does not cross
```
it—hence the name "near miss". The parameter can be taken as x^(T}^ which
```
PURSUIT-EVASION GAMES 463
Figure 8.23: Different maneuvers, all leading to near misses.
```
satisfies the constraint 0 < xz(T] < min(^i, 1*2/^1) and determines 0(T) through
```
```
sin#(T) = ujiX2(T)/V2- The terminal conditions for the costate equations are
```
```
VXI(T) > 0, VX2(T) = Ve(T) = 0. Type 6 resembles type a, the difference being
```
```
that it2* = — 1. Types c and d have 6(T] = 7r/2 and end with a singular arc
```
by P2. The length of this arc is the parameter. These two types only occur if
```
(jj\t\ > V2- For LJ\II < V2, the types c and d do not arise, but instead we have
```
```
the types e, /, g and h, for which X2(T) — t\. For types e. and h the parameter
```
```
is 9(T], for / and g it is the length of the singular arc of P2's trajectory. Types
```
i and j resemble e and h, respectively, but now PI is almost destroyed at the
far end of P2's confrontation range. In e and h it is P2 who can barely escape
the far end of Pi's confrontation range. Note that in the figures corresponding
to i and j, the relative coordinate system at t = T has been drawn with respect
```
to P2 (instead of PI, as in all other types). In these two latter types, P2 has a
```
464 T. BA§AR AND G. J. OLSDER
Figure 8.24: The trajectory corresponding to type a in Fig. 8.23, now in relative
space.
singular arc, the length of which is the parameter.
In Fig. 8.21, we have drawn the line segments corresponding to the applicable
```
near miss cases for the example with characteristics (8.50), which are the types
```
o, b, c and d and their mirror images a', &', c', d'. The crucial question now is
```
whether all the line segments (both simultaneous confrontation and near miss)
```
enclose a region, together with AI. It should be clear from the figure that two
such regions exist: an "inner one" and an "outer one". Closer scrutiny reveals
that some corners of the outer region leak, and the semipermeable surfaces
which enclose the inner region constitute the barrier which separates RI from
R2.
To summarize, we have given a complete characterization of the regions
leading to victory for PI and P2, and have arrived at the qualitative result that
```
if PI plays optimally along the boundary of his region of victory (Ri), then P2
```
```
is captured within RI (and likewise with the roles of PI and P2 interchanged).96
```
If he cannot stay within RI forever, then he has to leave this region via AI which
```
leads to termination of the game (with Pi's victory).
```
8.8 Problems
1. Instead of the pursuit-evasion game of Example 8.2, consider the equivalent
problem described by
```
where T = min{t : xi(t) = 0}, -1 < u* < 0, i = 1,2. Obtain the
```
```
final condition for the costate equation by making use of (8.14), and show
```
that the result is in complete agreement with the corresponding condition
derived in Example 8.2.
96Note that since i\ = oo, RS is empty.
PURSUIT-EVASION GAMES 465
2. Calculate the value function V(x) for Example 8.5, and show that both V
and dV/dx are continuous along the universal surface.
3. Consider the "lady in the lake" with an instantaneous informational ad-
```
vantage to E, i.e., the strategies are of the form ul(t] = 71(x(£)),u2(£) =
```
```
72 (x(t),M1 (t)). E starts at the center of the pond and spirals outward as
```
```
fast as possible, while maintaining 0(t) = TT. Show that the circle r — Rv<2
```
will be reached in finite time.
Figure 8.25: Geometrical derivation of the saddle-point solution.
4. This problem addresses the derivation of the equilibrium solution of the
"lady in the lake" geometrically, without utilizing Thm. 8.2. Start from
the decision point EQ in Fig. 8.25, and suppose that P chooses u1 — — 1
```
(i.e., he moves such that the pond is to his left in real space); the case
```
```
u1 = +1 can be dealt with analogously. E will then swim in a straight line
```
to the shore. The question is: in which direction will she swim? Suppose
```
that she swims in the direction j3 (see Fig. 8.25).
```
```
(i) Why can /3 be restricted to \(3\ < Tr/2?
```
```
(ii) Show geometrically that swimming towards £3 is better for E then
```
```
to swim to EI, where £3 is only an e-distance away from £2- (Show
```
that the time needed for P to cover the distance £2 £3 is greater than
the time needed for E to cover the distance £^£3, where E'^ is the
```
projection of £2 on EQE^.}
```
```
(iii) Show that swimming towards £4 is the best E can do. (Make use of
```
the fact that the circle through the points £4, EQ and C lies inside
```
the pond.)
```
5. Consider the following two-cars model:
```
where PI chooses u\ and u^, subject to the constraint (i4)2 + (wl)21'€ < 1>
```
where e is a small positive number. P2 chooses, as before, w2, without any
restrictions. For e = 0 this problem reduces to Problem 6. It is conjectured
that the present more general version features a switching envelope. Prove
or disprove this conjecture, and also investigate the limiting behavior of
the solution when t [ 0.
8. The kinematics of this problem are described by (8.22) with v-2 = |. The
```
only restriction on the controls is |w1(t)| < 1. The target set of PI is given
```
by
466 T. BA§AR AND G. J. OLSDER
```
together with the constraints |w*(i)l 5; 1» * = 1»2. The target set A is
```
```
defined by x\ + (z2 + R)2 - R2 = 0, where R = \/2/8. PI tries to avoid
```
A, whereas P2 would like to have the state enter A.
```
(i) Determine the barrier(s) which separate(s) points from where avoid-
```
ance is possible from those from where it is not, and show that each
one lies partly inside A. Therefore, the semipermeable surface deter-
```
mined cannot be the barrier(s) sought.
```
```
(ii) Determine the actual barrier, in a way similar to that employed in the
```
```
dolichobrachistochrone problem (see Chigir (1976) or the first edition
```
```
of Ba§arand Olsder (1982)).
```
6. Consider the system described by
```
with Iw1) < 1 and no constraints imposed on u2. The target set is the
```
```
half line (xi > 0, ^2 = 0). PI wants to steer the system to the target set
```
as soon as possible, whereas P2 wants to do the opposite. The game is
played in the half plane x-2 > 0.
```
(i) Show that a barrier starts at the origin (x\ = 0, #2 = 0) and ends at
```
```
X2 = I -
```
```
(ii) Show that the continuation of the barrier, for ^2 > 1, is an equivocal
```
line.
7. The differential game addressed in this problem is the same as that of
Problem 6, except that the dynamics are replaced by
PURSUIT-EVASION GAMES 467
```
Pi's objective is to force (0:1,£2) into AI without passing through A2,
```
whereas P2 wants to have the state enter A2 without passing through AI.
Show that the region of victory Ri for Pi is as indicated in the following
```
figure:
```
Check also that the semipermeable curve DE lies completely outside A2
and that the corner E does not leak.
9. We are given a zero-sum differential game for which the Isaacs condition
does not hold everywhere, but instead the Isaacs equation admits two
different solutions depending on the order of the ram and max operations.
Prove that the associated differential game does not admit a saddle-point
solution in feedback strategies, but it admits a saddle point when one
or the other player is given an informational advantage on the current
action of his opponent. Make use of this result in providing a possible
verification for the existence of a saddle point in the differential game
"lady in the lake."
8.9 Notes
Section 8.2. The theory of deterministic pursuit-evasion differential games was
single-handedly created by Isaacs in the early 1950s, which culminated in his book
```
(Isaacs, 1975; first edn. 1965). Blaquiere and Leitmann, independently of Isaacs, also
```
```
obtained Thms. 8.1 and 8.2 (see Blaquiere et al., 1969). Their geometric approach
```
is essentially the same as that of Isaacs, but it is stated in a mathematically more
precise language. In this chapter, we follow essentially Isaacs' approach. Starting in
the 1960s, many researchers have worked on a rigorous establishment of the validity of
the Isaacs equation. The starting point is often a time-discretization and a carefully
defined information structure. Depending on this information structure, lower and
upper values of the game are defined, and then appropriate limiting arguments are
```
incorporated, under which these upper and lower values approach each other; the re-
```
sulting limiting value, in each case, is declared to be the value of the game. References
```
in this connection are Fleming (1961, 1964), Varaiya and Lin (1969), Roxin (1969),
```
and the target set of P2 is given by
468 T. BA§AR AND G. J. OLSDER
```
Friedman (1971), Elliott and Kalton (1972), Elliott (1977) and Elliott et al. (1973).
```
```
The relationship with viscosity solutions is given in Evans and Souganidis (1984) and
```
```
in Barren, Evans and Jensen (1984). These two references contain results for fixed
```
```
terminal time problems only; see also Elliott (1987). For results on viscosity solu-
```
tions for problems with variable terminal time the reader is referred to Bardi and
```
Soravia (1991a, b). For extended solution concepts of ordinary differential equations,
```
in which, for instance, the /-function in the state equation is not Lipschitz-continuous,
```
the reader is referred to Krasovskii and Subbotin (1988), which also includes a rigorous
```
```
definition of strategy, and also to Hajek (1979).
```
The notions of playability and nontermination aided in the understanding of the
```
so-called "bang-bang-bang surface" introduced in Isaacs (1969); see also Ciletti (1970)
```
```
and Lewin (1976).
```
Section 8.3. The concept of capturability also originates in Isaacs' works. A
mathematically more rigorous approach to capturability is given in Krasovskii and
```
Subbotin (1988), leading to the so-called "Theorem of the Alternative". A different
```
```
approach to capturability has been developed by Pontryagin (1967) and subsequently
```
```
by Hajek (1975). In this set-up the pursuer knows the current action of the evader,
```
```
which he "neutralizes" by means of his own control. The left-over power (if any) is
```
used by the pursuer to steer the system to the target set.
The famous "homicidal chauffeur game" is very versatile in that its solution fea-
tures many singular surfaces. We do not provide here the complete solution to this
```
game; it was partly solved in Isaacs (1975). The complete solution is spread out over
```
```
Merz (1971) and Breakwell (1973). The notion of a leaking corner was introduced in
```
```
Bernhard (1971).
```
Section 8.4. The transition, dispersal, universal and equivocal lines were first in-
```
troduced in Isaacs (1975). The focal line and the switching envelope appeared for the
```
```
first time in Merz (1971) and Breakwell (1973).
```
Introduction of noise in the systems equations tends to "smoothen" the singular
surfaces which will therefore no longer exist as such. The addition of noise to the
```
system dynamics is the starting point for viscosity solutions; see Chapter 5. For a
```
```
"noisification" of the homicidal chauffeur game, see Pachter and Yavin (1979).
```
Section 8.5. The "lady in the lake" appeared in the Russian translation of Isaacs'
```
book; see also Breakwell (1977). Another well-known differential game, which has a
```
safe contact in its solution, is the dolichobrachistochrone problem, stated and partly
```
solved in Isaacs (1975). The complete solution was given later in Breakwell (1971) and
```
```
Chigir (1976). In the survey article (Ho and Olsder, 1983), a rather exhaustive list
```
of other known solved or partly solved zero-sum differential games is given, including
such games as "lion and man", "obstacle tag", "conic surveillance evasion" and the
"suicided pedestrian". References to extensions with "two pursuers and one evader"
or "one pursuer and two evaders" are also given there. A recent book in the vein of
```
Isaacs is Lewin (1994); it contains new developments and examples.
```
Section 8.6. The application in maritime ship collision avoidance follows the works
```
of Sharma (1976), and Vincent and Peng (1973). See also Merz (1973) and Olsder and
```
```
Walter (1978).
```
PURSUIT-EVASION GAMES 469
Section 8.7. Role determination and its application in aeronautics were first dis-
```
cussed in Olsder and Breakwell (1974). Similar problems, but with different target
```
```
sets, have been considered in Merz (1976). For other aeronautical applications see
```
```
Peng and Vincent (1975) and Vincent et al. (1976).
```
```
Section 8.8. Problem 5 is from Vincent and Peng (1973). Problem 6 is a spe-
```
```
cial version of a problem considered in Isaacs (1975), Problem 8 is from Getz and
```
```
Pachter (1980).
```
This page intentionally left blank
Appendix A
Mathematical Review
This appendix provides some background material on those aspects of real anal-
```
ysis and optimization which are frequently used in the text; it also serves to
```
introduce the reader to our notation and terminology. For a more detailed ex-
```
position on the topics covered here, a standard reference is Luenberger (1969).
```
A.I Sets
```
A set S is a collection of elements. If s is a member (element) of 5, we write
```
```
s € S; if s does not belong to 5", we write s £ S. If 5 contains a finite number
```
of elements, it is called a finite set] otherwise it is called an infinite set. If the
```
number of elements of an infinite set is countable (i.e., if there is a one-to-one
```
```
correspondence between its elements and positive integers), we say that it is a
```
```
denumerable (countable) set, otherwise it is a nondenumerable set.
```
A set 5 with some specific structure attached to it is called a space, and it
```
is called a linear (vector) space if this specific structure is of algebraic nature
```
with certain well-known properties which we assume that the reader is familiar
with. If 5 is a vector space, a subset of S which is also a vector space is called
a subspace. An example of a vector space is the n- dimensional Euclidean space
```
(denoted by Rn) each element of which is determined by n real numbers. An
```
```
x £ Rn can either be written as a row vector x = (xi,..., xn), where x i , . . . , xn
```
are real numbers and denote the components of x, or as a column vector which
```
is the "transpose" of (xi,... ,xn) (written as x = (xi,... ,xn)'}. We shall adopt
```
the latter convention in this text, unless indicated otherwise.
Linear independence
Given a finite set of vectors s i , . . . , sn in a vector space 5, we say that this set
of vectors is linearly independent if the equation X^=i OiiSi = ® imPnes ai — 0>
```
i = 1 , . . . , n. Furthermore, if every element of 5 can be written as a linear
```
combination of these vectors, we say that this set of vectors generates S. Now,
```
if S is generated by such a linearly independent finite set (say, .X"), it is said to
```
471
Convergent sequences, Cauchy sequence
```
An infinite sequence of vectors (si, 5 2 , . . . , Si...} in a normed vector space S is
```
said to converge to a vector 5 if, given an arbitrary e > 0, there exists an N,
which may depend on e, such that \\s — Sj|| < e for all i > N. In this case, we
```
write Si —» s, or limj—^oo Si = s, and call s the limit point of the sequence {si}.
```
```
More generally, a point s is said to be a limit point of an infinite sequence {sj}
```
```
if the latter has an infinite subsequence {sik} that converges to s.
```
```
An infinite sequence {s^} in a normed vector space is said to be a Cauchy
```
sequence if, given an e > 0, there exists an N such that ||sn — sm\\ < e for all n,
m > N. A normed vector space S is said to be complete, or a Banach space, if
every Cauchy sequence in 5 is convergent to an element of 5.
Open, closed and compact sets
Let S be a normed vector space. Given an s € 5 and an e > 0, the set
```
Nc(s) — {x e S : \\x — s\\ < e} is said to be an e-neighborhood of s. A subset X
```
```
of S is open if, for every x 6 X, there exists an e > 0 such that Ne(x) C X. A
```
```
subset X of 5" is closed if its complement in 5 is open; equivalently, X is closed
```
if every convergent sequence in X has its limit point in X. Given a set X C S,
the largest subset of X which is open is called the interior of X and denoted as
X.
A subset X of a normed vector space 5 is said to be compact if every infinite
sequence in X has a convergent subsequence whose limit point is in X. If X is
finite dimensional, compactness is equivalent to being closed and bounded.
Transformations and continuity
A mapping / of a vector space S into a vector space T is called a transformation
```
or a function, and is written symbolically / : 5 —> T or y = f ( x ) , for x € 5,
```
```
y € T; / is said to be a functional if T = R.
```
472 T. BA§AR AND G. J. OLSDER
be finite dimensional with its unique "dimension" being equal to the number of
```
elements of X; otherwise, S is infinite dimensional.
```
```
A.2 Normed Linear (Vector) Spaces
```
```
A normed linear vector space is a linear (vector) space S which has some ad-
```
ditional structure of topological nature. This structure is induced on S by a
real-valued function which maps each element u E S into a real number ||u||
called the norm of u. The norm satisfies the following three axioms:
APPENDIX A 473
Let / : S —> T, where S and T are normed linear spaces. The mapping /
is said to be continuous at XQ € S if, for every e > 0, there exists a 8 > 0 such
```
that /(#) € AT€(/(xo)) for every x 6 N$(XQ}. If / is continuous at every point
```
of 5 it is said to be continuous everywhere or, simply, continuous.
A.3 Matrices
```
An (m x n) matrix A is a rectangular array of numbers, called elements or
```
entries, arranged in m rows and n columns. The element in the ith row and
jth column of A is denoted by a subscript r?, such as a^ or [A]ij, in which case
```
we write A = {a^}. A matrix is said to be square if it has the same number of
```
```
rows and columns; an (n x n) square matrix A is said to be an identity matrix
```
```
if an = 1, i = 1 , . . . , n, and aij — 0, i ^ j, i, j = 1,..., n. An (n x n) identity
```
matrix will be denoted by In or, simply, by / whenever its dimension is clear
from the context.
```
The transpose of an (m x n) matrix A is the (n x m) matrix A' with elementsa
```
```
ij = aji- A square matrix A is symmetric if A = A!\ it is nonsingular if
```
```
there is an (n x n) matrix, called the inverse of A, denoted by A~l, such that
```
A~1A = I = AA~1.
Eigenvalues and quadratic forms
If A is a square matrix, a scalar A and a nonzero vector x satisfying the equation
```
Ax = Xx are said to be, respectively, an eigenvalue and an eigenvector of A.
```
```
A square symmetric matrix A whose eigenvalues are all positive (respectively,
```
```
nonnegative) is said to be positive definite (respectively, nonnegative definite or
```
```
positive semidefinite). An equivalent definition is as follows. A symmetric (nxn)
```
```
matrix A is said to be positive definite (respectively, nonnegative definite) if
```
```
x'Ax > 0 (respectively, x'Ax > 0) for all nonzero x € Rn. The matrix A is said
```
```
to be negative definite (respectively, nonpositive definite] if the matrix (—A)
```
```
is positive (respectively, nonnegative) definite. We symbolically write A > 0
```
```
(respectively, A > 0) to denote that A is positive (respectively, nonnegative)
```
definite.
A.4 Convex Sets and Functionals
A subset C of a vector space 5" is said to be convex if for every u, v G C and
```
every a £ [0,1], we have au + (l — a)v € C. A functional / : C —> R defined over
```
a convex subset C of a vector space S is said to be convex if, for every w, v e C
```
and every scalar a E [0,1], we have /(cm + (1 — a)v) < af(u] + (1 — a}f(v). If
```
```
this is a strict inequality for every a € (0,1), then / is said to be strictly convex.
```
```
The functional / is said to be concave if (—/) is convex, and strictly concave if
```
```
(—/) is strictly convex.
```
```
A functional / : Rn —> R is said to be differentiate if, with x = (xi,..., xn}' €
```
Rn, the partial derivatives of / with respect to the components of x exist, in
```
V/(rr) is called the gradient of / at x and is a row vector. We shall also use the
```
```
notation fx(x) or df(x)/dx to denote the same quantity. If we partition x into
```
two vectors y and z of dimensions n\ and n — ni, respectively, and are interested
only in the partial derivatives of / with respect to the components of y, then
```
we use the notation Vyf(y, z) or df(y, z)/dy to denote this partial gradient.
```
Let g : Rn —>• Rm be a vector-valued function whose components are dif-
```
ferentiable with respect to the components of x € Rn. Then, we say that g(x)
```
```
is differentiable, with the derivative dg(x)/dx being an (m x n) matrix whose
```
```
ijth element is dgi(x)/dxj. (Here <ft denotes the zth component of g.) The
```
```
gradient Vf(x) being a vector, its derivative (which is the second derivative of
```
```
/ : Rn —>• R) will thus be an (n x n) matrix, assuming that f ( x ) is twice contin-
```
uously differentiable in terms of the components of x. This matrix, denoted by
```
V2/(x), is symmetric, and is called the Hessian matrix of f at x. This Hessian
```
matrix is nonnegative definite for all x G Rn if, and only if, / is convex.
Separating hyperplane theorem
```
Given a convex set 5, one can always find a (hyper)plane such that 5 lies on one
```
side of it. This result, which should be intuitive in finite-dimensional spaces, also
holds in infinite-dimensional linear vector spaces, and is referred to as support
theorem or separating hyperplane theorem. Toward giving a precise statement
for the most general case, let us first define what a hyperplane is: A hyperplane
```
H in a linear vector space 5 is a linear variety H (that is, translation of a
```
```
subspace of 5), with the property that H ^ 5, and if V is any linear variety
```
containing H, then either V = S or V — H. Then the separating hyperplane
```
theorem says that if C is a convex set (as a subset of S) with nonempty interior,
```
and x a point in S and not an interior point of C, there is a closed hyperplane
H containing x such that H lies on one side of C. The point x can of course be
chosen as a boundary point of C.
A.5 Optimization of Functionals
Given a functional / : 5 —> R, where 5' is a vector space, and a subset X C S,
by the optimization problem
```
minimize f ( x ) subject to
```
```
we mean the problem of finding an element x* € X (called a minimizing dement
```
or an optimal solution] such that
```
If such an a:* e X exists, then we use the notation x* = arg minxex/(^)-
```
This is sometimes also referred to as a globally minimizing solution, in order
474 T. BA§AR AND G. J. OLSDER
which case we write
```
as the optimal value of the optimization problem. If {f(x) : x G X} is not
```
```
bounded below, i.e., inf^x f ( x ) = —oo, then neither an optimal solution nor
```
an optimal value exists.
An optimization problem which involves maximization instead of minimiza-
tion may be converted into a minimization problem by simply replacing / by
-/. Any optimal solution of this minimization problem is also an optimal solu-
tion for the initial maximization problem, and the optimal value of the latter,
```
denoted supx€JV /(x), is equal to minus the optimal value of the former. If a
```
```
maximizing element x* G X exists, then supx€^ f ( x ) = maxxex f ( x ) — f ( x * ) .
```
Existence of optimal solutions
In the minimization problem formulated above, an optimal solution exists if X is
a finite set, since then there is only a finite number of comparisons to make. If X
```
is not finite, however, existence of an optimal solution is not always guaranteed;
```
it is guaranteed if / is continuous and X is compact—a result known as the
Weierstrass theorem. For the special case when X is finite dimensional, we
should recall that compactness is equivalent to being closed and bounded.
Necessary and sufficient conditions for optimality
Let 5 = Rn, and / : Rn —> R be a differentiable function. If X is an open set,
a first-order necessary condition for an optimal solution to satisfy is
APPENDIX A 475
to differentiate it from the other alternative—a locally minimizing solution. An
element x° G X is called a locally minimizing solution if we can find an e > 0
such that
```
i.e., we compare f ( x ° ) with values of f ( x ) in that part of a certain e-neighborhood
```
of x°, which lies in X.
For a given optimization problem, it is not necessary that an optimal solution
```
exists; an optimal solution will exist if the set of real numbers {f(x) : x G X} is
```
```
bounded below and there exists an x* e X such that inf{/(:r) : x e X} = /(#*),
```
in which case we write
```
If such an x* cannot be found, even though inf{/(a:) : x G -X"} is finite, we
```
```
simply say that an optimal solution does not exist; but we declare the quantity
```
If, in addition, / is twice continuously differentiable on Rn, a second-order
necessary condition is
476 T. BA§AR AND G. J. OLSDER
```
The pair of conditions {V/(o;*) = 0, V2/(,o;*) > 0} is sufficient for x* e X to
```
be a locally minimizing solution. These conditions are also sufficient for global
optimality if, in addition, X is a convex set and / is a convex functional on X.
These results, by and large, hold also for the case when S is infinite dimension-
al, but then one has to replace the gradient vector and the Hessian matrix by
```
first and second Gateaux (or Prechet) derivatives, and the positive-definiteness
```
```
requirement by "strong positiveness" of an operator. See Luenberger (1969) for
```
these extensions.
Appendix B
Some Notions of
Probability Theory
This appendix briefly presents some notions of probability theory which are used
in the text. For more complete exposition the reader should consult standard
```
texts on probability theory, such as Feller (1971), Papoulis (1965), Ash (1972)
```
```
and Loeve (1963). In two of the sections of this book (viz. Sections 5.3 and
```
```
6.7) we have used some material on stochastic processes and stochastic differ-
```
```
ential equations which is, however, not covered in this appendix; for this, the
```
```
reader should consult Wong and Hajek (1985), Gikhman and Skorohod (1972),
```
```
Fleming and Rishel (1975), Fleming and Soner (1993) and the references cited
```
in Section 6.7.
B.I Ingredients of Probability Theory
Let 0 denote a set whose elements are the outcomes of a random experiment.
```
This random experiment might be the toss of a coin (in which case f2 has only
```
```
two elements) or selection of an integer from the set [0, oo) (in which case £1
```
```
is countably infinite) or the continuous roulette wheel which corresponds to a
```
nondenumerable f£. Any subset of 0 on which a probability measure can be
defined is known as an event. Specifically, if F denotes the class of all such
```
events (i.e., subsets of 17), then it has the following properties:
```
```
(1) Q e F .
```
```
(2) If A 6 F, then its complement Ac — {u> e £7 : u; ^ ^4} also belongs to F.
```
```
(The empty set, 0, being the complement of 17, also belongs to F.)
```
```
(3) If A\,Ai e F, then A\ n A2 and A\ U A2 also belong to F.
```
```
(4) If A\, A-2,..., A i , . . . denote a countable number of events, the countable
```
```
intersection r\^lAi and the countable union U^A; are also events (i.e.,
```
```
they belong to F).
```
477
```
The triple (O,F,"P) defined above is known as a probability space, while the
```
```
pair (O, F) is called a measurable space. If fi = Rn, then its subsets of interest
```
are the n-dimensional rectangles, and the smallest cr-algebra generated by these
rectangles is called the n-dimensional Borel cr-algebra and is denoted Bn. Ele-
```
ments of Bn are Borel sets, and the pair (Rn, Bn) is a Borel (measurable) space.
```
A probability measure defined on this space is known as a Borel probability
measure.
Finite and countable probability spaces
```
If Q is a finite set (say, Q = {uji,u;2,...,cjn}), we can assign probability weights
```
on individual elements of Ci, instead of on subsets of fi, in which case we
write Pi to denote the probability of the single event u^. We call the n-tuple
```
(p\iP2i- • • iPn) a probability distribution over O. Clearly, we have the restriction
```
0 < p i < l V i = l , . . . , n , and furthermore as the elements of fi are all disjoint
events, we have the property Y^i=iPi = 1- The same convention applies when
```
Jl is a countable set (i.e., fi = {wi, 0^2, • • • ,^t, • • •})> in which case we simply
```
replace n by oo.
B.2 Random Vectors
```
Let (Qi, FI) and (f^ F2) be two measurable spaces and / be a function defined
```
from the domain set Q.\ into the range set ^2- If for every A G F2 we have
```
f~l(A) = {uj € D! : f((jj} 6 A} 6 FI, then / is said to be a measurable
```
```
function, or a measurable transformation from (fii,Fi) into (^2^2). If the
```
latter measurable space is a Borel space, then / is said to be a Borel function,
in which case we denote it by x. For the special case when the Borel space is
```
(fJ2> ^2) = (R> B), the Borel function x is called a random variable. For the case
```
```
when (^2)^2) = (Rn)Bn), x is known as an n-dimensional random vector.
```
```
If there is a probability measure P defined on (f2i,Fi)—which we hence-
```
```
forth write simply as (0,, F)—then the random vector x will induce a probabil-
```
```
ity measure Px on the Borel space (Rn,Bn), so that for every B £ Bn we have
```
```
PX(B] = P(x~l(B}}. Since every element of Bn is an n-dimensional rectan-
```
```
gle, the arguments of Px are in general infinite sets; however, considering the
```
478 T. BA§AR AND G. J. OLSDER
```
The class F, thus defined, is called a sigma algebra (a-algebra] and a probability
```
measure P is a nonnegative functional defined on the elements of this <j-algebra.
The probability measure P also satisfies the following axioms:
```
(1) For every event
```
```
(3) Let {Ai} denote a (countably) infinite sequence in F, with the properties
```
Ai+i C Ai and n^jAj = 0. Then, the limit of the sequence of real
```
numbers {P(Ai}} is zero (i.e., linii-.oo P(Ai) = 0).
```
Probability density function
A measure defined on subintervals of the real line and which equals the length
```
of the corresponding subinterval(s) is called a Lebesgue measure. It assigns zero
```
weight to countable subsets of the real line, and its definition can readily be
extended to n-dimensional rectangles in Rn.
```
Let P be a Borel probability measure on (Rn, Bn) such that any element of
```
```
Bn which has a Lebesgue measure of zero has also a "P-measure of zero; then
```
we say that P is absolutely continuous with respect to the Lebesgue measure.
```
Now, a well-established result of probability theory says that (cf. Loeve, 1963),
```
```
if x : (fi,F,P) —> (Rn,~Bn,Px) is a random vector and if Px is absolutely
```
continuous with respect to the Lebesgue measure, there exists a nonnegative
```
Borel function px(-) such that, for every A e Bn,
```
APPENDIX B 479
```
collection of sets {£ 6 Rn : & < c^, i = 1 , . . . , n} in Bn where ai (i = 1 , . . . , n)
```
are real numbers, restriction of Px to this class is also a probability measure
whose argument is now a finite set. We denote this probability measure by
```
PX — Px(fli)a 2, • • • ,ctn) and call it a probability distribution function of the
```
random vector x. Note that
where Xj is a random variable denoting the ith component of x. Whenever
```
n > 1, Px is sometimes also called the cumulative (joint) probability distribution
```
function. It is a well-established fact that there is a one-to-one correspondence
between Px and Px and the subspace on which Px is defined can generate the
```
whole Bn (cf. Loeve, 1963).
```
Independence
```
Given the probability distribution function of a random vector x = ( x i , . . . , xn)'
```
```
the (marginal) distribution function of each random variable Xi can be obtained
```
from
```
The random variables x i , . . . ,xn are said to be (statistically) independent if
```
for all scalars a i , . . . , an.
```
Such a function px(-) is called the probability density function of the random
```
vector x. In terms of the distribution function Px, the preceding relation can
be written as
for every scalar a i , . . . , an.
480 T. BA§AR AND G. J. OLSDER
B.3 Integrals and Expectation
```
Let x : (fi,F,P) -> (R^B",^) be a random vector and / : (Rn,Bn) -»
```
```
(R™,Bm) be a nonnegative Borel function. Then, / can also be considered as
```
```
a random vector from (£2,F) into (Rm,Bm), and its average value (expected
```
```
value) is defined either by fnf(x(u})P(du>)or by Jpn f(£)Px(d£)depending
```
on which interpretation we adopt. Both of these integrals are well defined and
are uniquely equal in value. If / changes signs, then we take f = f+ — f~ where
both /+ and /" are nonnegative, and write the expected value of / as
which is a Lebesgue-Stieltjes integral and which is the convention that we shall
```
adopt. For the special case when f(x) = x we have
```
```
If fi consists only of a finite number of disjoint events u;i,u;2,... ,u>n, then the
```
integrals are all replaced by the single summation
where pi denotes the probability of occurrence of event o^. For a countable set
fi, we have the counterpart:
```
provided that at least one of the pair E[f+(x}] and E[f (x)] is finite. Since, by
```
```
definition, Px(d£) = PX(£ + d£) - Px(£], this integral can further be written as
```
```
which is known as the mean (expected) value of x. The covariance of the n-
```
dimensional random vector x is defined as
```
which is a nonnegative definite matrix of dimension (n x n). Now, if Px is abso-
```
lutely continuous with respect to the Lebesgue measure, E[f] can equivalently
be written, in terms of the corresponding density function px, as
APPENDIX B 481
B.4 Norms and the Cauchy—Schwarz Inequality
```
Given a random vector x : (Q,F,P) —> (Rn,Bn,"Px), the quantity defined by
```
```
provided that it is finite, is called the (£2) norm of x, and indeed satisfies
```
all the properties of a norm introduced in the previous appendix. A random
vector with a finite £2-norm is called a second-order random vector. Note that
a second-order random vector has a well-defined covariance.
If x and y are two n-dimensional second-order random vectors defined on
the same probability space, then we have the following useful inequality, known
as the Cauchy-Schwarz inequality:
The inequality here is an equality if, and only if, x = Ay, w.p. 1 for some scalar
A, or y — 0 w.p. 1.
This page intentionally left blank
Appendix C
Fixed Point Theorems
```
In this appendix we give, without proof, three theorems (on fixed points) which
```
have been used in Chapters 3 and 4 of the text. Proofs of these theorems are
rather lengthy and can be found in the references cited. Some general references
```
for these as well as other results on fixed points are (Istratescu, 1981), (Joshi
```
```
and Bose, 1985), and (Smart, 1974).
```
Fixed point theorems
```
Theorem C.I (Brouwer fixed point theorem) If S is a compact and convex
```
subset of Rn and f is a continuous function mapping S into itself, then there
```
exists at least one x £ S such that f(x] = x.
```
Several proofs exist for this fixed point theorem, one of the most elementary
```
ones being given by Kuga (1974). Its original version has appeared in 1910
```
```
(Brouwer, 1910). A generalization of this theorem is the Kakutani fixed point
```
```
theorem (Kakutani, 1941) given below.
```
```
Definition C.I (Upper semicontinuity) Let f be a function defined on a normed
```
```
linear space X, and associating with each x € X a subset f ( x ) of some (other)
```
```
normed linear space Y. Then, f is said to be upper semicontinuous (use) at
```
```
a point XQ € -X" if, for any sequence {xi} converging to XQ and any sequence
```
```
{yi € f ( x i } } converging to yo, we have yo € /(^o)- The function f is upper
```
semicontinuous if it is use at each point of X.
```
Theorem C.2 (Kakutani) Let S be a compact and convex subset o/Rn, and
```
let f be an upper semicontinuous function which assigns to each x €S a closed
```
and convex subset of S. Then there exists some x G 5 such that x £ /(£)•
```
Another generalization of the Brouwer fixed point theorem is due to Schauder
```
(1930), and involves the space of real-valued bounded continuous functions on
```
```
a subset S of Rn, to be denoted C(S}. Here, we will first need the definition of
```
```
equicontinuity:
```
483
```
Theorem C.3 (Schauder) Let S be a bounded subset of~Rn, and let C(S] be
```
the space of real-valued bounded continuous functions on S, endowed with the
```
sup norm. Let F C C(S) be nonempty, closed, bounded and convex. Then if
```
```
the mapping T : F —> F is continuous and the family T(F) is equicontinuous,
```
T has a fixed point in F.
```
A proof of this theorem can be found in (Istratescu, 1981) and (Hutson
```
```
and Pym, 1980). This is a special case of a more general theorem (also due
```
```
to Schauder) which says that "every real-valued continuous function mapping a
```
convex compact subspace of a Banach space into itself has a fixed point." The
```
space C(S) defined above is indeed one such space, and equicontinuity assures
```
```
that convex compact subsets of C(S) are mapped into themselves.
```
For interesting discussions on the Brouwer fixed point theorem and its vari-
```
ous extensions, see Franklin (1980).
```
484 T. BA§AR AND G. J. OLSDER
```
Definition C.2 (Equicontinuity) Let the space of all real-valued bounded con-
```
```
tinuous functions on S, denoted C(S), be endowed with the sup norm. A subset
```
```
F of C(S] is equicontinuous if for every e > 0 there exists a 6 > 0 such that
```
Bibliography
S. ALPERN, Games with repeated decisions, SI AM Journal on Control and
```
Optimization, 26 (1988), pp. 468-477.
```
S. ALPERN, Cycles in extensive form perfect information games. Journal of
```
Mathematical Analysis and Applications, 159 (1991), pp. 1-17.
```
B. ANDERSON AND J. B. MOORE, Optimal Control, Prentice-Hall, Englewood
Cliffs, NJ, 1989.
R. B. ASH, Real Analysis and Probability, Academic Press, New York, NY,
1972.
J. AUBIN, Mathematical Methods of Game and Economic Theory, North- Hol-
land, Amsterdam, The Netherlands, 1980.
R. J. AUMANN, Borel structures for function spaces, Illinois Journal of Math-
```
ematics, 5 (1961), pp. 614-630.
```
, Mixed and behavior strategies in infinite extensive games, in Advances in
Game Theory, M. Dresher, L. S. Shapley, and A. W. Tucker, eds., Princeton
University Press, Princeton, NJ, 1964, pp. 627-650.
R. J. AUMANN AND M. MASCHLER, Some thoughts on the minimax principle,
```
Management Science, 18 (1972), pp. 54-63.
```
A. BAGCHI AND G. J. OLSDER, Linear stochastic pursuit evasion games,
```
Journal of Applied Mathematics and Optimization, 7 (1981), pp. 95-123.
```
A. V. BALAKRISHNAN, Applied Functional Analysis, Springer-Verlag, New
York, NY, 1976.
R. B. BAPAT AND T. RAGHAVAN, Nonnegative Matrices and Applications,
Cambridge University Press, Cambridge, England, 1997.
M. BARDI, A boundary value problem for the minimum-time function, SIAM
J. Control and Optimization, 27 (1989), pp. 776-785.
M. BARDI AND C. SARTORI, Convergence results for Hamilton-Jacobi equa-
```
tions in variable domains, Differential and Integral Equations, 5 (1992),
```
pp. 805-816.
M. BARDI AND P. SORAVIA, Hamilton-Jacobi equations with singular bound-
ary conditions on a free boundary and applications to differential games,
485
al
486 BIBLIOGRAPHY
```
Transactions of the American Mathematical Society, 325 (1991), pp. 205-
```
229.
M. BARDI AND V. STAICU, The Bellman equation for time-optimal control of
non-controllable nonlinear systems, Tech. Report SISSA 132/91/M, SISSA-
ISAS, Strada Costiera 11, 34014 Trieste, Italy, 1991.
E. N. BARRON, L. C. EVANS, AND R. JENSEN, Viscosity solutions of Isaacs'
equations and differential games with Lipschitz controls, Journal of Differ-
```
ential Equations, 53 (1984), pp. 213-233.
```
T. BA§AR, On the relative leadership property of Stackelberg strategies, Journal
```
of Optimization Theory and Applications, 11 (1973), pp. 655-661.
```
A counter example in linear-quadratic games: Existence of non-linear
Nash solutions, Journal of Optimization Theory and Applications, 14
```
(1974), pp. 425-430.
```
Nash strategies for M-person differential games with mixed information
```
structures, Automatica, 11 (1975), pp. 547-551.
```
On the uniqueness of the Nash solution in linear-quadratic differential
```
games, International Journal of Game Theory, 5 (1976a), pp. 65-90.
```
, Some thoughts on saddle-point conditions and information structures in
zero-sum differential games, Journal of Optimization Theory and Applica-
```
tions, 18 (1976b), pp. 165-170.
```
Existence of unique equilibrium solutions in nonzero-sum stochastic dif-
ferential games, in Differential Games and Control Theory II, E. O. Roxin,
P. T. Liu, and R. Sternberg, eds., Marcel Dekker, Inc., 1977a, pp. 201-228.
Informationally nonunique equilibrium solutions in differential games,
```
SIAM Journal on Control and Optimization, 15 (1977b), pp. 636-660.
```
Multicriteria optimization of linear deterministic and stochastic sys-
```
tems: A survey and some new results. Marmara Research Institute Publica-
```
tion, Applied Mathematics Division, no. 37,Gebze Kocaeli, Turkey, 1977c.
Two general properties of the saddle-point solutions of dynamic games,
```
IEEE Transactions on Automatic Control, AC-22 (1977d), pp. 124-126.
```
-, Decentralized multicriteria optimization of linear stochastic systems,
```
IEEE Transactions on Automatic Control, AC-23 (1978), pp. 233-243.
```
Hierarchical decision making under uncertainty, in Dynamic Optimiza-
tion and Mathematical Economics, P. T. Liu, ed., Plenum Press, New York,
NY, 1979a, pp. 205-221.
Information structures and equilibria in dynamic games, in New Trends
in Dynamic System Theory and Economics, M. Aoki and A. Marzollo, eds.,
Academic Press, New York and London, 1979b, pp. 3-55.
Stochastic stagewise Stackelberg strategies for linear-quadratic sys-
tems, in Stochastic Control Theory and Stochastic Differential Systems,
M. Kohlmann and W. Vogel, eds., Lecture Notes in Control and Informa-
tion Sciences, Springer-Verlag, New York, 1979c, ch. 16, pp. 264-276.
BIBLIOGRAPHY 487
Memory strategies and a general theory for Stackelberg games with par-
tial dynamic information, in Proceedings of the 4th International Con-
ference on the Analysis and Optimization of Systems, Versailles, France,
December 1980a, Springer-Verlag, New York, pp. 397-415.
On the existence and uniqueness of closed-loop sampled-data Nash con-
trols in linear-quadratic stochastic differential games, in Optimization Tech-
niques, K. Iracki et al., eds., Lecture Notes in Control and Information
Sciences, Springer-Verlag, New York, 1980b, ch. 22, pp. 193-203.
Equilibrium strategies in dynamic games with multi-levels of hierarchy,
```
Automatica, 17 (1981a), pp. 749-754.
```
, A new method for the Stackelberg solution of differential games with
sampled-data state information, in Proceedings of the 8th IFAC World
Congress, Kyoto, Japan, Pegamon Press, New York, August 1981b,
pp. 139-144.
On the saddle-point solution of a class of stochastic differential games,
```
Journal of Optimization Theory and Applications, 33 (1981c), pp. 539-556.
```
A general theory for Stackelberg games with partial state information,
```
Large Scale Systems, 3 (1982), pp. 47-56.
```
Affine incentive schemes for stochastic systems with dynamic informa-
```
tion, SIAM Journal on Scientific and Statistical Computing, 22 (1984),
```
pp. 199-210.
ed., Dynamic Games and Applications in Economics, vol. 265 of Lecture
Notes in Economics and Mathematical Systems, Springer-Verlag, Berlin,
1986a.
, A tutorial on dynamic and differential games, in Dynamic Games and
Applications in Economics, T. Ba§ar, ed., vol. 265 of Lecture Notes in
Economics and Mathematical Systems, Springer-Verlag, New York, 1986b,
pp. 1-25.
Relaxation techniques and asynchronous algorithms for on-line com-
putation of noncooperative equilibria, Journal of Economic Dynamics and
```
Control, 71 (1987), pp. 531-549.
```
Stochastic incentive problems with partial dynamic information and mul-
```
tiple levels of hierarchy, European J. Political Economy, 5 (1989a), pp.
```
203-217.
Time consistency and robustness of equilibria in noncooperative dynamic
games, in Dynamic Policy Games in Economics, F. Van der Ploeg and A. de
Zeeuw, eds., North-Holland, Amsterdam, 1989b, pp. 9-54.
A dynamic games approach to controller design: Disturbance rejection
```
in discrete time, IEEE Transactions on Automatic Control, AC-36 (1991 a),
```
pp. 936-952.
488 BIBLIOGRAPHY
Generalized Riccati equations in dynamic games, in The Riccati Equa-
tion, S. Bittanti, A. Laub, and J. C. Willems, eds., Springer-Verlag, Berlin,
1991b, pp. 293-333.
On the application of differential game theory in robust control design
for economic systems, in Dynamic Economic Models and Optimal Control,
G. Feichtinger, ed., North-Holland, Amsterdam, 1992, pp. 171-186.
T. BA§AR AND P. BERNHARD, eds., Differential Games and Applications,
Lecture Notes in Control and Information Sciences, vol. 119, Springer-
Verlag, Berlin, 1989.
H°°- Optimal Control and Related Minimax Design Problems: A Dy-
namic Game Approach, Birkhauser, Boston, MA, 2nd ed., 1995.
T. BA§AR AND A. HAURIE, Feedback equilibria in differential games with
structural and modal uncertainties, vol. 1 of Advances in Large Scale Sys-
tems, J. B. Cruz, Jr., ed. JAI Press Inc., Connecticut, 1984, pp. 163-201.
T. BA§AR AND A. HAURIE, eds., Advances in Dynamic Games and Applica-
tions, Birkhauser, Boston, MA, 1994.
T. BA§AR, A. HAURIE, AND G. Ricci, On the dominance of capitalists lead-
ership in a feedback Stackelberg solution of a differential game model of
```
capitalism, Journal of Economic Dynamics and Control, 9 (1985), pp. 101-
```
125.
T. BA§AR AND Y. C. Ho, Informational properties of the Nash solutions of
```
two stochastic nonzero-sum games, Journal of Economic Theory, 7 (1974),
```
pp. 370-387.
T. BA§AR AND S. Li, Distributed algorithms for the computation of Nash
equilibria in linear stochastic differential games, SIAM Journal on Control
```
and Optimization, 27 (1989), pp. 563-578.
```
T. BA§AR AND M. MINTZ, On the existence of linear saddle-point strategies
for a two-person zero-sum stochastic game, in Proceedings of the IEEE
llth Conference on Decision and Control, New Orleans, LA, 1972, IEEE
Computer Society Press, Los Alamitos, CA, pp. 188-192.
A multistage pursuit-evasion game that admits a Gaussian random pro-
```
cess as a maximum control policy, Stochastics, 1 (1973), pp. 25-69.
```
T. BA§AR AND G. J. OLSDER, Mixed Stackelberg strategies in continuous-
```
kernel games, IEEE Transactions on Automatic Control, AC-25 (1980a),
```
pp. 307-309.
, Team-optimal closed-loop Stackelberg strategies in hierarchical control
```
problems, Automatica, 16 (1980b), pp. 409-414.
```
-, Dynamic Noncooperative Game Theory, Academic Press, London/New
```
York, 1982. (Second printing 1989.)
```
T. BA§AR AND H. SELBUZ, Properties of Nash solutions of a 2-stage nonzero-
```
sum game, IEEE Transactions on Automatic Control, AC-21 (1976),
```
pp. 48-54.
BIBLIOGRAPHY 489
Closed-loop Stackelberg strategies with applications in the optimal control
of multilevel systems, IEEE Transactions on Automatic Control, AC-24
```
(1979a), pp. 166-179.
```
A new approach for derivation of closed-loop Stackelberg strategies, in
Proceedings of the IEEE 17th Conference on Decision and Control, San
Diego, CA, January 1979b, IEEE Computer Society Press, Los Alamitos,
CA, pp. 1113-1118.
T. BA§AR, S. J. TURNOVSKY, AND V. D'OREY, Optimal macroeconomic
policies in interdependent economies: A strategic approach, in Dynamic
Games and Applications in Economics, T. Ba§ar, ed., Lecture Notes in Eco-
nomics and Mathematical Systems, volume 265, Springer-Verlag, Berlin,
1986, pp. 134-178.
R. BELLMAN, Dynamic Programming, Princeton University Press, Princeton,
NJ, 1957.
, Introduction to Matrix Analysis, McGraw-Hill, New York, NY, 2nd ed.,
1970.
A. BENSOUSSAN, Saddle points of convex concave functionals, in Differential
Games and Related Topics, H. W. Kuhn and G. P. Szego, eds., North-
Holland, Amsterdam, The Netherlands, 1971, pp. 177-200.
Points de Nash dans le cas de fonctionelles quadratiques et jeux differen-
tiels lineaires a N personnes, SIAM Journal on Control and Optimization,
```
12 (1974), pp. 237-243.
```
-, Perturbation Methods in Optimal Control, John Wiley, Gauthier-Villars,
Chichester, England, 1988.
L. D. BERKOVITZ, A variational approach to differential games, in Advances in
Game Theory, M. Dresher, L. S. Shapley, and A. W. Tucker, eds., Princeton
University Press, Princeton, NJ, 1964, pp. 127-174.
Optimal Control Theory, Springer-Verlag, Berlin, 1974.
P. BERNHARD, Condition de coin pour les jeux differentiels. Seminaire des 21-
25 Juin, Les Jeux Differentiels, Centre d'Automatique de 1'Ecole Nationale
Superieure de Mines de Paris, 1971.
Singular surfaces in differential games: an introduction, in Differential
Games and Applications, P. Hagedorn, H. W. Knobloch, and G. J. Olsder,
eds., Springer-Verlag, Berlin, 1977, pp. 1-33. Lecture Notes in Control and
Information Sciences, vol. 3.
-, Linear-quadratic two-person zero-sum differential games: Necessary and
sufficient conditions, Journal of Optimization Theory and Applications, 27
```
(1979), pp. 51-69.
```
D. P. BERTSEKAS, Dynamic Programming: Deterministic and Stochastic
Models, Prentice-Hall, Englewood Cliffs, NJ, 1987.
D. P. BERTSEKAS AND J. N. TSITSIKLIS, Parallel and Distributed Computa-
```
tion: Numerical Methods, Prentice-Hall, Englewood Cliffs, NJ, 1989.
```
490 BIBLIOGRAPHY
D. P. BERTSEKAS AND J. N. TSITSIKLIS, Convergence rate and termination
of asynchronous interative algorithms, in Proceedings of the 1989 Interna-
tional Conference on Supercomputing, Irakleion, Greece, 1989, pp. 561-470.
Some aspects of parallel and distributed iterative algorithms - a survey,
```
Automatica, 27 (1991), pp. 3-21.
```
K. BlNMORE, Fun and Games, D. C. Heath and Company, Lexington, MA,
1992.
D. BLACKWELL AND M. A. GIRSHICK, Theory of Games and Statistical De-
cisions, John Wiley and Sons, New York, NY, 1954.
A. BLAQUIERE, ed., Topics in Differential Games, North-Holland, Amster-
dam, The Netherlands, 1973.
Une generalisation du concept d'optimalite et des certaines notions
geometriques qui's rattachent Institute des Hautes Etudes de Belgique,
Cahiers du centre d'etudes de recherche operationelle, vol. 18, no. 1-2,
Bruxelles, pp. 49-61, 1976.
-, Differential games with piece-wise continuous trajectories, in Differential
Games and Applications, P. Hagedorn, H. W. Knobloch, and G. J. Olsder,
eds., Springer-Verlag, Berlin, 1977, ch. 3, pp. 34-69.
A. BLAQUIERE, F. GERARD, AND G. LEITMANN, Quantitative and Qualita-
tive Games, Academic Press, New York and London, 1969.
V. G. BOLTYANSKI, Optimal Control of Discrete Systems, Hallsted Press, John
Wiley, New York, NY, 1978.
E. BOREL, The theory of play and integral equations with skew symmetrical
```
kernels: On games that involve chance and skill of the players, On systems
```
of linear forms of skew symmetric determinants and the general theory of
```
play, trans, by L. J. Savage, Econometrica, 21 (1953), pp. 97-117.
```
D. BRAESS, Uber eine Paradoxen aus der Verkehersplanung, Un-
```
ternehmensforschung, 12 (1968), pp. 258-268.
```
J. V. BREAKWELL, Examples elementaires. Seminaire des 21-25 Juin, Les
Jeux Differentiels, Centre d'Automatique de 1'Ecole Nationale Superieure
des Mines de Paris, 1971.
Some differential games with interesting discontinuities. Internal Re-
port, Stanford University, Stanford, CA, 1973.
Zero-sum differential games with terminal payoff, in Differential Games
and Applications, P. Hagedorn, H. W. Knobloch, and G. J. Olsder, eds.,
Springer-Verlag, Berlin, 1977, ch. 3, pp. 70-95.
T. F. BRESNAHAN, Duopoly models with consistent conjectures, American Eco-
```
nomic Review, 71 (1981), pp. 934-945.
```
R. W. BROCKETT, Finite Dimensional Linear Systems, John Wiley and Sons,
New York, NY, 1970.
BIBLIOGRAPHY 491
L. E. J. BROUWER, Uber abbildung von mannigfaltigkeiten, Math. Annalen.,
```
71 (1910), pp. 97-115.
```
G. W. BROWN, Iterative solutions of games by fictitious play, in Activity
Analysis of Production and Allocation, T. C. Koopmans, ed., John Wiley
and Sons, New York, NY, 1951, ch. 13, pp. 374-376. Cowles Commission
Monograph.
G. W. BROWN AND J. VON NEUMANN, Solutions of games by differential
equations, in Contributions to the Theory of Games, H. W. Kuhn and A. W.
Tucker, eds., Princeton University Press, Princeton, NJ, 1950, pp. 73-79.
Vol. I, no. 24.
A. E. BRYSON, JR. AND Y. C. Ho, Applied Optimal Control, Hemisphere,
Washington, DC, 1975.
E. BURGER, Einfuhrung in die Theorie der Spiele, (2 Auflage), Walter de
Gruyter and Company, Berlin, 1966.
M. D. CANON, C. D. CULLUM, JR., AND E. POLAK, Theory of Optimal
Control and Programming, McGraw-Hill, New York, NY, 1970.
D. H. CANSEVER AND T. BA§AR, A minimum sensitivity approach to incen-
```
tive design problems, Large Scale Systems, 5 (1983), pp. 233-244.
```
, On stochastic incentive control problems with partial dynamic informa-
```
tion, Systems &; Control Letters, 6 (1985a), pp. 69-75.
```
Optimum/near optimum incentive policies for stochastic decision prob-
```
lems involving parametric uncertainty, Automatica, 21 (1985b), pp. 575-
```
584.
J. H. CASE, Equilibrium Points of N-person differential games, PhD thesis,
University of Michigan, Ann Arbor, MI, 1967. Department of Industrial
Engineering, Tech. report no. 1967-1.
Toward a theory of many player differential games, SIAM Journal on
```
Control and Optimization, 7 (1969), pp. 179-197.
```
Applications of the theory of differential games to economic problems,
in Differential Games and Related Topics, H. W. Kuhn and G. P. Szego,
eds., North-Holland, Amsterdam, The Netherlands, 1971, pp. 345-371.
, Economics and the Competitive Process, New York University Press,
New York, NY, 1979.
D. A. CASTANON, Equilibria in Stochastic Dynamic Games of Stackelberg
Type, PhD thesis, M.I.T. Electronics Systems Laboratory, Cambridge, MA,
1976.
D. CHAZAN AND W. MIRANKER, Chaotic relaxation, Linear Algebra and Ap-
```
plications, 2 (1969), pp. 199-222.
```
C. I. CHEN AND J. B. CRUZ JR., Stackelberg solution for two-person games
with biased information patterns, IEEE Transactions on Automatic Con-
```
trol, AC-17 (1972), pp. 791-798.
```
492 BIBLIOGRAPHY
S. A. CHIGIR, The game problem on the dolichobrachistochrone, PMM, 30
```
(1976), pp. 1003-1013.
```
M. D. ClLETTI, On the contradiction of the bang-bang-bang surfaces in differ-
```
ential games, Journal of Optimization Theory and Applications, 5 (1970),
```
pp. 163-169.
S. CLEMHOUT, G. LEITMANN, AND H. Y. WAN, JR., A differential game
```
model of oligopoly, Journal of Cybernetics, 3 (1973), pp. 24-39.
```
E. A. CODDINGTON AND N. LEVINSON, Theory of Ordinary Differential Equa-
tions, McGraw-Hill, New York, NY, 1955.
J. E. COHEN, The counterintuitive in conflict and cooperation, American Sci-
```
entist, (1988).
```
J. E. COHEN AND P. HOROWITZ, Paradoxal behaviour of mechanical and
```
electrical networks, Nature, 352 (1991), pp. 699-701.
```
A. COURNOT, Recherches sur les principes mathematigues de la theorie des
```
richesses. Hachette, Paris. (English edition published in 1960 by Kelley,
```
New York, under the title 'Researches into the Mathematical Principles of
the Theory of Wealth', trans, by N. T. Bacon, 1838.
M. CRANDALL AND P. LIONS, Viscosity solutions of Hamilton-Jacobi equa-
```
tions, Transactions of the American Mathematical Society, 277 (1983),
```
pp. 1-42.
M. G. CRANDALL, L. C. EVANS, AND P. L. LIONS, Some properties of viscos-
ity solutions of Hamilton-Jacobi equations, Transactions of the American
```
Mathematical Society, 282 (1984), pp. 487-502.
```
J. B. CRUZ, JR., Leader-follower strategies for multilevel systems, IEEE Trans-
```
actions on Automatic Control, AC-23 (1978), pp. 244-255.
```
G. B. DANTZIG, Linear Programming and Extensions, Princeton University
Press, Princeton, NJ, 1963.
M. DAVIS, On the existence of optimal policies in stochastic control, SIAM
```
Journal on Control and Optimization, 11 (1973), pp. 587-594.
```
M. DAVIS AND P. P. VARAIYA, Dynamic programming conditions for partially
observable stochastic systems, SIAM Journal on Control and Optimization,
```
11 (1973), pp. 226-261.
```
M. A. DAVIS, Linear Estimation and Stochastic Control, Chapman and Hall,
London, 1977.
E. DOCKNER AND G. FEICHTINGER, Dynamic advertising and pricing in an
```
oligopoly: A Nash equilibrium approach, in Dynamic Games and Applica-
```
tions in Economics, T. Ba§ar, ed., Lecture Notes in Economics and Math-
ematical Systems, volume 265, Springer-Verlag, Berlin, 1986, pp. 238-251.
J. DOYLE, K. GLOVER, P. KHARGONEKAR, AND B. FRANCIS, State-space
solutions to standard HZ and HQQ control problems, IEEE Transactions on
```
Automatic Control, AC-34 (1989), pp. 831-847.
```
BIBLIOGRAPHY 493
P. DUBEY AND M. SHUBIK, Information conditions, communications and gen-
```
eral equilibrium, Mathematics of Operations Research, 6 (1981), pp. 186-
```
189.
F. Y. EDGEWORTH, Paper Relating to Political Economy, vol. 1, Macmillan,
London, 1925.
A. EHRENFEUCHT AND J. MYCIELSKI, Positional strategies for mean payoff
```
games, International Journal of Game Theory, 8 (1979), pp. 109-113.
```
T. ElSELE, Nonexistence and nonuniqueness of open-loop equilibria in linear-
quadratic differential games, Journal of Optimization Theory and Applica-
```
tions, 37 (1982), pp. 443-468.
```
R. J. ELLIOTT, The existence of value in stochastic differential games, SIAM
```
Journal on Control and Optimization, 14 (1976), pp. 85-94.
```
Feedback strategies in deterministic differential games, in Differential
Games and Applications, P. Hagedorn, H. W. Knobloch, and G. J. Olsder,
eds., Springer-Verlag, Berlin, 1977, pp. 136-142. Lecture Notes in Control
and Information Sciences, vol. 3.
-, Viscosity Solutions and Optimal Control, Longman Scientific and Tech-
nical, Wiley, New York, NY, 1987.
R. J. ELLIOTT AND N. J. KALTON, The existence of value in differential
```
games of pursuit and evasion, Journal of Differential Equations, 12 (1972),
```
pp. 504-523.
R. J. ELLIOTT, N. J. KALTON, AND L. MARKUS, Saddle-points for linear
```
differential games, SIAM Journal on Control and Optimization, 11 (1973),
```
pp. 100-112.
J. C. ENGWERDA, On the open-loop Nash equilibrium in LQ-games, Journal
```
of Economic Dynamics and Control, 22 (1998), pp. 729-762.
```
R. A. EPSTEIN, The Theory of Gambling and Statistical Logic, Academic
Press, New York and London, 1967.
L. EVANS AND P. SOUGANIDIS, Differential games and representation formu-
las for solutions of Hamilton-Jacobi-Isaacs equations, Indiana Univ. Math.
```
J., 33 (1984), pp. 773-797.
```
K. FAN, Minimax theorems, Proceedings of the Academy of Sciences, 39
```
(1953), pp. 42-47.
```
W. FELLER, An Introduction to Probability Theory and Its Applications, vol. 2,
John Wiley and Sons, New York, NY, 1971.
W. H. FLEMING, The convergence problem for differential games, Journal of
```
Mathematical Analysis and Applications, 3 (1961), pp. 102-116.
```
, The convergence problem for differential games, in Advances in Game
Theory, M. Dresher, L. S. Shapley, and A. W. Tucker, eds., Princeton
University Press, Princeton, NJ, 1964, pp. 195-210. Annals of Mathematics
Studies, no. 52.
494 BIBLIOGRAPHY
Optimal continuous-parameter stochastic control, SIAM Review, 11
```
(1969), pp. 470-509.
```
W. H. FLEMING AND R. W. RISHEL, Deterministic and Stochastic Optimal
Control, Springer-Verlag, Berlin, 1975.
W. H. FLEMING AND H. M. SONER, Controlled Markov Processes and Viscos-
ity Solutions, Applications of Mathematics, vol. 25, Springer-Verlag, 1993.
J. G. FOREMAN, The princess and the monster on the circle, SIAM Journal
```
on Control and Optimization, 15 (1977), pp. 841-856.
```
M. Fox AND G. S. KlMELDORF, Noisy duels, SIAM Journal of Applied Math-
```
ematics, 17 (1969), pp. 353-361.
```
B. A. FRANCIS, A Course in H^ Control Theory, vol. 88 of Lecture Notes in
Control and Information Sciences, Springer-Verlag, New York, NY, 1987.
J. FRANKLIN, Methods of Mathematical Economics, Springer-Verlag, Berlin,
1980.
A. FRIEDMAN, Differential Games, John Wiley and Sons, New York, NY,
1971.
Stochastic differential games, Journal of Differential Equations, 11
```
(1972), pp. 79-108.
```
J. W. FRIEDMAN, Oligopoly and the Theory of Games, North-Holland, Ams-
terdam, The Netherlands, 1977.
D. FUDENBERG AND J. TlROLE, Game theory, MIT Press, Cambridge, MA,
1991.
Z. GAJIC, D. PETKOVSKI, AND X. SHEN, Singularly Perturbed and Weakly
Coupled Linear Control Systems: A Recursive Approach, Springer-Verlag,
New York, NY, 1990.
S. GAL, Search Games, Academic Press, New York, 1980.
W. M. GETZ AND M. PACHTER, Two-target pursuit-evasion differential games
```
in the plane, Journal of Optimization Theory and Applications, 34 (1981),
```
pp. 383-403.
I. I. GlKHMAN AND A. V. SKOROHOD, Stochastic Differential Equations,
Springer-Verlag, Berlin, 1972.
I. L. GLICKSBERG, Minimax theorem for upper and lower semicontinuous pay-
offs. Rand Corporation Research Memorandum RM-478, Santa Monica,
CA, 1950.
C. GONZAGA, Path-following methods for linear programming, SIAM Review,
```
34 (1992), pp. 167-224.
```
M. GREEN AND D. LIMEBEER, Linear Robust Control, Prentice-Hall, Engle-
wood Cliffs, NJ, 1995.
P. HAGEDORN, H. W. KNOBLOCH, AND G. J. OLSDER, eds., Differential
Games and Applications, vol. 3 of Lecture Notes in Control and Information
Sciences, Springer-Verlag, Berlin, 1977.
al
al
al
D
BIBLIOGRAPHY 495
O. HAJEK, Pursuit Games, Academic Press, New York and London, 1975.
Discontinuous differential equations, I and II, Journal of Differential
```
Equations, 32 (1979), pp. 149-185.
```
A. H ALAN AY, Differential games with delay, SI AM Journal on Control and
```
Optimization, 6 (1968), pp. 579-593.
```
J. HALE, Theory of Functional Differential Equations, Springer-Verlag, Berlin,
1977.
R. P. HAMALAINEN, Nash and Stackelberg solutions to general linear-quadratic
two-player difference games, Systems Theory Laboratory B-29, Helsinki
University of Technology publication, Espoo, Finland, 1976.
R. P. HAMALAINEN AND H. K. EHTAMO, eds., Differential Games: Develop-
ments in Modelling and Computation, vol. 156, Springer-Verlag, New York,
August 1991.
H. HERMES AND J. LASALLE, Functional analysis and time optimal control,
Academic Press, New York, 1969.
Y. C. Ho, Differential games, dynamic optimization and generalized control
theory, survey paper, Journal of Optimization Theory and Applications, 6
```
(1970), pp. 179-209.
```
Y. C. Ho, A. E. BRYSON, JR., AND S. BARON, Differential games and opti-
mal pursuit-evasion strategies, IEEE Transactions on Automatic Control,
```
AC-10 (1965), pp. 385-389.
```
Y. C. Ho, P. B. LUH, AND G. J. OLSDER, A control-theoretic view on
incentives, in Proceedings of the Fourth International Conference on Anal-
ysis and Optimization of Systems, A. Bensoussan and J. L. Lions, eds.,
Springer-Verlag, Berlin, 1980, pp. 359-383. Lecture Notes in Control and
Information Sciences, vol. 28.
```
A control-theoretic view on incentives, Automatica, 18 (1982), pp. 167-
```
180.
Y. C. Ho AND S. K. MlTTER, Directions in Large Scale Systems, Plenum
Press, New York, NY, 1976.
Y.-C. Ho AND G. OLSDER, Aspects of the Stackelberg game problem,— in-
centive, bluff and hierarchy, in Proceedings of the 8th IFAC world congress,
Kyoto, Japan, 1981, IFAC, Laxenburg, 1982, pp. 1359-1363.
Y.-C. Ho AND G. J. OLSDER, Differential games: Concepts and applications,
in Mathematics of Conflict, M. Shubik, ed., North-Holland, Amsterdam,
The Netherlands, 1983, pp. 127-186.
N. HOWARD, Paradoxes of Rationality: Theory of Metagames and Political
Behavior, The MIT Press, Cambridge, MA, 1971.
V. HUTSON AND J. S. PYM, Applications of Functional Analysis and Operator
Theory, Operator Theory, Academic Press, London, 1980.
496 BIBLIOGRAPHY
M. D. INTRILIGATOR, Mathematical Optimization and Economic Theory,
Prentice-Hall, Englewood Cliffs, NJ, 1971.
R. ISAACS, Differential games I, II, III, IV. Rand Cooperation Research
Memorandum RM-1391, 1399, 1411, 1468, Santa Monica, CA, 1954-1956.
Differential games: Their scope, nature and future, Journal of Opti-
```
mization Theory and Applications, 3 (1969), pp. 283-295.
```
-, Differential Games, Kruger Publishing Company, Huntington, NY,
```
2nd ed., 1975. (First edition: Wiley, NY, 1965.)
```
J. ISBELL, Finitary games, in Annals of Mathematical Studies, vol. 39, Prince-
ton University Press, Princeton, NJ, 1957, pp. 79-95.
H. ISHII, Perron's method for Hamilton-Jacobi equations, Duke Math. J, 55
```
(1987), pp. 369-384.
```
A boundary value problem of the dirichlet type for Hamilton-Jacobi
```
equations, Ann. Sc. Norm. Sup. Pisa, XVI (1989), pp. 105-135.
```
V. I. ISTRATESCU, Fixed Point Theory, D. Reidel Publishing Company, Dor-
drecht, Holland, 1981.
D. H. JACOBSON, On values and strategies for infinite-time linear quadratic
```
games, IEEE Transactions on Automatic Control, AC-22 (1977), pp. 490-
```
491.
S. JORGENSEN, Optimal dynamic pricing in an oligopolistic market: A sur-
vey, in Dynamic Games and Applications in Economics, T. Ba§ar, ed., Lec-
ture Notes in Economics and Mathematical Systems, volume 265, Springer-
Verlag, Berlin, 1986, pp. 179-237.
M. C. JOSHI AND R. K. BOSE, Some Topics in Nonlinear Functional Analysis,
John Wiley and Sons, New York, NY, 1985.
T. KAILATH, Linear Systems, Prentice-Hall, Englewood Cliffs, NJ, 1980.
S. KAKUTANI, Generalization of Brouwer's fixed point theorem, Duke Journal
```
of Mathematics, 8 (1941), pp. 457-459.
```
M. I. KAMIEN AND N. L. SCHWARTZ, Conjectural variations, Canadian J.
```
Economics, 16 (1983), pp. 191-211.
```
S. KARLIN, Matrix Games, Programming and Mathematical Economics, vols. I
and II, Addison-Wesley, Reading, MA, 1959.
N. KARMARKAR, A new polynomial-time algorithm for linear programming,
```
Combinatorica, 4 (1984), pp. 373-395.
```
P. P. KHARGONEKAR, State-space HOQ control theory and the LQG control
problem, in Mathematical System Theory: The Influence of R. E. Kalman,
A. C. Antoulas, ed., Springer-Verlag, Berlin, 1991, pp. 159-176.
P. P. KHARGONEKAR, K. N. NAGPAL, AND K. R. POOLLA, H°° control
```
with transients, SIAM Journal on Control and Optimization, 29 (1991),
```
pp. 1373-1393.
BIBLIOGRAPHY 497
E. KoHLBERG AND J.-F. MERTENS, On the strategic stability of equilibria,
```
Econometrica, 54 (1986), pp. 1003-1037.
```
N. KRASOVSKII AND A. SUBBOTIN, Game-theoretical Control Problems,
Springer-Verlag, New York, 1988.
D. KREPS, Game Theory and Economic Modelling, Oxford University Press,
Oxford, England, 1990.
D. M. KREPS AND R. WILSON, Sequential equilibria, Econometrica, 50 (1982),
pp. 863-894.
K. KUGA, Brouwer's fixed point theorem: An alternative proof, SIAM Journal
```
of Mathematical Analysis, 5 (1974), pp. 893-897.
```
H. W. KUHN, Extensive games and the problems of information, in vol. 29 of
Contributions to the Theory of Games, Princeton University Press, Prince-
ton, NJ, 1953, pp. 193-216. H. W. Kuhn and A. W. Tucker, eds.
H. W. KUHN AND G. P. SZEGO, eds., Differential Games and Related Topics,
North-Holland, Amsterdam, The Netherlands, 1971.
H. W. KUHN AND A. W. TUCKER, eds., Contributions to the Theory of
Games, vol. I, no. 24, Annals of Mathematical Studies, Princeton University
Press, Princeton, NJ, 1950.
eds., Contributions to the Theory of Games, vol. I, no. 28, Annals of
Mathematical Studies, Princeton University Press, Princeton, NJ, 1953.
P. R. KUMAR AND T. H. SHIAU, Existence of value and randomized strate-
gies in zero-sum discrete-time stochastic dynamic games, SIAM Journal on
```
Control and Optimization, 19 (1981), pp. 617-634.
```
F. KYDLAND, Noncooperative and dominant player solutions in discrete
```
dynamic games, International Journal of Economic Review, 16 (1975),
```
pp. 321-335.
F. KYDLAND AND E. PRESCOTT, Rules rather than discretion: The inconsis-
```
tency of optimal plans, Journal of Political Economy, 85 (1977), pp. 473-
```
491.
S. LAKSHMIVARAHAN AND K. S. NARENDRA, Learning algorithms for two-
person zero-sum stochastic games with incomplete information, Mathemat-
```
ics of Operations Research, 6 (1981), pp. 379-286.
```
G. LEITMANN, Cooperative and Noncooperative Many Player Differential
Games, vol. 190 of CISM Monograph, Springer-Verlag, Vienna, 1974.
Many player differential games, in Differential Games and Applications,
P. Hagedorn, H. W. Knobloch, and G. J. Olsder, eds., Springer-Verlag,
Berlin, 1977, pp. 153-171. Lecture Notes in Control and Information Sci-
ences, vol. 3.
On generalized Stackelberg strategies, Journal of Optimization Theory
```
and Applications, 26 (1978).
```
498 BIBLIOGRAPHY
G. LEITMANN AND H. Y. WAN, JR., Macro-economic stabilization policy for
an uncertain dynamic economy, in New Trends in Dynamic System Theory
and Economics, M. Aoki and A. Marzollo, eds., Academic Press, London
and New York, 1979, pp. 105-136.
C. E. LEMKE AND J. F. HOWSON, Equilibrium points of bi-matrix games,
```
SIAM Journal on Applied Mathematics, 12 (1964), pp. 413-423.
```
J. LEVINE, Two-person zero-sum differential games with incomplete
information-a Bayesian model, in Differential Games and Control Theory
III, P. T. Liu and E. O. Roxin, eds., Plenum Press, New York, NY, 1979,
pp. 119-151. Proceedings of the 3rd Kingston Conference 1978, Part A.
Lecture Notes in Pure and Applied Mathematics, vol. 44.
R. E. LEVITAN AND M. SHUBIK, Noncooperative equilibrium and strategy
spaces in an oligopolistic market, in Differential Games and Related Top-
ics, H. W. Kuhn and G. P. Szego, eds., North-Holland, Amsterdam, The
Netherlands, 1971, pp. 429-448.
J. LEWIN, The bang-bang-bang problem revisited, Journal of Optimization The-
```
ory and Applications, 18 (1976), pp. 429-432.
```
Differential Games: Theory and Methods for Solving Game Problems
with Singular Surfaces. Springer-Verlag, New York, 1994.
J. LEWIN AND G. J. OLSDER, Conic surveillance evasion, Journal of Opti-
```
mization Theory and Applications, 19 (1979), pp. 107-125.
```
S. Li AND T. BA§AR, Distributed algorithms for the computation of noncoop-
```
erative equilibria, Automatica, 23 (1987), pp. 523-533.
```
D. LIMEBEER, B. ANDERSON, P. KHARGONEKAR, AND M. GREEN, A game
theoretic approach to H^ control for time-varying systems, SIAM J. Con-
```
trol Optim., 30 (1992), pp. 262-283.
```
P. LlONS, Generalized Solutions of Hamilton-Jacobi Equations, Pitman Ad-
vanced Publishing Program, 1982.
M. LOEVE, Probability Theory, Van Nostrand, Princeton, NJ, 3rd ed., 1963.
R. D. LUCE AND H. RAIFFA, Games and Decisions, John Wiley and Sons,
New York, NY, 1957.
D. G. LUENBERGER, Optimization by Vector Space Methods, John Wiley and
Sons, New York, NY, 1969.
Introduction to Linear and Nonlinear Programming, Addison-Wesley,
Reading, MA, 1973.
D. L. LUKES, Equilibrium feedback control in linear games with quadratic costs,
```
SIAM Journal on Control and Optimization, 9 (1971), pp. 234-252.
```
D. L. LUKES AND D. L. RUSSELL, A global theory for linear quadratic dif-
ferential games, Journal of Mathematical Analysis and Applications, 33
```
(1971), pp. 96-123.
```
BIBLIOGRAPHY 499
E. F. MAGEIROU, Values and strategies for infinite duration linear quadratic
```
games, IEEE Transactions on Automatic Control, AC-21 (1976), pp. 547-
```
550.
A. MAITRA AND T. PARTHASARATHY, On stochastic games, Journal of Opti-
```
mization Theory and Applications, 5 (1970), pp. 289-300.
```
M. MARCUS AND H. MlNC, A Survey of Matrix Theory and Matrix Inequali-
ties, Allyn and Bacon, Inc., Boston, MA, 1964.
J. McKlNSEY, Introduction to the Theory of Games, McGraw-Hill, New York,
NY, 1952.
J. MEDANIC, Closed-loop Stackelberg strategies in linear-quadratic problems,
in Proceedings of the 1977 JACC, San Francisco, CA, 1977, pp. 1324-1329.
J. F. MERTENS, Formulation of Bayesian analysis for games with incomplete
```
information, International Journal of Game Theory, 14 (1985), pp. 1-29.
```
Stable equilibria - a reformulation. Part II Discussion of the defini-
```
tion and further results, Mathematics of Operations Research, 16 (1991),
```
pp. 694-753.
A. W. MERZ, The homicidal chauffer-a differential game, Tech report, Guid-
ance and Control Laboratory 418, Stanford University, Stanford, CA, 1971.
Optimal evasive maneuvers in maritime collision avoidance, Navigation,
```
20 (1973), pp. 144-152.
```
A differential game solution to the coplanar tail-chase aerial combat
problem. Tech report NASA-Cr-137809, NASA Langley Research Center,
Hampton, VA, 1976.
T. MILOH AND S. D. SHARMA, Maritime collision avoidance as a differential
game, Berich 329, Institut fur Schiffbau der Universitat Hamburg, 1976.
R. B. MYERSON, Refinement of the Nash equilibrium concept, International
```
Journal of Game Theory, 7 (1978), pp. 73-80.
```
Game Theory: Analysis of Conflict, Harvard University Press, Cam-
bridge, MA, 1991.
J. NASH, Noncooperative games, Annals of Mathematics, 54 (1951), pp. 286-
295.
H. NlKAlDO AND K. ISODA, Note on non-cooperative convex games, Pacific
```
Journal of Mathematics, 5 (1955), pp. 807-815.
```
K. OKUGUCHI, Expectations and Stability in Oligopoly Models, Lecture Notes
in Economics and Mathematical Systems, Springer-Verlag, Berlin, 1976,
ch. 138.
G. OLSDER, A critical analysis of a new equilibrium concept, in Axiomatics
and Pragmatics of Conflict Analysis, J. Paeling and P. H. Vossen, eds.,
Gower Pub. Co., Aldershot, England, 1987, pp. 80-100.
500 BIBLIOGRAPHY
G. J. OLSDER, Some thoughts about simple advertising models as differential
games and the structure of coalitions, in Directions in Large Scale Systems,
Y. C. Ho and S. K. Mitter, eds., Plenum Press, New York, 1976, pp. 187-
206.
, Information structures in differential games, in Differential Games and
Control Theory II, E. O. Roxin, P. T. Liu, and R. L. Sternberg, eds., Marcel
Dekker, New York, NY, 1977a, pp. 99-136.
-, On observation costs and information structures in stochastic differen-
tial games, in Differential Games and Applications, P. Hagedorn, H. W.
Knobloch, and G. J. Olsder, eds., Springer-Verlag, Berlin, 1977b, ch. 3,
pp. 172-185.
G. J. OLSDER AND J. V. BREAKWELL, Role determination in an aerial dog-
```
fight, International Journal of Game Theory, 3 (1974), pp. 47-66.
```
G. J. OLSDER AND G. P. PAPAVASSILOPOULOS, About when to use the search-
```
light, Journal of Mathematical Analysis and Applications, 136 (1988a),
```
pp. 466-478.
A Markov chain with dynamic information, Journal of Optimization
```
Theory and Applications, 59 (1988b), pp. 467-486.
```
G. J. OLSDER AND J. WALTER, Collision avoidance of ships, in Proceedings
8th IFIP Conference on Optimization Techniques, J. Stoer, ed., Springer-
Verlag, Berlin, 1978, ch. 6, pp. 264-271.
G. OWEN, Game Theory, Saunders, Philadelphia, PA, 1968.
Existence of equilibrium pairs in continuous games, International Jour-
```
nal of Game Theory, 5 (1974), pp. 97-105.
```
, Game Theory, Academic Press, New York, NY, 1982.
M. PACHTER AND Y. YAVIN, A stochastic homicidal chauffeur pursuit-evasion
differential game, Technical Report TWISK 95, WNNR, CSIR, Pretoria,
South Africa, 1979.
Z. PAN AND T. BA§AR, H°°-optimal control for singularly perturbed sys-
```
tems. Part I: Perfect state measurements, Automatica, 29 (1993), pp. 401-
```
423.
H°° -optimal control for singularly perturbed systems. Part II: Imper-
fect state measurements, IEEE Transactions on Automatic Control, AC-39
```
(1994a), pp. 280-299.
```
H°°-optimal control of singularly perturbed systems with sampled-state
measurements, in Advances in Dynamic Games and Applications, T. Ba§ar
and A. Haurie, eds., Birkhauser, Boston, MA, 1994b, pp. 23-55.
G. P. PAPAVASSILOPOULOS AND J. B. CRUZ, JR., Nonclassical control prob-
lems and Stackelberg games, IEEE Transactions on Automatic Control,
```
AC-24 (1979a), pp. 155-166.
```
BIBLIOGRAPHY 501
On the existence of solutions to coupled matrix Riccati differential equa-
tions in linear quadratic Nash games, IEEE Transactions on Automatic
```
Control, AC-24 (1979b), pp. 127-129.
```
, Sufficient conditions for Stackelberg and Nash strategies with memory,
```
Journal of Optimization Theory and Applications, 31 (1980), pp. 233-260.
```
G. P. PAPAVASSILOPOULOS, J. V. MEDANIC, AND J. B. CRUZ, JR., On the
existence of Nash strategies and solutions to coupled Riccati equations in
linear-quadratic games, Journal of Optimization Theory and Applications,
```
28 (1979), pp. 49-76.
```
G. P. PAPAVASSILOPOULOS AND G. J. OLSDER, On the linear-quadratic,
closed-loop, no-memory Nash game, Journal of Optimization Theory and
```
Applications, 42 (1984), pp. 551-560.
```
A. PAPOULIS, Probability, Random Variables and Stochastic Processes,
McGraw-Hill, New York, NY, 1965.
T. PARTHASARATHY AND T. RAGHAVAN, Some Topics in Two-person Games,
Elsevier, New York, NY, 1971.
T. PARTHASARATHY AND M. STERN, Markov games - a survey, in Differential
Games and Control Theory II, E. O. Roxin, P. T. Liu, and R. L. Sternberg,
eds., Marcel Dekker, New York, NY, 1977, pp. 1-46.
W. Y. PENG AND T. L. VINCENT, Some aspects of aerial combat, AIAA
```
Journal, 13 (1975), pp. 7-11.
```
R. S. PlNDYCK, Optimal economic stabilization policies under decentralized
control and conflicting objectives, IEEE Transactions on Automatic Con-
```
trol, AC-22 (1977), pp. 517-530.
```
M. POHJOLA, Applications of dynamic game theory to macroeconomics, in Dy-
namic Games and Applications in Economics, T. Ba§ar, ed., Lecture Notes
in Economics and Mathematical Systems, volume 265, Springer-Verlag,
Berlin, 1986, pp. 103-133.
L. S. PONTRYAGIN, Linear differential games, I, Soviet Math. Doklady, 8
```
(1967), pp. 769-771.
```
L. S. PONTRYAGIN, V. G. BOLTYANSKII, R. V. GAMKRELIDZE, AND E. F.
MlSHCHENKO, The Mathematical Theory of Optimal Processes, Interscience
Publishers, New York, NY, 1962.
W. POUNDSTONE, Prisoner's Dilemma, Doubleday, New York, NY, 1992.
O. POURTALLIER AND B. TOLWINSKI, Discretization of Isaacs' equation: A
convergence result, Tech. report, INRIA, Sophia-Antipolis, France, 1992.
R. RADNER, Monitoring cooperative agreements in a repeated principal-agent
```
relationship, Econometrica, 49 (1981), pp. 1127-1148.
```
Repeated principal-agent games with discounting, Econometrica, 53
```
(1985), pp. 1173-1198.
```
502 BIBLIOGRAPHY
T. RAGHAVAN AND J. FILAR, Algorithms for Stochastic Games - A Survey,
```
ZOR - Methods and Models of Operations Research, 35 (1991), pp. 437-
```
472.
W. T. REID, Riccati Differential Equations, Academic Press, New York and
London, 1972.
R. RESTREPO, Tactical problems involving several actions, in Contributions
to the Theory of Games, M. Dresner, A. W. Tucker, and P. Wolfe, eds.,
Princeton University Press, Princeton, NJ, 1957, ch. Ill, pp. 313-335.
J. ROBINSON, An iterative method of solving a game, Annals of Mathematics,
```
54 (1951), pp. 296-301.
```
J. B. ROSEN, Existence and uniqueness of equilibrium points for concave n-
```
person games, Econometrica, 33 (1965), pp. 520-534.
```
E. O. ROXIN, The axiomatic approach in differential games, Journal of Opti-
```
mization Theory and Applications, 3 (1969), pp. 153-163.
```
Differential games with partial differential equations, in Differential
Games and Applications, P. Hagedorn, K. W. Knobloch, and G. J. Ols-
der, eds., Springer-Verlag, Berlin, 1977, vol. 3, pp. 186-204. Lecture Notes
in Control and Information Sciences.
W. RUPP, e-Gleichgewichtspunkte in n-Personenspielen, in Mathematical Eco-
```
nomics and Game Theory; Essays in Honor of Oskar Morgenstern, R. Henn
```
and O. Moeschlin, eds., Springer-Verlag, Berlin, 1977, pp. 128-138.
H. SAGAN, Introduction to the Calculus of Variations, McGraw-Hill, New
York, NY, 1969.
R. C. ScALZO, n-person linear quadratic differential games with constraints,
```
SIAM Journal on Control, 12 (1974), pp. 419-425.
```
H. SCARF, The approximation of fixed points of a continuous mapping, SIAM
```
Journal on Applied Mathematics, 15 (1967), pp. 1328-1343.
```
J. SCHAUDER, Der Fixpunktsatz in Funktionalraumen, Studia Mathematica, 2
```
(1930), pp. 171-180.
```
W. E. SCHMITENDORF, Existence of optimal open-loop strategies for a class
of differential games, Journal of Optimization Theory and Applications, 5
```
(1970), pp. 363-375.
```
R. SELTEN, Reexamination of the perfectness concept for equilibrium points in
```
extensive games, International Journal of Game Theory, 4 (1975), pp. 25-
```
55.
L. S. SHAPLEY, Stochastic games, in Proceedings of the National Academy of
Sciences, vol. 39, 1953, pp. 1095-1100.
A note on the Lemke-Howson algorithm. Pivoting and extensions, Math-
```
ematical Programming Study, 1 (1974), pp. 175-189.
```
L. D. SHARMA, On ship maneuverability and collision avoidance. International
report, Institut fur Schiffabu der Universitat Hamburg, 1976.
BIBLIOGRAPHY 503
M. SHUBIK, Game Theory in the Social Sciences, MIT Press, Cambridge, MA,
1983.
M. SlMAAN AND J. B. CRUZ, JR., Additional aspects of the Stackelberg strat-
egy in nonzero sum games, Journal of Optimization Theory and Applica-
```
tions, 11 (1973a), pp. 613-626.
```
On the Stackelberg strategy in nonzero sum games, Journal of Optimiza-
```
tion Theory and Applications, 11 (1973b), pp. 533-555.
```
R. R. SINGLETON AND W. F. TYNDAL, Games and Programs, Freeman, San
Francisco, CA, 1974.
M. SlON, On general minimax theorems, Pacific Journal of Mathematics, 8
```
(1958), pp. 171-176.
```
M. SIGN AND P. WOLFE, On a game without a value, in Contributions to the
Theory of Games, M. Dresher, A. W. Tucker, and P. Wolfe, eds., Princeton
University Press, Princeton, NJ, 1957, ch. Ill, pp. 209-306.
D. R. SMART, Fixed Point Theorems, Cambridge University Press, London,
1974.
M. J. SOBEL, Noncooperative stochastic games, Ann. of Math. Statist., 42
```
(1971), pp. 1930-1935.
```
R. SRIKANT AND T. BA§AR, Iterative computation of noncooperative equilibria
in nonzero-sum differential games with weakly coupled players, Journal of
```
Optimization Theory and Applications, 71 (1991), pp. 137-168.
```
Sequential decomposition and policy iteration schemes for M-player
```
games with partial weak coupling, Automatica, 28 (1992), pp. 95-106.
```
V. STAICU, Minimal time function and viscosity solutions, Journal of Opti-
```
mization Theory and Applications, 60 (1989), pp. 81-91.
```
A. W. STARR AND Y. C. Ho, Further properties of nonzero-sum differen-
```
tial games, Journal of Optimization Theory and Applications, 3 (1969a),
```
pp. 207-219.
Nonzero-sum differential games, Journal of Optimization Theory and
```
Applications, 3 (1969b), pp. 184-206.
```
A. A. STOORVOGEL, The HOO Control Problem: A State Space Approach,
Prentice-Hall, New York, 1992.
J. SZEP AND F. FORGO, Introduction to the Theory of Games, D. Reidel
Publishing Co, Boston, MA, 1985.
S. H. Tus, Semi-infinite and infinite matrix games and bimatrix games, PhD
thesis, University of Nijmegen, The Netherlands, 1975.
B. TOLWINSKI, NumericaL-solution of N-person non-zero-sum differential
```
games, Control and Cybernetics, 7 (1978a), pp. 37-50.
```
On the existence of Nash equilibrium points for differential games with
```
linear and nonlinear dynamics, Control and Cybernetics, 7 (1978b), pp. 57-
```
69.
504 BIBLIOGRAPHY
Stackelberg solution of dynamic game with constraints, in Proceedings
of the IEEE 19th Conference on Decision and Control, Albuquerque, NM,
1980, IEEE Computer Society Press, Los Alamitos, CA.
Closed-loop Stackelberg solution to multi-stage linear-quadratic games,
```
Journal of Optimization Theory and Applications, 35 (1981a), pp. 485-502.
```
-, Equilibrium solutions for a class of hierarchical games, in Applications
of Systems Theory to Economics, Management and Technology, J. Outen-
baum and M. Niergodka, eds., PWN, Warsaw, Poland, 1981b.
J. N. TSITSIKLIS, On the stability of asynchronous iterative processes, Math.
```
Systems Theory, 20 (1987), pp. 137-153.
```
, A comparison of Jacobi and Gauss-Seidel parallel iterations, Applied
```
Mathematical Letters, 2 (1989), pp. 167-170.
```
S. J. TURNOVSKY, T. BA§AR, AND V. D'OREY, Dynamic strategic mone-
tary policies and coordination in interdependent economics, The American
```
Economic Review, 78 (1988), pp. 341-361.
```
K. UCHIDA, On the existence of Nash equilibrium point in n-person nonzero-
sum stochastic differential games, SIAM Journal on Control and Optimiza-
```
tion, 16 (1978), pp. 142-149.
```
A note on the existence of a Nash equilibrium point in stochastic dif-
```
ferential games, SIAM Journal on Control and Optimization, 17 (1979),
```
pp. 1-4.
K. UCHIDA AND M. FUJITA, On the central controller: Characterizations via
differential games and LEQG control problems, Systems &: Control Letters,
```
13 (1989), pp. 9-13.
```
E. VAN DAMME, A relation between perfect equilibria in extensive games and
proper equilibria in normal form, International Journal of Game Theory,
```
13 (1984), pp. 1-13.
```
Stability and Perfection of Equilibria, Springer-Verlag, New York, 1987.
, Stable equilibria and forward induction, Journal of Economic Theory,
```
48 (1989), pp. 476-496.
```
P. P. VARAIYA, The existence of solutions to a differential game, SIAM Jour-
```
nal on Control, 5 (1967), pp. 153-162.
```
N-person nonzero-sum differential games with linear dynamics, SIAM
```
Journal on Control, 8 (1970), pp. 441-449.
```
-, N-player stochastic differential games, SIAM Journal on Control and
```
Optimization, 14 (1976), pp. 538-545.
```
P. P. VARAIYA AND J. G. LIN, Existence of saddle-point in differential games,
```
SIAM Journal on Control, 7 (1969), pp. 141-157.
```
M. VlDYASAGAR, A new approach to N-person, nonzero-sum, linear differen-
```
tial games, Journal of Optimization Theory and Applications, 18 (1976),
```
pp. 171-175.
BIBLIOGRAPHY 505
J. A. VlLLE, Sur la theorie generate des jeux ou interment I'habilitedes joueurs.
In E. Borel, Traite du calcul des probabilites et de ses applications IV, 1938,
pp. 105-113.
T. L. VINCENT AND W. Y. PENG, Ship collision avoidance. Workshop on
differential games, Naval Academy, Annapolis, MO, 1973.
T. L. VINCENT, D. L. STRICHT, AND W. Y. PENG, Aircraft missile avoid-
```
ance, Journal of Operations Research, 24 (1976).
```
J. VON NEUMANN, Zur theorie der gesellschaftspiele, Mathematische Annalen,
```
100 (1928), pp. 295-320.
```
Uber ein okonomisches gleichungssytem und eine verallgemeinerung des
Brouwerschen fixpunktsatzes, Ergebnisse eines Mathematik Kolloquiums, 8
```
(1937), pp. 73-83.
```
A numerical method to determine optimum strategy, Naval Research
```
Logistics Quarterly, 1 (1954), pp. 109-115.
```
J. VON NEUMANN AND O. MORGENSTERN, Theory of Games and Economic
Behavior, Princeton University Press, Princeton, NJ, 2nd ed., 1947.
H. VON STACKELBERG, Marktform und Gleichgewicht. Springer-Verlag, Vi-
```
enna. (An English translation appeared in 1952 entitled The Theory of the
```
Market Economy, published by Oxford University Press, Oxford, England,
1934.
N. H. VOROB'EV, Game Theory, Springer-Verlag, Berlin, 1977.
A. WALD, Generalization of a theorem by von Neumann concerning zero-sum
```
two-person games, Annals of Mathematics, 46 (1945), pp. 281-286.
```
J. WARGA, Optimal Control of Differential and Functional Equations, Aca-
demic Press, New York and London, 1972.
A. WASHBURN, Deterministic graphical games, Journal of Mathematical Anal-
```
ysis and Applications, 153 (1990), pp. 84-96.
```
R. L. WEIL, Game theory and eigensystems, SIAM Review, 10 (1968), pp. 360-
367.
J. C. WlLLEMS, Least squares stationary optimal control and the algebraic
```
Riccati equation, IEEE Transactions on Automatic Control, AC-16 (1971),
```
pp. 621-634.
J. D. WILLIAMS, The Compleat Strategyst, McGraw-Hill, New York, NY, 1954.
J. L. WlLLMAN, Formal solutions for a class of stochastic pursuit-evasion
```
games, IEEE Transactions on Automatic Control, AC-14 (1969), pp. 504-
```
509.
D. J. WILSON, Differential games with no information, SIAM Journal on
```
Control and Optimization, 15 (1977), pp. 233-246.
```
D. WlSHART AND G. J. OLSDER, Discontinuous Stackelberg solutions, Inter-
```
national Journal of Systems Science, 10 (1979), pp. 1359-1368.
```
506 BIBLIOGRAPHY
H. J. WiTSENHAUSEN, On the relations between the values of a games and its
```
information structure, Information and Control, 19 (1971).
```
H. S. WiTSENHAUSEN, On information structures, feedback and causality,
```
SIAM Journal on Control, 9 (1971), pp. 149-160.
```
Alternatives to the tree model, for extensive games, in The Theory and
Applications of Differential Games, J. D. Grote, ed., Reidel Publishing
Company, The Netherlands, 1975, pp. 77-84.
E. WONG AND B. E. HAJEK, Stochastic Processes in Engineering Systems,
Springer-Verlag, New York, NY, 1985.
L. A. ZADEH, The concepts of system, aggregate and state in system theory, in
System Theory, L. A. Zadeh and E. Polak, eds., McGraw-Hill, New York,
NY, 1969, pp. 3-42.
G. ZAMES, Feedback and optimal sensitivity: Model reference transformation,
multiplicative seminorms and approximate inverses, IEEE Transactions on
```
Automatic Control, AC-26 (1981), pp. 301-320.
```
Y. P. ZHENG AND T. BA§AR, Existence and derivation of optimal affine incen-
tive schemes for Stackelberg games with partial information: A geometric
```
approach, International Journal of Control, 35 (1982), pp. 997-1012.
```
Y. P. ZHENG, T. BA§AR, AND J. B. CRUZ, JR., Incentive Stackelberg strate-
gies for deterministic multi-stage decision processes, IEEE Transactions on
```
Systems, Man and Cybernetics, SMC-14 (1984), pp. 10-24.
```
Corollaries, Definitions,
Examples, Lemmas,
Propositions, Remarks and
Theorems
Chapter 1
Examples
No. 1.1
1.2
1.3
1.4
p. 4
6
10
10
Definitions
No. 2.1
2.2
2.3
2.4
2.5
2.6
2.7
2.8
2.9
2.10
2.11
2.12
2.13
2.14
2.15
p. 21
23
25
25
38
39
40
40
44
44
46
47
54
59
63
Chapter 2
Corollaries
No. 2.1
2.2
2.3
2.4
2.5
p. 22
26
28
45
53
Examples
No. 2.1
2.2
2.3
2.4
2.5
p. 39
49
50
52
61
Lemmas
No. 2.1 p. 26
2.2 33
507
508 LIST OF COROLLARIES ETC.
Propositions
No. 2.1
2.2
2.3
2.4
2.5
2.6
2.7
Remarks
No. 2.1
2.2
2.3
2.4
2.5
2.6
Theorems
No. 2.1
2.2
2.3
2.4
2.5
p. 42
42
47
50
51
52
54
p. 25
39
39
39
44
59
p. 19
21
25
27
34
Chapter 3
Definitions
No. 3.1
3.2
3.3
3.4
3.5
3.6
3.7
3.8
3.9
3.10
3.11
3.12
3.13
3.14
3.15
3.16
3.17
3.18
3.19
3.20
3.21
3.22
3.23
3.24
3.25
3.26
3.27
3.28
3.29
3.30
3.31
3.32
3.33
3.34
3.35
3.36
3.37
3.38
p. 78
79
79
81
84
85
88
90
91
97
98
99
103
103
105
106
107
108
112
112
119
120
121
121
122
134
134
134
137
138
141
141
143
143
146
146
146
149
Corollaries
No. 3.1
3.2
87
97
LIST OF COROLLARIES ETC. 509
Examples
No. 3.1
3.2
3.3
3.4
3.5
3.6
3.7
3.8
3.9
3.10
3.11
3.12
3.13
3.14
3.15
3.16
3.17
3.18
p. 79
84
89
91
94
101
109
110
112
114
116
123
139
141
144
147
150
151
Propositions
No. 3.1
3.2
3.3
3.4
3.5
3.6
3.7
3.8
3.9
3.10
3.11
3.12
3.13
3.14
3.15
3.16
3.17
3.18
3.19
p. 81
81
86
90
93
96
103
118
121
122
126
127
127
130
131
135
137
142
147
Remarks
No. 3.1
3.2
3.3
3.4
3.5
3.6
3.7
3.8
3.9
3.10
3.11
3.12
3.13
3.14
3.15
3.16
3.17
3.18
3.19
3.20
3.21
Theorems
No. 3.1
3.2
3.3
Chapter 4
Corollaries
No. 4.1
4.2
4.3
4.4
4.5
Definitions
No. 4.1
4.2
4.3
4.4
4.5
4.6
4.7
4.8
4.9
4.10
p. 81
84
86
97
103
105
105
106
108
109
116
118
118
119
122
131
134
134
135
135
144
p. 85
91
134
p. 167
174
178
181
195
p. 163
164
169
170
172
179
180
182
187
189
510
Examples
No. 4.1
4.2
4.3
4.4
4.5
4.6
Propositions
No. 4.1
4.2
4.3
4.4
4.5
4.6
4.7
Remarks
No. 4.1
4.2
4.3
4.4
4.5
4.6
4.7
4.8
Theorems
No. 4.1
4.2
4.3
4.4
4.5
4.6
4.7
4.8
p. 162
167
178
182
199
200
p. 175
182
183
185
189
191
197
p. 175
179
181
187
189
191
195
197
p. 164
167
173
176
177
177
178
180
LIST OF COROLLARIES ETC.
Chapter 5
Definitions
No. 5.1
5.2
5.3
5.4
5.5
5.6
5.7
5.8
5.9
5.10
5.11
5.12
5.13
5.14
5.15
5.16
Examples
No. 5.1
5.2
Propositions
No. 5.1
5.2
5.3
5.4
5.5
5.6
5.7
5.8
Remarks
No. 5.1
5.2
5.3
5.4
5.5
5.6
5.7
5.8
5.9
5.10
5.11
5.12
5.13
5.14
p. 219
221
222
223
224
225
229
232
232
232
248
249
250
250
257
260
p. 228
244
p. 234
236
239
241
253
254
256
259
p. 222
223
226
235
238
238
239
240
243
248
253
254
255
257
LIST OF COROLLARIES ETC. 511
Theorems
No. 5.1
5.2
5.3
5.4
5.5
5.6
5.7
Chapter 6
Corollaries
No. 6.1
6.2
6.3
6.4
6.5
6.6
6.7
6.8
6.9
6.10
6.11
6.12
6.13
Definitions
No. 6.1
6.2
6.3
6.4
6.5
6.6
6.7
Examples
No. 6.1
6.2
6.3
6.4
6.5
6.6
6.5
6.8
Lemmas
No. 6.1
6.2
6.3
6.4
p. 226
230
237
243
246
258
258
p. 280
282
282
306
323
326
335
335
344
344
353
354
354
p. 269
278
296
298
312
321
335
p. 307
315
329
332
336
338
345
349
p. 273
284
290
317
Propositions
No. 6.1
6.2
6.3
6.4
6.5
6.6
6.7
6.8
p. 278
286
289
294
294
297
322
337
Remarks
No. 6.1
6.2
6.3
6.4
6.5
6.6
6.7
6.8
6.9
6.10
6.11
6.12
6.13
6.14
6.15
6.16
6.17
6.18
6.19
6.20
6.21
6.22
6.23
p. 271
275
281
281
282
286
286
287
298
299
302
306
311
313
319
324
324
324
326
329
329
349
354
512 LIST OF COROLLARIES ETC.
Theorems
No. 6.1
6.2
6.3
6.4
6.5
6.6
6.7
6.8
6.9
6.10
6.11
6.12
6.13
6.14
6.15
6.16
6.17
6.18
6.19
6.20
6.21
6.22
6.23
6.24
6.25
6.26
6.27
Chapter 7
Corollaries
No. 7.1
7.2
Definitions
No. 7.1
Examples
No. 7.1
7.2
7.3
7.4
7.5
7.6
Lemmas
No. 7.1
7.2
7.3
p. 267
269
272
274
276
278
285
291
300
303
310
312
316
317
320
322
327
331
333
334
335
336
340
341
347
349
351
p. 371
375
p. 394
p. 376
382
393
394
401
404
p. 367
386
408
Propositions
No. 7.1
7.2
7.3
7.4
7.5
7.6
7.7
7.8
Remarks
No. 7.1
7.2
7.3
7.4
7.5
7.6
7.7
7.8
7.9
7.10
7.11
7.12
7.13
7.14
7.15
Theorems
No. 7.1
7.2
7.3
7.4
7.5
7.6
p. 380
384
387
397
399
406
415
417
p. 370
370
373
373
374
375
381
381
382
384
391
391
406
412
417
p. 368
374
388
403
409
412
Chapter 8
Definitions
No. 8.1
8.2
Examples
No. 8.1
8.2
8.3
8.4
8.5
8.6
p. 433
433
p. 429
431
436
442
444
447
Remarks
No. i
I
I
i
J
5.1
3.2
5.3
5.4
5.5
p. 428
429
433
433
433
Theorems
No. 8.1
8.2
8.3
8.4
p. 427
428
434
434
Appendix C
Definitions
No. C.I
C.2
Theorems
No. C.I
C.2
C.3
p. 483
484
p. 483
483
484
LIST OF COROLLARIES ETC. 513
This page intentionally left blank
Index
Absolute continuity, 479
Action, 5
Action set, 219
Admissible Nash equilibrium solu-
tion, 79
Advertising game, 6
Aeronautics application, 456
Affine-quadratic continuous-time op-
timal control, 238
Affine-quadratic differential game, 312
Affine-quadratic discrete-time opti-
mal control, 234
Average Stackelberg cost, 182
Average value, 480
Banach space, 472
Bang-bang-bang surface, 468
Barrier, 245, 435, 447
Battle of the sexes, 80, 87
Behavioral Stackelberg equilibrium,
143
Behavioral strategies, 44, 143
Behavioral strategies in infinite dy-
namic game, 216
Behavioral strategies in multi-act games,
50
Better pair of strategies, 79
Bimatrix games, 78
Borel set, 478
Braess paradox, 203
Brouwer's fixed point theorem, 92
Canonical equations, 243
Capturability, 434
Cauchy problem, 255
Cauchy sequence, 472
Cauchy-Schwarz inequality, 481
Causality, 217
Closed set, 472
Closed-loop, 247
```
Closed-loop imperfect state (CLIS)
```
information, 221
```
Closed-loop no-memory (CLNM) Nash
```
equilibrium solution, 267
```
Closed-loop perfect state (CLPS) in-
```
formation, 221, 225
Collision avoidance, 451
Compact set, 472
Complete vector space, 472
Completely mixed Nash equilibrium,
86
Concavity, 473
Concurrent Stackelberg solution, 186
Conflict, 1
Conic surveillance evasion, 468
Constant strategy, 7, 41
Constant-sum game, 3
Continuity, 473
Control function, 224
Cooperative game, 5
```
Cost function(al), 98, 221, 223
```
```
Costate vector (variable), 242
```
Cournot duopoly situation, 199
```
Cumulative (joint) probability, 479
```
Cusp, 441
Decision maker, 1
Decision point, 445, 451
Decision rule, 5
Delayed commitment, 108, 112, 121
Differential game, 2, 224
Dimension of a set, 472
Dimensional random vector, 478
Dirichlet problem, 255
515
516 INDEX
Discount factor, 259
Discrete-time infinite game, 223
Dispersal line, 443
```
Dispersal surface (line), 442
```
Divide and rule, 396
Duel, 66
Duopoly, 199
Dynamic game, 2
Dynamic programming, 3
Dynamic single-act game, 112
Eigenvalue, 473
Eigenvector, 473
Epsilon-delayed closed-loop perfect
```
state (eDCLPS), 226
```
Epsilon-neighborhood, 472
Epsilon-perfect equilibrium, 129
Equicontinuous, 484
Equivocal line, 447
Evader, 424
Event, 477
```
Expected (average) cost functional,
```
223
Extensive form, 36, 97, 119, 149,
220
Feedback, 10
Feedback imperfect state informa-
tion, 221
Feedback Nash equilibrium, 277
Feedback Nash equilibrium solution
```
(FNES), 120, 267, 278, 321
```
Feedback perfect state information
```
(FB), 221, 226
```
Feedback saddle-point strategy, 47
Feedback Stackelberg cost, 137
Feedback Stackelberg solution, 136
Feedback strategies, 247
Finite game, 3
Finite set, 471
Finite-dimensional set, 472
Focal line, 447
Follower, 11, 131, 146
Followers' group, 146
Function, 472
Functional, 472
Game of degree, 220, 435
Game of kind, 220, 434
Global Stackelberg, 136
Globally minimizing solution, 474
Gradient, 474
Graphical solution, 30
```
Hamilton-Jacobi-Bellman (HJB) equa-
```
tion, 236
Hamilton-Jacobi-Isaacs equation, 326
Hessian matrix, 474
Hierarchical equilibrium, 146
Hierarchical equilibrium strategy, 146
Homicidal chauffeur game, 436, 437
Identity matrix, 473
Incentives, 392
Independent, 479
Infinite dimensional set, 472
Infinite game, 10
Infinite set, 471
Infinite-horizon, 262
Information, 219
Information field, 225
Information set, 37, 98, 149
Information space, 220
Information structure, 225
Informational inferiority, 103, 122
Informational nonunique Nash equi-
libria, 265
Informational nonuniqueness, 105
Informationally inferiority, 296
Initial state, 219
Interior, 472
Isaacs condition, 326, 353, 428
Isaacs equation, 326, 423
Iso-cost curve, 169
Isochrone, 245
Kruzkov-transform, 259
Ladder-nested form, 105
Lady in the lake, 10, 429
Leader, 11, 131
Leader's Stackelberg cost, 180
Leaking corner, 440
INDEX 517
Lebesgue measure, 479
Lemke-Howson algorithm, 96
Limit point, 472
Linear independence, 471
Linear programming, 31
Linear-quadratic continuous-time op-
timal control, 238
Linear-quadratic discrete-time opti-
mal control, 234
Linearly independent vectors, 471
Lion and man, 468
Lipschitz-continuity, 227
Locally stable Nash equilibrium, 171
Loop model, 216
Loss-ceiling, 18
Lower value, 19, 164, 432
Marginal distribution function, 479
Markov games, 262
Matrix elements, 473
Matrix entries, 473
```
Mean (expected) value, 480
```
Measurable function, 478
Measurable space, 478
Measurable transformation, 478
Memoryless perfect state informa-
```
tion (MPS), 221, 226
```
Mental poker, 154
Minimal cost, 233
Minimax solution, 11
Minimax theorem, 26, 211
Minimax values of a bimatrix game,
84
Minimizing element, 474
Minimum cost, 233
Minimum principle, 246
Miss distance, 452
Mixed epsilon Nash equilibrium so-
lution, 166
Mixed saddle-point strategy, 163
Mixed security strategy, 25, 163
Mixed Stackelberg equilibrium strat-
egy, 141, 182
Mixed strategies, 11, 23, 91, 182,
232
```
Mixed-strategy noncooperative (Nash)
```
equilibrium solution, 91
Multi-act game, 39
Multi-act zero-sum games, 45
n-dimensional Euclidean space, 471
N-person deterministic multi-stage
game, 219
N-person nonzero-sum feedback game
in extensive form, 119
N-person nonzero-sum feedback game
in ladder-nested form, 119
Nash condition, 352
Nash equilibrium, 4
Nash equilibrium solution, 163
Nature, 2, 223
Near miss, 458
Near miss maneuver, 462
Negative definite, 473
Nested, 105
Nested form, 119
Nested information, 405
Noisy observation, 362
Nonconcurrent Stackelberg solution,
186
```
Noncooperative (Nash) equilibrium
```
outcome, 79, 85, 89, 91
```
Noncooperative (Nash) equilibrium
```
solution, 78, 85, 88, 99
Noncooperative game, 1
Nondenumerable set, 471
Nonnegative definite, 473
Nonnested information, 405
Nonpositive definite, 473
Norm, 472
Normal form, 36, 221
Normed linear vector space, 472
Observation, 219
Obstacle tag, 468
Oligopoly, 198
One-step delayed observation shar-
```
ing pattern (1DOS), 222
```
One-step memory, 248
Open set, 472
```
Open-loop (OL), 10, 221, 225
```
518 INDEX
Open-loop games, 53
Open-loop Nash equilibrium solution,
267
Open-loop realization, 247
Open-loop strategies, 247
Open-loop value, 247
Optimal control theory, 2
Optimal response, 169
Optimal response set, 133, 143, 146
Optimal solution, 474
Optimal strategy, 134, 146, 180
Optimal value, 475
Optimality, 11
Ordered interchangeability of sad-
dle points, 177
Pareto-optimal solution, 5
```
Partial differential equation (PDE),
```
236
Pay-off function, 60
Penetration, 436
Perfect equilibrium, 129
```
Person-by-person (pbp) optimality,
```
196
Payability, 229
Playable strategies, 229
Player, 1
Player sets, 98
Players' set, 219, 224
Positive definite, 473
Positive semidefinite, 473
Precedence of players, 105
Price game, 201
Princess and the monster, 10
Prior commitment, 54, 108
Prisoners' dilemma, 83
Probability density function, 479
Probability distribution, 149, 478,
479
Probability measure, 478
Probability space, 478
Proper equilibrium, 130
Pure strategy, 21, 38
Pursuer, 424
Quadratic games, 190
Qualitative game, 220
Quantitative game, 220
Quantity game, 201
Quasi-perfect equilibrium, 131
Random variable, 478
Randomized strategies, 56
Rational reaction set, 169
Reaction curve, 169
Reaction function, 169
Relative coordinate system, 448
Repeated games, 159
Representation of a strategy, 248,
297
Robust Nash solution, 171
Rope-pulling game, 4
Saddle point in behavioral strate-
gies, 44
Saddle point in mixed strategies, 25
Saddle-point equilibrium, 21
Searchlight game, 66
Security level, 18
Security strategy, 18
Semi-infinite bimatrix game, 166
Semipermeable, 427
Semipermeable surface, 435, 460
Separating hyperplane theorem, 474
Sequential equilibrium, 131
Ship collision avoidance, 458
Sigma algebra, 478
Simplex, 23
Simultaneous confrontation, 458
Single-act game, nonzero-sum, 40
Single-act game, zero-sum, 41
Singleton set, 374
Singular lines, 442
```
Singular surfaces (line), 442
```
Stable Nash equilibrium, 171
Stackelberg cost, 141
Stackelberg equilibrium, 133, 134,
179
Stackelberg game, 11
Stage of the game, 219
Stage-additive cost function, 221
INDEX 519
```
Stagewise (Nash) equilibrium solu-
```
tion, 298
State equation, 219
State space, 219
State trajectory, 224
```
Static (open-loop) version, 122
```
Static normal form, 103, 122
Static sub-extensive form, 106
Static version of a dynamic game,
103
Stationary strategies, 262
Stochastic differential equation, 230
Stochastic differential game, 230, 263
Stochastic games, 222
Stochastic incentive problems, 405
Stochastic systems, 252
Strategic equilibrium, 131
Strategic equivalence, 34, 81, 90
Strategic form, 3
Strategy, 5, 18, 220
Strategy set, 39, 99
Strategy space, 39, 91, 99
Strict dominance, 35
Strong positive definiteness, 190
Strong time consistency, 250
Switching envelope, 447
Switching function, 244
Switching surface, 442
Symmetric game, 71
Target set, 228, 424
Team problem, 81, 196, 299, 378
Team-optimal solution, 196
Terminal cost functional, 222
Terminal time, 228
Termination, 220, 229, 431
Termination dilemma, 432
Threat, 392
Threat strategy, 380
Time consistency, 249
Time inconsistency, 250
Trajectory space, 224
```
Transition surface (line), 442
```
Tree structure, 36
Trembling hand equilibrium, 129
Two-cars model, 437
Two-target games, 456
Undecomposable extensive form, 112
```
Universal surface (line), 444
```
Unstable Nash equilibrium, 171
Upper value, 19, 164, 432
```
Usable part (UP), 245, 435
```
Value, 21, 164
Value function, 234, 236, 424
Value in mixed strategies, 26
Vector space, 471
Viscosity solution, 257, 432
Viscosity subsolution, 257, 260
Viscosity supersolution, 257, 260
Weak time consistency, 250
Weierstrass theorem, 475
Wiener process, 230
Zero-sum game, 3
Zero-sum games with chance moves,
57