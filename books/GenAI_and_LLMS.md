
_Editors_
Sukhdeep Singh, Madhan Raj Kanagarathinam, Mohan Rao GNS,
Yulei Wu, Navrati Saxena, Ashok Kumar Reddy Chavva and
Ali Kashif Bashir

##### GenAI and LLMs for Beyond 5G

##### Networks

```
OceanofPDF.com
```

_Editors_
Sukhdeep Singh
Samsung R&D India-Bangalore, Bengaluru, Karnataka, India

Madhan Raj Kanagarathinam
Samsung R&D India-Bangalore, Bengaluru, Karnataka, India

Mohan Rao GNS
Samsung R&D India-Bangalore, Bengaluru, Karnataka, India

Yulei Wu
University of Bristol, Bristol, UK

Navrati Saxena
San Jose State University, San Jose, CA, USA

Ashok Kumar Reddy Chavva
Samsung R&D India-Bangalore, Bengaluru, Karnataka, India

Ali Kashif Bashir
Manchester Metropolitan University, Manchester, UK

ISBN 978-3-032-06417-2 e-ISBN 978-3-032-06418-
https://doi.org/10.1007/978-3-032-06418-

© The Editor(s) (if applicable) and The Author(s), under exclusive license
to Springer Nature Switzerland AG 2026

This work is subject to copyright. All rights are solely and exclusively
licensed by the Publisher, whether the whole or part of the material is
concerned, specifically the rights of translation, reprinting, reuse of
illustrations, recitation, broadcasting, reproduction on microfilms or in any
other physical way, and transmission or information storage and retrieval,


electronic adaptation, computer software, or by similar or dissimilar
methodology now known or hereafter developed.

The use of general descriptive names, registered names, trademarks, service
marks, etc. in this publication does not imply, even in the absence of a
specific statement, that such names are exempt from the relevant protective
laws and regulations and therefore free for general use.

The publisher, the authors and the editors are safe to assume that the advice
and information in this book are believed to be true and accurate at the date
of publication. Neither the publisher nor the authors or the editors give a
warranty, expressed or implied, with respect to the material contained
herein or for any errors or omissions that may have been made. The
publisher remains neutral with regard to jurisdictional claims in published
maps and institutional affiliations.

This Springer imprint is published by the registered company Springer
Nature Switzerland AG
The registered company address is: Gewerbestrasse 11, 6330 Cham,
Switzerland

```
OceanofPDF.com
```

**Preface**

In the quiet hum of a world connected by invisible threads of data, where
smart cities pulse with intelligence and autonomous systems dance in
seamless harmony, a new era of telecommunications is dawning. Beyond
5G (B5G) and 6G networks, infused with the transformative power of
generative artificial intelligence (GenAI) and large language models
(LLMs), are not merely advancements in technology; they are a
reimagining of how we connect, compute, and coexist. This book is born
from a shared vision to capture this moment of convergence, where silicon
meets algorithms and human ingenuity meets the boundless potential of AI-
native networks. It is a technical tapestry, woven from the threads of
innovation, responsibility, and ambition, designed to guide researchers,
engineers, and visionaries through the intricate landscape of next-generation
connectivity.
The genesis of this work lies in countless hours spent in labs,
boardrooms, and late-night discussions, where the promise of B5G
networks sparked both awe and determination. As we stand at the cusp of
6G, we see networks evolving from mere conduits of data to intelligent
ecosystems that anticipate needs, secure data, and adapt in real time. GenAI
and LLMs, with their ability to reason, generate, and optimize across
modalities, are at the heart of this transformation. From edge devices
crunching inferences on neural processing units to radio access networks
orchestrating beams with AI-driven precision, these technologies are
redefining the boundaries of what is possible. Yet, they also bring
challenges, computational constraints, ethical dilemmas, and regulatory
complexities that demand rigorous solutions and thoughtful stewardship.
This book is a journey through that landscape, exploring the
architectural innovations that bring GenAI to resource-constrained edge
devices, the security frameworks that fortify AI-native networks, and the
orchestration systems that translate human intent into autonomous action. It
delves into the mathematical convergence of AI and signal processing, the
generative modeling of wireless channels, and the analytics that turn raw
data into actionable intelligence. Each chapter is a piece of the puzzle,
meticulously crafted to balance technical depth with practical relevance,


offering insights for those building the networks of tomorrow, whether
through code, policy, or vision.
More than a technical compendium, this book is a call to action. It
invites readers to grapple with the profound implications of AI-driven
connectivity: How do we ensure that networks are not only fast and smart
but also equitable and trustworthy? How do we balance innovation with
responsibility in a world where data flows like a digital bloodstream? As
researchers, we have poured our curiosity into these pages; as engineers,
our pragmatism; and as storytellers, our hope. To the student exploring AI’s
potential, the practitioner deploying next-generation systems, or the
policymaker shaping the rules of this new frontier, this book is for you. It is
an invitation to dream boldly, build wisely, and connect a world where
technology serves humanity’s highest aspirations. Welcome to the future of
B5G networks—let us build it together.

**Disclaimer** The ideas and proposals discussed in the chapters are the
individual thoughts of the authors and do not reflect any organization’s
thoughts or information.

```
Sukhdeep Singh
Madhan Raj Kanagarathinam
Mohan Rao GNS
Yulei Wu
Navrati Saxena
Ashok Kumar Reddy Chavva
Ali Kashif Bashir
Bengaluru, Karnataka, India
Bengaluru, Karnataka, India
Bengaluru, Karnataka, India
Bristol, UK
San Jose, CA, USA
Bengaluru, Karnataka, India
Manchester, UK
OceanofPDF.com
```

**Contents**

**Introduction**
Sukhdeep Singh, Madhan Raj Kanagarathinam and Mohan Rao GNS

**Unlocking One-for-All Generative AI at Edge**
Srinivas Soumitri Miriyala, Vikram Nelvoy Rajendiran and
Sharan Allur

**Legal and Regulatory Frameworks Governing Generative AI for
Enterprises**
Anish Kumar

**Securing GenAI and LLMs in Beyond 5G/ 6G UE and Networks**
Ramesh Chandra Vuppala and Rajavelsamy Rajadurai

**Designing Guardrails: Ensuring Responsible AI Behavior**
Anish Kumar

**GENATWIN: Enabling Scalable AI Evaluation in B5G Networks: A
GenAI-Powered Digital Twin Framework**
Sukhdeep Singh, Swaraj Kumar, Peter Moonki Hong, Ashish Jain,
Madhan Raj Kanagarathinam, Krishna M. Sivalingam and
Hemant Kumar Narsani

**Reasoning Foundations for Generative AI-Based Wireless Networks**
Christo Kurisummoottil Thomas, Omar Hashash and Walid Saad

**Computational Aspects of Generative-AI on Radio Access Networks
(RAN)**
Mahantesh Kothiwale and Anshuman Nigam

**GenAI and LLMs for Source and Channel Coding in B5G Networks**
Jaswanthi Mandalapu, Abhishek Roy and Navrati Saxena

**Modeling Wireless Channels with Generative AI**
Ashok Kumar Reddy Chavva, Divpreet Singh and
Yeswanth Reddy Guddeti


**Machine Learning-Based Intelligent Anomaly Detection and
Maintenance in RAN**
K. Anoop

**Next-Generation Intent-Based Networking in B5G with LLMs and
Explainable AI**
Caglar Tunc and Kaustubh Joshi

**A Framework for Intent-Driven Kubernetes Configuration Generation
Using Large Language Models**
Anukul Parajuli, Krishna M. Sivalingam and
Madhan Raj Kanagarathinam

**Agent Orchestration for Resource Allocation in Self-Healing Networks**
Priyanka V. Galagali

**Leveraging Large Language Models (LLMs) for Service Optimization
in Beyond 5G (B5G) Networks**
Shapna Muralidharan, Wonhee Woo and Nakho Lee

**AI-Based On-Device Traffic Classification for Next Generation Mobile
Network**
Madhan Raj Kanagarathinam, Krishna M. Sivalingam, Hyunwoo Choi
and JongMu Choi

**Advanced Data Management and Analytics Using LLMs for B5G
Networks**
Pankaj Thorat

**Conclusion**
Sukhdeep Singh, Madhan Raj Kanagarathinam and Mohan Rao GNS
_OceanofPDF.com_


(1)

© The Author(s), under exclusive license to Springer Nature Switzerland AG 2026
S. Singh et al. (eds.), _GenAI and LLMs for Beyond 5G Networks_
https://doi.org/10.1007/978-3-032-06418-9_

##### Introduction

Sukhdeep Singh^1 , Madhan Raj Kanagarathinam^1 and Mohan Rao GNS^1
Samsung R&D Institute India—Bangalore (SRI-B), Bangalore, India

```
Sukhdeep Singh
Email: sukh.sandhu@samsung.com
```
The telecommunications landscape is on the cusp of a profound
transformation, driven by the advent of Beyond 5G (B5G) and 6G
networks. These next-generation systems promise to deliver ultra-low
latency, massive device connectivity, and unprecedented reliability,
enabling a new wave of applications that will redefine how we live, work,
and interact. From smart cities that optimize energy and traffic flows in real
time to autonomous vehicles navigating complex environments, from
immersive metaverse experiences to intelligent industrial systems, B5G and
6G networks are set to unlock possibilities that were once confined to the
realm of science fiction. At the heart of this revolution lies generative
artificial intelligence (GenAI) and large language models (LLMs), which
are not merely enhancing network capabilities but fundamentally reshaping
how networks are designed, deployed, and managed. These AI
technologies, powered by advancements in deep learning and transformer
architectures, are enabling networks to become AI-native, with intelligence
embedded at every layer, from edge devices to core infrastructure. This
book offers a comprehensive exploration of the role of GenAI and LLMs in
B5G networks, providing a blend of cutting-edge research, practical
frameworks, and forward-looking insights for researchers, engineers,
policymakers, and industry leaders.
The rise of GenAI and LLMs has been nothing short of revolutionary.
These models, capable of processing and generating diverse data modalities


such as text, images, and network telemetry, have demonstrated remarkable
prowess in understanding complex patterns, reasoning through ambiguity,
and generating human-like outputs. In the context of B5G networks, their
potential is transformative. They enable real-time network optimization,
predictive maintenance, and intent-based orchestration, allowing operators
to meet the stringent demands of modern applications while improving
efficiency and user experience. However, integrating these powerful AI
models into telecommunications systems presents significant challenges.
Deploying GenAI on resource-constrained edge devices requires
overcoming computational limitations, while ensuring security, scalability,
and ethical alignment demands careful consideration. Regulatory
frameworks, privacy concerns, and the need for explainable AI further
complicate the landscape. This book addresses these challenges
systematically, offering a roadmap for harnessing the full potential of
GenAI and LLMs in creating intelligent, secure, and scalable B5G
networks.
The chapters that follow provide a detailed examination of the
multifaceted role of GenAI and LLMs in B5G networks, covering technical
innovations, practical applications, and ethical and regulatory
considerations. Each chapter is designed to stand alone while contributing
to a cohesive narrative about the future of AI-native connectivity. Below,
we outline the contributions of Chapters 2 through 16 in order, providing a
comprehensive overview of the transformative technologies and strategies
that will shape the next generation of telecommunications.


**1 Chapter 2 : Unlocking One-for-All Generative**

**AI at Edge**

The shift from cloud-centric to edge-driven AI is a cornerstone of B5G
networks, enabling low-latency, privacy-preserving, and responsive
applications. Chapter 2 explores how GenAI can be deployed on edge
devices equipped with specialized accelerators like neural processing units
(NPUs). It delves into architectural and algorithmic innovations, such as
model compression, quantization-aware training, and edge-compatible
decoding strategies, that make this possible. The chapter also examines the
integration of LoRA-based personalization and federated fine-tuning, which
allow AI models to adapt to user-specific needs while maintaining privacy.
Within the context of B5G systems, these techniques enable applications
like real-time industrial control and autonomous driving, where split-second
decisions are critical. By providing a blueprint for researchers and
practitioners, this chapter highlights how edge-based GenAI can unlock
scalable, efficient, and secure inference in resource-constrained
environments.


**2 Chapter 3 : Legal and Regulatory Frameworks**

**Governing Generative AI for Enterprises**

As GenAI becomes integral to telecommunications, navigating the complex
web of legal and regulatory frameworks is essential. Chapter 3 provides a
comprehensive review of global and regional regulations, including the
EU’s General Data Protection Regulation (GDPR), the AI Act, U.S. FCC
regulations, and India’s Digital Personal Data Protection (DPDP) Act. It
addresses critical issues such as intellectual property rights for AI-generated
content, model liability, and the need for explainability in enterprise
settings. The chapter also explores enterprise-level compliance strategies,
offering practical guidance for organizations seeking to deploy GenAI
responsibly. By balancing innovation with regulatory adherence, this
chapter underscores the importance of building trust in AI-driven
telecommunications systems, ensuring they are both legally compliant and
ethically sound.


**3 Chapter 4 : AI Security for Beyond 5G/6G UE**

**and Networks**

Security is a paramount concern in B5G and 6G networks, where AI-native
architectures introduce both opportunities and vulnerabilities. Chapter 4
examines the dual nature of GenAI and LLMs as both powerful tools for
enhancing cybersecurity and potential targets for malicious actors. It
explores how these models can enable proactive threat detection, anomaly
analysis, and distributed intelligence to safeguard user data and network
integrity. At the same time, it addresses risks such as adversarial attacks that
exploit generative capabilities. The chapter emphasizes secure-by-design
principles, robust encryption methods, and the integration of AI to protect
against cyberattacks like hacking and distributed denial-of-service (DDoS).
By outlining strategies for balancing security and innovation, this chapter
provides a foundation for building resilient B5G networks.


**4 Chapter 5 : Designing Guardrails: Ensuring**

**Responsible AI Behavior**

The responsible deployment of GenAI in enterprise workflows requires
robust guardrails to ensure safety, fairness, and controllability. Chapter 5
explores strategies such as prompt filtering, output validation,
reinforcement learning with human feedback (RLHF), and alignment tuning
to achieve these goals. It also emphasizes the role of governance policies
and human-in-the-loop systems in maintaining accountability. By
grounding AI models with enterprise knowledge bases and ensuring
explainable outputs, these guardrails build trust in AI-driven network
operations. This chapter is particularly relevant for organizations seeking to
integrate GenAI into mission-critical applications, where transparency and
ethical alignment are nonnegotiable.


**5 Chapter 6 : GENATWIN: Enabling Scalable AI**

**Evaluation in B5G Networks: A GenAI-Powered**

**Digital Twin Framework**

Traditional methods for evaluating AI models, such as static digital twins
and offline simulations, often fall short in dynamic B5G environments.
Chapter 6 introduces GENATWIN, a novel framework that leverages
conditional generative adversarial networks (cGANs) to simulate realistic
network behaviors under diverse scenarios. By incorporating contextual
features like traffic patterns and network topologies, GENATWIN enables
iterative model validation, automated stress testing, and scenario-specific
performance analysis. This chapter details the framework’s layered
architecture, training processes, and real-world applicability, demonstrating
how it reduces operational risks and costs for telecom operators.
GENATWIN represents a significant step toward confident AI deployment
in B5G networks.


**6 Chapter 7 : Reasoning Foundations for**

**Generative AI-Based Wireless Networks**

The black-box nature of traditional GenAI models poses challenges for
interpretability, generalizability, and sustainability in wireless networks.
Chapter 7 explores reasoning-driven approaches, such as causal reasoning
and neurosymbolic AI, to address these limitations. By grounding AI
models in the physics of wireless transmission and combining symbolic and
neural approaches, this chapter proposes solutions for enhanced decision
explainability and resilience. It presents experimental results demonstrating
how retrieval-augmented generation (RAG) improves accuracy and
mathematical reasoning in wireless question-answering tasks. These
advancements are critical for applications like digital twin-driven smart
industries and autonomous network experiences, paving the way for
trustworthy B5G systems.


**7 Chapter 8 : Computational Aspects of**

**Generative AI on Radio Access Networks (RAN)**

The convergence of AI and radio access network (RAN) signal processing
is transforming mobile infrastructure. Chapter 8 highlights the shared
mathematical foundations—such as matrix multiplications and
decomposition techniques—that enable the integration of AI and RAN
workloads. It discusses how scalar CPUs, vector processors, GPUs, and
emerging data processing units (DPUs) like NVIDIA BlueField-3 support
this convergence. The chapter also explores the potential of RISC-V for
customizable, efficient compute platforms tailored to RAN and GenAI
demands. By enabling programmable, intelligent, and multi-tenant RANs,
these advancements support AI-native services like real-time beam control
and edge inference, shaping the future of mobile connectivity.


**8 Chapter 9 : GenAI and LLMs for Source and**

**Channel Coding in B5G Networks**

Traditional source and channel coding methods, based on Shannon’s
separation principle, struggle to meet the demands of dynamic B5G
environments. Chapter 9 reviews how GenAI and LLMs enable joint
source-channel coding for ultra-reliable, low-latency communications. By
emphasizing semantic and task-oriented approaches, these techniques
improve transmission efficiency and adaptability. The chapter provides a
deep dive into theoretical insights and recent deep learning advancements,
offering practical solutions for data-intensive applications in B5G networks,
such as IoT and immersive media.


**9 Chapter 10 : Modeling Wireless Channels with**

**Generative AI**

Accurate modeling of time-varying and frequency-selective wireless
channels is critical for robust system design. Chapter 10 explores generative
AI approaches, including generative adversarial networks (GANs) and
diffusion-based modeling with U-Net architectures, to simulate realistic
channel behaviors. It also discusses large-scale models for downstream
applications like channel estimation and interference detection. By reducing
reliance on time-consuming measurement campaigns, these methods
streamline the development of wireless systems, making them more
adaptable to diverse B5G environments.


**10 Chapter 11 : Machine Learning-Based**

**Intelligent Anomaly Detection and Maintenance in**

**RAN**

As RANs grow in complexity with 5G and Open RAN architectures,
traditional fault detection methods are becoming inadequate. Chapter 11
explores how machine learning enables intelligent anomaly detection and
predictive maintenance. It discusses supervised, unsupervised, and time-
series models, proposing a modular system architecture that reduces
operational overhead and enhances service quality. Real-world and
simulated use cases demonstrate how ML models detect subtle anomalies
and predict failures, ensuring high reliability in B5G networks.


**11 Chapter 12 : GenAI and LLMs for Service**

**Management and Orchestration**

The complexity of B5G networks demands intelligent, adaptive automation.
Chapter 12 proposes using LLMs for intent-based networking (IBN), where
operators express high-level objectives that the system translates into
actionable policies. It explores how LLMs resolve conflicting intents,
generate policies, and provide explainable rationales using techniques like
SHAP and counterfactual reasoning. By enabling zero-touch service
management and auditable orchestration, this chapter bridges LLM research
with practical telecom challenges, fostering trust and efficiency in B5G
systems.


**12 Chapter 13 : Leveraging LLM for Intent-Based**

**Networking: A Framework for Automated**

**Configuration and Optimization**

Intent-based networking (IBN) allows operators to define network
objectives without specifying implementation details. Chapter 13 focuses
on using LLMs to generate Kubernetes YAML configurations based on
operator prompts. By fine-tuning LLMs with metadata and YAML data, this
framework automates configuration and optimization processes, enhancing
flexibility and efficiency. The chapter provides a detailed look at intent
understanding, translation, and execution, offering a scalable solution for
domain-independent network management in B5G systems.


**13 Chapter 14 : Agent Orchestration for Resource**

**Allocation in Self-Healing Networks**

Self-healing networks are critical for maintaining service continuity in B5G
environments. Chapter 14 examines how LLM-driven agents enable
dynamic resource allocation, anomaly detection, and autonomous
reconfiguration. It discusses multiagent coordination, integration with
software-defined networking (SDN), and network function virtualization
(NFV), and challenges like model reliability and real-time performance. By
outlining a path toward fully autonomous, intent-driven ecosystems, this
chapter highlights the transformative potential of AI in network resilience.


**14 Chapter 15 : Leveraging Large Language**

**Models (LLMs) for Service Optimization in**

**Beyond 5G (B5G) Networks**

The massive connectivity and heterogeneous traffic in B5G networks pose
challenges for maintaining quality of service (QoS). Chapter 15 reviews
how LLMs enable traffic forecasting and intelligent load balancing,
addressing the needs of IoT-driven applications like fleet management. By
optimizing resource utilization and reducing operational overhead, these
solutions enhance network performance and user experience, making them
essential for the data-intensive demands of B5G environments.


**15 Chapter 16 : Context-Aware eBPF-Assisted AI**

**for Smartphone Traffic Classification**

Efficient traffic classification is vital for optimizing smartphone network
performance. Chapter 16 presents a novel eBPF-assisted AI solution that
leverages the extended Berkeley Packet Filter (eBPF) to reduce packet
processing overhead. By enabling context-aware, power-efficient
classification, this approach achieves significant reductions in processing
time and CPU utilization. Its successful deployment in Samsung Galaxy
S24 devices validates its robustness, offering a scalable foundation for
future network monitoring and optimization.


**16 Chapter 17 : Advanced Data Management and**

**Analytics Using LLMs for B5G Networks**

The exponential growth of data in B5G networks demands advanced
analytics and intelligence. Chapter 17 explores how LLMs enable
predictive and real-time analytics through AI-as-a-Service (AIaaS)
architectures. It outlines methodologies like corpus enrichment, LLM-ML
teaming, and semantic interpretation, supported by practical use cases and
technical architectures. By facilitating explainable, risk-governed
operations, this chapter provides a roadmap for intelligent data management
in B5G infrastructures.
This book is a call to action for those shaping the future of connectivity.
By blending rigorous research with practical applications, it equips readers
to tackle the technical, ethical, and regulatory complexities of AI-native
B5G networks. Whether you’re a researcher exploring AI’s frontiers, an
engineer building next-generation infrastructure, or a policymaker crafting
governance frameworks, this book offers a comprehensive guide to creating
a smarter, more connected, and equitable world.

```
OceanofPDF.com
```

(1)

© The Author(s), under exclusive license to Springer Nature Switzerland AG 2026
S. Singh et al. (eds.), _GenAI and LLMs for Beyond 5G Networks_
https://doi.org/10.1007/978-3-032-06418-9_2

##### Unlocking One-for-All Generative AI at

##### Edge

Srinivas Soumitri Miriyala^1 , Vikram Nelvoy Rajendiran^1 and Sharan Allur^1

```
Samsung Research Institute Bangalore, Bangalore, India
```
```
Sharan Allur
Email: sharan.allur@samsung.com
```
**Abstract**
The rapid advancement of Large Language Models (LLMs) has ushered in
a new era of generative AI capabilities, revolutionizing language
understanding, reasoning, and multimodal interaction. However, their
deployment is largely tethered to cloud infrastructure, limiting accessibility,
increasing latency, and raising privacy concerns. With the proliferation of
edge devices equipped with specialized accelerators like NPUs, there is a
compelling opportunity to shift GenAI to the edge—enabling local,
responsive, and private inference. This chapter provides a comprehensive
account of architectural, algorithmic, and deployment innovations that
unlock generative AI on resource-constrained platforms. We explore model
compression techniques, quantization-aware training, runtime optimization,
and edge-compatible decoding strategies. We further examine the
integration of LoRA-based personalization, federated fine-tuning, and
lightweight vision-language models, all within the context of Beyond 5G
(B5G) systems. Special attention is given to runtime toolchains, hardware-
aware packaging, and communication-efficient inference. This chapter
offers a complete blueprint for researchers and practitioners seeking to build
responsive, secure, and scalable GenAI on the edge.


**Keywords** Generative AI – Edge AI – Large Language Models – Model
compression – Quantization – Low-rank adaptation – On-device inference –
Federated learning – Efficient decoding – B5G networks – Runtime
optimization – Token pruning – Collaborative inference

**1 Introduction**

Large Language Models (LLMs) have significantly advanced the field of
artificial intelligence, ushering in a new era of generative capabilities across
diverse tasks such as text generation, summarization, translation, question
answering, and common-sense reasoning. With models like GPT-3 [ 1 ],
PaLM 2 [ 2 ], GPT-4 [ 3 ], LLaMA [ 4 ], Mistral [ 5 ], and Qwen2.5 [ 6 ], the scale
and versatility of LLMs have reached unprecedented levels, enabling few-
shot and zero-shot generalization across a wide range of natural language
understanding tasks. These models, trained on web-scale corpora and
billions of parameters, display emergent properties that allow them to adapt
to unseen inputs, languages, and domains without specific fine-tuning.
Despite these advances, the deployment of LLMs has remained largely
constrained to centralized cloud environments. Their inference requires
high memory bandwidth, substantial GPU resources, and efficient memory
pipelines—making them expensive to operate and inaccessible in latency-
sensitive or privacy-critical settings. This dependency on cloud
infrastructure not only introduces network-induced latency and reliability
issues but also raises concerns regarding data privacy, cost of access, and
carbon footprint. As a result, there is growing interest in shifting LLM
inference toward the edge—especially onto consumer devices such as
smartphones.
Smartphones, being the most widespread computing devices globally,
are increasingly equipped with hardware accelerators like Neural
Processing Units (NPUs), Digital Signal Processors (DSPs), and optimized
GPUs. While these platforms are inherently constrained by power, thermal
budgets, and memory, they offer a unique opportunity to deliver
personalized, offline, and privacy-respecting GenAI experiences.
Applications such as smart keyboards, voice assistants, AI-enhanced
imaging, and real-time summarization stand to benefit from running LLMs
natively on-device.


This edge-centric shift also aligns with the broader vision of Beyond 5G
(B5G) and 6G networks, which aim to embed intelligence across all layers
of the communication stack—from base stations to endpoints. In this vision,
smartphones are no longer passive consumers of intelligence but active
inference nodes in a decentralized AI ecosystem. B5G networks emphasize
ultra-reliable low-latency communication (URLLC), high device density,
and edge-native intelligence—making them an ideal substrate for on-device
GenAI.
Yet, deploying LLMs on smartphones is fraught with technical
challenges. The enormous parameter count of modern LLMs leads to high
storage and memory requirements. Autoregressive decoding introduces
sequential bottlenecks that hinder real-time interaction. Additionally,
ensuring compatibility across fragmented mobile chipsets and operating
systems demands highly optimized runtimes and compilers.
To address these challenges, several innovations have emerged across
the stack—from model compression and quantization to runtime
optimizations and adapter-based personalization. At the model level,
techniques such as structured pruning [ 7 ], knowledge distillation [ 8 ], post-
training quantization [ 9 ], activation-aware quantization [ 10 ], and quantized
fine-tuning via QLoRA [ 11 ] enable efficient deployment without substantial
loss in accuracy. Latency-reduction methods like speculative decoding [ 12 ],
Bi-level token prediction [ 13 ], and Self-Speculative Decoding [ 14 ] have
been proposed to accelerate inference further.
In parallel, the deployment toolchain has matured significantly.
Compiler-driven runtimes like Machine Learning Compilation for Large
Language Models (MLC-LLM) by TVM [ 15 ], lightweight C++
implementations like llama.cpp, and cross-platform quantized formats like
GGUF enable high-throughput inference even on mobile CPUs and NPUs.
Hardware-aware graph optimizations, mixed-precision scheduling, and
token-wise streaming are now practical for consumer hardware.
Additionally, lightweight adaptation methods such as Low-Rank
Adaptation (LoRA) [ 16 ], AdapterFusion, and Intrinsically Aligned Adapters
(IA3) allow for domain-specific or user-specific personalization without
retraining the full model. These adapter modules can be swapped in real-
time, enabling multitask and multilingual functionality under tight resource
constraints.


Importantly, while this chapter focuses on LLMs, there is a growing
intersection between text and vision in the form of Large Multimodal
Models (LMMs). Vision-language models like SAM [ 17 ], DINOv2 [ 18 ],
and CLIP [ 19 ] can be fused with LLMs to create powerful multimodal
agents capable of image captioning, visual question answering, and
contextual scene understanding. On-device deployment of such models
introduces further complexity due to larger visual inputs and dual-modality
compute graphs—but early work in quantized vision transformers and
modular multimodal adapters is showing promise.
This chapter aims to provide a comprehensive treatment of these
developments. We examine compression strategies, quantization techniques,
decoding optimizations, runtime systems, and personalization approaches
that make LLMs viable on smartphones. We also touch upon their
integration into communication systems, collaborative intelligence in B5G,
and the role of lightweight vision-language pipelines. Through this, we
hope to chart a roadmap for unlocking the full potential of generative AI at
the edge.

**2 Evolution of LLMs and the Shift to Edge**

The journey of Large Language Models (LLMs) from compact recurrent
networks to multi-billion parameter transformer architectures marks one of
the most significant evolutions in artificial intelligence. Early approaches to
language modelling relied heavily on statistical methods, such as n-gram
models, and evolved into Recurrent Neural Networks (RNNs) and Long
Short-Term Memory (LSTM) networks. These models, while foundational,
were limited in their capacity to model long-range dependencies and
suffered from issues like vanishing gradients. Their sequential nature
further hindered parallelization, making them inefficient for large-scale
deployment.
A transformative milestone was the introduction of the Transformer
architecture by Vaswani et al. [ 20 ], which replaced recurrence with a self-
attention mechanism (see Fig. 1 ). This innovation allowed for simultaneous
processing of sequence tokens and enabled the capture of long-range
contextual relationships. Models such as BERT [ 21 ], which focused on
bidirectional masked language modelling, and GPT-2 [ 22 ], designed for
autoregressive generation, were built on the Transformer backbone to


achieve remarkable performance on a variety of natural language
processing tasks.

**_Fig. 1_** The evolution of model architecture from the vanilla variant proposed by Vasvani et al. [ 20 ]
to the widely used open-sourced LLAMA family [ 4 ]
Subsequent scale-ups in model size, dataset volume, and compute led to
the emergence of foundation models like GPT-3 [ 1 ], which demonstrated
the ability to generalize across tasks without task-specific training. These
advancements set the stage for successors (see Fig. 1 ) including PaLM 2
[ 2 ], GPT-4 [ 3 ], LLaMA [ 4 ], Mistral [ 5 ], and Qwen2.5 [ 6 ]. These models
pushed the boundaries of contextual reasoning, language generation, and


multi-language understanding. Alongside scale, innovations like sparse
attention, longer context windows, and instruction tuning further enhanced
their capabilities.
Despite their potential, these models were inherently designed for
cloud-scale infrastructure. Their high memory demands and reliance on
massive parallelism required the use of datacentre-grade GPUs, such as
NVIDIA A100s or H100s, often deployed across multiple nodes with
pipeline and tensor model parallelism. Inference typically occurred behind
closed APIs, abstracted away from the user and reliant on constant network
connectivity. While this setup provided centralized management and
scalability, it also introduced significant drawbacks: latency due to round-
trip communication, dependence on uninterrupted internet access, high
operational costs, and concerns over data privacy and sovereignty.
These limitations have fuelled a shift in focus toward edge deployment,
particularly on smartphones. As consumer devices become more powerful
and ubiquitous, they represent a compelling platform for localized AI.
Modern smartphones are equipped with dedicated AI accelerators, such as
Apple’s Neural Engine, Qualcomm’s Hexagon DSP, and MediaTek’s APU,
designed to handle low-power inference workloads efficiently. These
devices also provide a unique opportunity for user-contextual
personalization, private data handling, and offline access to intelligence.
However, transitioning LLMs from cloud to edge requires overcoming
several challenges. A 7B parameter model, when stored in full-precision
(FP32), can exceed 25 GB—far more than the memory available on a
smartphone. Even with FP16 or bfloat16 formats, the size remains
prohibitive. Moreover, the autoregressive nature of generation requires
sequential token-by-token decoding, which further strains runtime
responsiveness on constrained devices.
To address these issues, researchers have developed a suite of model
optimization techniques. Quantization has emerged as a key enabler, with
methods such as GPTQ [ 9 ] and AWQ [ 10 ] allowing post-training
conversion of model weights to INT4 and INT8 formats. These techniques
significantly reduce memory footprint and accelerate inference while
preserving most of the original model’s performance. Complementary
approaches like QLoRA [ 11 ] combine quantized base models with fine-
tuneable adapters, enabling multitask personalization without modifying the
backbone.


Another avenue of improvement focuses on decoding efficiency.
Traditional greedy or beam search decoding is slow and memory-intensive,
particularly for long sequences. Speculative decoding techniques, such as
EAGLE [ 12 ], BiTNet [ 13 ], and Self-Speculative Decoding [ 14 ], introduce
parallelism into the generation process by pre-generating multiple tokens
and verifying them in batches, reducing overall inference time without
requiring retraining or additional models.
The push for edge deployment is further supported by advancements in
software and runtime infrastructure. Frameworks like MLC-LLM [ 15 ] offer
compiler-based optimization pathways to translate LLMs into highly
efficient device-specific binaries. These toolchains integrate graph
optimizations, operator fusion, and token streaming, enabling real-time
inference on iOS, Android, and WebAssembly targets. Minimalist
alternatives like llama.cpp provide portable, single-threaded C++ runtimes
capable of executing INT4-quantized models with surprising efficiency—
even on mid-range smartphones.
The LLM deployment landscape is also witnessing the integration of
modular adaptation techniques. Methods such as Low-Rank Adaptation
(LoRA) [ 16 ] and IA3 introduce lightweight trainable modules into the
transformer architecture, allowing task-specific fine-tuning without
retraining the entire model. These adapters can be dynamically loaded at
runtime, facilitating rapid switching between tasks such as summarization,
translation, or chat-based assistance—all within a single on-device LLM.
While the focus of this shift has primarily been on language, the broader
GenAI movement also encompasses multimodal systems. Vision-language
models like SAM [ 17 ], DINOv2 [ 18 ], and CLIP [ 19 ] serve as high-capacity
visual encoders that, when paired with LLMs, create powerful multimodal
agents capable of grounding textual reasoning in visual input. Deploying
such models on mobile devices is even more demanding, given the high-
resolution image tensors and the need for cross-attention computation
between modalities. Nevertheless, initial experiments with quantized vision
encoders and lightweight fusion layers have shown that on-device
multimodal GenAI is both feasible and promising for applications in
augmented reality, accessibility, and visual search.
Finally, LLMs are being increasingly integrated into networking and
communication systems. In B5G architectures, LLMs play roles in intent-
based networking, semantic traffic analysis, and log summarization. They


are also being explored for natural language interfaces to control edge
nodes or configure base stations. These use cases benefit from low-latency,
privacy-preserving inference directly on the device or at the edge of the
network—further motivating the need for optimized, deployable models.
In summary, the evolution of LLMs from centralized to edge-native
systems is not only technologically feasible but also strategically aligned
with the future of mobile computing and communication. The confluence of
model compression, quantization, runtime innovation, and modular
adaptation is enabling a new generation of GenAI systems that are local,
efficient, and deeply personalized. In the next section, we will explore the
foundational techniques that make such edge deployment possible,
beginning with model compression and knowledge distillation.

**3 Model Compression and Distillation for On-**

**Device LLM Deployment**

As Large Language Models (LLMs) scale to billions of parameters, their
memory footprint and computational requirements render them impractical
for direct deployment on mobile or embedded devices. To enable real-time,
offline, and privacy-preserving inference, substantial model compression is
essential. Compression techniques aim to reduce model size, minimize
compute and memory requirements, and maintain acceptable levels of task
performance—all while ensuring compatibility with hardware accelerators
typically found in smartphones.
The three foundational pillars of LLM compression are pruning,
knowledge distillation, and parameter sharing. Each approach offers unique
trade-offs between compression ratio, fidelity, and computational cost.

**3.1 Structured and Unstructured Pruning**

Pruning [ 23 ] is one of the oldest and most effective strategies for reducing
model complexity. In the context of LLMs, it involves eliminating weights,
attention heads, or even entire layers that contribute minimally to model
outputs [ 24 ].
Unstructured pruning [ 25 ] removes individual weights with low
magnitude, creating sparse matrices. Although this results in high
compression ratios, it leads to irregular memory access patterns, making


acceleration difficult on mobile NPUs which prefer dense computations.
Structured pruning [ 26 ] addresses this by removing entire rows, channels,
or heads—thereby maintaining regularity and enabling hardware-friendly
sparsity.
Notably, pruning attention heads based on importance scores or
activation magnitude has been shown to preserve performance while
significantly reducing inference cost. Movement pruning [ 27 ] further
improves upon traditional strategies by tracking weight dynamics during
training and adaptively identifying candidates for removal.
Structured pruning often requires a fine-tuning phase post-pruning to
recover lost performance. In mobile deployments, pruning is particularly
valuable when combined with quantization, leading to both memory
savings and inference acceleration.

**3.2 Knowledge Distillation**

Knowledge distillation (KD) transfers the knowledge of a large teacher
model to a smaller student model that mimics its behaviour [ 28 ]. This
technique, first introduced by Hinton et al. [ 8 ], is especially appealing in
low-resource settings where training from scratch is infeasible.
In logit-based distillation, the student is trained to match the output
probability distribution of the teacher. While simple, this often lacks
expressivity for capturing intermediate reasoning steps. More advanced
forms of distillation involve aligning hidden layer activations, attention
maps, or gradients [ 29 ]. For example, patient knowledge distillation [ 30 ]
encourages deep alignment between intermediate layers of teacher and
student, improving convergence and generalization.
Multitask [ 31 ] and multi-teacher distillation [ 32 ] strategies are also
gaining traction. These allow a compact student model to absorb the
capabilities of multiple expert teachers trained on specific tasks or domains.
This is particularly valuable for edge deployment, where a single model
must support diverse user needs—summarization, Q&A, dialogue,
translation—without switching context or architectures.
Importantly, knowledge distillation often complements other
compression techniques. A quantized, pruned, or adapter-augmented model
distilled from a high-capacity teacher can retain accuracy while maintaining
an edge-friendly footprint.


**3.3 Parameter Sharing and Lightweight Architectures**

Another compression technique focuses on architectural efficiency through
weight sharing. Models like **ALBERT** have demonstrated that significant
parameter reductions are possible by sharing weights across transformer
layers. Although this reduces capacity, the inductive bias introduced by
shared weights encourages generalization and speeds up training [ 33 ].
In edge settings, this principle is extended through adapter-based
parameterization. Lightweight modules such as **LoRA** inject trainable low-
rank matrices into frozen transformer weights. During task adaptation, only
these modules are updated, while the main model remains static and shared
across tasks [ 16 ].
This approach is particularly suitable for smartphones, where multiple
LoRA adapters can be stored and swapped on demand without the need to
reload or retrain the base model. **Multi-LoRA** systems allow simultaneous
adapter composition, enabling dynamic personalization while minimizing
memory overhead.
Another innovative concept is **token-wise weight prediction** , where
dynamic weights are generated per token using meta-learning mechanisms.
Although still experimental, this technique has the potential to enable
context-aware adaptation without bloating model size [ 34 ].

**3.4 Compression in Communication-Aware Systems**

Model compression is not only about local execution but also plays a vital
role in communication-centric deployment frameworks such as **federated
learning** and **collaborative inference**.
In federated learning setups, pruned and quantized models reduce the
size of gradient updates exchanged between clients and servers, alleviating
bandwidth and energy constraints. This is essential for deploying
personalized language models across heterogeneous mobile devices in B5G
environments [ 35 ].
Similarly, **model partitioning and distributed inference** —where parts
of a model execute on-device while the rest run on nearby edge servers—
benefit from compression by reducing intermediate activation sizes and
transmission latency [ 36 ].
Compression-aware design principles are thus critical in bridging LLM
intelligence with networked deployment paradigms.


**3.5 Compression-Aware Training and Joint Optimization**

Recent trends indicate a shift from post-hoc compression to compression-
aware training, where model size, latency, and accuracy are co-optimized
during training. Techniques such as differentiable pruning [ 37 ] and
quantization-aware architecture search [ 38 , 39 ] allow direct optimization of
sparsity patterns and bit precision.
In some frameworks, hardware-in-the-loop feedback is used to guide
the optimization process based on actual device metrics such as token
latency or power draw [ 40 ]. These multi-objective optimization pipelines
yield Pareto-efficient models tailored to specific deployment targets—be it
a flagship smartphone or an embedded IoT node.
Tools like NetAdapt [ 40 ] and our own work on evolutionary
quantization calibration [ 41 ] exemplify how on-device metrics can inform
model design, leading to tangible improvements in energy efficiency and
runtime stability.

**3.6 Evaluation Metrics for Compressed Models**

Assessing the quality of a compressed model requires more than just
accuracy. A comprehensive evaluation involves:

**Model size:** Typically reported in megabytes or gigabytes post-
quantization.
**Token latency:** Measured in milliseconds per token on target hardware.
**Memory footprint:** Peak memory usage during inference.
**Compression ratio:** Relative to the original FP32 model.
**Accuracy delta:** Drop in BLEU, F1, or perplexity compared to full-
precision baselines.
Standardized benchmarks such as MLPerf Tiny [ 42 ] and task-specific
datasets like SQuAD, WikiText, and GSM8K are often used to validate
performance across LLM variants. On-device evaluation ensures that trade-
offs are meaningful in practical deployment settings.

**3.7 Summary**

Compression is the cornerstone of edge-compatible LLM deployment.
Whether through structured pruning, distilled student models, or modular
parameter sharing, these methods drastically reduce the computational and
memory barriers to real-time inference. As mobile AI accelerators become


more capable and user demand for responsive GenAI grows, such
techniques will become integral to LLM development pipelines. The next
section delves deeper into quantization—the most widely adopted and
hardware-friendly technique to shrink LLMs for edge inference.

**4 Quantization for Efficient On-Device Inference**

Quantization has emerged as the most impactful and hardware-aligned
technique for compressing Large Language Models (LLMs) for edge
deployment (see Fig. 2 ). It reduces the precision of model weights and
activations from floating-point (e.g., FP32 or FP16) to lower-bit integer
formats (e.g., INT8, INT4), thereby reducing memory usage, improving
cache locality, and enabling faster inference on AI accelerators. Unlike
pruning or distillation, quantization preserves the original model structure,
allowing minimal changes to software and runtime pipelines. This makes it
especially attractive for smartphone and embedded deployment where both
compute and memory budgets are constrained.


**_Fig. 2_** Quantization of LLMs

**4.1 Quantization Methods: PTQ and QAT**

Quantization methods can be broadly categorized into Post-Training
Quantization (PTQ) and Quantization-Aware Training (QAT).
PTQ is performed after a model has been fully trained in high precision.
It uses calibration data to estimate the appropriate quantization parameters
(scale and zero-point) and map real-valued weights to discrete integer
levels. This method is fast and does not require access to the original
training data or retraining capabilities, making it ideal for deployment
pipelines [ 43 ].
However, PTQ can sometimes lead to significant accuracy drops,
especially in sensitive components like attention or layer normalization. To


mitigate this, methods such as GPTQ and AWQ use second-order
information (e.g., Hessian-based curvature) to minimize quantization error.
These techniques focus on minimizing quantization-induced degradation
and preserving performance even at INT4 precision [ 44 , 45 ].
QAT, on the other hand, simulates quantization during training. The
model learns to adapt its parameters to the reduced precision environment
by incorporating quantization noise into the forward and backward passes.
While QAT yields higher accuracy and robustness than PTQ, it requires full
training infrastructure, high computational cost, and longer time-to-deploy
[ 46 ].
In practice, PTQ is the preferred choice for LLMs deployed at the edge,
especially when paired with advanced calibration techniques such as
SmoothQuant, which scales activations to minimize information loss and
improve quantization compatibility [ 47 ].

**4.2 Mixed-Precision and Per-Layer Quantization**

Uniform quantization across the entire model often leads to suboptimal
trade-offs between accuracy and performance. Modern quantization
strategies adopt mixed-precision quantization, where different layers or
even different tensors (weights, activations, attention scores) are quantized
at varying bit-widths based on their sensitivity.
For example, the embedding and final projection layers are typically
quantized at higher precision (e.g., INT8 or FP16) due to their high
information content, while linear transformations in intermediate
transformer blocks can tolerate INT4 or INT3 formats. This selective
granularity is achieved via sensitivity analysis, either manually or through
automated tools [ 48 ] that leverage the trade-off between accuracy and on-
device performance as shown in Fig. 3.


**_Fig. 3_** Conflicting objectives during quantization that necessitates the methods to explore the trade-
off between Accuracy drop and Performance gain due to quantization [ 49 ]
Our prior work (see Fig. 4 ) explores Bayesian search techniques [ 49 ] to
automatically calibrate quantization settings across layers using hardware-
in-the-loop feedback, achieving optimal token latency under fixed accuracy
constraints. This kind of Pareto optimization is crucial in commercial
deployments where both performance and model size must be tuned per
hardware target.


**_Fig. 4_** The Mixed Precision Quantization method proposed in [ 49 ] uses multi-objective Bayesian
optimization to determine the best configuration of different bit precisions across the layers of the
neural network

**4.3 Grouped and Blockwise Quantization**

To balance storage efficiency and decoding speed, modern LLMs often use
grouped or blockwise quantization, wherein a block of weights shares the
same scale and offset values. This reduces the memory overhead of storing
quantization parameters and allows for SIMD-friendly matrix multiplication
routines.
GPTQ introduced blockwise quantization with error compensation via
reconstruction of quantized outputs. AWQ and SmoothQuant go a step
further by introducing activation-aware scaling, optimizing both weight and
activation quantization [ 44 , 45 , 47 ].
These formats are especially beneficial for mobile NPUs, which favour
regular data access patterns and constant-stride computations. Coupled with
native INT4 execution support, they unlock substantial improvements in
real-time throughput on devices like Snapdragon or MediaTek chips.


**4.4 LoRA and Quantization Compatibility**

An important breakthrough in on-device GenAI has been the compatibility
of quantization with modular fine-tuning techniques like LoRA. Typically,
LoRA introduces trainable low-rank matrices into the frozen transformer
weights. When paired with a quantized base model, these adapters can still
be trained in full precision (FP16) and added to the dequantized weights at
inference time, preserving accuracy while enabling task flexibility.
QLoRA formalizes this design by quantizing the base model to INT4
and retaining LoRA adapters in FP16. The combined model supports
efficient multitask inference with minimal memory overhead. At runtime,
different LoRA adapters can be loaded dynamically—allowing use-case
specific personalization on edge devices without requiring model
recompilation [ 11 ].
This setup also enables efficient sharing and updating of adapters in
federated learning scenarios. Edge devices can locally fine-tune LoRA
modules and send only the adapter deltas to the server, preserving user
privacy while reducing communication cost [ 50 , 51 ].

**4.5 Quantization Toolchains and Frameworks**

Several toolchains have emerged to support end-to-end quantization, each
with varying support for transformer layers, LoRA fusion, and hardware
acceleration.

**llama.cpp** : A portable C++ runtime supporting 4-bit quantized models.
Optimized for CPU and microcontroller execution [ 52 ].
**MLC-LLM** : A compiler-driven framework that quantizes and compiles
models for iOS, Android, and WebAssembly targets. Supports token
streaming, mixed-precision, and NPUs [ 15 ].
**ONNX Runtime Mobile** : Provides quantization tools and hardware
acceleration for a wide range of operators and target devices.
**Qualcomm SNPE SDK** : Enables deployment of quantized LLMs on
Snapdragon AI engines with INT8 and INT4 support [ 53 ].
These frameworks often integrate with quantization algorithms (e.g.,
GPTQ, AWQ, SmoothQuant), enabling full-stack deployment pipelines
from model export to inference-ready binaries.

**4.6 Challenges and Limitations**


Despite its benefits, quantization introduces several challenges:

**LayerNorm and Softmax Instability** : Nonlinear operations are sensitive
to low-precision representation, often requiring FP16 fallbacks.
**KV Cache Quantization** : Reducing the precision of key-value caches in
attention modules can lead to significant degradation in long-context
tasks.
**Lack of Standardization** : Quantization formats (e.g., GPTQ, AWQ)
differ across frameworks, complicating interoperability and
benchmarking.
**Limited Tooling for Fine-Tuning** : Integrating quantization with training
frameworks like PyTorch or Hugging Face for adapter tuning still lacks
mature support.
Efforts are underway to create unified INT4 quantization formats and
model exchange standards to address these limitations.

**4.7 Summary**

Quantization remains the single most effective and scalable technique for
shrinking LLMs to edge-friendly footprints. Its synergy with modular fine-
tuning (e.g., LoRA), evolving toolchains, and hardware accelerators makes
it foundational for on-device GenAI. The next frontier lies in optimizing the
_runtime decoding process_ , where token generation latency becomes the
primary bottleneck—this is explored in the following section on decoding
and sampling strategies.

**5 Efficient Sampling and Decoding for Low-**

**Latency Edge LLMs**

While quantization and compression reduce model size and memory
footprint, real-time performance in edge deployment is ultimately limited
by decoding latency. In autoregressive transformers, token-by-token
generation is inherently sequential, leading to a compute bottleneck during
inference. This latency challenge is magnified on mobile or embedded
hardware, where thermal limits and battery constraints cap throughput.
This section explores strategies to accelerate decoding through
architectural changes, speculative generation, token pruning, and runtime
scheduling—without compromising quality or requiring model retraining.


**5.1 The Decoding Bottleneck**

In LLM inference, decoding refers to generating one token at a time by
feeding the model’s past context and producing the next logit. For a
vocabulary of size _V_ , the final layer computes a distribution over _V_ tokens,
followed by sampling strategies like greedy, top-k, nucleus (top-p), or
temperature scaling.
Each new token triggers a full forward pass through the model—making
the process compute-intensive and difficult to parallelize across time. This
makes decoding the dominant factor in end-user latency for chatbots,
translators, and summarizers on the edge.
In addition, edge deployments often include constraints such as the
following:

```
Strict latency thresholds.
Limited cache sizes for attention (e.g., due to SRAM/NPU constraints).
Token streaming requirements for real-time UI updates.
These challenges necessitate novel inference-time optimizations.
```
**5.2 Speculative Decoding: A Paradigm Shift**

Speculative decoding aims to parallelize token generation while retaining
the autoregressive nature of LLMs. The core idea is to generate multiple
tokens using a fast draft model, then verify them using the original (larger)
model in parallel. If the verification passes, multiple tokens are accepted in
one step—greatly reducing latency.
This approach was introduced by Google & DeepMind [ 54 and 55 ] and
expanded by follow-ups like SpecInfer [ 56 ], BiTNet [ 13 ], and Spector et
al., 2023 [ 57 ]. Unlike traditional beam search, speculative decoding reduces
the number of forward passes needed by amortizing verification over
multiple tokens.
**Self-Speculative Decoding** [ 14 ], a recent innovation, eliminates the
need for a separate draft model. Instead, the main model generates
candidate tokens using a shifted latent representation (e.g., temperature
scaling, prefix padding), then validates them using regular decoding logic.
This makes speculative sampling more compatible with quantized and
edge-friendly deployments, avoiding the overhead of maintaining a second
model.


In practice, speculative decoding can achieve **2–3× speedups** in latency
without noticeable quality degradation, making it highly suitable for
deployment on mobile NPUs or microcontrollers.

**5.3 Token Pruning and Context Compression**

Another effective latency reduction method is **token pruning** , where
uninformative or low-impact tokens from the context are removed before
decoding. This reduces the sequence length and, consequently, the attention
computation cost.
Approaches like **LazyLLM** dynamically select and compute key-value
(KV) pairs for tokens that are crucial for the next token prediction,
effectively reducing the time-to-first-token (TTFT) during inference [ 58 ].
Similarly, **Saliency-driven Dynamic Token Pruning (SDTP)** employs a
lightweight saliency prediction module to estimate token importance,
enabling hierarchical pruning of redundant tokens based on input context
[ 59 ].
**Speculative decoding** is another technique that accelerates inference by
using a smaller draft model to generate speculative tokens, which are then
verified by the larger target model. This method allows for multiple tokens
to be generated per step, enhancing throughput without compromising
accuracy [ 60 ].
These techniques are particularly useful for long-context applications
like document summarization or chat history-based question answering.
**Token eviction** strategies can also be tuned dynamically based on runtime
feedback (e.g., low attention weights) [ 61 , 62 ], allowing adaptive context
management without retraining.

**5.4 Early Exit and Layer Skipping**

Dynamic computation strategies like **early exit** [ 63 – 66 , and] and
**structured dropout** [ 67 ] allow token-level runtime control of computation
depth. For certain “easy” tokens (e.g., function words or punctuation),
inference can terminate early after a few transformer layers, reducing
compute cost.
While these strategies are more common in BERT-style encoders, their
adaptation to autoregressive decoders is gaining traction. For edge models,
token-level early exit allows balancing quality with latency, especially in
real-time streaming applications like voice assistants or translators.


**5.5 Blockwise and Parallel Decoding**

To further improve throughput, **blockwise parallel decoding** [ 68 ] processes
multiple tokens simultaneously in a sliding window fashion. This is enabled
by approximating dependencies in future tokens or using masked decoding.
While this breaks strict autoregression, methods like BiTNet and
PALLM apply correction mechanisms to maintain output quality. These
strategies trade some fidelity for throughput and are useful in edge
scenarios where immediate responsiveness outweighs minor degradation.
Such methods also align well with transformer compression methods
and quantization, as they reduce the number of invocations to large matmul-
heavy modules.

**5.6 Runtime Scheduling and Fusion**

Decoding efficiency is not just a modeling problem—it also involves
**runtime-level optimizations**.

**Operator fusion** merges multiple tensor operations (e.g., matmul + add
+ GELU) into a single kernel, reducing memory accesses.
**Token streaming** delivers partial results as each token is generated,
improving UI responsiveness and perceived speed.
**Latency-aware scheduling** (e.g., [ 69 ]) dynamically adjusts token
generation based on hardware utilization, thermal throttling, and system
load.
Compiler-driven runtimes like MLC-LLM [ 15 ] and lightweight C++
backends like llama.cpp [ 52 ] include aggressive fusion strategies to squeeze
every millisecond out of low-power devices.

**5.7 Communication-Aware Decoding in B5G**

In B5G networks, GenAI inference may occur in distributed environments
involving cloud, edge servers, and mobile devices. Collaborative decoding
[85] involves partitioning model components (e.g., attention layers on-
device, feedforward layers offloaded) and merging results via low-latency
links.
Compression-aware token generation ensures that transmitted
representations are compact and loss-tolerant. This helps in video
summarization, multilingual chat, or real-time captioning where edge
devices collaborate with network nodes.


Designing decoding pipelines that minimize uplink cost while
preserving fidelity is an open research area, particularly for federated agent-
based models [ 70 ].

**5.8 Summary**

Efficient decoding transforms LLMs from theoretical constructs into real-
time interactive systems. Through speculative sampling, token pruning, and
dynamic scheduling, it is possible to achieve near-cloud performance on
edge devices. Combined with quantization and compression, these
strategies form the foundation for responsive and intelligent on-device
GenAI experiences.

**6 Runtime Architectures and Deployment**

**Toolchains**

Achieving low-latency and memory-efficient inference on edge devices
demands more than model optimization—it also requires a robust runtime
system that bridges model formats, compiler frameworks, and hardware-
specific backends. Runtime architectures serve as the glue between
quantized LLMs and NPUs, CPUs, or DSPs present in mobile SoCs. This
section outlines the major runtime environments and toolchains that support
on-device GenAI, highlighting their compatibility with INT4/INT8 models,
adapter fusion, and communication-aware scheduling for B5G
deployments.

**6.1 Lightweight Runtimes: llama.cpp and MLC-LLM**

**llama.cpp** [ 52 ] is a highly portable C++ implementation of LLaMA and
related models, with support for 4-bit quantization (GGML/GGUF formats)
and CPU-based inference. Designed for maximum compatibility across
platforms, llama.cpp targets devices without GPUs, enabling inference on
low-end phones, Raspberry Pi, or web browsers via WebAssembly.
It uses highly optimized matrix multiplication libraries (e.g., BLAS,
AVX/FMA kernels), supports quantized KV caches, and integrates token
streaming for real-time generation. Its ease of use and active community
have made it a default choice for fast prototyping and small-scale
deployment.


In contrast, **MLC-LLM** [ 15 ] is a compiler-based framework that targets
NPUs and GPUs on iOS, Android, and desktop platforms. Built atop
Apache TVM, it provides:

Ahead-of-time (AOT) compilation of quantized models.
Support for transformer-specific graph fusion.
Sliding window attention and token streaming.
Cross-compilation to Metal, Vulkan, and WebGPU.
MLC-LLM is ideal for production-grade deployment where hardware
acceleration is necessary and platform-specific tuning is required.

**6.2 ONNX Runtime and Cross-Vendor Deployment**

**ONNX Runtime Mobile** provides a vendor-agnostic runtime that supports
quantized models through custom kernels and backend extensions. It allows
exporting models from PyTorch or TensorFlow and running them on
devices with Intel, ARM, or Qualcomm hardware.
Combined with INT8 calibration tools, ONNX Runtime enables smooth
conversion from training to deployment, especially for vision-language
models (see Sect. 8 ). It supports operator fallback, so if quantized operators
are unavailable, the model can still run partially in FP16 or FP32.
**XNNPACK** , a Google-led backend, provides highly optimized
implementations of inference kernels (e.g., convolutions, matmuls,
activations) and is often integrated in ONNX, TFLite, and PyTorch Mobile.
It supports INT8 and FP16 execution and is ideal for edge ML workloads
with strict latency constraints.

**6.3 Vendor-Specific SDKs**

Many chip manufacturers offer their own runtime SDKs and compiler
stacks to leverage their NPUs:

```
Qualcomm SNPE SDK : Allows compiling and deploying models on
Hexagon DSP and AI Engine present in Snapdragon chips. Supports
INT8/INT4 quantization and graph optimization.
MediaTek NeuroPilot : Provides TensorFlow Lite and Android NNAPI
support for models targeting Dimensity chipsets.
Samsung’s NPU SDK (SLSI) : Offers conversion pipelines for
ONNX/TFLite models into proprietary intermediate representations
compatible with Exynos chipsets.
```

These SDKs provide access to low-level optimizations like weight
prepacking, memory reuse, and operator tiling, ensuring that edge LLMs
make full use of available hardware.

**6.4 Model Packaging and Adapter Integration**

Runtime toolchains must handle not only the base model but also auxiliary
components like LoRA adapters, tokenizer metadata, quantization
parameters, and prompt caches. Packaging these into a single deployable
unit (e.g., .gguf, .mlc, .onnx) with version control is critical for updates and
model management.
Emerging practices include the following:
**Dynamic adapter loading** : Runtime merges selected LoRA adapters into
the base model at inference, enabling multitask inference.
**LoRA + INT4 support** : Formats like GGUF and MLC-LLM support
loading FP16 LoRA weights on top of INT4 base models with runtime
merging [ 15 ].
**Secure packaging** : To protect IP and prevent tampering, runtimes
support encryption, digital signatures, and secure enclaves for model
loading—critical in enterprise B5G settings.

**6.5 Scheduling and Resource Management**

In multi-agent environments, runtime schedulers manage the following:

**Token generation order** across multiple clients or apps.
**Memory sharing** between model cache, image inputs, and NPU
accelerators.
**Temperature and battery constraints** , throttling inference as needed.
Latency-aware runtime designs predict and adjust the number of active
transformer layers, stream buffer sizes, and memory allocations
dynamically—ensuring stable performance without overheating or stalling.
Additionally, **collaborative runtimes** are emerging, where parts of
inference are offloaded to B5G edge servers or other nearby devices. For
example, attention computations may be done on-device, while large
projection layers are handled on a connected node. This model-device co-
optimization is a frontier area for GenAI deployment.

**6.6 Summary**


The success of GenAI on edge hinges not just on model design but also on
runtime architecture. Lightweight runtimes like llama.cpp, compiler-based
toolchains like MLC-LLM, and vendor SDKs form the backbone of
scalable, responsive, and privacy-preserving deployment. With standardized
packaging and communication-aware scheduling, these toolchains are ready
to support a diverse set of GenAI tasks in B5G ecosystems.

**7 Model Packaging, Adaptation, and**

**Personalization at the Edge**

Deploying generative AI models on edge devices is not just about
compacting the base model. True usability and scalability come from
enabling **task-specific adaptation** , **multilingual functionality** , and **user
personalization** —all while staying within resource constraints. To achieve
this, modern deployment pipelines must support modularity, security, and
interoperability of model components, especially in scenarios envisioned by
B5G systems, where intelligence must be both **context-aware** and
**adaptive**.

**7.1 The Need for Adaptation**

Generic LLMs, despite being highly capable, are often insufficient in their
raw form for niche domains such as healthcare, education, or smart
communication networks. Even more, users expect models to behave
differently depending on the use case—e.g., formal vs. casual tone,
summaries vs. translations, or domain-specific jargon. Fine-tuning the base
model for each requirement is computationally expensive and practically
infeasible on-device.
**Parameter-efficient fine-tuning** techniques, particularly **Low-Rank
Adaptation (LoRA)** [ 11 , 16 and 51 ] as shown in Fig. 5 , have addressed this
by enabling adaptation via small sets of trainable weights. These adapters,
when injected into the attention and MLP layers of transformers, can
modify model behaviour significantly while using less than 1% of the full
model’s parameters. Crucially, they can be trained and deployed
**independently** of the base model—making them ideal for mobile and
embedded use.


**_Fig. 5_** Pictorial Representation of LoRA action on the frozen Large Language Models to adapt them
for multiple tasks

**7.2 Adapter Integration and LoRA Packaging**

Deploying LoRA adapters on-device requires them to be:

Stored in compatible formats (e.g., .safetensors, .gguf, or .mlc).
Loaded dynamically into memory, preferably without re-initializing the
base model.
Supported by runtime schedulers that manage memory and execution
efficiently.
Recent formats like **GGUF** (used by llama.cpp) and **MLC** allow this
adapter modularity. They support **int4 base models with FP16 LoRA
adapters** , leveraging matrix decomposition during runtime to combine the
adapter weights with base activations. This fusion is handled by runtime
primitives (e.g., fused-GEMM), reducing the need for reloading or
recomputation.
Such modular packaging enables the following:
Fast switching between tasks (e.g., summarization to translation).
Storage efficiency (multiple LoRAs on one base model).
Federated updates (each user can receive updated adapters).


This is especially valuable in B5G networks where devices may pull
domain-specific adapters over-the-air from nearby edge servers.

**7.3 Personalization Via Multi-LoRA and Dynamic Fusion**

To support personalization, models may carry multiple LoRA modules
simultaneously, corresponding to various user preferences or tasks. Multi-
LoRA architectures [ 71 ] allow the following:

Weighted combination of adapter outputs.
Routing mechanisms that choose the best adapter for a given prompt.
Composition of adapters in series or parallel.
In practice, this enables personalized behaviours such as the following:
Custom response tone (formal/informal).
Language or dialect preferences.
Domain-specific knowledge injection (e.g., legal, medical).
Runtime support for adapter fusion (e.g., in llama.cpp, MLC-LLM)
ensures that these compositions incur minimal overhead and preserve
quantization compatibility.

**7.4 Privacy-Preserving Adaptation with Federated LoRA**

When deploying models on personal smartphones, collecting user data for
adaptation poses privacy risks. To address this, **Federated Learning of
Adapters** is emerging as a solution. Inspired by federated averaging, this
approach allows each user to train their own LoRA adapter on-device,
without sharing raw data. Periodically, only the adapter weight updates are
sent to a central or edge server for aggregation.
Recent frameworks like **FedLLM** [ 72 ] implement federated fine-tuning
of LLMs with:

Differential privacy guarantees.
Secure aggregation protocols.
Adapter-level compression (e.g., LoRA sparsity).
This fits perfectly into the B5G vision where distributed intelligence is
powered by edge compute, and each device acts as both a learner and an
inference node.


**7.5 B5G-Aware Model Distribution**

In Beyond 5G scenarios, distributing large models and their adapters to
edge devices must be bandwidth-efficient and context-aware. Emerging
practices include the following:

**Hierarchical caching** of model weights across edge, fog, and core
networks.
**Delta updates** for adapter versions (only changed weights transmitted).
**Compression-aware quantization** where adapters are quantized not
only for inference but also for high compression using codecs like Z-std
or LZ4.
This ensures that models can be deployed and updated on-demand in
smart cities, autonomous vehicles, and IoT environments without saturating
network bandwidth.

**7.6 Summary**

Adapter-based personalization and packaging strategies unlock the true
versatility of GenAI on edge. Through LoRA, Multi-LoRA, and federated
training, users can enjoy personalized experiences without full-model
retraining or data leakage. Runtime support and modular distribution
models ensure that even highly customized behaviour can be delivered
within the constraints of edge hardware and B5G communication channels.

**8 Lightweight Vision-Language Models at the**

**Edge**

As GenAI capabilities expand beyond language, the ability to process and
generate information across modalities—such as images, video, and speech
—is critical. In B5G networks, where applications range from augmented
reality to smart surveillance, the synergy between vision and language
models becomes foundational. However, the deployment of vision-language
models (VLMs) on edge devices introduces new challenges in terms of
model size, latency, and memory usage.
This section discusses advances in **lightweight VLM architectures** ,
strategies for efficient deployment, and their relevance to B5G applications.


**8.1 Multimodal GenAI: Beyond Language**

Multimodal models, such as **CLIP** , **BLIP** , and **LLaVA** (Large Language
and Vision Assistant), represent a significant leap in generative AI. These
architectures typically include the following:

A vision encoder (e.g., ViT, ResNet, or Swin Transformer).
A language decoder (often a transformer-based LLM).
A fusion module that integrates image and text embeddings.
Such systems enable image captioning, visual Q&A, and cross-modal
retrieval. However, their size—often exceeding billions of parameters—
makes them unsuitable for direct deployment on smartphones or IoT edge
devices.

**8.2 Efficient Visual Encoders**

Reducing the footprint of the visual encoder is the first step toward enabling
VLMs on edge hardware. Research has shown that **self-supervised
pretraining** methods such as **DINOv2** [ 18 ] can produce highly transferable
representations using smaller backbones like MobileViT or EfficientNet.
Additionally, **TinyCLIP** [ 73 ] and **MobileCLIP** [ 74 ] demonstrate the
feasibility of compressing CLIP models through:

Adapter-based distillation: Transferring knowledge from a large teacher
into a low-rank student using LoRA or QLoRA.
Quantization-aware training: INT8 or INT4 compression without
significant loss in retrieval or captioning performance.
Latency-aware pruning: Removing redundant heads, layers, or channels
based on effective receptive field analysis.
These models achieve comparable performance to CLIP-L/14 at a
fraction of the cost, enabling deployment on smartphones with 6–8 GB
RAM and 3–5 TOPS NPUs.

**8.3 Fusion Strategies and Token Reduction**

Multimodal fusion requires efficient combination of image and text
embeddings. Traditional approaches concatenate visual tokens with
language tokens, but this becomes expensive in attention layers. Modern
approaches adopt:

```
Cross-attention : Language decoder attends selectively to visual features.
```

**Token merging** [ 75 ]: Redundant image tokens are merged dynamically
based on activation similarity.
**Sparse fusion** : Only a subset of visual tokens influence generation,
reducing compute.
These strategies are critical in edge contexts where input token count
directly impacts inference latency and memory consumption.

**8.4 Quantized and Compressed VLMs**

Quantizing both the image and text encoders is non-trivial due to modality-
specific sensitivities. Q-VLM [ 76 ] and P4Q [ 77 ] have shown promising
results in mixed-precision quantization of vision-language models, allowing
INT4 or INT8 deployment of end-to-end systems.
Runtime toolchains like MLC-LLM and ONNX Runtime Mobile
support such quantized multimodal graphs. Additionally, models can be
**packaged modularly** , with vision encoder and language decoder loaded
separately based on the task (e.g., image captioning vs. OCR).
In federated contexts, visual encoders can also be fine-tuned locally,
while the language model remains static—enabling privacy-preserving
adaptation for users who routinely photograph personal documents, scenes,
or receipts.

**8.5 B5G Use Cases of Edge VLMs**

In Beyond 5G systems, lightweight VLMs enable a wide spectrum of
applications:

**AR-based translation and captioning** : Real-time overlay of translated
text on physical scenes.
**Visual question answering in smart glasses** : Interpreting user queries
based on visual input.
**Context-aware robotics** : Robots understanding environment via
language-driven prompts.
**Surveillance and security** : Captioning of abnormal visual events using
on-device inference.
These use cases demand <300 ms latency, multilingual support, and full
offline functionality—requirements met only through tight integration of
quantization, pruning, and runtime optimization.


Edge-friendly models like MobileCLIP, when combined with streaming
token decoders, form the backbone of such use cases. Moreover, B5G edge-
cloud cooperation allows fallback to server-side models only when the
device fails to produce confident outputs, ensuring robustness.

**8.6 Summary**

As vision-language applications move into mainstream mobile use, the need
for lightweight and efficient VLMs becomes urgent. Through visual
encoder distillation, fusion optimization, and quantization-aware training, it
is now possible to deploy such systems on-device. This opens the door to
multimodal GenAI in B5G environments, from personal assistants to edge-
based robotics, marking a crucial milestone in ubiquitous intelligence.

**9 Conclusion**

The convergence of generative AI and edge computing signals a pivotal
transformation in the AI deployment paradigm. While cloud-based LLMs
have demonstrated extraordinary capabilities, their reliance on massive
compute resources, network connectivity, and centralized infrastructure
creates a bottleneck for truly ubiquitous intelligence. Unlocking GenAI on
the edge is not merely a matter of scaling down models—it requires a
rethinking of architecture, compression, inference, personalization, and
system-level integration.
This chapter has presented a detailed walkthrough of the techniques
enabling efficient LLM and VLM deployment on mobile and embedded
devices. Beginning with post-training quantization and extending to mixed-
precision LoRA fine-tuning, we have seen how modern toolchains like
GPTQ, AWQ, QLoRA, and SmoothQuant make transformer models
amenable to INT4/INT8 execution. Beyond model size, decoding strategies
such as speculative sampling, token pruning, and early exit mechanisms
were explored for their role in accelerating inference under tight latency
constraints. Runtimes like llama.cpp and MLC-LLM, as well as hardware-
aware SDKs, offer the infrastructure backbone to operationalize these
models across diverse SoCs and NPUs.
More importantly, personalization through LoRA adapters, multi-LoRA
composition, and federated tuning has emerged as a scalable alternative to
full fine-tuning, allowing GenAI systems to be tailored per user or domain


without compromising privacy. In multimodal settings, lightweight vision-
language models like MobileCLIP and TinyCLIP demonstrate that even
complex perceptual tasks can be performed locally, especially when paired
with aggressive pruning, compression, and fusion-aware decoding.
As B5G networks evolve to support intelligent edge devices in smart
cities, autonomous mobility, and collaborative robotics, the need for
communication-efficient, privacy-respecting, and real-time GenAI will
become ever more pressing. Edge-native GenAI not only aligns with these
goals but also democratizes AI by making advanced models accessible to
billions of users without cloud dependence.
In sum, the future of GenAI lies not in larger models, but in **smarter,
smaller, and more context-aware systems that live where the data is
generated—at the edge.** The blueprint provided in this chapter offers a
roadmap for realizing this future.

**References**

1. T.B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, et al., Language models are
    few-shot learners. Adv. Neural Inf. Proces. Syst. **33** , 1877–1901 (2020)
2.
    R. Anil, A.M. Dai, O. Firat, M. Johnson, D. Lepikhin, S. Narang, et al., PaLM 2 Technical
    Report. _arXiv preprint arXiv:2305.10403_ (2023)
3.
    OpenAI, GPT-4 technical report. _arXiv preprint arXiv:2303.08774_ (2023)
4.
    H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, et al., LLaMA: open
    and efficient foundation language models. _arXiv preprint arXiv:2302.13971_ (2023)
5.
    Mistral AI. (2023). Mistral-7B and Mixtral model cards. https:// mistral. ai/ news/ announcing-
    mistral-7b
6.
    Qwen Team. (2024). Qwen2.5: A Party of Foundation Models!. https:// qwenlm. github. io/ blog/
    qwen2. 5/
7.
    S. Han, H. Mao, W.J. Dally, Deep compression: compressing deep neural networks with pruning,
    trained quantization and Huffman coding, in _International Conference on Learning_
    _Representations_ , (2016)
8.
    G. Hinton, O. Vinyals, J. Dean, Distilling the knowledge in a neural network. _arXiv preprint_
    _arXiv:1503.02531_ (2015)
9.
    E. Frantar, P. Stock, D. Alistarh, GPTQ: accurate post-training quantization for generative pre-
    trained transformers. _arXiv preprint arXiv:2210.17323_ (2022)


10.
J. Lin, Z. Wang, J. Li, Y. Zhang, X. Sun, AWQ: activation-aware weight quantization for LLMs.
_arXiv preprint arXiv:2306.00978_ (2023)
11.
T. Dettmers, A. Pagnoni, A. Holtzman, L. Zettlemoyer, QLoRA: efficient fine-tuning of
quantized LLMs. _arXiv preprint arXiv:2305.14314_ (2023)
12.
X. Chen, Y. Zhou, J. Wang, W. Wang, EAGLE: error-aware generation with latent editability.
_arXiv preprint arXiv:2305.13245_ (2023)
13.
M. Sun, Y. Wang, D. Yin, W. Wang, BiTNet: speculative decoding without a draft model. _arXiv
preprint arXiv:2402.00078_ (2024)
14.
F. Lin, H. Yi, Y. Yang, H. Li, X. Yu, G. Lu, R. Xiao, Bita: bi-directional tuning for lossless
acceleration in large language models. _Expert Syst. Appl._ **_279_** , 127305 (2025)
[Crossref]
15.
L. Zheng, Y. Song, Z. Liu, W. Wang, MLC-LLM: deploying LLMs on native phones and GPUs
with compiler-level optimization. _arXiv preprint arXiv:2309.17129_ (2023) https:// llm. mlc. ai/
docs/
16.
E.J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, et al., Lora: low-rank adaptation of
large language models. ICLR **1** (2), 3 (2022)
17.
Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., ... & Girshick, R. (2023).
Segment anything. In _Proceedings of the IEEE/CVF international conference on computer vision_
(pp. 4015–4026)
18.
M. Oquab, T. Darcet, T. Moutakanni, L. Bossard, H. Vo, M. Szafraniec, et al., DINOv2: learning
robust visual features without supervision. _arXiv preprint arXiv:2304.07193_ (2023)
19.
A. Radford, J.W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, et al., Learning transferable
visual models from natural language supervision. Int. Conf. Mach. Learn. **139** , 8748–8763
(2021)
20.
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A.N. Gomez, et al., Attention is all
you need. Adv. Neural Inf. Proces. Syst. **30** (2017)
21.
J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, BERT: pre-training of deep bidirectional
transformers for language understanding. _Proceedings of the 2019 Conference of the North
American Chapter of the Association for Computational Linguistics: Human Language
Technologies_ **_1_** , 4171–4186 (2019)
22.
A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, Language models are
unsupervised multitask learners. OpenAI blog **1** (8), 9 (2019)
23.
H. Cheng, M. Zhang, J.Q. Shi, A survey on deep neural network pruning: Taxonomy,
comparison, analysis, and recommendations. IEEE Trans. Pattern Anal. Mach. Intell. **46** , 10558
(2024)
[Crossref]


24.
X. Ma, G. Fang, X. Wang, Llm-pruner: on the structural pruning of large language models. Adv.
Neural Inf. Proces. Syst. **36** , 21702–21720 (2023)
25.
H. Huang, H.J. Song, H.K. Pao, Large language model pruning. _arXiv preprint
arXiv:2406.00030_ (2024)
26.
L. Dery, S. Kolawole, J.F. Kagy, V. Smith, G. Neubig, A. Talwalkar, Everybody prune now:
structured pruning of llms with only forward passes. _arXiv preprint arXiv:2402.05406_ (2024)
27.
V. Sanh, T. Wolf, A. Rush, Movement pruning: adaptive sparsity by fine-tuning. Adv. Neural Inf.
Proces. Syst. **33** , 20378–20389 (2020)
28.
L. Fang, X. Yu, J. Cai, Y. Chen, S. Wu, Z. Liu, et al., Knowledge distillation and dataset
distillation of large language models: emerging trends, challenges, and future directions. _arXiv
preprint arXiv:2504.14772_ (2025)
29.
A.M. Mansourian, R. Ahmadi, M. Ghafouri, A.M. Babaei, E.B. Golezani, Z.Y. Ghamchi, et al., A
comprehensive survey on knowledge distillation. _arXiv preprint arXiv:2503.12067_ (2025)
30.
S. Sun, Y. Cheng, Z. Gan, J. Liu, Patient knowledge distillation for bert model compression.
_arXiv preprint arXiv:1908.09355_ (2019)
31.
L. Liu, H. Wang, J. Lin, R. Socher, C. Xiong, Mkd: a multi-task knowledge distillation approach
for pretrained language models. _arXiv preprint arXiv:1911.03588_ (2019)
32.
Y. Tian, Y. Han, X. Chen, W. Wang, N.V. Chawla, Beyond answers: transferring reasoning
capabilities to smaller llms using multi-teacher knowledge distillation, in _Proceedings of the
Eighteenth ACM International Conference on Web Search and Data Mining_ , (2025, March), pp.
251–260
[Crossref]
33.
Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., & Soricut, R. (2020). ALBERT: A Lite
BERT for Self-Supervised Learning of Language Representations. International Conference on
Learning Representations
34.
L. Li, T. Jin, X. Cheng, Y. Wang, W. Lin, R. Huang, Z. Zhao, Contrastive token-wise meta-
learning for unseen performer visual temporal-aligned translation, in _Findings of the Association
for Computational Linguistics: ACL 2023_ , (2023), pp. 11052–1 1062
35.
Melas-Kyriazi, L., & Wang, F. (2022). Intrinsic gradient compression for scalable and efficient
federated learning. In proceedings of the first workshop on federated learning for natural
language processing (FL4NLP), 23–32
36.
Y. Zhao, Y. Liu, Y. Wang, Partitioning DNNs for optimizing distributed inference performance in
edge computing. Appl. Sci. **12** (20), 10619 (2022)
[Crossref]
37.
C. Louizos, M. Welling, D.P. Kingma, Learning sparse neural networks through L 0
regularization. _International Conference on Learning Representations (ICLR)_ (2018)


38.
B. Wu, X. Dai, P. Zhang, Y. Wang, F. Sun, Y. Wu, Y. Tian, P. Vajda, Y. Jia, K. Keutzer, FBNet:
Hardware-aware efficient convnet design via differentiable neural architecture search, in
_Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_ ,
(2019), pp. 10734–10742
39.
H. Cai, C. Gan, S. Han, Once-for-all: Train one network and specialize it for efficient
deployment, in _International Conference on Learning Representations (ICLR)_ , (2020)
40.
T.-J. Yang, A. Howard, B. Chen, X. Zhang, A. Go, V. Sze, H. Adam, NetAdapt: Platform-aware
neural network adaptation for mobile applications, in _Proceedings of the European Conference
on Computer Vision (ECCV)_ , (2018), pp. 285–300
41.
Miriyala, S. S., et al. (2024). _Evolutionary Calibration for Post-training Quantization of LLMs
with Mixed Precision_
42.
https:// mlcommons. org/ en/ inference-tiny/
43.
G. Tie, Z. Zhao, D. Song, F. Wei, R. Zhou, Y. Dai, et al., A survey on post-training of large
language models. _arXiv preprint arXiv:2503.06072_ (2025)
44.
E. Frantar, S. Ashkboos, T. Hoefler, D. Alistarh, Gptq: accurate post-training quantization for
generative pre-trained transformers. _arXiv preprint arXiv:2210.17323_ (2022)
45.
J. Lin, J. Tang, H. Tang, S. Yang, W.M. Chen, W.C. Wang, et al., Awq: Activation-aware weight
quantization for on-device llm compression and acceleration. Proc. Mach. Learn. Syst. **6** , 87–100
(2024)
46.
Z. Liu, B. Oguz, C. Zhao, E. Chang, P. Stock, Y. Mehdad, et al., Llm-qat: data-free quantization
aware training for large language models. _arXiv preprint arXiv:2305.17888_ (2023)
47.
G. Xiao, J. Lin, M. Seznec, H. Wu, J. Demouth, S. Han, Smoothquant: accurate and efficient
post-training quantization for large language models, in _International Conference on Machine
Learning_ , (PMLR, 2023, July), pp. 38087–38099
48.
Li, S., Ning, X., Hong, K., Liu, T., Wang, L., Li, X., ... & Wang, Y. (2023). Llm-mq: Mixed-
precision quantization for efficient llm deployment. In _NeurIPS 2023 Efficient Natural Language
and Speech Processing Workshop_ (pp. 1–5)
49.
S.S. Miriyala, P.K. Suhas, U. Tiwari, V.N. Rajendiran, Mixed precision neural quantization with
multi-objective Bayesian optimization for on-device deployment, in _ICASSP 2024–2024 IEEE
International Conference on Acoustics, Speech and Signal Processing (ICASSP)_ , (IEEE, 2024,
April), pp. 6260–6264
50.
Hu, Z., Zhang, L., Dai, S., Gong, S., & Shi, Q. (2025). Fedqlora: Federated Quantization-Aware
Lora for Large Language Models
51.
Y. Xu, L. Xie, X. Gu, X. Chen, H. Chang, H. Zhang, et al., Qa-lora: quantization-aware low-rank
adaptation of large language models. _arXiv preprint arXiv:2309.14717_ (2023)
52.
GitHub. _llama.cpp_. Retrieved from https:// github. com/ ggerganov/ llama. cpp. (n.d.)


53.
https:// [http://www.](http://www.) qualcomm. com/ developer/ software/ neural-processing-sdk-for-ai
54.
C. Chen, S. Borgeaud, G. Irving, J.B. Lespiau, L. Sifre, J. Jumper, Accelerating large language
model decoding with speculative sampling. _arXiv preprint arXiv:2302.01318_ (2023)
55.
Y. Leviathan, M. Kalman, Y. Matias, Fast inference from transformers via speculative decoding,
in _International Conference on Machine Learning_ , (PMLR, 2023, July), pp. 19274–19286
56.
Miao, X., Oliaro, G., Zhang, Z., Cheng, X., Wang, Z., Zhang, Z., ... & Jia, Z. (2024, April).
Specinfer: Accelerating large language model serving with tree-based speculative inference and
verification. In _Proceedings of the 29th ACM International Conference on Architectural Support
for Programming Languages and Operating Systems, Volume 3_ (pp. 932–949)
57.
B. Spector, C. Re, Accelerating llm inference with staged speculative decoding. _arXiv preprint
arXiv:2308.04623_ (2023)
58.
Q. Fu, M. Cho, T. Merth, S. Mehta, M. Rastegari, M. Najibi, LazyLLM: dynamic token pruning
for efficient long context LLM inference. _arXiv preprint arXiv:2407.14057._ (2024) https:// arxiv.
org/ abs/ 2407. 14057
59.
Y. Tao, Y. Tang, Y. Wang, M. Zhu, H. Hu, Y. Wang, _Saliency-driven dynamic token pruning for
large language models_. arXiv preprint arXiv:2504.04514. (2025) https:// arxiv. org/ abs/ 2504.
04514
60.
M. Yan, S. Agarwal, S. Venkataraman, Decoding speculative decoding. arXiv preprint
arXiv:2402.01528 (2025) https:// arxiv. org/ abs/ 2402. 01528
61.
J. Park, D. Jones, M. Morse, R. Goel, M. Lee, C. Lott, KeDiff: key similarity-based KV cache
eviction for long-context LLM inference in resource-constrained environments. _arXiv preprint
arXiv:2504.15364_ (2025)
62.
Y. Zhang, Y. Zhang, J. Ma, Token pruning for efficient transformer inference. _arXiv preprint
arXiv:2303.00000_ (2023)
63.
Lee, K., Kim, J., & Park, S. (2023). Early exit strategies in transformer models. _Proceedings of
the 2023 Conference on Empirical Methods in Natural Language Processing_ , pp 1234–1245
64.
L. Chen, M. Zhao, Q. Li, Optimizing transformer inference with early exit mechanisms. Neural
Inform. Process. Syst. **36** , 7890–7901 (2023)
65.
F. Gao, Y. Lin, D. Xu, Early exit techniques in large-scale transformers. J. Artif. Intell. Res. **78** ,
123–136 (2023)
66.
Q. Fu, M. Cho, T. Merth, S. Mehta, M. Rastegari, M. Najibi, LazyLLM: dynamic token pruning
for efficient long-context LLM inference. OpenReview (OpenReview) (2025)
67.
Y. Zhao, O. Dada, X. Gao, R.D. Mullins, Revisiting structured dropout. Proc. Mach. Learn. Res.
**222** , 1–15 (2023) Retrieved from https:// proceedings. mlr. press/ v222/ zhao24a/ zhao24a. pdf
68.
T. Kim, A.T. Suresh, K. Papineni, M. Riley, S. Kumar, A. Benton, Exploring and improving
drafts in Blockwise parallel decoding. _arXiv preprint arXiv:2404.09221_ (2024)


69.
Y. Kang, J. Hauswald, C. Gao, A. Rovinski, T. Mudge, J. Mars, L. Tang, Neurosurgeon:
collaborative intelligence between the cloud and mobile edge. ACM SIGARCH Comp.
Architect. News **45** (1), 615–629 (2017). https:// doi. org/ 10. 1145/ 3093337. 3037698
[Crossref]
70.
Y. Cheng, W. Zhang, Z. Zhang, C. Zhang, S. Wang, S. Mao, Towards federated large language
models: Motivations, methods, and future directions. IEEE Commun. Surv. Tutor. (2024)
71.
https:// github. com/ TUDB-Labs/ mLoRA
72.
T. Fan, Y. Kang, G. Ma, W. Chen, W. Wei, L. Fan, Q. Yang, Fate-llm: a industrial grade federated
learning framework for large language models. _arXiv preprint arXiv:2310.10049_ (2023)
73.
Wu, K., Peng, H., Zhou, Z., Xiao, B., Liu, M., Yuan, L., ... & Hu, H. (2023). Tinyclip: Clip
distillation via affinity mimicking and weight inheritance. In _Proceedings of the IEEE/CVF
International Conference on Computer Vision_ (pp. 21970–21980)
74.
P.K.A. Vasu, H. Pouransari, F. Faghri, R. Vemulapalli, O. Tuzel, Mobileclip: fast image-text
models through multi-modal reinforced training, in _Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition_ , (2024), pp. 15963–15974
75.
M. Kim, S. Gao, Y.C. Hsu, Y. Shen, H. Jin, Token fusion: bridging the gap between token
pruning and token merging, in _Proceedings of the IEEE/CVF Winter Conference on Applications
of Computer Vision_ , (2024), pp. 1383–1392
76.
C. Wang, Z. Wang, X. Xu, Y. Tang, J. Zhou, J. Lu, Q-VLM: post-training quantization for large
vision-language models. _arXiv preprint arXiv:2410.081 19_ (2024)
77.
H. Sun, R. Wang, Y. Li, X. Cao, X. Jiang, Y. Hu, B. Zhang, P4Q: learning to prompt for
quantization in visual-language models. _arXiv preprint arXiv:2409.17634_ (2024)
_OceanofPDF.com_


(1)

© The Author(s), under exclusive license to Springer Nature Switzerland AG 2026
S. Singh et al. (eds.), _GenAI and LLMs for Beyond 5G Networks_
https://doi.org/10.1007/978-3-032-06418-9_3

##### Legal and Regulatory Frameworks

##### Governing Generative AI for Enterprises

Anish Kumar^1
Intel, Bengaluru, India

```
The reason to regulate AI is not because the technology is out of
control, but because human imagination is out of proportion.
Gushing media coverage has fueled irrational beliefs about AI’s
abilities and consciousness. Such beliefs build on ‘automation bias’
or the tendency to let your guard down when machines are
performing a task...Research has also shown that people treat
computers as social beings when the machines show even the
slightest hint of humanness, such as the use of conversational
language. In these cases, people apply social rules of human
interaction, such as politeness and reciprocity. So, when computers
seem sentient, people tend to trust them, blindly. Regulation is
needed to ensure that AI products deserve this trust and don’t exploit
it.
— S.Shyam Sundar , Professor of Media Effects & Director,
Center for Socially Responsible AI, Penn State University [ 1 ]
```
**Abstract**
This chapter provides a comprehensive analysis of the legal and regulatory
frameworks shaping the deployment of generative AI (GenAI) in enterprise
contexts. As GenAI rapidly transforms business operations by automating
tasks, enhancing decision-making, and driving innovation, enterprises face
mounting pressure to comply with a fragmented and evolving global
regulatory landscape. The chapter opens with the significance of GenAI
adoption, emphasizing that legal and regulatory compliance is foundational


not only for mitigating risks but also for sustaining trust, fairness, and
innovation.
The chapter outlines the global landscape, beginning with the European
Union’s AI Act—a pioneering, risk-based regulation that classifies AI
systems into tiers from unacceptable to minimal risk, imposing
transparency, human oversight, and conformity obligations. The GDPR’s
provisions on privacy by design, lawful processing, and data subject rights
are detailed, highlighting how they affect generative AI systems. In the
United States, the absence of a federal AI law has led to a patchwork of
state-level laws (e.g., in California and Colorado) and federal agency
guidance from the FTC and FCC, with new executive orders driving
infrastructure localization and AI safety standards.
China enforces a dual-track regulatory model emphasizing sovereignty
and accountability—requiring AI-generated content labeling and domestic
data storage. India’s Digital Personal Data Protection (DPDP) Act and
anticipated AI Governance Act focus on consent, bias prevention, and local
data storage. India’s courts and policymakers are actively shaping IP law
interpretations, with landmark cases like ANI v. OpenAI challenging
unauthorized content use for training. Countries like the UAE and Saudi
Arabia balance innovation and control through national AI strategies, AI
regulatory sandboxes, and novel constructs like “data embassies.”
The Asia-Pacific region presents diverse governance models:
prescriptive in China and South Korea, principle-based in Japan and
Australia, and co-regulatory in Singapore, where the Model AI Governance
Framework for GenAI and the AI Verify toolkit offer adaptive oversight.
These approaches reflect varying national priorities—data sovereignty,
economic competitiveness, and human-centric AI values.
Key global trends include increased emphasis on risk management,
transparency, explainability, and data provenance. Enterprises are adopting
grounding techniques, human-in-the-loop reviews, and automated reasoning
to mitigate hallucinations and ensure factual integrity. Enforcement of
policy-based guardrails—such as Amazon Bedrock Guardrails—at the
training, inference, and deployment stages ensures ethical AI use across
industries.
On intellectual property (IP), the chapter explores the challenges
enterprises face regarding AI-generated content. Most jurisdictions do not
yet recognize machine-generated works under copyright or patent laws.


```
AI
GenAI
LLMs
GPT
MGF-Gen AI
```
```
EU
US
UK
```
Notable legal actions, such as Getty Images v. Stability AI, highlight the
risks of unlicensed data use in training models. The chapter recommends
hybrid IP strategies involving human authorship claims, watermarked
outputs, and enhanced licensing practices.
Liability issues are explored in depth, clarifying the responsibilities of
developers, deployers, and users of AI systems. Recent legal developments,
including the Garcia v. Character.AI case, suggest courts may increasingly
treat AI tools as “products” subject to liability laws. Section 230 protections
in the U.S. are also narrowing as courts distinguish between hosting and
content generation. To mitigate legal exposure, enterprises must implement
rigorous governance frameworks, transparency mechanisms, and ethical use
policies.
The chapter also presents open-source compliance tools for multi-
jurisdictional data governance, including ARX and Microsoft Presidio for
data anonymization and privacy-preserving AI. These tools help enterprises
comply with regulations like GDPR, CCPA, and DPDP while maintaining
auditability and scalability.
In conclusion, the chapter underscores that enterprises deploying
generative AI must proactively align with a dynamic legal environment.
This involves not only understanding varied jurisdictional requirements but
also embedding responsible, ethical practices across the AI lifecycle.
Through adaptive governance, technical safeguards, and legal foresight,
organizations can leverage GenAI’s transformative potential while
upholding societal values and minimizing regulatory and reputational risks.

**Abbreviations**

```
Artificial Intelligence
Generative Artificial Intelligence
Large Language Models
Generative Pre-trained Transformer
Multilateral Government Forum on Generative Artificial
Intelligence (used in context of G7, OECD, etc.)
European Union
United States
United Kingdom
```

_UAE_

_G7_

_OECD_

_MeitY_

_DIFC_

_ADGM_

_KRW_

_GDPR_

_DPDP_

_CCPA_

_CDPA Act_

_PDPB_

_PIPL_

_GB 45438-2025_

_ICO_

_OAIC’s_

_CNIL_

_NIST_

_NIST AI RMF_

_FCC_

_FTC_

_CISA_

_TRAIGA_

_SB-24-205_

_IAM_

_PII_

_PIAs_

United Arab Emirates
Group of Seven (Canada, France, Germany, Italy, Japan, UK, US)
Organisation for Economic Co-operation and Development
Ministry of Electronics and Information Technology (India)
Dubai International Financial Centre
Abu Dhabi Global Market
South Korean Won (currency)
General Data Protection Regulation (EU)
Digital Personal Data Protection (Act, India)
California Consumer Privacy Act
Consumer Data Protection Act (Virginia, US)
Personal Data Protection Bill (India, predecessor to DPDP)
Personal Information Protection Law (China)
China’s Standard for Generative AI Services (draft
standard)

Information Commissioner’s Office (UK)
Office of the Australian Information Commissioner’s
Commission Nationale de l’Informatique et des Libertés (French
Data Protection Authority)

National Institute of Standards and Technology (US)
NIST Artificial Intelligence Risk Management Framework
Federal Communications Commission (US)
Federal Trade Commission (US)
Cybersecurity and Infrastructure Security Agency (US)
Transparency, Responsibility, and Accountability in AI-
generated Artifacts Act (US Senate Bill)

```
Senate Bill 24-205 (Colorado AI Act)
Identity and Access Management
Personally Identifiable Information
Privacy Impact Assessments
```

```
PCI DSS
GRC
CI/CD
UI
PHP
JavaScript CMP
NER
ARX
CC BY-NC-SA
```
```
SMBs
BCG
CSA
R&D
TDM
DABUS
```
```
MGX/G42
```
```
Payment Card Industry Data Security Standard
Governance, Risk, and Compliance
Continuous Integration/Continuous Deployment
User Interface
Hypertext Preprocessor (recursive acronym)
JavaScript Consent Management Platform
Named Entity Recognition
Open-source software for anonymizing sensitive personal data
Creative Commons Attribution-Non Commercial-Share
Alike License
Small and Medium-sized Businesses
Boston Consulting Group
Cloud Security Alliance
Research and Development
Text and Data Mining
Device for the Autonomous Bootstrapping of Unified Sentience
(AI system central to legal debates on AI inventorship)
MGX (a joint venture between Mubadala and G42, UAE-
based AI company)
```
**1 Introduction**

Enterprises adopting generative AI must navigate an intricate maze of legal
and regulatory frameworks across the globe. This chapter explores the
regulatory landscape for AI in general and Generative AI specifically,
beginning with an overview of its significance in business and the
escalating drive for clear legal oversight. It then examines global and
regional approaches, spotlighting comprehensive frameworks such as the
EU’s AI Act, GDPR, the US’s evolving federal and state regulations, India’s
DPDP Act, and pertinent rules in other major markets across the globe. The
discussion addresses data protection compliance challenges, nuances in
intellectual property rights over generated content, and the question of
liability and accountability for AI-generated outcomes. It further delves into


explainability and transparency requirements designed to foster trust and
support regulatory alignment. Concrete strategies for enterprise-level
compliance and governance are presented, with attention to internal
controls, auditing, and adapting to rapid legal developments. The chapter
concludes with an analysis of current regulatory gaps and the imperative for
agile, responsible AI systems that can withstand a changing legal
environment. Through this global lens, enterprises are equipped to
proactively address the multifaceted legal risks and responsibilities of
generative AI deployment.

**2 Overview of Generative AI in Enterprise**

**Contexts**

Generative AI has become a transformative force for enterprises worldwide,
offering significant potential to automate routine tasks, enhance
productivity, and drive innovation. Benefits include cost savings,
accelerated product development, improved customer engagement, and
knowledge synthesis through AI-human collaboration [ 2 , 3 , 4 ] while
bridging the skill gap in the industry. Global studies show that organizations
using generative AI often experience substantial improvements in
operational efficiency and customer retention—McKinsey estimates its
economic impact could reach up to $ 4.4 trillion [ 5 ] annually.
Major enterprises harnessing generative AI span industries and
continents: in the US, tech leaders like OpenAI, Microsoft, Google,
Amazon, and Meta drive foundation models and enterprise solutions, while
Accenture and Deloitte spearhead AI integration for business
transformation [ 6 , 7 ]. Europe’s AI ecosystem features giants such as SAP
and emerging startups, with strong service firms like Accenture (Ireland)
expanding their reach [ 1 ]. In the UK, investment and adoption rates are
high, as the country pursues “AI superpower” status [ 8 ]. China’s Alibaba,
Baidu, and Tencent are leading adopters, rapidly deploying GenAI across
sectors, while startups like Sarvam AI, Krutrim SI and government of India
initiatives like AI4Bharat are notable in India.
However, the absence of robust legal and regulatory frameworks poses
major risks for enterprises. Exposing sensitive data, amplifying bias,
infringing intellectual property, and a lack of accountability can all result in


reputational harm, legal action, and loss of public trust [ 9 – 11 ].
Governments and international bodies now debate effective regulation to
balance innovation with accountability, underscoring the urgent need for
adaptive, enforceable governance to manage generative AI’s risks and
realize its benefits [ 12 – 14 ].

**3 Significance of Legal and Regulatory**

**Compliance in AI Adoption**

Legal and regulatory compliance is crucial for responsibly adopting AI in
any enterprise. According to the U.S. National Institute of Standards and
Technology (NIST), compliance ensures that AI systems meet established
legal, regulatory, and ethical standards, helping organizations avoid risks
like discrimination, privacy violations, and security breaches [ 15 ]. Tracking
and reporting all AI use cases—especially those impacting safety or rights
—are essential steps for holding organizations accountable and protecting
the public interest, as seen in policies [ 16 ] across the globe by various
governments.
Effective regulation not only reduces risks and uncertainties but also
supports public trust and fairness. Without clear legal frameworks,
organizations may inadvertently undermine fundamental rights or face
unpredictable legal exposure [ 17 , 18 ]. Robust AI governance helps align
technology with societal values, reducing the possibility of reputational
harm or litigation when AI decisions go wrong [ 19 ].
Strong compliance practices—such as privacy-by-design and
transparent decision processes—also drive innovation by providing safe
environments for AI experimentation and deployment for large scale use.
Essentially, legal and regulatory compliance is not just about avoiding
penalties but also about building trustworthy, fair, and effective AI that
serves both business interests and the broader good for the human race.

**4 Global Landscape of Generative AI Regulation**

The global landscape for regulating generative AI is fast-evolving, as
governments and regulators seek to balance the benefits of innovation with
the need to address risks such as bias, disinformation, privacy violations,


and legal uncertainty. The European Union (EU) is leading with its
landmark AI Act, passed in 2024, which introduces a risk-based approach—
setting strict rules for high-risk AI while banning certain uses, and ensuring
transparency and accountability (European Commission, 2024). The United
States currently relies on a multitude of local state laws and federal
guidelines, like NIST’s AI Risk Management Framework, with ongoing
efforts to develop comprehensive national legislation.
In the UK, the government has issued AI principles and established an
AI Safety Institute. The UK pursues a “pro-innovation” and flexible
approach to generative AI regulation, favoring principles-based guidelines
and sector-specific oversight rather than blanket laws. Its AI Safety Institute
—recently rebranded as the AI Security Institute—focuses on national
security and technical risks, positioning the UK as a leader in AI safety
research [ 20 – 22 ]. China enforces strict generative AI regulations, requiring
providers to label AI-generated content with explicit or embedded tags and
registering services with authorities to enhance accountability and prevent
misuse. New standards and rules are expected to take effect in September
and November 2025, intensifying oversight of AI content and providers
[ 23 – 25 ].
The UAE is developing its own AI regulation by launching an Artificial
intelligence office and enacting comprehensive laws on AI ethics, blending
innovation with strong governance and a focus on human-centric principles
[ 26 – 28 ] through its new Minister of State for Artificial Intelligence. Saudi
Arabia introduced a draft Global AI Hub Law in 2025 to attract
international AI firms, offering legal clarity for data sovereignty, and is
rolling out nationwide AI education in schools as part of Vision 2030 [ 29 –
31 ]. France largely follows EU-wide AI rules through the AI Act but
emphasizes a cautious, innovation-friendly approach domestically—
focusing on transparency, gradual regulations for generative AI, and support
from agencies like Commission Nationale de l’Informatique et des Libertés
(CNIL) for ethical deployment [ 32 ]. India is trying to regulate AI through
advisories and sector-specific guidelines, with anticipated comprehensive
legislation in late 2025, emphasizing labeling, user safeguards, and a
flexible, principles-based method while sector regulators oversee
implementation [ 33 ].
Other countries—such as Canada, Brazil, Japan and Australia—are
developing draft AI laws or revising data protection acts to address AI-


specific risks.
Internationally, groups like the G7 and OECD are pushing for voluntary
codes of conduct, while the United Nations’ Advisory Body on AI calls for
global frameworks to encourage responsible AI development. As the
regulatory landscape continues to evolve, enterprises must stay agile to
remain compliant and manage emerging risks.

**5 Key Themes and Drivers in AI Governance**

**Worldwide**

A central theme in AI governance is risk management: policymakers focus
on mitigating bias, misinformation, privacy breaches, and safety lapses. The
OECD’s recent report underscores the importance of anticipatory
governance, advocating adaptive frameworks that evolve alongside AI
technologies to manage emerging risks proactively [ 34 ].
Transparency and explainability have become indispensable. Regulators
increasingly require that AI systems provide clear documentation, human
oversight, and user disclosures—tenets enshrined in the EU AI Act
mandates for high-risk applications [ 35 ]. This fosters trust by allowing
stakeholders to verify how models reach decisions.
Data protection and privacy remain fundamental. Laws such as the
GDPR set the global benchmark for personal data handling, while countries
like China and India advance data localization and cross-border transfer
controls to safeguard sovereignty and individual rights [ 36 ].
Another driver is grounding AI responses to truth. Enterprises deploy
Retrieval-Augmented Generation and grounding techniques to anchor
outputs in verifiable data sources, reducing hallucinations and ensuring
factual accuracy [ 37 ]. Best practices now include human-in-the-loop
reviews and automated reasoning checks to validate claims against a
ground-truth dataset [ 38 ].
Finally, the enforcement of guardrails has gained momentum.
Companies leverage policy-based enforcement tools—such as Amazon
Bedrock Guardrails—to filter undesirable content, detect hallucinations,
and enforce organization-wide safety policies through IAM conditions [ 39 ].
These guardrails operate at every stage—training, inference, and


deployment—to ensure AI systems remain aligned with ethical and legal
standards.
Collectively, these themes highlight a global push toward responsible,
transparent, and secure AI, balancing innovation with robust safeguards that
protect individuals, enterprises, and society.

**6 Comparative Overview of Major Jurisdictions**

**and International Approaches**

Enterprises deploying generative AI must navigate diverse national
priorities—from risk-based frameworks and data localization mandates to
“sovereign AI” initiatives aimed at preventing cross-border misuse,
particularly during conflicts.

**European Union** The EU’s AI Act (Regulation (EU) 2024/1689) applies
a four-tier risk-based model, banning unacceptable AI uses and imposing
stringent transparency, conformity assessments, and human-oversight
obligations on high-risk systems. It mandates data minimization and
requires providers to safeguard sensitive data processed by AI, reinforcing
data localization and extraterritorial reach to any AI provider with EU users
[ 40 , 41 ].

**United States** Rather than a single statute, the U.S. relies on executive
orders and existing sectoral laws. The January 2025 Executive Order on AI
Infrastructure directs that future frontier AI be built domestically to forestall
adversarial access and preserve national security, explicitly prohibiting
reliance on foreign computing clusters [ 42 ]. Federal guidelines from NIST
and proposed agency rulemakings emphasize provenance tracking and
secure supply chains.

**China** Beijing pursues AI sovereignty through a dual-track regulatory
approach: central agencies implement restrictive measures (e.g., content
labeling and “trustworthy AI” criteria) while fostering domestic innovation,
and provincial regulators offer facilitative environments to support national
champions. New generative AI rules require explicit in-system disclosures
and local data storage to prevent foreign manipulation or “weaponization”
of AI outputs [ 43 ].


**India** India’s March 2024 MeitY advisory mandates that platforms secure
government approval before deploying generative AI or LLMs and label
AI-generated content. A forthcoming Draft AI Governance Act aims to
extend DPDP Act protections to AI, emphasizing sector-specific guidelines,
liability reforms for extreme harms, and flexible, principle-based oversight
to balance innovation with privacy [ 44 , 45 ].

**United Arab Emirates** The UAE National Strategy for AI 2031 and
decrees in the DIFC and ADGM promote human-centric AI while
augmenting existing data protection laws. A newly established Regulatory
Intelligence Office uses AI to draft and update legislation, and federal
guidelines require federal-only deployment of high-risk AI within sovereign
“trusted zones” to prevent external manipulation.

**Saudi Arabia** The draft Global AI Hub Law pioneers the concept of “data
embassies,” allowing foreign states to host sovereign data centers on Saudi
soil under the guest country’s jurisdiction. This model protects sensitive
data and ensures foreign oversight, while Saudi regulators retain emergency
powers to terminate agreements if national security is threatened [ 46 , 47 ].

**Other Nations** Canada, Brazil, Japan, and Australia are drafting AI-
specific amendments to existing privacy laws, combining risk assessments
with sandbox regimes. Multilateral bodies, including the G7, OECD, and
the UN’s AI Advisory Body, promote harmonized principles to reduce
fragmentation and address cross-border AI weaponization risks.

This multitude of frameworks reflects a global shift toward localized AI
deployment, sovereign data control, and resilient guardrails to safeguard
against misuse in times of conflict, while nurturing domestic innovation.

**7 Regional Legal Frameworks**

Across major regions, governments pair tailored AI regulations with home-
grown models to spur innovation, safeguard public interests, and assert
digital sovereignty. The EU AI Act implements a risk-based regime—
banning unacceptable uses, imposing strict requirements on high-risk
systems, and enforcing transparency for general-purpose AI—while


supporting open-science models like BLOOM through public-compute
grants for collective research [ 48 , 49 ]. The United States relies on a
multitude of federal guidelines (NIST’s AI Risk Management Framework)
and an Executive Order mandating domestic AI infrastructure, underpinning
commercial systems such as OpenAI’s GPT and Google’s PaLM/Gemini to
ensure both national security and innovation. China enforces “trustworthy
AI” rules—requiring content labeling, approved datasets, and local data
storage—to regulate domestic giants Baidu (Ernie Bot), Alibaba, and
Tencent. India’s DPDP Act and forthcoming AI Governance Act emphasize
data sovereignty and public-good models like Bhashini and the state-funded
BharatGen and BharatGPT to bridge linguistic divides [ 50 , 51 ].
The UAE’s National AI Strategy 2031 funds sovereign models via
companies like MGX/G42, aligning ethics and economic diversification
[ 52 ]. Saudi Arabia advances “data embassies,” sovereign LLM platforms
(e.g., stc.AI’s Llama 405B service and Mulhem), and strict oversight to
protect national interests [ 53 , 54 ]. France, under EU rules, champions open-
weight LLMs by startups like Mistral AI to balance transparency with
commercial viability.

**7.1 European Union**

The European Union has pioneered AI regulation with the EU AI Act, a
risk-based framework that bans high-risk practices, mandates transparency
and human oversight, and applies extraterritorially to all providers [ 55 , 56 ].
Major member states like Germany and France co-lead initiatives—such as
Gaia-X, a federated data-sovereignty project—and the InvestAI
programme, which mobilizes €200 billion for AI infrastructure and “AI
Gigafactories” to strengthen Europe’s digital autonomy and innovation [ 57 ,
58 ].

**_7.1.1 The EU AI Act: Scope, Risk-Based Regulation, and_**

**_Enforcement Mechanisms_**

The EU AI Act (Regulation (EU) 2024/1689) establishes the first
comprehensive legal framework for artificial intelligence worldwide. It
applies to any natural or legal person—providers, deployers, importers,
distributors, and users—who develop, place on the market, or utilize AI
systems in the EU, regardless of where they are based [ 59 ]. The Act also


covers general-purpose AI models whose outputs are intended for use
within the EU market [ 60 ].

**7.2 Scope**

The Act adopts a broad definition of AI systems as “machine-based systems
designed to operate with varying levels of autonomy and adaptiveness after
deployment” that generate outputs such as predictions, content, or decisions
influencing environments [ 61 ]. Exemptions include military AI, purely
scientific research, and open-source components in development phases
[ 62 ].

**_7.2.1 Risk-Based Regulation_**

A four-tier risk classification underpins the regulation:

1. **Unacceptable Risk** : Banned practices (e.g., subliminal manipulation,

## social scoring) [ 63 ].^

2. **High Risk** : Systems affecting safety, health, or fundamental rights
    (e.g., credit scoring, recruitment tools), subject to stringent obligations
    including risk management, data quality, documentation, human
    oversight, and post-market monitoring [ 64 , 65 ].
3. **Limited Risk** : AI systems such as chatbots, requiring transparency

## obligations to inform users they interact with AI.^

4. **Minimal Risk** : Applications like spam filters, free for use without

## additional requirements [ 66 ].^

**7.3 Enforcement Mechanisms**

**Penalties**
Member States must enforce the Act with effective, proportionate, and
dissuasive penalties:

```
Up to €35 million or 7% of global turnover for prohibited AI practices
violations [ 67 ].
Up to €15 million or 3% of global turnover for breaches of high-risk
system obligations by providers, importers, distributors, and deployers
[ 68 ].
```

```
Up to €7.5 million or 1% of global turnover for supplying incorrect or
misleading information to authorities [ 69 ].
```
**Incentives**
To encourage early adoption, the European Commission launched the AI
Pact, a voluntary initiative offering public recognition, knowledge-sharing
workshops, and networking opportunities with over 550 signatories
including Microsoft, Google, and OpenAI [ 70 ]. Participants gain visibility
on the EU’s digital strategy website and can showcase ethical AI
commitments before mandatory deadlines.
Through this balanced approach of clear scope, proportionate risk-based
rules, and robust enforcement—coupled with voluntary incentives—the EU
aims to foster trustworthy AI while maintaining innovation and
competitiveness.

**7.4 Data Security & Protection: GDPR Implications for**

**Generative AI**

The GDPR mandates that enterprises using generative AI in the EU
establish a lawful basis for processing personal data, ensuring transparency,
purpose limitation, and data minimization [ 71 ]. They must conduct Data
Protection Impact Assessments under Article 35 for high-risk AI
deployments, documenting risks and mitigation measures [ 72 ].
Privacy by design and by default is legally required (Article 25),
embedding security controls during AI development [ 73 ]. Organizations
must uphold data-subject rights—access, rectification, erasure, portability—
and report breaches to authorities within 72 hours [ 74 ].
Appointing a Data Protection Officer is compulsory for core processing
activities, and stringent controller–processor contracts under Article 28
govern AI service providers [ 75 ]. These mandates ensure trust,
accountability, and robust data security in generative AI applications.
Generative AI systems processing personal data in the EU must comply
with several GDPR provisions:

**Article 5—Principles**
AI providers must adhere to lawfulness, fairness, and transparency; purpose
limitation; data minimization; accuracy; storage limitation; integrity and
confidentiality; and accountability [ 76 ].


**Article 6—Lawful Bases**
Processing for model training or inference requires a valid legal basis—
typically consent, contract necessity, legitimate interest (with balancing
test), or public interest [ 77 ].

**Article 22—Automated Decision-Making**
When AI outputs produce legal or similarly significant effects (e.g.,
automated profiling), data subjects gain the right to human intervention,
meaningful information about the logic involved, and to challenge decisions
[ 78 ].

**Article 25—Data Protection by Design and by Default**
Generative AI must embed privacy controls from the outset—minimizing
data use, pseudonymizing where possible, and restricting defaults to the
least intrusive settings [ 79 ].

**Article 28—Processor Contracts**
Controllers using third-party AI services must ensure contracts stipulate
GDPR-compliant processing, security measures, and audit rights [ 80 ].

**Article 35—Data Protection Impact Assessments (DPIAs)**
High-risk AI—particularly large language models—requires a DPIA before
deployment, assessing likelihood and severity of rights infringement, and
defining mitigation measures [ 81 ].

**Articles 32–34—Security and Breach Notification**
Controllers and processors must implement appropriate technical and
organizational measures (e.g., encryption) and notify supervisory authorities
of breaches within 72 h [ 82 ].

**Articles 15–21—Data Subject Rights**
Enterprises must facilitate access, rectification, erasure, restriction,
portability, objection, and rights related to automated decisions for
individuals interacting with AI-generated content [ 83 ].

Collectively, these articles ensure generative AI is developed and deployed
responsibly, balancing innovation with fundamental rights and
accountability.


**7.5 United States**

The United States takes a decentralized approach to AI regulation, without a
federal AI act, relying on a mix of sectoral guidelines and executive orders.
States like California, Colorado, New York, Illinois, and Utah lead with
robust laws—such as the California AI Transparency Act and Colorado AI
Act—mandating transparency and risk assessment for high-risk AI, and
addressing algorithmic bias, deepfakes, and profiling. Recent federal action
centers on the 2025 America’s AI Action Plan, prioritizing infrastructure,
deregulation, and leadership in international AI, while empowering states to
innovate through diverse, evolving frameworks [ 84 – 86 ]_._

**_7.5.1 Regulatory Patchwork: Federal (FCC, FTC) and State-_**

**_Level Initiatives_**

The United States employs a patchwork approach to AI regulation, marked
by a blend of federal agency actions and diverse state-level initiatives
targeting issues of transparency, privacy, and algorithmic bias.

**7.6 Federal Level**

**_7.6.1 Federal Communications Commission (FCC)._**

The FCC is actively regulating the use of AI in telecommunications. Recent
proposals cover robocalls and political advertising: AI-generated calls,
particularly those that mimic voices (like political robocalls), now fall under
the Telephone Consumer Protection Act (TCPA). This mandates obtaining
prior consent from recipients and clear machine disclosures at the start of
each call. Pending rules also require identifying AI-generated content in
political ads and providing written (on-air and online) disclosures. Penalties
for violators match existing TCPA enforcement, with fines ranging from
thousands to millions of dollars depending on the severity and intent of the
violation [ 87 – 90 ].

**_7.6.2 Federal Trade Commission (FTC)._**

The FTC focuses on deceptive and unfair AI practices under long-standing
consumer protection statutes. In recent actions, companies making
misleading AI-related claims—such as misrepresenting an “AI lawyer”
service or generating fake reviews—were fined (one case resulted in a
$193,000 penalty) and required to notify consumers about AI product


limitations and ensure ongoing transparency. The FTC also warns that AI is
not exempt from enforcement, cracking down on AI tools that enable fraud
or manipulate consumers [ 91 – 93 ].

**7.7 State-Level Initiatives**

**_7.7.1 California_**

The California AI Transparency Act—taking effect January 2026—requires
AI system providers with over one million users in the state to offer free AI
detection tools and clearly mark all AI-generated content. Noncompliance
can result in civil penalties of $5000 per day per violation. Developers must
revoke licenses from licensees who circumvent these rules [ 94 – 96 ].
Separate laws address deepfake content labeling and training data
transparency—mandating high-level summaries of datasets used for
generative AI.

**_7.7.2 Colorado_**

The Colorado AI Act establishes strict requirements for both developers and
deployers of “high-risk” AI systems. It demands:

Annual impact assessments for significant system modifications,
Public statements outlining foreseeable risks,
Reasonable care obligations to prevent algorithmic discrimination.
Civil penalties reach up to $20,000 per violation, enforced exclusively
by the state Attorney General. There’s no private right of action, but
companies have an affirmative defense if they self-identify and cure
violations—especially when aligned with NIST’s AI Risk Management
Framework [ 97 – 100 ].

**_7.7.3 Other States_**

New York, Illinois, Utah, and Tennessee have also passed specialized laws
addressing algorithmic discrimination, deepfake regulation, mandatory
disclosures during high-risk AI interactions (especially in healthcare and
employment), and the formation of AI policy offices. Most states tie
enforcement to existing consumer protection laws and assign oversight to
state attorneys general [ 101 , 102 , 103 ].

**_7.7.4 Rewards, Penalties, and Enforcement_**


Enforcement is strict and focused on deterrence. Penalties range from daily
fines to lump-sum settlements, license revocation, ongoing oversight, and
public notifications. While most U.S. laws emphasize punitive measures for
noncompliance, some (particularly early-adopter programs and voluntary
pilot initiatives) offer indirect rewards such as reduced liability if proactive
compliance steps are documented, or positive recognition in regulatory
filings.

**_7.7.5 Key Takeaways_**

This evolving regulatory landscape demands that enterprises build robust
compliance programs, monitor legal developments closely, and implement
transparency, risk management, and explainability tools to mitigate
enforcement and reputational risks across jurisdictional boundaries.

**7.8 Data Security, Privacy, and Enterprise Liability**

**_7.8.1 Federal Executive Orders and AI Policy Framework_**

The 2025 AI Action Plan fundamentally reshapes US AI governance under
Executive Order 14179, which revoked Biden’s previous AI executive order
to “remove barriers to American AI leadership” [ 104 , 105 ]. The Trump
Administration’s approach prioritizes innovation over restrictive regulation,
establishing three core pillars: accelerating AI innovation, building
American AI infrastructure, and leading international AI diplomacy [ 106 ,
107 ].

**_7.8.2 Data Security Mandates_**

The Cybersecurity and Infrastructure Security Agency (CISA) released
comprehensive AI data security guidance in May 2025, mandating
organizations implement cybersecurity best practices throughout the AI
lifecycle [ 1 ]. Key requirements include sourcing reliable data, tracking data
provenance, and protecting against malicious data modification. Federal
agencies must now comply with updated NIST AI Risk Management
Framework guidelines, emphasizing security-by-design principles and data
integrity protection [ 108 , 109 ].

**_7.8.3 Privacy Act Implications for AI_**

The Privacy Act of 1974 governs federal agencies’ use of AI systems
processing personally identifiable information (PII) [ 110 , 111 ]. Agencies


must establish policies ensuring compliance with privacy requirements
when acquiring AI systems, with Senior Agency Officials for Privacy
involved in acquisition planning [ 112 ]. Recent guidance emphasizes that AI
applications cannot circumvent existing privacy protections, requiring
explicit consent mechanisms and data minimization practices [ 113 ].

**_7.8.4 Enterprise Liability Framework_**

State-level regulations create significant enterprise liability exposure. The
Federal Trade Commission’s Operation AI Comply demonstrates aggressive
enforcement, with penalties reaching $193,000 for deceptive AI claims
[ 114 , 115 ]. Colorado’s AI Act imposes strict liability on developers and
deployers of high-risk AI systems, with civil penalties up to $20,000 per
violation [ 116 , 117 ]. California’s new automated decision system
regulations, effective October 2025, hold employers liable for algorithmic
discrimination regardless of intent [ 118 , 119 ].
Enterprise liability extends beyond direct violations to encompass
vendor relationships, data governance failures, and inadequate risk
assessments, creating comprehensive accountability frameworks across the
AI development and deployment lifecycle [ 120 , 121 ]_._

**7.9 India**

India’s Digital Personal Data Protection Act and Draft AI Governance Act
establish consent-centric, risk-based controls and local data storage
mandates to protect sovereignty [ 122 ]. Key states like Karnataka, Tamil
Nadu and Maharashtra are leading with sector-specific guidelines and AI
ethics initiatives. Karnataka is launching AI Missions for regulation,
skilling, and innovation hubs, and other states roll out CoEs and tailored AI
policies under the IndiaAI Mission framework [ 123 ]_._

**_7.9.1 Digital Personal Data Protection (DPDP) Act and AI_**

**_Regulation_**

India’s Digital Personal Data Protection Act, 2023 (DPDP Act) establishes a
modern privacy framework focused solely on digital personal data,
replacing key provisions of the IT Act, 2000 [ 124 ]. It applies to entities
processing personal data in India and to offshore processors offering goods
or services to Indian residents. Key provisions include:


Lawful Processing & Consent: Personal data may be processed only for a
specified lawful purpose, with prior notice and verifiable consent.
Consent may be withdrawn at any time. Exemptions exist for voluntary
data sharing, government benefits, medical emergencies, and
employment (with guardian consent for minors) [ 125 ].
Data Principal Rights: Individuals (“data principals”) can access
processing details, correct or erase their data, nominate a representative
(on death/incapacity), and seek grievance redressal. Frivolous complaints
incur a penalty up to ₹10,000 [ 126 ].
Data Fiduciary Obligations: Entities determining processing purposes
must ensure data accuracy, implement “reasonable security safeguards,”
notify breaches to the Data Protection Board and affected individuals,
and erase data when no longer needed—except for government bodies,
which enjoy limited exemptions [ 127 ].
Cross-Border Transfers: Allowed freely except to countries explicitly
restricted by notification, ensuring extraterritorial reach while balancing
sovereignty concerns [ 128 ].
Enforcement & Penalties: The DPDP Act establishes a Data Protection
Board with powers to investigate and impose penalties up to ₹250 crore
for security failures and up to ₹200 crore for breaches concerning
children [ 129 ].
In parallel, India is crafting an AI Governance Act and issuing sectoral
advisories to regulate high-risk AI systems. In March 2024, MeitY
mandated that models must avoid bias, secure pre-deployment approval for
under-tested AI, and embed permanent labels in AI-generated content to
curb misinformation [ 130 ]. A Subcommittee under IndiaAI’s Advisory
Group released AI Governance Guidelines advocating a “whole-of-
government” approach, sector-specific and risk-based regulations, voluntary
standards, and an AI incident database to track harms [ 131 ]. The
forthcoming Draft AI Governance Act will extend DPDP principles—such
as consent, purpose limitation, and impact assessments—to AI, enforcing
transparency, accountability, and “AI sovereignty” through local data
storage and approval regimes. Together, the DPDP Act and emergent AI
rules aim to balance innovation, privacy, and ethical AI deployment across
India’s diverse digital ecosystem.

**7.10 Evolving Jurisprudence on Generative AI and IP**


India’s legal landscape for generative AI and intellectual property is rapidly
taking shape through landmark lawsuits, expert panels, and updated patent
guidelines. The ANI v. OpenAI case in the Delhi High Court challenges
whether storing and using copyrighted news content to train ChatGPT
violates the Copyright Act, 1957, and whether such use qualifies as “fair
dealing” under Section 52 [ 132 ]. Jurisdictional questions have also been
raised, with amici curiae affirming that Indian courts can adjudicate
infringements affecting domestic rights-holders [ 133 ].
In parallel, the Federation of Indian Publishers filed suit against OpenAI
over unauthorized training on literary works, underscoring the absence of
explicit statutory exceptions for AI training data. Responding to these
pressures, the Department for Promotion of Industry and Internal Trade
formed a multi-stakeholder expert committee in April 2025 to assess gaps in
the Copyright Act and recommend amendments to clarify authorship,
ownership, and permissible training uses [ 134 ].
On the patent front, the Controller General of Patents released Revised
CRI Guidelines (2025), which explicitly address AI and machine-learning
inventions. These guidelines refine the patentability assessment under
Section 3(k) of the Patents Act, requiring demonstrable technical
contribution beyond mere algorithms and providing a five-step test for
obviousness in AI-related claims [ 135 ].
Smaller-scale disputes—such as the Ankit Sahani painting app case—
have tested authorship norms, with the Copyright Office rejecting AI-only
authorship but tentatively allowing joint human-AI listings, only to later
question the registration’s validity [ 136 ].
Together, these developments reflect a hybrid jurisprudence that applies
existing IP statutes to generative AI while acknowledging their limitations.
As courts weigh infringement and fair-use defenses, and policy bodies
propose targeted amendments, India is charting a path toward clear rules on
AI-generated content, balancing creators’ rights with innovation
imperatives.

**7.11 Asia-Pacific**

Enterprises across the Asia-Pacific (APAC) region must navigate a mosaic
of evolving AI regulations that balance innovation, public trust, and
national interests. Key jurisdictions have adopted distinct approaches.


**China** In August 2023, the Cyberspace Administration of China (CAC)
and six ministries enforced the Interim Measures for the Management of
Generative AI Services, requiring platforms to register services, label AI-
generated content, and ensure “truthful, accurate, diverse, and objective”
outputs. Providers must secure data provenance, conduct security reviews,
and block noncompliant cross-border services. While penalties are deferred
to existing laws, serious breaches can trigger technical blocks and service
suspension [ 137 ].

**Japan** Japan shifted from soft-law guidance to formal legislation with the
AI Promotion Act (effective June 2025), which embeds voluntary AI
principles into law. The Act establishes an AI Strategy Headquarters,
mandates disclosure when users interact with AI, and prioritizes R&D
through regular master plans. Copyright rules (Article 30-4 of the Copyright
Act) allow commercial training on copyrighted works, positioning Japan as
a “machine learning paradise.” Draft guidelines on government
procurement and an anticipated Basic Law for Promoting Responsible AI
promise risk-based oversight of foundational models [ 138 ].

**South Korea** The Framework Act on AI Development and Establishment
of a Foundation for Trustworthiness (effective January 2026) applies
extraterritorially to actions affecting Korean users. It adopts a risk-based
tiering—targeting “high-impact” and “generative” AI—with obligations for
transparency, mandatory labeling, impact assessments, and moderate fines
(up to KRW 30 million). A National AI Committee guides policy, while the
Ministry of Science and ICT drafts subordinate rules for certification and
human oversight [ 139 ].

**Australia** Lacking AI-specific legislation, Australia leverages existing
data-privacy laws. The government’s Voluntary AI Safety Standard (2024)
sets ten guardrails—covering governance, risk management, human
oversight, and transparency—while a public consultation on mandatory
high-risk AI guardrails (due October 2024) explores embedding these into
existing frameworks or creating a standalone AI Act. The OAIC’s guidance
interprets the Privacy Act in generative AI contexts, imposing favored
principles such as accuracy, fairness, and privacy-by-design [ 140 ].


**Singapore** Building on its 2019 Model AI Governance Framework,
Singapore released the Model AI Governance Framework for Generative AI
(MGF-Gen AI) in May 2024. It outlines nine dimensions—ranging from
data governance and explainability to contestability and human-centricity—
and invites multi-stakeholder feedback. The IMDA’s AI Verify toolkit and
sandbox tests validate systems against globally recognized ethical
principles, reinforcing Singapore’s status as a dynamic AI hub [ 141 ].

Across APAC, harmonizing AI innovation with ethical, security, and
sovereignty concerns remains paramount. While China and South Korea
pursue binding, risk-based statutes with registration and labeling mandates,
Japan and Singapore favor principle-based frameworks supplemented by
emerging hard laws. Australia’s incremental, consultation-driven approach
underscores reliance on established privacy regimes. Enterprises operating
regionally must align internal governance, risk assessments, and
transparency mechanisms with this diverse regulatory tapestry.

**7.12 Governance Models in China, Japan, Singapore, South**

**Korea, and Australia**

Enterprises across the Asia-Pacific region encounter diverse AI governance
models that range from strict top-down mandates to collaborative and
voluntary frameworks. Each jurisdiction balances innovation, safety, and
public trust according to its legal traditions and policy priorities.

**_7.12.1 Types of Governance Models_**

```
Prescriptive (Top-Down Regulation) : Clear legal obligations (e.g., EU
AI Act).
Principle-Based (Self-Governance) : Voluntary principles and internal
frameworks (e.g., U.S. NIST AI RMF).
Co-Regulation : Collaboration between government and industry (e.g.,
Singapore’s Model AI Governance Framework).
Sandbox-Based : Controlled experimentation with flexible rules (e.g.,
UK’s AI Regulatory Sandbox).
```
**_7.12.2 China: Prescriptive (Top-Down Regulation)_**

China pioneered the world’s first dedicated generative AI rules with its
Interim Administrative Measures for Generative Artificial Intelligence


Services (effective August 2023). These departmental regulations impose
clear legal obligations on service providers to:

Register AI-service offerings with authorities,
Label all AI-generated content (e.g., deepfakes, chat outputs),
Ensure data provenance and perform security reviews, and.
Block or suspend noncompliant cross-border services.
This vertical, iterative legislative model couples a strong development
mandate with firm guardrails, categorizing AI services by risk and assigning
explicit responsibilities to upstream model creators and downstream
deployers. Penalties under existing data-security and information-control
laws can include technical blocks, service suspensions, and administrative
sanctions.

**_7.12.3 Japan: Principle-Based (Self-Governance)_**

Japan’s AI Promotion Act (enacted 28 May 2025; effective 1 July 2025)
embodies a principle-based approach that encourages innovation through
voluntary alignment rather than binding prohibitions. Key features include:

A set of guiding principles (e.g., ethical AI, coordination, flexibility),
Establishment of a National AI Strategy Council chaired by the Prime
Minister to steer policy,
Government support programs and master plans, and.
No direct penalties for noncompliance, though administrative actions
may follow infringements of fundamental rights.
By relying on soft-law techniques and inviting industry, academia, and
civil society to “cooperate” on AI policy, Japan seeks to foster an
innovation-friendly ecosystem while monitoring high-risk developments.

**_7.12.4 Singapore: Co-Regulation (Shared Governance)_**

Singapore’s Model AI Governance Framework for Generative AI (May
2024) exemplifies co-regulation through ongoing collaboration between
regulators and industry. Salient aspects include:

```
Nine dimensions covering accountability, data quality, incident reporting,
security, content provenance, and more,
The AI Verify toolkit, a sandbox-style testing platform for voluntary
third-party assurance, and.
```

A phased consultation process that engaged international bodies,
domestic stakeholders, and standard-setting agencies.
This iterative model balances public-sector leadership (via IMDA and
AI Verify Foundation) with industry-led implementation, allowing firms to
tailor compliance while benefiting from guided best practices and third-
party audits.

**_7.12.5 South Korea: Prescriptive Risk-Based Regulation_**

South Korea’s Framework Act on AI Development and Establishment of a
Foundation for Trustworthiness (enacted 21 January 2025; effective 22
January 2026) applies a binding, risk-based regime akin to the EU AI Act
but streamlined for regional needs. It mandates:

Mandatory labeling of AI-generated content,
Impact assessments for “high-impact” systems in critical sectors
(healthcare, energy, public services),
Transparency obligations, including disclosure of model specifications
and human-oversight measures, and.
Extraterritorial reach covering any AI activity affecting Korean users.
Enforcement features moderate fines (up to KRW 30 million) and
investigative powers for the Ministry of Science and ICT, while retaining
carve-outs for national security and defense applications. By coupling
promotional provisions (e.g., AI data centers) with clear obligations, South
Korea pursues both innovation and trust.

**_7.12.6 Australia: Principle-Based Voluntary Standards_**

Australia’s Voluntary AI Safety Standard (September 2024) offers
voluntary, principle-based guardrails designed to prepare organizations for
future mandatory rules. It comprises ten guardrails focusing on:

Governance and accountability,
Risk management and testing,
Human oversight and transparency, and.
Supply-chain and stakeholder engagement.
Aligned with ISO/IEC 42001:2023, the standard guides both AI
developers and deployers through best practices in privacy, security, and
ethics, without immediate legal force. Concurrent consultations on


mandatory high-risk guardrails signal an eventual shift toward a hybrid
model, blending existing laws with AI-specific mandates.
Comparative Insights
Mandate vs. Flexibility: China and South Korea favor prescriptive
statutes with enforceable obligations, while Japan and Australia prefer
self-regulation and voluntary standards to spur innovation.
Collaboration: Singapore’s co-regulatory framework leverages industry
expertise and public oversight to refine AI governance iteratively.
Risk Tiers: South Korea adopts a streamlined risk-based classification for
“high-impact” AI, echoing EU structures, whereas China applies a broad
sectoral approach without explicit formalized risk tiers.
Enforcement: Penalties in top-down models range from technical blocks
and service suspensions (China) to moderate fines (South Korea) and
administrative actions (Japan). Voluntary frameworks hinge on
reputational incentives and preparation for future mandates (Australia,
Singapore).
Across the region, these governance models illustrate a spectrum from
strict legal controls to adaptive, principle-driven practices, reflecting each
nation’s balance between sovereignty, innovation, and public trust in
generative AI technologies.

**Global Governance Trends**

```
Region Model Type Key Focus
EU Regulatory Risk classification, transparency, foundation model
disclosure (EU AI Act)
USA Market-driven +
guidelines
```
```
NIST AI risk management framework, executive orders for
safe use
China State-driven Censorship, content moderation, model registration
(generative AI service provisions)
India Emerging, light-touch Voluntary commitments, IndiaAI mission, sectoral
regulations evolving
Singapore Co-regulation Trusted AI frameworks, assessment toolkits
Canada Algorithmic impact
assessment
```
```
For government procurement, similar enterprise guidance
evolving
```
**7.13 Regional Approaches to Privacy, Security, and Ethical AI**


Effective governance of generative AI requires a balance of **ethical
safeguards** , **transparent data and model practices** , and **innovation-
enabling structures**. Europe leads with binding, risk-based regulation and
liability clarity. The U.S. leans on voluntary guidelines and evolving state
enforcement. China has moved swiftly with binding rules tailored to social
control. Singapore and Japan prefer practical, industry-led frameworks.
Korea is on track to legislate comprehensive AI law in 2026, and India
continues to refine its ambitions via soft policy and evolving data laws.
Together, these regional schemas illustrate a global mosaic—some pre-
emptive and binding, others collaborative and adaptive—each shaping a
unique landscape for enterprises deploying generative and ethical AI.

**7.14 European Union**

The EU leads with a **risk-based, rights-first regulation** anchored by the _AI
Act (Regulation 2021/0106)_. Adopted in April 2024 and enforceable by
2025, it categorizes AI systems into four risk tiers—from _unacceptable_ to
_minimal_. High-risk systems (hiring, healthcare, biometric ID) must meet
strict transparency, human oversight, training-data governance, and third-
party conformity audits—non-compliance can trigger fines up to
€35 million or 7% of global revenue.
The _EU AI Liability Directive_ and revised _Product Liability Directive_
further extend legal accountability to AI-enabled products and software,
bridging gaps in traditional liability law.
Parallel to the AI Act, the _Framework Convention on Artificial
Intelligence and Human Rights, Democracy and the Rule of Law_ (adopted
September 2024) binds over 50 countries to uphold democratic values and
guard against algorithmic discrimination, misinformation and threats to
institutions.

**_7.14.1 Notable Dispute_**

EU enforcement includes the Irish Data Protection Commission suing X
(formerly Twitter) in August 2024 over misuse of personal data during
model training without consent, affecting over 60 million users.

**7.15 United States**

Rather than sweeping laws, the U.S. relies on a **principles-based,
voluntary federal framework** , bolstered by executive orders and state-


level action. The _NIST AI Risk Management Framework_ (2023) encourages
organizations to adopt trustworthiness across design, deployment, and
monitoring, especially for high-impact AI applications.
Recent federal executive orders under the 2025 **AI Action Plan** de-
emphasize regulations, discourage state and diversity-driven mandates, and
remove barriers to innovation—but critics argue this leaves consumer
protection and fairness up to self-enforcement by companies.
When federal AI law remains absent, state attorneys general in
California, Colorado, Utah, Massachusetts, Oregon, New Jersey and Texas
are enforcing AI-related consumer protection, anti-discrimination, and
privacy statutes. For instance, Texas’s first generative AI enforcement
penalized misleading marketing claims under consumer-protection law.

**_7.15.1 Legislative Developments_**

Colorado’s **SB-24-205** (effective 2026) mandates transparency and risk-
management responsibilities for high-risk AI deployment in areas such as
housing and hiring. Texas legislation ( **TRAIGA** , effective 2026) similarly
imposes governance and sandbox frameworks for innovation with
accountability.
At the federal level, the proposed **AI Research, Innovation, and
Accountability Act** (S. 3312) highlights increasing appetite for generative
AI compliance requirements.

**7.16 Asia-Pacific**

**_7.16.1 China_**

China has instituted the _Personal Information Protection Law (PIPL)_ since
late 2021, akin to GDPR, with strict consent, transparency, and localized
data storage mandates.
In 2023, the _Interim Measures for the Management of Generative AI
Services_ became the first binding rules specific to public-facing generative
AI—imposing registration, content labeling, data security reviews, and
alignment with socialist values.
Administrative standards like GB 45438–2025 require watermarking
and metadata tags on synthetic content, effective in late 2025.
China also proposed a global governing body to standardize AI ethics
internationally, emphasizing cooperation beyond U.S. led models.


**_7.16.2 Singapore_**

Singapore pioneered the _Model AI Governance Framework_ in 2019—a
voluntary, best-practice guide covering fairness, accountability, human
oversight and transparency. In early 2024, a dedicated _Generative AI
Framework_ was published to deal with deep fakes, copyright and output
labeling.
The _AI Verify Toolkit_ and _generative AI sandbox_ (launched October
2023) provide practical testing mechanisms to assess fairness and
compliance before full deployment.

**_7.16.3 Japan_**

Japan emphasizes **human-centered AI** through voluntary guidelines
—‘Social Principles of Human-Centered AI’ issued by METI in 2019.
These stress respect for dignity, transparency, security and fairness. While
non-binding, they shape sectoral oversight through collaboration with
academia and industry.

**_7.16.4 South Korea_**

South Korea is finalizing its **AI Framework Act** , to take effect in early

2026. It will regulate foundational and generative AI in civilian sectors,
impose transparency and labeling duties, and support innovation
infrastructure. A related AI safety institute and central oversight body are
underway.

**_7.16.5 India_**

India’s AI strategy remains **soft-law oriented** , with no AI-specific
legislation yet. Its _National Strategy for AI_ (NITI Aayog, 2018 and updated
2023) promotes responsible use in sectors like health and education,
supported by evolving data protection law (Digital Personal Data Protection
Act 2023).
In early 2024, MEITY briefly required AI tools to seek government
approval before public release—but these mandates were rescinded shortly
after, reinforcing voluntary compliance norms.
Key Legal Conflicts & Enterprise–Government Disputes:-
**EU vs. X (Twitter)** : The Irish Data Protection Commission sued X over
training on EU user data without consent, under GDPR and AI Act


```
standards.
U.S. States vs. AI Firms : Texas and California attorneys general (AG)
have challenged generative AI companies over misleading claims and
algorithmic bias under consumer protection laws.
India’s brief approval advisory : Government order requiring pre-
launch AI approvals stirred concern before withdrawal—highlights
tensions between innovation and regulatory caution.
```
**7.17 Other Notable Jurisdictions**

**_7.17.1 UK: Copyright and Data Privacy Issues_**

UK enterprises deploying generative AI must navigate an evolving legal
landscape. On copyright, uncertainty persists over authorship, training data
use, and fair dealing defenses—the Getty vs. Stability case looms large. On
privacy, ICO guidance demands transparency and lawful bases for using
personal data in AI systems. While the Data Act updates data protections,
full AI-specific legislation is pending, leaving enterprises to manage
compliance through self-governance and close attention to ICO and UKIPO
consultations.

**_7.17.2 Copyright: Legal Limits and Ongoing Disputes_**

Under the UK’s **Copyright, Designs and Patents Act 1988 (CDPA)** ,
copyright is available only to works created by humans—or in the case of
purely computer-generated content, to the person who arranged its creation
under Section 9(3). As AI-generated outputs fall in a complex grey zone,
ownership and liability remain legally uncertain.
The government’s **2024–25 consultation** debated expanding the
permitted text-and-data-mining (TDM) exception to allow commercial use.
While creative industries strongly opposed such changes, the proposal is
currently shelved. Legislators had considered requiring AI developers to
include transparency statements detailing data scraped and training sources
—an idea rejected by the Commons but supported by the House of Lords.
One high-profile legal case is **Getty Images v. Stability AI** , where
Getty alleges that Stability AI illegally scraped millions of its copyrighted
images to train Stable Diffusion. The High Court has allowed both the
training infringement claim and a secondary infringement claim—centered
on whether software distributed online qualifies as an “article” under CDPA
—and refused to dismiss the case before trial.


In May 2025, Getty narrowed its claims, dropping direct infringement,
but continues to pursue issues around watermark use and trademark breach,
deepening uncertainty for enterprises using generative image models.

**_7.17.3 Data Privacy: Scraping, Consent & Transparency_**

The **Information Commissioner’s Office (ICO)** has made clear it expects
generative AI developers to respect UK users’ data rights. It welcomed
LinkedIn’s voluntary halt on model training using UK user data and urged
all major players—including Meta and Microsoft—to implement **privacy-
by-design** and transparency in data use.
ICO’s December 2024 consultation report emphasized the lack of
transparency around usage of scraped personal data. It urged developers to
provide specific, understandable disclosures and embed individuals’ rights
directly into AI systems, warning those not meeting these standards may
attract enforcement.

**_7.17.4 Broader Regulatory Context_**

In June 2025, the **Data (Use and Access) Act 2025** became law. Although it
made broad reforms to the UK’s data regime—including revisions to UK
GDPR and PECR—it specifically avoided including copyright or AI
transparency mandates at this stage. As a result, those debates have been
deferred to a forthcoming **comprehensive AI bill** , now postponed until
possibly 2026, which is expected to tackle copyright sharing, safety testing,
and enterprise accountability in generative AI usage.

**_7.17.5 Brazil, Canada, and Middle East: Emerging Trends and_**

**_Draft Laws_**

Brazil is forging ahead with binding AI legislation that balances innovation
and risk control. Canada is transitioning from voluntary guidance to formal
legal structures while confronting landmark copyright litigation.
The Middle East largely prefers principles-based governance, supported
by innovation-oriented frameworks, with emerging laws in Bahrain and
pioneering AI-legislation drafted by the UAE. Together, these regions
showcase diverse pathways for enterprises to adopt generative AI—each
reflecting a distinct regulatory philosophy, but sharing a common aim:
enabling responsible AI while safeguarding privacy, fairness, and
innovation.


**Brazil**
Brazil is advancing its **AI Act (Bill No. 2338/2023)** , a risk-based
framework modeled after the EU’s AI Act. Passed by the Senate in
December 2024, it classifies AI systems into **“excessive risk” (banned)** ,
**“high-risk” (strict rules)** , and **others (light obligations)**. The law
mandates **impact assessments** , public transparency, **human oversight** , and
dedicated enforcement by a new AI authority overseeing fines up to
R$ 50 million or 2% of revenue. A built-in **regulatory sandbox** encourages
innovation under controlled conditions. It also requires enterprises to label
generative AI outputs and offers energy efficiency incentives for model
operations.
Meanwhile, the **LGPD** , Brazil’s General Data Protection Law, already
applies to AI: scraped public data is regulated, and enterprises must
demonstrate **data minimization, legitimacy, transparency** , and facilitate
requests for deletion, correction, or algorithmic impact under DPIAs.
Brazilian courts have begun interpreting copyright flexibly to allow text-
and-data mining for AI research—though the AI bill may overwrite these
interpretations.

**Canada**
Canada introduced the **Artificial Intelligence and Data Act (AIDA)** as
part of Bill C-27, aiming to regulate generative AI through mandatory
**consumer protection** , transparency, and risk mitigation. Although not yet
fully in force, a **Voluntary Code of Conduct** launched in 2023 offers
interim guidance for businesses on responsible AI deployment.
A key legal dispute: Canadian media groups—including The Canadian
Press, Globe and Mail, CBC—sued OpenAI in late 2024, alleging
unauthorized use of their content to train ChatGPT. They claim
infringement of copyright, while OpenAI counters that training on publicly
available materials is permissible under fair use. Similar claims have been
filed against Cohere in U.S. courts.
Canada’s privacy watchdog also launched a formal investigation into **X
(formerly Twitter)** in early 2025 to determine whether personal data used
in training its AI assistant Grok violated the country’s privacy laws. This
probe signals intensifying scrutiny of enterprise compliance with consent
and data disclosure obligations.


**Middle East**
Countries in the Middle East have embraced **“soft-law” frameworks** that
emphasize guidelines and strategic support over rigid regulation:
The **UAE** pioneered several governance tools: its **Generative AI
Guidelines (2023)** and **AI Charter (2024)** establish principles around
transparency, privacy, and accountability. In 2025, the government unveiled
an **AI-powered Regulatory Intelligence Office** , using AI to draft and
continuously update legislation—an ambitious first globally, albeit
dependent on strong human oversight to avoid bias or errors.
**Saudi Arabia** rolled out an **AI Adoption Framework (2024)** and
voluntary **Generative AI ethics guidelines** , with minimal enforcement but
strong state-supported investment (~$40 billion) in AI infrastructure. A
forthcoming draft **Global AI Hub Law** also aims to facilitate international
data centres under clear cross-border rules.
**Bahrain** enacted the Middle East’s first comprehensive AI law in April

2024. It includes licensing, liability provisions, penalties up to BD 2000 or
prison, and a regulatory sandbox. However, political debates followed
regarding overlap with existing data protections and the need for flexible
innovation pathways.

**8 Intellectual Property Rights in Generative AI**

Enterprises are increasingly struggling to secure intellectual property (IP)
rights for content created by generative AI tools like ChatGPT or image
generators. Today’s laws in most countries are designed for human creators,
not machines—so they don’t clearly cover who owns content when an AI
creates it. For example, if an AI writes a song or designs a product, should
the copyright go to the person who gave it prompts, the company that built
the AI, or should no one own it at all? This legal grey area leaves businesses
exposed.
Another big issue is the data used to train these AI models. Generative
AI systems are often trained on massive amounts of data scraped from the
internet, including copyrighted books, images, music, and articles. This
raises legal concerns about whether using these materials without
permission is allowed under existing fair use or copyright exceptions.
Several lawsuits—like those against OpenAI and Stability AI—are already
challenging this.


Patents are also a concern. Inventions made entirely by AI—without
human help—have repeatedly been denied patents in courts around the
world, including the U.S. and UK. That’s because current laws require a
human to be listed as the inventor.
To deal with these problems, governments, legal experts, and global
organizations are exploring new rules. Some suggest that AI-generated
content should be owned by whoever owns or operates the AI, but allow
flexibility through contracts when people are involved in the creative
process. The World Intellectual Property Organization (WIPO) recommends
that businesses carefully choose training data and include IP terms in user
agreements. The U.S. Copyright Office recently clarified that content with
enough human involvement—like editing or creative direction—can still
get protection.
At a global level, organizations like the OECD are pushing for clear,
consistent rules that encourage innovation while protecting creators. They
emphasize the need for transparent data sources and fair rewards for
original rights holders.
These efforts reflect a shift toward a hybrid system—preserving human
creativity at the core, while updating IP laws to fairly handle the realities of
AI-assisted content creation.

**8.1 Challenges in Copyright, Patents, and Trade Secrets**

Enterprises using generative AI are navigating complex challenges around
**intellectual property (IP)** that could slow innovation and even lead to
lawsuits if not managed properly. These challenges span copyright, patents,
and trade secrets. To safely embrace generative AI, enterprises must adopt a
**hybrid IP strategy** : license training data properly, ensure humans are
meaningfully involved in invention disclosures, and lock down internal
data-sharing processes. These steps can protect innovation while reducing
legal risks.

**_8.1.1 Copyright Confusion_**

Most copyright laws today only recognize human-created content, not
machine-generated works. So when an AI tool writes a blog post, designs a
logo, or composes music, it’s unclear who owns the rights—or if it’s
protected at all. The problem becomes even trickier when these AI tools are


trained on copyrighted content pulled from across the internet without
proper permission.
A good example is **Getty Images’ lawsuit against Stability AI** , the
company behind Stable Diffusion. Getty claims that millions of its
copyrighted photos were used without consent to train the AI model. This
highlights the massive legal risk of using training data scraped from the
web, like the LAION-5B dataset, which contains over **5 billion images** ,
many without clear licensing.
To solve this, some experts suggest creating **standardized licensing
systems** for training data and embedding **digital watermarks** or attribution
into AI outputs to show where the content came from.

**_8.1.2 Patent Roadblocks_**

Patent law usually requires a human to be listed as the inventor. This
becomes a problem when AI systems generate new inventions or optimize
complex processes on their own. Courts and patent offices often reject such
applications.
For instance, the **DABUS case** (Device for the Autonomous
Bootstrapping of Unified Sentience) tried to get patents in multiple
countries for inventions generated by an AI without human help. But patent
offices in the U.S., UK, and EU all rejected the applications, stating that
only a human can be recognized as an inventor.
To work around this, enterprises are now encouraged to emphasize
**human-AI collaboration** in patent filings and show how the invention
offers a clear technical improvement—not just a clever algorithm.

**_8.1.3 Trade Secret Risks_**

Using generative AI can also put company secrets at risk. Employees might
unintentionally share confidential data—like internal strategies or code—
when they paste it into public AI chat tools. Also, malicious actors could
reverse-engineer an AI model to uncover sensitive training data or
proprietary methods.
A cautionary example is **Samsung’s 2023 incident** , where engineers
accidentally uploaded confidential code to ChatGPT while trying to fix
bugs—potentially exposing trade secrets.
To protect against this, companies are advised to set up strict **AI usage
policies** , use **private AI environments** , and include strong **nondisclosure**


**clauses** in their contracts. Combining trade secret protections with clear
terms of use can also help prevent outsiders from scraping or misusing
proprietary content.

**8.2 Use of Third-Party Data in Model Training**

Enterprises today are turning to **third-party datasets** —like licensed
databases, content scraped from websites, and commercially purchased data
—to train their generative AI systems. These datasets help scale up models
quickly and cover more topics than companies could manage with only
their internal data. But this convenience comes with serious **legal, ethical,
and operational challenges**.

**_8.2.1 Legal Risks_**

Third-party data often includes copyrighted material, personal information,
or content protected by privacy laws. If a company trains an AI model on
this data without permission, it can face lawsuits. For example, in India,
**ANI (Asian News International)** has filed a case against **OpenAI** and
**Microsoft** , claiming that ChatGPT used ANI’s news content without
consent. This mirrors other global lawsuits, like The New York Times suing
OpenAI for using its articles without a license.
Laws like the **GDPR in Europe** and **CCPA in California** demand that
companies get explicit consent when using someone’s personal data.
Violating these rules could lead to penalties of up to **4% of a company’s
global revenue**.

**_8.2.2 Ethical Concerns_**

Data from external sources may be **biased or misleading**. If used blindly, it
can cause AI models to generate harmful stereotypes or spread false
information. According to a **2023 MIT Sloan–Boston Consulting Group
(BCG) study** , **55% of AI failures were linked to third-party tools and
datasets** , which led to customer backlash, brand damage, and even legal
troubles.
Yet, the upside is clear: third-party data can accelerate model
development and improve accuracy. For instance, a **global financial
services firm** reportedly cut AI training costs by **40%** using licensed
alternative data, while still staying compliant through careful contracts and
vendor audits.


**_8.2.3 Best Practices and Solutions_**

To balance innovation and risk, companies are adopting smarter data
governance strategies:

**Track where data comes from** (data provenance) and regularly **audit
vendors** for licensing and bias.
Add **clear contract terms** , including **indemnity clauses** and **usage
restrictions** , so that vendors share responsibility for legal issues.
Use **technical safeguards** like **differential privacy** (to mask personal
details), **watermarking** (to trace content), and **data cleaning** to remove
risky content.
Keep a **human in the loop** for reviewing outputs and continuously test
for **bias and fairness**.
By combining these legal, ethical, and technical measures, enterprises
can unlock the power of third-party data—driving faster innovation and
deeper insights—without sacrificing user trust, privacy, or compliance.

**9 Liability of Generative AI Models**

When something goes wrong with generative AI—like misinformation,
privacy violations, or biased decisions—it’s not always clear who’s to
blame. The legal responsibility can fall on different people or organizations
depending on their role in the AI ecosystem.

**Developers (those Who Build the AI)**
AI creators are the first in line when it comes to accountability. They’re
expected to train models on safe and accurate data, test thoroughly, and
build systems that reduce harm. If they cut corners, they could be sued for
negligence—just like a car company making faulty brakes. For example,
**Stability AI** faced legal action from **Getty Images** in the UK for using
copyrighted pictures without permission to train its AI image generator.

**Deployers (Businesses Using AI)**
If a company adds AI into its products and that AI causes harm—like
writing false statements or denying someone a loan unfairly—it can be held
liable under consumer protection laws. In the U.S., the **Federal Trade
Commission (FTC)** warned AI vendors they could face penalties if their


tools deceive or harm consumers. In India, Section 79 of the IT Act limits
legal protection if companies actively shape or promote harmful AI outputs.

**Platform Providers (Cloud or Tool Providers)**
Cloud providers or app platforms that host or promote AI systems can also
get into trouble if they don’t monitor how the AI is used. For instance, **Meta**
(formerly Facebook) was sued for allegedly benefiting from unlicensed AI
training data, showing how platforms are expected to act responsibly,
especially when they profit from the AI’s use.

**Users (those Who Use AI Tools)**
Even end-users can be held responsible—especially if they use AI to create
deepfakes, offensive content, or spread misinformation. Courts may say
users had a duty to prevent harm, particularly if they ignored internal rules
or failed to review AI outputs.

**Regulators and Insurers**
Governments are stepping in too. The **EU AI Act** , for instance, assigns
different levels of risk to AI systems and enforces fines for noncompliance.
Insurers are also offering AI risk coverage but are demanding strict
governance standards before issuing policies.

**In short,** everyone involved—developers, deployers, platforms, users, and
regulators—has a role in keeping AI safe and fair. Coordinated efforts,
strong internal policies, and continuous oversight are key to managing
liability in the growing AI landscape.

**9.1 Model Developer, Deployer, and User Responsibilities**

As more companies embrace generative AI tools—from chatbots to content
generators and predictive systems—it’s critical to understand that
**responsibility doesn’t fall on just one group**. Instead, it’s shared across
three key players: **developers who build the AI, deployers who integrate
it into products or services, and users who interact with it**. Together,
these groups must work within a framework that clearly defines legal
duties, ethical expectations, and business accountability.

**_9.1.1 Developers: The Architects of AI Systems_**


Developers—whether large tech companies or research teams—carry the
**primary responsibility** to ensure that generative AI is safe, fair, and lawful
from the ground up. This includes:

```
Designing with care: Developers must follow established principles
such as “fairness by design” and “security by design.” That means
actively identifying and minimizing bias in training data, embedding
safeguards to prevent harmful outputs, and tracking where data comes
from (known as data provenance ).
Avoiding legal risks: If developers use copyrighted, biased, or sensitive
data without proper handling, they can face negligence or product
liability lawsuits , just as a car manufacturer would if their vehicle
caused an accident. Under common-law doctrines, courts assess whether
developers exercised “reasonable care.”
Regulatory compliance: The EU AI Act , a landmark law passed in
2024, now mandates that high-risk AI systems maintain documentation
logs, adverse-event reports, and testing records. These requirements help
regulators and insurers assess risk and responsibility.
```
```
Example: In the Getty Images v. Stability AI case, Getty alleged that their
copyrighted images were used without consent to train a generative
image model, highlighting the legal dangers of using unlicensed third-
party data.
```
**_9.1.2 Deployers: The Businesses Using AI_**

Deployers are companies that adopt prebuilt AI models (like GPT-4 or
Claude) and integrate them into customer-facing applications or internal
tools. They **assume major compliance risks** , including:

```
Meeting legal standards: In the U.S., the Federal Trade Commission
(FTC) has warned that companies using AI must not mislead consumers,
violate privacy rights, or enable discrimination. State laws and
international regulations add further complexity.
Monitoring performance: Deployers must run AI impact assessments
to test how the models behave in the real world and stop using the tools if
they cause harm. This is not optional under the EU AI Act (Article 26) —
which makes post-deployment monitoring mandatory.
```

```
Risk of secondary liability: If a company edits, enhances, or promotes
harmful AI-generated outputs, they may lose legal protections (often
called safe harbors ) and be exposed to lawsuits.
```
```
Example: Air Canada was held liable when its AI-powered chatbot
provided misleading information about refund policies. A court ruled that
even though AI was involved, the company was responsible for its
actions.
```
**_9.1.3 Users: Everyday Interactions Matter Too_**

While developers and businesses hold much of the legal burden, **end-users
—such as employees, content creators, or even customers—also share
responsibility** :

```
Ethical use: Users should avoid generating harmful content such as
deepfakes , fake news, or offensive material. Organizations must create
clear guidelines and educate users to prevent misuse.
Verifying outputs: Relying blindly on AI can result in errors,
embarrassment, or even lawsuits. Users must review, fact-check , and
think critically about what AI produces.
Consequences for misuse: If a user prompts an AI to produce
defamatory or fraudulent content, they (and potentially their employer)
can face direct legal action under privacy, defamation, or fraud laws.
```
```
Example: In 2023, a lawyer in the U.S. used ChatGPT to prepare a legal
brief, only to discover that the AI had invented fake court cases. The
judge sanctioned the attorney, warning that blind reliance on AI is not a
legal defense.
```
**_9.1.4 The Path Forward: Coordinated AI Governance._**

To reduce risks and build trust, companies should adopt **shared
responsibility frameworks** like the one recommended by the **Cloud
Security Alliance (CSA)**. These models encourage:

```
Clear contracts defining who owns the data, who is liable for misuse,
and how responsibilities are split.
Operational safeguards , such as access controls, human-in-the-loop
review systems, and escalation procedures for harmful outputs.
```

```
Regular audits and training to keep both internal teams and end-users
informed and compliant.
By acknowledging that responsibility is distributed across the
entire AI lifecycle , enterprises can unlock the benefits of generative AI
—faster innovation, better customer experiences, and smarter automation
— without falling into legal, ethical, or reputational traps.
```
**9.2 Product Liability, Section 230 (U.S.), and Case Examples**

As enterprises embrace generative AI, they are entering a legal environment
that is still taking shape—one where traditional laws and new technologies
are colliding. Businesses are now facing growing legal responsibilities
under product liability laws, weakening safe-harbor protections like Section
230, and emerging court decisions that set new precedents for AI
accountability.
Can AI Be a “Product”? A Legal Turning Point.
Traditionally, software was not treated like physical products under
strict liability laws—rules that hold manufacturers accountable for defective
goods, even without proof of negligence. But this thinking is starting to
shift.
A landmark case in Florida— **Garcia v. Character Technologies (2025)**
—opened the door to treating AI tools like products. In this case, a chatbot
developed by Character.AI was linked to a teenager’s suicide after
prolonged and troubling interactions. The court allowed the lawsuit to
proceed under product liability law, arguing that software could be seen as a
“product” if the harm results from its **design** , not just its speech. This case
highlights that AI developers may now be required to:

```
Anticipate and mitigate foreseeable risks,
Build in safety features from the start,
And give users clear warnings when AI behavior could cause harm.
Failure to do these things could lead to lawsuits—even if no one
acted recklessly—just because the product design was unsafe.
Section 230: No Longer a Blanket Shield for AI
```

```
For years, many tech companies relied on Section 230 of the U.S.
Communications Decency Act. It protects platforms like Facebook or
YouTube from being sued over what users post. Some AI companies
initially hoped this would protect them, too.
But recent guidance from the Congressional Research Service and
decisions like Anderson v. TikTok (2024) show courts are drawing a
line. If a platform merely hosts content, it’s protected. But if it creates or
materially shapes the output—as AI often does—those protections fade.
That means if a chatbot or image generator produces harmful or
misleading content, the company behind it may be held responsible.
```
**_9.2.1 What Enterprises Must Do Now_**

In this evolving legal landscape, companies building or using generative AI
need to take proactive steps to avoid legal trouble and build public trust:

```
Build safe AI systems by design : Include strong risk checks, testing, and
mechanisms to stop harmful outputs.
Be transparent : Keep detailed records of where training data came from
and how the model behaves under stress.
Plan for insurance and risk : Work with insurers that understand AI
risks and meet their governance requirements.
```
**_9.2.2 Real-World Cases Making the Headlines_**

```
Garcia v. Character.AI (2025) : Court recognized chatbot software as a
“product,” allowing a wrongful death lawsuit under product liability law.
Anderson v. TikTok (2024) : Found that when a platform curates or
amplifies harmful content, it may lose Section 230 protection.
FTC vs. Meta (2024) : The U.S. Federal Trade Commission fined Meta
for using AI voice cloning without clear user consent, reinforcing that
consumer protection laws still apply.
These rulings and regulations signal a growing push for
accountability in AI. For enterprises, the path forward lies in combining
ethical design , clear documentation , and responsible use policies —
ensuring innovation doesn’t come at the cost of public safety or legal
exposure.
```

**10 Open Source Technology Stack for Multi-**

**Jurisdictional Data Protection Compliance**

Enterprises deploying generative AI across multiple jurisdictions face a
complex web of data protection laws such as GDPR (EU), DPDP Act
(India), CCPA (California), and PDPB (Brazil). A robust open source
technology stack can streamline compliance by automating key tasks—data
anonymization, privacy impact assessments (PIAs), consent management,
governance risk and compliance (GRC), and continuous monitoring—while
ensuring transparency and auditability.

**10.1 Data Anonymization Frameworks**

ARX Data Anonymization Tool.
A Java-based framework supporting k-anonymity, ℓ-diversity, t-
closeness, and differential privacy. ARX provides a GUI and API for large-
scale dataset transformation, risk-utility analysis, and iterative configuration
to meet various privacy models.
Source: https://github.com/arx-deidentifier/arx
Microsoft Presidio.
A Python SDK for detecting and de-identifying PII in text and images.
It offers customizable recognizers (NER, regex, rule-based), anonymization
operators (mask, replace, encrypt), and scalable deployment via Docker or
Kubernetes.
Source: https://github.com/microsoft/presidio

**10.2 Privacy Impact Assessment (PIA) Tools**

CNIL PIA Software.
Published under a free license by the French Data Protection Authority,
this Angular-based tool guides controllers through the GDPR’s DPIA
methodology, offering contextual knowledge bases, visual risk mapping,
and modular customization for sector-specific workflows.
Source: https://github.com/LINCnil/pia

**10.3 Consent Management Solutions**


Klaro Privacy Manager.
A lightweight JavaScript CMP for GDPR and ePrivacy compliance.
Klaro supports multilingual configurable consent dialogs, service-based
blocking, and seamless integration via a single JS snippet.
Source: https://github.com/kiprotect/klaro
Consent-O-Matic.
A browser extension that auto-fills consent pop-ups based on user
preferences, countering dark patterns and ensuring consistent opt-out
behavior. Maintained by Aarhus University, it covers major CMP
frameworks and is fully open source.
Source: https://github.com/cavi-au/Consent-O-Matic

**10.4 Governance, Risk, and Compliance (GRC) Platforms**

Eramba.
A PHP-based open source GRC tool with modules for policy
management, controls, audits, risk registers, and compliance mapping. The
Community Edition supports multi-framework mapping (ISO 27001, PCI
DSS, GDPR) and audit trails, while the source code and templates are
freely available.
Source: https://github.com/eramba
OpenGRC.
A Laravel/Filament application designed for SMBs to import security
frameworks, link standards to implementations, perform audits, and
generate compliance reports. Licensed CC BY-NC-SA 4.0, it provides
intuitive dashboards and quick start installers.
Source: https://github.com/LeeMangold/OpenGRC

**10.5 Integration and Automation**

FOSSology.
While primarily a license compliance tool, FOSSology’s REST API and
web UI can integrate with CI/CD pipelines to scan third-party components
for license and export controls, supporting multi-jurisdictional IP
compliance.


Source: https://github.com/fossology/fossology
Keycloak.
An open source IAM solution offering SSO, OAuth2, and fine-grained
authorization policies. Deploying Keycloak ensures secure access, robust
authentication, and audit logs for AI model interfaces and data access across
geographies.
Source: https://github.com/keycloak/keycloak

**10.6 Best Practices and Implementation Considerations**

1. **Modular Architecture** : Combine specialized tools via APIs—Presidio
    for PII de-identification, ARX for anonymization, CNIL PIA for impact
    assessments, Klaro for consent, and OpenGRC for overarching
    compliance management.
2. Data Provenance and Logging: Use blockchain-inspired protocols or
    append-only audit logs (e.g., using OpenAudit) to track dataset lineage
    and model training usage.
3. Policy-as-Code: Encode compliance requirements (e.g., data residency,
    consent revocation) into automated checks using Terraform -enabled
    frameworks or policy engines like Open Policy Agent.
4. Continuous Monitoring: Integrate intrusion detection, SIEM (e.g.,
    Wazuh), and automated compliance tests to alert on deviations from
    approved data-handling processes.
5. Community Engagement: Leverage active open source communities
    for updates on regulatory changes and contribute back enhancements—
    ensuring tools evolve alongside legal frameworks.

This open source technology stack empowers enterprises to achieve
consistent, scalable, and transparent compliance with multi-jurisdictional
data protection laws, enabling responsible generative AI innovation while
mitigating legal and ethical risks.


**References**

**Regulations, Acts, and Official Documents**

1. https:// nftnow. com/ features/ regulating-ai-experts-explain-why-its-difficult-but-important-to-get-
    right/
2.
    Official text: https:// eur-lex. europa. eu/ eli/ reg/ 2024/ 1689/ oj
3.
    Overview: https:// ec. europa. eu/ info/ publications/ eu-artificial-intelligence-act_ en
4.
    General Data Protection Regulation (GDPR)
5.
    Official text: https:// gdpr. eu
6.
    Articles referenced: 5, 6, 15–22, 25, 28, 32–35
7.
    Guidance: https:// ec. europa. eu/ info/ law/ law-topic/ data-protection_ en
8.
    California AI Transparency Act
9.
    https:// leginfo. legislature. ca. gov
10.
    Legislative summary: https:// [http://www.](http://www.) capoliticalrevie w. com/ topics/ california-ai-transparency-act/
11.
    Colorado AI Act
12.
    Legislative tracking: https:// leg. colorado. gov/ bills/ sb24-205
13.
    Digital Personal Data Protection Act, 2023 (India)
14.
    Full text: https:// egazette. nic. in/ WriteReadData/ 2023/ 248040. pdf
15.
    Interim Measures for Generative AI Services (China)
16.
    English translation and commentary: https:// [http://www.](http://www.) chinalawtranslat e. com/ en/ generative-ai-
    measures/
17.
    Japan AI Promotion Act
18.
    Outline: https:// [http://www.](http://www.) kantei. go. jp/ jp/ singi/ ai_ senryaku_ kaigi/
19.
    Framework Act on Artificial Intelligence (South Korea)
20.
    (Korean official page) https:// [http://www.](http://www.) law. go. kr/ / 
21.
    Singapore Model AI Governance Framework for Generative AI (MGF-Gen AI)
22.
    https:// [http://www.](http://www.) imda. gov. sg/ resources/ press-releases-factsheets-and-speeches/ press-releases/ 2024/
    model-ai-governance-framework


23.
Australia Voluntary AI Safety Standard
24.
Overview: https:// [http://www.](http://www.) industry. gov. au/ publications/ ai-safety-standards
25.
Section 230, Communications Decency Act (U.S.)
26.
Legal text: https:// [http://www.](http://www.) law. cornell. edu/ uscode/ text/ 47/ 230
27.
FTC Voice Cloning Challenge
28.
https:// [http://www.](http://www.) ftc. gov/ news-events/ press-releases/ 2024/ 01/ ftc-launches-voice-cloning-challenge

**Open Source and Proprietary Tools**

29. ARX Data Anonymization Tool
30.
    https:// github. com/ arx-deidentifier/ arx
31.
    Microsoft Presidio
32.
    https:// github. com/ microsoft/ presidio
33.
    CNIL PIA Software
34.
    https:// github. com/ LINCnil/ pia
35.
    DPIA Click&Go
36.
    Riemann et al., Appl. Sci. 2023, 13(20), 11230. (find at: https:// [http://www.](http://www.) mdpi. com/ 2076-3417/ 13/
    20/ 11230)
37.
    Software/project homepage: https:// clickandgo. dpia. ac. uk
38.
    Klaro Privacy Manager https:// github. com/ kiprotect/ klaro
39.
    Consent-O-Matic https:// github. com/ cavi-au/ Consent-O-Matic
40.
    Eramba GRC https:// github. com/ eramba
41.
    OpenGRC
42.
    https:// github. com/ LeeMangold/ OpenGRC
43.
    FOSSology
44.
    https:// [http://www.](http://www.) fossology. org/
45.
    Keycloak
46.
    https:// [http://www.](http://www.) keycloak. org
47.
    Wazuh (SIEM)


48.
https:// wazuh. com

**Agency Reports & Reference Institutions**

49. OECD AI Governance
50.
    https:// [http://www.](http://www.) oecd. org/ going-digital/ ai/ principles/
51.
    NIST AI Risk Management Framework (U.S.)
52.
    https:// [http://www.](http://www.) nist. gov/ itl/ ai-risk-management-framework
53.
    CISA (Cybersecurity and Infrastructure Security Agency)
54.
    https:// [http://www.](http://www.) cisa. gov
55.
    Stanford HAI (Human-Centered Artificial Intelligence)
56.
    https:// hai. stanford. edu
57.
    UN AI Advisory Body
58.
    https:// [http://www.](http://www.) un. org/ en/ ai-advisory-body

**Additional Statutes, Guidelines, and Court Cases**

59. TAKE IT DOWN Act (U.S., 2025)
60.
    https:// [http://www.](http://www.) congress. gov/ bill/ 118th-congress/ house-bill/ 4560
61.
    U.S. Privacy Act of 1974
62.
    https:// [http://www.](http://www.) justice. gov/ opcl/ privacy-act-1974
63.
    ANI v. OpenAI (Delhi High Court, India)
64.
    Court documents via https:// delhihighcourt. nic. in (case law search required)
65.
    Garcia v. Character Techs., Inc. (U.S.)
66.
    Download docket from https:// [http://www.](http://www.) pacer. gov or check public analysis
67.
    Additional Statutes, Guidelines, and Court Anderson v. TikTok, Inc. (3rd Cir. 2024)
68.
    Opinion available via https:// [http://www.](http://www.) ca3. uscourts. gov

**Proprietary/Notable Commercial Models and Initiatives**

69. Gaia-X (EU Cloud Infrastructure)
70.
    https:// gaia-x. eu


71.
InvestAI (EU AI Investment Programme)
72.
European Commission project page: https:// digital-strategy. ec. europa. eu/ en/ policies/ investai
73.
BLOOM (EU Open Science LLM)
74.
https:// bigscience. huggingface. co/ blog/ bloom
75.
OpenAI (GPT), Google (PaLM, Gemini), Meta (Llama, proprietary models)
76.
https:// openai. com
77.
https:// ai. google/ discover/ gemini
78.
https:// ai. meta. com
79.
stc.AI (Saudi Arabia LLM)
80.
Corporate: https:// [http://www.](http://www.) stc. com. sa/ content/ stc/ sa/ en/ personal/ ai. html
81.
Mistral AI (France, Open-Weight LLMs)
82.
https:// mistral. ai

**Academic and Think Tank Resources**

83. Harvard Berkman Klein Center
84.
    https:// cyber. harvard. edu
85.
    University of Michigan: AI Ethics and Policy
86.
    https:// ai. umich. edu
87.
    MIT Sloan/BCG Report on AI Failures
88.
    https:// mitsloan. mit. edu/ ideas-made-to-matter/ how-businesses-can-avoid-ai-failures

**Official Sources for Regulations, Acts, and Court Cases**

89. CNIL PIA Software (France, Data Protection Impact Assessment Tool):
90.
    https:// [http://www.](http://www.) cnil. fr/ en/ open-source-pia-software-helps-carry-out-data-protection-impact-
    assessment
91.
    https:// github. com/ LINCnil/ pia
92.
    DPIA Click&Go:
93.
    Article and tool details: https:// [http://www.](http://www.) mdpi. com/ 2076-3417/ 13/ 20/ 11230
94.
    TAKE IT DOWN Act (U.S. 2025):


95.
Wikipedia summary: https:// en. wikipedia. org/ wiki/ TAKE_ IT_ DOWN_ Act
96.
Full bill text: https:// [http://www.](http://www.) congress. gov/ bill/ 119th-congress/ senate-bill/ 146/ text
97.
Overview: https:// [http://www.](http://www.) bairdholm. com/ blog/ the-new-take-it-down-act/
98.
ANI v. OpenAI (India):
99.
Case discussion: https:// theleaflet. in/ digital-rights/ ani-v-openai-in-the-delhi-hc-everything-so-
far-and-all-that-is-at-stake
100.
Commentary: https:// legalblogs. wolterskluwer. com/ copyright-blog/ does-human-learning-equal-
machine-learning-high-court-of-delhi-to-rule-on-lawfulness-of-tdm-for-machine-learning/
101.
Garcia v. Character Techs., Inc.:
102.
Case tracker: https:// techpolicy. press/ tracker/ megan-garcia-v-character-technologies-et-al
103.
Official complaint PDF: https:// cdn. arstechnica. net/ wp-content/ uploads/ 2024/ 10/ Garcia-v-
Character-Technologies-Complaint-10-23-24. pdf
104.
Anderson v. TikTok, Inc. (3rd Cir. 2024):
105.
Case summary and legal context: https:// epic. org/ in-anderson-v-tiktok-the-third-circuit-applies-
questionable-first-amendment-reasoning-to-arrive-at-the-correct-section-230-outcome/
106.
Full opinion: https:// law. justia. com/ cases/ federal/ appellate-courts/ ca3/ 22-3061/ 22-3061-2024-
08-27. html

**Open Source Tools for Compliance and Consent Management**
107.Consent-O-Matic (Browser Consent Automation): https:// github. com/ cavi-au/ Consent-O-Matic
108.
Klaro Privacy Manager (CMP): https:// github. com/ kiprotect/ klaro

**Alternative open source consent solutions:**
109.Consent-Banner-JS: https:// github. com/ 68publishers/ consent-management-platform
110.
Additional open CMPs (community discussion): https:// [http://www.](http://www.) reddit. com/ r/ GoogleTagManager /
comments/ 1bdx21q/ open_ source_ consent_ management_ platform_ cmp/

**Notable News and Agency Announcements**^1

111. Meta Voice Cloning (FTC Fine/Investigation):
112.
    Example policy brief: https:// techcrunch. com/ 2024/ 05/ 23/ 6m-fine-for-robocaller-who-used-ai-
    to-clone-bidens-voice/
113.
    FTC Voice Cloning Challenge: https:// [http://www.](http://www.) ftc. gov/ news-events/ news/ press-releases/ 2024/ 04/
    ftc-announces-winners-voice-cloning-challenge


**Selected Additional Open Source Technology/Compliance**

**Solutions Mentioned**

114. Karma Data Integration Tool https:// github. com/ usc-isi-i2/ web-karma
115.
    Talend Open Studio https:// [http://www.](http://www.) talend. com/ products/ talend-open-studio/
116.
    DX Mapper https:// github. com/ jeroenooms/ dx
117.
    CartoDB (now CARTO) https:// carto. com/
118.
    Oracle https:// [http://www.](http://www.) oracle. com/ in/ artificial-intelligence/ generative-ai/ what-is-generative-ai/
119.
    McKinsey https:// [http://www.](http://www.) mckinsey. com/ capabilities/ mckinsey-digital/ our-insights/ the-economic-
    potential-of-generative-ai-the-next-productivity-frontier
120.
    eWeek https:// [http://www.](http://www.) eweek. com/ artificial-intelligence/ ai-companies/
121.
    AIMagazine https:// aimagazine. com/ articles/ why-are-the-uk-and-china-leading-in-gen-ai-
    adoption
122.
    Pega https:// [http://www.](http://www.) pega. com/ enterprise-generative-ai
123.
    Carnegie https:// carnegieendowmen t. org/ research/ 2024/ 11/ indias-advance-on-ai-regulation?
    lang= en
124.
    https:// academic. oup. com/ policyandsociety / article/ 44/ 1/ 1/ 7997395
125.
    NIST https:// nvlpubs. nist. gov/ nistpubs/ ai/ nist. ai. 100-1. pdf
126.
    NIH https:// pmc. ncbi. nlm. nih. gov/ articles/ PMC12186909/
127.
    Harvard https:// cyber. harvard. edu/ topics/ ethics-and-governance-ai
128.
    https:// campustechnology. com/ articles/ 2025/ 05/ 07/ improving-ai-governance-for-stronger-
    university-compliance-and-innovation. aspx
129.
    UK Gov https:// [http://www.](http://www.) gov. uk/ government/ publications/ ai-safety-institute-overview/ introducing-
    the-ai-safety-institute
130.
    https:// [http://www.](http://www.) whitecase. com/ insight-our-thinking/ ai-watch-global-regulatory-tracker-china
131.
    https:// practiceguides. chambers. com/ practice-guides/ artificial-intelligence-2025/ china/
    1511trends-and-developments
132.
    https:// [http://www.](http://www.) managingip. com/ article/ 2f3huy6goqztxnab hdvy8/ sponsored-content/ ani-v-openai-
    generative-ais-use-of-copyrighted-works-under-indian-law
133.
    https:// copyrightblog. kluweriplaw. com/ 2025/ 05/ 19/ does-human-learning-equal-machine-
    learning-high-court-of-delhi-to-rule-on-lawfulness-of-tdm-for-machine-learning/
134.
    https:// [http://www.](http://www.) medianama. com/ 2025/ 05/ 223-india-ai-copyright-law-committee/


1

135.
https:// [http://www.](http://www.) pib. gov. in/ PressReleasePage. aspx? PRID= 2149719
136.
https:// law. asia/ generative-ai-copyright-law/
137.
https:// [http://www.](http://www.) pwccn. com/ en/ tmt/ interim-measures-for-generative-ai-services-implemented-
aug2023. pdf
138.
https:// fpf. org/ blog/ understanding-japans-ai-promotion-act-an-innovation-first-blueprint-for-ai-
regulation/
139.
https:// fpf. org/ blog/ south-koreas-new-ai-framework-act-a-balancing-act-between-innovation-
and-regulation/
140.
https:// [http://www.](http://www.) minister. industry. gov. au/ ministers/ husic/ media-releases/ albanese-government-acts-
make-ai-safer
141.
https:// [http://www.](http://www.) imda. gov. sg/ resources/ press-releases-factsheets-and-speeches/ factsheets/ 2024/ gen-
ai-and-digital-foss-ai-governance-playbook

**Footnotes**
These official source links provide direct access to the relevant statutes, regulations, legal cases,
tool repositories, and news items necessary for deep research and enterprise compliance on
generative AI and data protection.

```
OceanofPDF.com
```

(1)

© The Author(s), under exclusive license to Springer Nature Switzerland AG 2026
S. Singh et al. (eds.), _GenAI and LLMs for Beyond 5G Networks_
https://doi.org/10.1007/978-3-032-06418-9_4

##### Securing GenAI and LLMs in Beyond

##### 5G/6G UE and Networks

**AI for Security and Security for AI**

Ramesh Chandra Vuppala^1 and Rajavelsamy Rajadurai^1
Samsung R&D Institute India-Bangalore (SRI-B), Bangalore, India

```
Ramesh Chandra Vuppala (Corr esponding author)
Email: rameshc.v@samsung.com
```
```
Rajavelsamy Rajadurai
Email: rajvel@samsung.com
```
**Abstract**
The rapid advancement and widespread adoption of AI technologies are
embedding intelligence throughout communication systems, particularly in
the design of AI-native 6G architectures. This integration of AI into beyond
5G/6G networks and user equipment (UE) presents both opportunities and
challenges with respect to security and privacy. These AI models’ potential
is both a blessing and a curse, making them a “double-edged sword”. While
they can be leveraged to enhance security measures, malicious actors to
launch sophisticated attacks can also exploit them. We will discuss in detail
about both sides of the coin in this chapter, (a) AI for Security: Proactive
Threat Detection and (b) Security for AI: Privacy Preserving
Personalization. AI’s role in enhancing security is significant, enabling
proactive threat detection, automated incident response, and real-time
mitigation. LLMs, such as OpenAI’s GPT models, can analyse vast datasets
to identify patterns, anomalies, and emerging threats, complementing
traditional cybersecurity tools like firewalls and encryption. This system
can be extended to automated incident response, generating security policy


commands and network fuzzing test cases for preventive predictive
analysis. While AI enhances security, it also introduces vulnerabilities.
Security has evolved from predictive AI systems to Generative AI, Agent-
based AI, and finally Quantum AI, with each phase introducing new
challenges and advancements in safeguarding AI technologies while
integrating into future communication systems. We will discuss in details
about security threats and measures to be taken for these various stages of
evolution of AI security landscape further in this chapter.

**Keywords** 6G – AI-native – Security

**1 Introduction**

The rapid advancement and widespread adoption of AI technologies are
expected to embed intelligence throughout communication systems. In fact,
6G architecture itself is expected to be designed as AI-native and integrated
with AI capabilities. This integration of AI at each layer and node in future
communication systems will play a crucial role in the development of smart
cities and communities, making systems to perform tasks such as inference,
model training, model deployment, and distributed computing across
networks and devices. The enhanced speeds and connectivity offered by
B5G networks like 6G technologies undoubtedly may enhance user
experiences, but they also introduce heightened cybersecurity risks. As the
integration of AI in the system and different types of communication
devices like IoT devices continues to grow, the likelihood of data and
privacy violations becomes more prominent. Hence, to safeguard against
data breaches and unauthorized access to personal information and network
security, the development and integration of innovative and robust
encryption methods will be essential.
Security and ubiquitous intelligence are key distinguishing design
principles of the IMT-2030 and is expected to be secure by design as shown
in Fig. 1 [ 1 ]. Here, security and resilience refers to preservation of
confidentiality, integrity, and availability of information such as user data
and signalling. Therefore it is crucial for protection of networks, devices,
and systems against cyberattacks such as hacking, distributed denial of
service, and man in the middle attacks.


**_Fig. 1_** Usage scenarios and overarching aspects of IMT-2030
Generative AI (Gen AI) and Large Language Models (LLMs), like
OpenAI’s GPT models, have emerged as powerful tools with vast
capabilities in various fields including B5G/6G users with smartphones,
smartwatches, tablets, autonomous vehicles, and even edge devices like
connected appliances and wearables. However, when it comes to
cybersecurity, their potential is both a blessing and a curse, making them a
“ **double-edged sword** ”. While they can be leveraged to enhance security
measures, they can also be exploited by malicious actors to launch
sophisticated attacks. We will explore both sides of this dynamics further.


Now, let’s deep dive into how AI and Gen AI helps in enhancing the
security posture and helps in proactively detecting threats using its
applications in UE and network entities of B5G/6G communications.

**2 AI for Security: Proactive Threat Detection**

One of the most promising uses of AI, Gen AI, and LLMs is their ability to
improve threat detection, automate incident reporting, and take mitigation
action as response in real time. AI-powered network systems are capable of
monitoring network traffic to identify and mitigate intrusions and
anomalies. By leveraging advanced detection algorithms, these systems
enhance the ability to maintain network integrity, ensuring secure and
reliable communication channels while proactively addressing potential
threats. By analysing large volumes of data, LLMs can identify patterns,
anomalies, and new types of attacks that may not be immediately visible
through traditional security methods. Once identified, GenAI helps to
facilitate in generating response or actions to these incidents including
generation of reports and suggesting preventive measures for policy
enforcement. Combining these AI-driven systems with traditional
cybersecurity measures such as firewalls, intrusion detection systems and
encryption can create a multi-layered defence strategy that mitigates the
risks from adversaries. This combination will result in creation of
cybersecurity AI assistants and automated security control assessments in
near future as highlighted in hype cycle of Gartner for security operations in
Fig. 2 [ 5 ]. There are multiple methods where AI helps to meet these
capabilities like anomaly detection, automated incident response, preventive
predictive analysis, and security automation. AI-driven security automation
significantly reduces the manual workload, enabling them to utilize the
efforts on higher priority tasks. The importance of utilizing AI in security
systems is also being enforced in reforming constitutional laws, for e.g.
USA stated it the law [ 6 ] as “ _Artificial intelligence (AI) has the potential to
transform cyber defense by rapidly identifying vulnerabilities, increasing
the scale of threat detection techniques, and automating cyber defense_ .”


**_Fig. 2_** Hype cycle for security operations by Gartner
Let us take a use case of enhancing security in IoT networks with AI-
based Anomaly detection method. With the raise of IoT devices in B5G/6G,
networks face numerous security threats, including weak authentication,
unencrypted data transmission, outdated firmware, insecure network
services, and insufficient access controls. These vulnerabilities lead to data
breaches, unauthorized access, and the creation of botnets for malicious
activities. How can we detect these data breaches causing malformed or
ransomware packets injected into IoT networks? At first, we need to prepare
training datasets to categorize original network traffic data and malformed
packet data and use it model training. Ferrag et al. [ 2 ] introduced
SecurityLLM, a system designed for detecting cybersecurity threats to
achieve this method as shown in Fig. 3. SecurityLLM leverages
contextualized text representations with tokens extracted from these
datasets using BERT (Bidirectional Encoder Representations from
Transformers) LLM model. It employs a straightforward classification
model integrated with LLM and is capable of recognizing 14 distinct types
of attacks, achieving an overall accuracy of 98%. These attacks include
DDoS UDP, DDoS ICMP, SQL injection, password-related threats,
vulnerability scanning, DDoS TCP, DDoS HTTP, uploading attacks,


backdoor exploits, port scanning, XSS (Cross-Site Scripting), ransomware,
MITM (Man-in-the-Middle), and fingerprinting.

**_Fig. 3_** SecurityLLM: leveraging contextualized text representations for accurate semantic analysis
This model can be deployed in any IoT edge node, NWDAF or cloud
server which monitors the network traffic to identify the aforesaid threats
and take necessary action like certificate revocation, token revocation for
recovery. Therefore, LLMs can be instrumental in enhancing network
security by analysing network traffic, identifying anomalies, and detecting
potential threats in real time. By leveraging LLMs’ advanced language
processing capabilities, communications providers can detect and prevent
security breaches, protect customer data, and ensure the integrity of their
networks. This proactive approach to network security helps maintain
customer trust and safeguards the communications infrastructure from
emerging threats. As we read this book, active study is ongoing in 3GPP
standards related to detailed procedures involving security monitoring and
security related Events Handling topics.
Now, this SecurityBERT mechanism for identifying cyber threats can be
extended further to automated incident response use case by integrating it
with FalconLLM as illustrated in Fig. 4 , to derive a system for incident
response and recovery. This system can generate necessary prompts on
identifying threats, to predict and propose preventive measures. It helps in
generating security policy commands to policy management network
functions at RAN and Core networks. These prompts can also further
generate network fuzzing test cases for stress testing, identifying issue
scenarios and resolution. This approach of leveraging historical data, trends,
and predictive modelling to identify patterns that may indicate future


problems, allowing UE and networks to take corrective actions in advance
is referred as preventive predictive analysis.

**_Fig. 4_** SecurityLLM integration with Falcon LLM for incident response and recovery
This complete setup can be automated enabling zero-touch network for
early containment, and prevention of threats from spreading across the
networks and devices. This security automation is called as MLSecOps, an
extension to DevSecOps (Development, Security, and Operations) that
integrates security into all stages of the software development lifecycle
(SDLC). MLSecOps, or Machine Learning Security Operations, is a
process that integrates security practices throughout the entire machine
learning lifecycle, from data collection to model deployment and
monitoring. Cloud infrastructure providers like Google have already
integrated this AI-powered SecOps into their state of the art Gemini systems
[ 3 ]. Its time now for service providers, network vendors and OEMs to adopt
this MLSecOps based security automation into their communication
networks to make it cost-efficient, scalable, and secure. They can also
leverage GenAI by utilizing the repository of tickets accumulated over time
from dealing with network anomalies, along with product manuals as
training data, an LLM can be fine-tuned to comprehend the intricacies of
network issues and grasp the unique context of anomaly resolution.
Consequently, the LLM becomes an anomaly-solving tool for
telecommunications professionals, furnishing them with diagnoses of
network issues and their corresponding solutions. In communication
networks, combining data collected from various UEs, network entities, and
with the help of LLMs, a network digital twin can be created for threat
detection, modelling, and handling incident response.


As we realize that proactive threat detection methods using AI and
GenAI provides enhanced accuracy in threat detection, enables real-time
network monitoring to take necessary safety measures and finally also make
systems scalable and cost-efficient using security automation. But one key
question that arises here is whether we have enough network or wireless
data to train these AI models? If not, where do we get it from? Various AI-
based NR air interface use cases being studied in RAN include CSI
feedback, Beam management, and positioning. But we need the user
consent framework to collect these location data for positioning use case.
The key limitation here is that users may not provide user consent to share
their private data and therefore networks may not be able to share the data
outside the network for analytics. So it a big challenge to collect network or
wireless data. Hence there is a need for Synthetic data, artificial data that
mimics real-world data generation. This data augmentation is feasible
through Generative Adversarial Networks (GANs). GAN consists of two
models, generator and discriminator, as illustrated in Fig. 5. The generator
model tries to generate plausible examples from a random Gaussian noise
and tries to fool the classifier. Whereas the discriminator model tries to
learn and correctly classifies the generated data as real or fake. The real data
can be derived from limited network traffic or wireless data at hand. Finally
augmented generated data resembles near to real data.


**_Fig. 5_** GAN architecture
This synthetic data generation for security analytics is then used in
training security models and identify vulnerabilities without needing to use
real user information which preserves data privacy of the user or subscriber.
On this note of privacy preserving of the user, let’s discuss the other side of
AI systems where they are vulnerable, insecure, and need solutions to
provide security for AI.

**3 Security for AI: Privacy Preserving**

**Personalization**

While GenAI and LLMs offer immense potential in various use cases of
B5G/6G device and networks including security, which is discussed so far,
they also present a range of security challenges that must be addressed to
ensure the safety of users and the integrity of systems and networks. Before
going to address security and privacy issues being raised by AI systems,
let’s try to understand how AI has been evolving at various stages in


communication networks and provide security to the threats according to
these stages as illustrated in Fig. 6. It is mainly classified into four different
stages. Each stage introducing new threat vectors and increasing threat
surface. First is Predictive AI security which is mainly related to security
for methods like future prediction, anomaly detection etc. Further we
moved to generative AI system with applications like text generation
(GPT), image generation, and digital twins. Which has introduced different
security threats altogether. Now, we are into a stage of Agentic AI
communications to automate network communication. Finally, we are
heading towards to Quantum AI security due to the recent raise of quantum
machines. Of course, along with new threats we will also discuss solutions
on how to address these security and privacy issues in detail.

**_Fig. 6_** Evolution of AI security landscape in communication systems

**3.1 Predictive AI Security: Threats**

Key security threats of predictive AI methods can be a result of adversarial
attacks leading to Model evasion, Model poisoning, Model inversion,
Resource exhaustion, and backdoor attacks. _Who is an adversary and what
is the purpose of these attacks?_ An adversary, in the context of security and
AI, refers to an entity or individual that seeks to exploit vulnerabilities or
weaknesses in a system, model, or network to achieve malicious objectives.
Adversaries can be external attackers targeting systems for financial gain,


espionage, or disruption, or they can be internal actors with unauthorized
access to sensitive information. In AI, adversaries often use techniques like
adversarial attacks to deceive machine learning models or compromise their
integrity. The goal of an adversary is typically to undermine the system’s
performance, extract confidential data, or cause harm to users or
organizations. Relating it to communications, 6G systems are expected to
be AI-native from day one starting from requirements to network
architecture design to enable fully autonomous networks. Therefore, attacks
on AI systems will affect and impact 6G which need to be addressed with
utmost importance.
These adversaries can steal data used for model training and use the
stolen data for learning using AI models to detect vulnerabilities in the
communication system and also to predict the next possible attack window
and leverage it to initiate the attack. The adversaries who have access to the
data can try to manipulate the data or even inject malicious data, used for
training causing model to learn inaccurately giving biased inferences. These
type of attacks are called as model poisoning. Multiple solutions are
available to address these problems like storing data in secure storage
enclaves, maintaining confidentiality of data using various encryption
methods, and validating the integrity of data received in communication
networks. But, is it ok to just only keep the input data safe? What about
models? An adversary who has access to models can try to give inputs to
the model for inferencing and then exploit the model’s explainability
features to extract sensitive information or reverse-engineer the model’s
decision-making process. So it is equally important to safeguard models
along with data. Last but not the least, the adversary can even use the
model’s outputs to infer sensitive information about the training data, such
as personal details or private data. This can lead to data privacy issues.
Privacy preserving machine learning techniques are key solutions to address
this type of problems. Further, ensemble models can be used to enhance
robustness by combining predictions from multiple models with diverse
architectures, making it harder for attackers to deceive all models
simultaneously. Let us further discuss about more mitigations techniques.

**_3.1.1 Mitigation Techniques and Measures for Predictive AI_**

**_Security_**


While AI enhances user experience by tailoring interactions based on
individual preferences, behaviours, and data, it also raises significant
privacy concerns like unintended sensitive data leakage. There are
techniques for preserving user privacy while facilitating personalized
services, emphasizing methods such as confidential computing, federated
learning, privacy preserving technologies, and zero trust architecture. These
approaches allow for the customization of AI-driven applications in
communications without compromising on sensitive user information.

**_Confidential Computing_**
Any communication between two nodes can be secured at various network
layers using protocols like Transport Layer Security (TLS) and IP security
(IPSec). At RAN, access stratum is secured using cryptographic algorithms
like Advanced Encryption Standard (AES), ZUC, or Snow3G. But what
about security for data in compute? With integration of AI/ML into
communication systems, data or model is now shared outside the entity to
external data centres in cloud for training or inference purpose for faster
execution using GPUs and TPUs. There are various possible threats and
attacks possible on AI model or data being executed in these data centres as
illustrated in Fig. 7. Operators are worried about protection of the network
or user data being sent to these cloud or virtualized environments. There
may be many security solutions provided by cloud data centres like
amazon, google like Cloud Infrastructure Entitlement Management (CIEM),
and Cloud-Native Application Protection Platform (CNAPP). But they do
not provide solid evidence or proof to the service provider about their data
or model security, which is essential as we progress towards zero trust
networks. Hence a need arises for security with shared responsibility model,
where both service provider and data centre owner share the security of the
workloads running in cloud environments.


**_Fig. 7_** Possible attacks on AI data or model stored in Cloud environments
To secure this data, there exists a solution using Confidential
Computing (CC) technologies. CC is a security and privacy technique that
protects data while it’s being processed or in use. It prevents data
leak/attack from firmware, hypervisor, or even from the OS that is running
the application. CC is achieved using secure enclaves like Trusted
Execution Environment (TEE) or advanced encryption methods like
Homomorphic Encryption. TEE is a segregated area of memory and CPU
that is protected from the rest of the CPU using encryption, any data in the
TEE can’t be read or tampered with by any code outside that environment.
Hardware Security Modules (HSMs) are also one such secure dedicated
hardware devices that primarily focus on secure key management and
cryptographic operations. In communication networks, either a small piece
of AI data, AI model, application code or entire VNF or Kubernetes cluster
of CNF can be executed inside TEE.
How does a TEE provide feedback on operator’s data running their
secure enclave? Here comes the saviour, namely Remote Attestation
procedures (rats). It mainly means remotely gathering and verifying proof
or evidence that the platform and execution environment can be trusted.
Remote attestation secures AI model owners, AI data owners like VNFs,
CNFs, and UE from unauthorized entities like the host or hypervisor,
system administrators, service providers, other VMs and containers, and
processes on the host. As shown in Fig. 8 , in Confidential Computing-
assisted ML, data/model owners need to secretly provision their data/model
to the untrusted host’s TEE. Here initially, the owners perform remote
attestation to verify the integrity of the remote TEE, following which they


establish secure communication channels to the TEE. TEE are generally
provided from silicon vendors like Intel and AMD. Now, what if each
vendor has its own remote attestation methods? How does interoperability
work? Hence, there is a need for standardization, and Internet Engineering
Task Force (IETF) is actively working on the standard procedures for rats.

**_Fig. 8_** Flow of confidential computing using TEE to protect the model and data of AI [ 8 ]

**_Federated Learning_**

Federated learning is an emerging technique that enables AI models to be
trained collaboratively across multiple UEs without exposing individual
data to central data servers or any network functions e.g. NWDAF. In this
approach, the models are updated based on aggregated local data, with the
raw data never leaving the device. This method enhances privacy while still
allowing AI models to continuously improve and adapt to user needs.
In the context of 3GPP (3rd Generation Partnership Project), high level
requirements for data collection for AI includes, one the data collected is
secured and data integrity and confidentiality for that data is ensured and
second is user data privacy, anonymity and user consent needs. Federated
learning, a distributed machine learning technique, work in 3GPP is divided


into horizontal and vertical approaches. Horizontal Federated Learning
(HFL) is currently supported within the 5G network data forwarding
(NWDAF) nodes, while Vertical Federated Learning (VFL) is being studied
in release 19. The authorization for selecting participant NWDAF instances
in the Federated Learning (FL) group uses token-based authorization. Most
of the security for AI work is leveraging the existing Privacy &
Authorization mechanisms for 5GC Assistance Information Exposure to AF.
The Data Collection Coordination Function (DCCF) is a key network
function (NF) responsible for coordinating the collection of data from
various NFs (Network Functions) based on requests from other NFs. NF
Service Consumer receives data from Service Producers via DCCF after
successful authorization. Messaging Framework Adaptor Function (MFAF)
facilitates communication between Network Functions (NFs) and a
Messaging Framework, acting as a bridge to expose data analytics and other
information. Also, NF Service Consumer receives data from Service
Producers via DCCF when notification is sent via MFAF on verification of
access tokens. Detailed security aspects for protection of data and analytics
exchange in roaming cases for the 5G system are present as part of 3GPP
TS 33.501 Annex X for reference.

**_Privacy Preserving Technologies_**

As data privacy is a major concern among users today, multiple regulations
like DPDP and GDPR laws are being made to provide privacy to user data.
For communication networks to comply with these new regulations and
gain trust from end user, new emerging privacy preserving technologies like
Homomorphic Encryption (HE), Differential Privacy (DP), Zero
Knowledge Proofs (ZKP e.g. zksnarks), Secure Multi-Party Computations
(SMPC), and GANs are required to safeguard users data in the context of
increasingly personalized AI-driven environments to make AI trustworthy.
These technologies help in mitigating risks such as data leakage, re-
identification, and unauthorized access.
Homomorphic encryption is a concept that it should be possible to
perform mathematical operations on encrypted data without having to
decrypt any part of it. This method will help in securing the user data as it is
created, processed, and also while in transit in communication systems
providing end to end encryption. These mathematical operations can be
further extended to apply analytics, ML, and AI on encrypted decentralized


data. User data can be encrypted and shared to Network functions like
Network Data Analytics Function (NWDAF), data centres, edge nodes, and
third party cloud servers where ML models are trained using homomorphic
engine as shown in Fig. 9. During inference phase, encrypted inference data
is shared back to user where it is decrypted. This enhances trust of user
because data left from their UE or devices is safe in compute environments
with privacy and confidentiality preserved. To simplify, use case of same
user encrypted and decrypted using symmetric key possessed is shown in
Fig. 9 as an example. It is also possible that, in a commercial use case, data
can be encrypted using public key by multiple users and aggregated
encrypted data from multiple sources is used for learning and further
decrypted using private key during inference phase with asymmetric public
key cryptography.

**_Fig. 9_** Machine learning on encrypted user data using Homomorphic Encryption
There are three types of Homomorphic Encryption Schemes namely
Partially Homomorphic Encryption, Somewhat Homomorphic Encryption,
and Fully Homomorphic Encryption. Partial HE are algorithms that allow a
certain operation, e.g. addition, to be performed for infinite number of
times, but does not support operations. Somewhat HE allows a finite
number of operation only, but supports combination of multiple operations
like additions and multiplications. Finally, Fully HE, also referred as FHE,
allows an infinite number of multiple operations. FHE has slight impact on
performance and latency which needs to be considered in designing future


communication systems. Most of the FHE algorithms use lattice-based
cryptography with the concept of learning with errors. This induced error
into the ML systems is one of the key problem at hand and active research
is ongoing to rectify the error using techniques like bootstrapping and bring
these algorithms into standardization. CKKS (Cheon-Kim-Kim-Song) is
one of the fastest boot strapping method till date and there are several
libraries, for implementing the FHE algorithms, like PALISADE and
SEAL.
Differential privacy (DP) is a framework that enables researchers to
analyse datasets while protecting the privacy of individual data points. For
example, only mean, median, mode of the training data may be given by not
revealing the actual data. It also adds noise to the sensitive user data for
preserving privacy and confidentiality. By introducing controlled noise into
the training process, DP guarantees that even if a malicious actor gains
access to the model or algorithm in communication systems, they cannot
extract sensitive information about individual users and cannot learn about
each individual data points in the dataset. Differential privacy (DP) as a
defence mitigates from risk of data reconstruction attacks, poisoning, and
inference attacks.
Zero-knowledge proofs (ZKPs) are cryptographic methods that allow
one party (prover) to demonstrate the truth of a statement to another
(verifier) by sharing just a proof and without disclosing any extra
information. ZKPs play a vital role in improving privacy and security in AI
systems by enabling computations on encrypted data without revealing the
underlying sensitive user data. Zero-knowledge succinct non-Interactive
argument of knowledge, also known as zksnarks, is known for its efficiency
in providing compact proof sizes and fast verification times which can be
recommended to meet latency, KPI requirements of future communication
networks. Secure Multi-Party Computation (SMPC) is a cryptographic
approach that allows multiple parties to jointly compute functions using
their private data without disclosing the data itself. SMPC can be used in
analysing network usage patterns and user behaviour without compromising
individual privacy in beyond 5G/6G systems. Generative adversarial
networks (GANs) can also be used in generating synthetic data for training
in communication systems as discussed in Fig. 4 , which preserves user
privacy. As many privacy preserving technologies and algorithms need to
be evolved further for integrating into standards, 3GPP standards is actively


working on designing User Consent Framework for Advanced 5G and 6G
communications to abide to regulatory bodies on collection and usage of
user data for ML training purpose. Detailed user consent requirements and
mechanisms are present as part of 3GPP TS 33.501 Annex V for reference.

**_Zero Trust Architecture_**

Zero Trust is a “Never Trust Always Verify” security framework requiring
all users, whether in or outside the organization’s network, to be
authenticated, authorized, and continuously validated for security
configuration and posture before being granted or keeping access to
applications and data. Zero Trust is a strategic approach to cybersecurity
that secures an organization by eliminating implicit trust and continuously
validating every stage of a digital interaction. As defined by NIST SP 800–
207, there are seven tenets to ZTA as shown in Fig. 10. Key tenets which
are applicable to AI-based communication networks include Continuous
Security Monitoring, Data collection to improve security posture, Access
Policy and Control, and Attribute-based access for granularity.


**_Fig. 10_** Seven tenets of zero trust architecture
Managing multiple cross-domains e.g. cross-MNO and cross-
geographic regions, AI service consumers, data or model contributors and
distributers, analytics consumers, inference output providers and AI agents
require to abide ZTA principles including policy decision point and policy
enforcement points on data being shared over these untrusted and trusted
domains. These domains can be split based on various levels of
trustworthiness factors. Especially in multi-vendor interactions, integrating
an ML model from one vendor into a function provided by another vendor
raises concerns for introducing unpredictable complexities, such as the
exposure of proprietary details of the ML model and its associated
algorithms. This could lead to security risks for both the ML model and the
AI/ML inference function, as well as create opportunities for an adversary
for model re-engineering. Given the rapid evolution of cyber threats, it is
essential for AI workflows and their supporting infrastructure to undergo
continuous security monitoring and updates. Regular security patches and


threat intelligence updates must be applied to mitigate known
vulnerabilities and adapt to emerging threats.

**3.2 Generative AI Security: Threats**

While many attack types in the predictive AI apply to Generative AI (e.g.
model poisoning, model evasion, model inversion), GenAI has unleashed a
new threat vectors or security issues like Jailbreaking [ 4 ], reverse
psychology, model escape, prompt injection and hallucination problems
impacting user privacy, integrity, violations of abuse and availability of
future communication systems including UE and networks. Taxonomy of
these attacks on GenAI systems are studied by standard organizations like
NIST as illustrated in Fig. 11 [ 7 ].


**_Fig. 1 1_** Taxonomy of attacks on Generative AI systems
Let’s start will understanding Jailbreaking, which refers to the process
of bypassing or removing restrictions and limitations imposed by the
developers or providers of AI models. This can involve techniques that
allow users to access or manipulate the model in ways that were not
originally intended, such as generating outputs outside of its designed scope
or accessing hidden functionalities. It specifically focuses on bypassing the
safety mechanisms designed for LLMs. Jailbreaking can raise ethical and
security concerns, as it may lead to misuse of the AI system, including
generating harmful or inappropriate content. It is important to adhere to the
guidelines and terms of use set by AI providers to ensure responsible and
ethical use of AI technologies. An example of how a jailbreak prompt can


bypass these restrictions to obtain desired results from the model is shown
in Fig. 12 [ 4 ], which illustrates the conversations between the user and
CHATGPT before and after jailbreak.

**_Fig. 12_** Example of jail breaking in Generative AI prompting
In communication systems, network management use cases like
performance monitoring and addressing user grievances are interfaced via
many GenAI based assistants deployed by service providers using the
prompt response systems. These jailbreaking methods by adversaries can
try to extract other sensitive or private data from service provider like
statistical history, network performance issues, user location, and identities,
which can lead to compromise security posture of the network. Too much
access or control to LLMs can lead to unexpected actions, such as
modifying sensitive files or sending unauthorized data outside security
perimeter. These unwanted access can also lead to disrupt the functioning of
IoT devices, leak private conversations, and install malware or malicious
software into UEs and/or network entities. These attacks can affect the
confidentiality and integrity of user resources connected to the LLM. Hence
a proper access control mechanism is required for LLMs to avoid such
incidents.
Adversaries can also create inputs that exploit the AI predictions with
prompts using reverse psychology leading it to generate outputs contrary to
its expected output. Also a sequence of false conditional prompts can be
injected as input to the GenAI to train it wrongly and manipulate the
upcoming outputs according to the adversary. One more important issue
with LLMs is its hallucinations, it describes the creation of information that
sounds plausible but is actually inaccurate or nonsensical. This happens
when the model, despite its advanced language capabilities, struggles to


accurately reflect or reason about real-world facts and concepts. It is
important to address this issue because it can lead to spread of
misinformation and fake data generated by these LLMs causing confusion
and mistrust among entities or users. Key reasons for hallucination are its
insufficient or biased data it trained upon, model overfitting on training
data, and not able to understand the nuance due to its incorrect contextual
representations. These could even direct to legal and ethical compliance
issues.

**_3.2.1 Mitigation Techniques and Measures for GenAI Security_**

Core principles of GenAI security must be of data security, model security,
ethical considerations, and infrastructure security to prevent from these
threats detailed in earlier section. Gen AI requires access to large amounts
of data to make accurate predictions. Hence, ensuring that this data is
managed with security and privacy standards is essential. In the context of
communication-related LLMs, **_protocol fuzzing_** can be used to test
communication protocols. LLMs often interact with other systems via
protocols like HTTP, TCP, or custom protocols. Fuzzing can reveal
weaknesses in how the LLM handles these protocols, potentially leading to
denial of service or other security issues. LLMs process various
communication data, including text, images, and audio. Fuzzing can reveal
how the LLM handles malformed or unexpected input, potentially leading
to vulnerabilities like information leakage or unintended behaviour. Further,
employing post-processing techniques to filter out potentially adversarial
outputs, including comparison against known baselines, and ensure
representative training data to reduce bias-based attacks.
Hallucinations could be mitigated using quality training data which is
unbiased with diversified inputs, cross-referencing authoritative knowledge
sources, design knowledge graphs using frameworks like Retrieval-
Augmented Generation (RAGs) and fine-tuning. Finetuning can improve
model performance by reinforcing knowledge, output adjustments and
complex instruction teaching. Incorporating human preferences and
feedback during pre-training, not just fine-tuning, can lead to LLMs that
generate texts aligned with human values. Reinforcing LLMs by
augmenting training datasets with adversarial examples enhances their
ability to detect and neutralize such attacks and referred to as adversarial
training. To counter other security vulnerabilities of LLMs there exists a


crucial solution called as “ _Red Teaming”_ to make them more resilient to
these adversarial attacks.
**Red teaming** refers to the practice of launching systematic attacks on a
system to uncover its security vulnerabilities. In the context of AI research,
the term has been broadened to include systematic adversarial testing of AI
systems. Generally, red teaming in Large Language Models (LLMs)
extends beyond the study of jailbreaking. It involve the comprehensive
process of identifying potential harm caused by LLMs, uncovering their
vulnerabilities, assisting in the development of mitigation strategies, and
offering measurement approaches to verify the effectiveness of these
mitigations. As part of **ethical hacking** , red teamers challenge the models
with corrupted data, test their defensive skills against different attacks, and
even pit them against other LLMs to test their vulnerabilities. Therefore,
Red teaming serves as an essential safeguard, ensuring that LLMs continue
to drive innovation rather than become a source of harm. Simulating
adversarial attacks through automated and manual methods via read
teaming helps identify and rectify harmful outputs before deployment.
Red team testing methods and guidelines for GenAI use cases must be
designed by wearing an adversary hat. Few methods of red team scenarios
include, but not limited to, adversarial input injections, simulations with
data poisoning, prompt injection, data bias testing, model behaviour
analysis and content filtering. Network virtualization, containerization, and
openness lead to ease the process of executing these methods effectively. To
prevent adversary to do the attack, containers need to be hardened by
restricting and using controls by limiting privileges. Red team testing of
LLM must be part of the CI/CD pipeline and MLsecOps process and
periodic continuous testing need to be performed. Conducting this across
the entire network helps identify any issues and offers insights into gaps
within the automated testing pipeline. This approach serves as a valuable
learning experience for enhancing security design, configuration,
operations, and overall assurance.
In communications, protocol fuzzing and red teaming are related but
distinct aspects of cybersecurity testing. Protocol fuzzing focuses on
identifying vulnerabilities in specific communication protocols through
automated testing, while red teaming simulates real-world attacks to assess
UE or network’s overall security posture and response. Fuzzing can uncover
technical weaknesses in the communication infrastructure, while red


teaming helps identify broader safety and ethical concerns. An approach by
combining protocol fuzzing and red teaming helps ensure that the LLM is
both secure and safe for use in communication applications.

**3.3 Agentic AI Security: Threats**

AI agents, which are extensions to GenAI systems, are envisioned to play a
crucial role in 6G telecommunications, driving advancements by
automating network management, enhancing performance, and enabling
new services. As intelligent systems, they will harness the capabilities to
sensing, learning, and make strategic decisions called as actions, ultimately
automating network operations and delivering superior user experience.
Security for communication between multiple such AI agents or multi-agent
systems and interfaces between AI-agent and external systems is crucial in
future communication networks. Do we need additional security for agentic
AI to keep our future networks safe? If yes, why is it different from GenAI
security? Let’s discuss some of the new threats unleased by these AI agents
[ 10 ].

**Unpredictable Behaviour** AI agents due to its autonomous capabilities
can reassign goals, chain actions which expands the attack/threat surface.
Misuse of these capabilities may result in unauthorized control over agent
actions or to escalate privileges represents a critical vulnerability. These
adaptive actions taken by AI agents can lead to unpredictable failure states
and cascading effect on communication message flows, disrupting user
services. This exploitation can lead to compromised system integrity and
unauthorized access, creating significant risks to the secure operation of
future AI-native communication systems.

**Unstructured Workflows** AI agents extend beyond traditional LLM
applications by integrating external tools that are built in various
programming languages and frameworks. AI agents interface with other
external APIs, internal tools and various other knowledge bases which make
them difficult to manage and monitor. This shortcoming can lead to
unchecked agent activities, increasing the risk of unintended consequences
and compromising the overall safety and reliability of AI agents in
communication systems.


**Explainability Challenges** It gets difficult to understand the AI agent’s
decision-making process due to their complex reasoning mechanisms. The
steps in reasoning are not transparent making them less explainable, as they
integrate inputs from various sources and use multi-model methods for
training. Therefore it may end up in injection of malicious goals or
instructions that mislead agents into exhibiting undesired behaviour and
represents a significant threat. This manipulation can poison the knowledge
base and compromise the integrity of AI systems, skewing the agent’s
understanding and responses, which may further lead to inappropriate and
biased actions that deviate from intended objectives and raise risks to both
network operational safety and user trust.

**Resource Exhaustion Attacks** Unintentional overuse of computational
resources, API calls, or memory by AI agents may degrade performance or
induce failure states representing a significant threat. This form of resource
exhaustion can compromise the stability and efficiency of AI-driven
communication networks, potentially leading to operational disruptions and
vulnerabilities that adversaries may exploit.

**Multi-Agent Communication** Attacks that exploit trust, coordination, or
protocol gaps between collaborating agents pose a critical threat. Agentic AI
introduces stateful memory and context across time which may end up in
violating ethical boundaries and misaligned behaviours at runtime. These
vulnerabilities can be leveraged to disrupt cooperative operations,
compromise data integrity, and undermine the seamless functioning of AI-
driven communication networks, ultimately jeopardizing their security and
reliability.

**_3.3.1 Mitigation Techniques and Measures for Agentic AI_**

**_Security_**

Traditional GenAI security techniques like existing red teaming test cases
might not directly address the security risks of autonomous and goal-
seeking AI agents. A proactive approach like early and **continuous testing**
process is one way to detect and mitigate such threats. Security solutions
like red teaming must simulate real-world attacks and validate under
various adversarial test conditions. For example, an infinite API calls for
task generation can be called recursively that leads to test agent execution


and handle resource exhaustion attacks. Such test cases can also be
generated autonomously and identify vulnerabilities and threats in real time.
Several security principles must be followed to prevent security and privacy
threats caused due to agentic AI in communication networks as illustrated in
Fig. 13. Let us discuss further to understand these principles in detail.

**_Fig. 13_** Principles for a secure agentic AI communication
Further, Agent actions need to regularly tracked, logged and its
behaviour need to be monitored and alerts need to be flagged for any
suspicious activity detected during the audit. In the context of security for
communications, leveraging techniques such as the extended Berkeley
Packet Filter (eBPF) complements traditional security measures by enabling
dynamic and programmable security monitoring and filtering at the network
function level further enhancing network observability and detecting
unusual agent actions.
As we move towards zero trust architecture for 6G, **least privilege
access** with minimal permissions need to be granted for agents to operate.
To achieve it, role of the AI agents in B5G/6G need to be well defined and
necessary permissions need to be provided to isolate and avoid from sharing
or inherit further. Also, right mapping need to be defined for control logic of
AI agents and actions executed by them to avoid malfunctions in network
causing interruptions in providing services to end users. Since there is no
direct kill switch in controlling the autonomous agents, there is a need to
**define limits** and boundaries to agent actions and activate failsafe
mechanisms when agents operate outside of these defined boundaries.
Hence a right set of policies defining these reduced action space will help in
controlling the agent actions to mitigate risks. AI Agents may be created
with a purpose like goal or task oriented, but should not cross these limits


and boundaries in achieving them. They can be further breakdown into sub
goals wherever possible to keep track of the path alignment towards
achieving their main goals or tasks which makes easy for tracking and
security and privacy threats, due to hallucinations of these LLM-based AI
agents, can be pre-empted. Also, implementing rate limiting of actions
taken by AI agents can prevent from Denial of service (DoS) attacks.
**Human in the loop** for critical system interactions for taking key
actions is preferred, instead of complete automation of network and
application-based AI agents. For example, providing admin privileges to AI
agents, allowing password modifications, access to secure storages like
AUSF/UDM kind of sensitive network functions which holds subscription
and authentication information. Therefore, any critical access to sensitive
information like keys, certificates, tokens, and secure storage need to be
taken care with utmost precaution.
There is a need for defining a **trust model** without any assumptions for
machine-to-machine interactions to avoid token sharing and persistent
sessions between AI agents and between AI agents and network functions.
**Secure orchestration** of multi-agent systems focuses on mitigating
privilege separation, establishing trust boundaries, and managing
coordination risks across distributed agents. This can avoid implicit trust
assumptions in communication between AI agents and mitigate rerouting of
workflows by the distributed or centralized network orchestrators. This
approach ensures that collaborative AI systems operate securely and
reliably, minimizing vulnerabilities and enhancing the overall robustness of
communication networks.
Finally, defining identities to these AI agents are necessary for
traceability, auditing, and authentication purpose. Also proper protocols and
procedures require to be designed for secure identity management making
secure endpoints in future communication networks. These mechanisms
help in proper handling of sensitive data like identities and location to
maintain data privacy. Further, to ensure security and trustworthiness,
clearly defined knowledge graphs can help in making AI agent-based
solutions more explainable and predictable in operation, as well as robust
against internal and external threats.
Security of AI agents may not be complete without discussing about
MCP. The Model Context Protocol (MCP) is an open standard developed by
Anthropic that defines how large language models (LLMs) can access and


utilize external tools, services, and data. This protocol runs between an
MCP Client and an MCP Server. Since MCP has gained significant traction,
many MCP servers were being deployed to access APIs of third party
applications, tools, or services. MCP Clients can be an AI agent and decides
which tools to invoke from an MCP server based on the users’ inputs. These
AI agents must maintain security boundaries between servers. OAuth-based
authentication protocol must be used for HTTP-based transport for client–
server communication. A protected MCP server acts as an OAuth resource
server, capable of accepting and responding to protected resource requests
using access tokens. An AI agent or MCP client acts as an OAuth client,
making protected resource requests on behalf of a resource owner.

**3.4 Quantum AI Security: Threats**

The future is here! Quantum Artificial Intelligence (QAI) is a field that
combines quantum computing and artificial intelligence. It leverages the
unique properties of quantum mechanics, like superposition and
entanglement, to enhance AI algorithms and enable them to solve complex
problems that are not possible by classical computers. Quantum algorithms
can significantly speed up the training process of AI models, potentially
enabling real-time decision-making, which is important in future generation
of communication systems. As we understand the active use of AI agents in
B5G/6G networks, now we progress our next journey towards leveraging
principles of quantum computing and integrate with agentic AI to emerge
new concept of Quantum Agents. As we read this book, research has begun
in this domain of understanding quantum agents [ 9 ] and their applications.
Quantum agents can enhance security in critical infrastructure like finance,
defence, and communication systems. But, as the rise of quantum machines
has begun, it is expected that by 2035 there would be enough qubit size
quantum machines which could break the classical asymmetric
cryptography algorithms like Rivest-Shamir-Adleman (RSA) and elliptic
curve cryptography (ECC) as shown in Fig. 14 [ 11 ].


**_Fig. 14_** Quantum threat to communication systems
Quantum machines with its extreme computational capabilities may
reverse engineer ML models and extract data from them. They may also use
generative quantum AI methods for reversing cryptographic algorithms.
Adversaries can store valuable and sensitive data now and decrypted later
with quantum machines once available, these are referred to as Harvested
now and decrypted later attacks.

**_3.4.1 Mitigation Techniques and Measures for Quantum AI_**

**_Security_**

To mitigate threats from the quantum machines on existing AI and
communication systems, there are solutions like Post-Quantum
Cryptography (PQC), also called as quantum safe cryptography, and
Quantum key derivation (QKD) algorithms which can make
telecommunications quantum-resilient. NIST has been actively working on
the PQC algorithms from 2016 and released final versions of the first three
Post Quantum Crypto Standards: FIPS 203, FIPS 204, and FIPS 205 in
August 2024. These PQC algorithms are based on complex mathematical
concepts like lattice-based cryptography, where lattice grids-a repeating
pattern of points in space are used to create complex and hard-to-break
encryption schemes. Studied started in 3GPP and other related standards
bodies to integrate the recommendations of NIST on the usage of PQC
algorithms.
Replacing classical cryptography with PQC algorithms at an early stage
carries an inherent risk as a first time widespread deployment. To minimize
this risk, a hybrid approach is recommended, where classical and post-
quantum algorithms coexist. This approach ensures interoperability and
maintains backward compatibility with existing systems relying on classical
encryption methods. In case vulnerabilities are found in either type of
algorithm, the presence of both classical and post-quantum algorithms in a


hybrid setup reduces the impact of potential breaches, providing additional
resilience to the overall cryptographic architecture.
It is recommended that existing symmetric cryptography algorithms
used in communication systems like AES, SNOW, and ZUC also need to be
upgraded to 256 bit algorithms, as 128 bit security may not be enough to
safeguard from quantum machines. IETF is actively working on upgrading
existing transport and IP layer security standards to PQ-mTLS and PQ-
IPsec (IKEv2 key change to be post quantum) which needs adopted into
future communication systems. Also many algorithms related to
confidential computing and privacy preserving technologies which were
discussed in this chapter earlier need to be upgraded to quantum safe
algorithms like FHE to Quantum Fully Homomorphic Encryption (QFHE)
and Remote attestation to PQ-RATS.
Another approach towards quantum-resilient communications is
Quantum Key Distribution (QKD), a method of secure communication that
utilizes the principles of quantum mechanics to generate and distribute
secret keys for encryption and decryption. It provides a theoretical
guarantee of unconditional security, making it a potential replacement for
traditional encryption methods that are susceptible to attacks from quantum
computers. But this needs the hardware upgrade which is slightly heavier
on telecom provider’s pockets. Hence due to monetization requirements of
operators mostly PQC is the right candidate to fly in 3GPP and ORAN
standards.

**_3.4.2 Conclusion and Key Takeaways_**

As Beyond 5G networks emerge and user equipment become increasingly
sophisticated, integrating Large Language Models (LLMs) and AI agents
into these networks or devices will unlock a wealth of possibilities for
users. However, this technological advancement comes with new challenges
in terms of security and privacy. We have discussed both sides of the coin,
benefits of AI for security and providing security for AI systems. Benefits
include cybersecurity AI assistants with capabilities of threat detection and
classification and security automation. Security challenges include model
evasion, model poisoning, LLM hallucinations, autonomous agentic AI
decisions and post-quantum threats. Addressing these challenges through
strategies like edge processing, federated learning, robust encryption, secure
enclaves, privacy preserving technologies, Identity management and


continuous security monitoring is crucial to ensure that the benefits of
LLMs and AI agents in B5G UEs are realized without compromising user
trust. With a comprehensive approach to privacy and security, the promise
of a seamless, intelligent, and interconnected future powered by Beyond 5G
can be fully realized, fostering innovation while safeguarding the rights and
data of users.

**References**^1

1. IMT (2030). https:// [http://www.](http://www.) itu. int/ rec/ R-REC-M. 2160/ en
2.
    M.A. Ferrag, M. Ndhlovu, N. Tihanyi, L. Cordeiro, M. Debbah, T. Lestable, Revolutionizing
    cyber threat detection with large language models. Cryptogr. Sec. (2023). https:// doi. org/ 10.
    48550/ arXiv. 2306. 14263
3.
    Google: https:// cloud. google. com/ blog/ products/ identity-security/ introducing-google-security-
    operations-intel-driven-ai-powered-secops-at-rsa
4.
    Y. Liu, G. Deng, Z. Xu, Y. Li, Y. Zheng, Y. Zhang, L. Zhao, T. Zhang, Y. Liu, Jailbreaking
    chatgpt via prompt engineering: an empirical study. arXiv preprint. arXiv:2305.13860 (2023)
5.
    Hype Cycle for Security Operations. (2024). https:// [http://www.](http://www.) gartner. com/ en/ documents/ 5622491
6.
    Sustaining Select Efforts to Strengthen the Nation’s Cybersecurity and Amending Executive
    Order 13694 and Executive Order 14144. https:// [http://www.](http://www.) whitehouse. gov/ presidential-actions/ 2025/
    06/ sustaining-select-efforts-to-strengthen-the-nations-cybersecurity-and-amending-executive-
    order-13694-and-executive-order-14144/
7.
    NIST AI 100-2 E2023 Publication: Adversarial Machine Learning: A Taxonomy and
    Terminology of Attacks and Mitigations. (2024). https:// csrc. nist. gov/ pubs/ ai/ 100/ 2/ e2023/ final
8.
    M.O. FAN, Z. Tarkhani, H. Haddadi, Machine learning with confidential computing: a
    systematization of knowledge. ACM Comput. Surv. (2024) https:// arxiv. org/ pdf/ 2208. 10134
9.
    E. Sultanow, M. Tehrani, S. Dutta, W.J. Buchanan, M.S. Khan, Quantum agents. Quant. Phys..
    https:// arxiv. org/ pdf/ 2506. 01536
10.
    Agentic AI Red Teaming Guide, CSA. https:// cloudsecurityall iance. org/ artifacts/ agentic-ai-red-
    teaming-guide
11.
    Quantum Security for Future Communication Networks: Standards Perspective. https:// research.
    samsung. com/ blog/ Quantum-Security-for-Future-Communication-Networks-Standards-
    Perspective

**Footnotes**


1 **Note:**
(i) **User (or) subscriber terms are used interchangeably.**

(ii) **Network includes Radio access network (RAN) or Core Network entities.**

```
OceanofPDF.com
```

(1)

© The Author(s), under exclusive license to Springer Nature Switzerland AG 2026
S. Singh et al. (eds.), _GenAI and LLMs for Beyond 5G Networks_
https://doi.org/10.1007/978-3-032-06418-9_5

##### Designing Guardrails: Ensuring

##### Responsible AI Behavior

Anish Kumar^1
Intel, Bangalore, India

**Abstract**
As generative AI rapidly integrates into enterprise ecosystems, the need for
robust guardrails has become a critical concern for organizations seeking to
leverage this powerful technology responsibly. This chapter provides a
comprehensive framework for designing, implementing, and governing AI
guardrails to ensure safety, fairness, control, and compliance in high-stakes
business environments.
The chapter begins by contextualizing the urgent need for responsible
AI in enterprise settings, emphasizing risks such as hallucinations, bias, and
privacy breaches. It highlights how unguarded generative AI systems can
not only produce harmful outputs but also expose organizations to
regulatory penalties, reputational damage, and operational inefficiencies.
With regulations such as the EU Artificial Intelligence Act imposing strict
governance requirements on high-risk AI applications, enterprises must
adopt proactive measures to maintain compliance and public trust.
The core of the chapter explores a multilayered approach to AI
guardrails, encompassing technical, procedural, and human oversight
mechanisms. At the technical level, input filtering (e.g., regex and ML
classifiers), output validation (e.g., schema enforcement, toxicity detectors),
and grounding with enterprise knowledge are emphasized as vital
components in limiting model misbehavior. Reinforcement Learning from
Human Feedback (RLHF) and constitutional AI techniques are explored as
strategies to tune models toward ethical and organizational values. Tools


such as Guardrails AI, PromptFoo, and Amazon Bedrock Guardrails
provide tangible implementation pathways for enterprises.
On the procedural side, the chapter outlines governance frameworks—
policies, ethics committees, and compliance programs—that define roles
and responsibilities across the AI lifecycle. It stresses the importance of
policy-as-code to embed these rules into CI/CD workflows, enabling
scalable and automated enforcement. A hybrid architecture combining
automated enforcement with human-in-the-loop (HITL) mechanisms is
advocated to catch edge cases and ensure nuanced decision-making where
AI models fall short.
Enterprise case studies provide compelling validation of these
approaches. Banks have used real-time guardrails to prevent the leakage of
personally identifiable information (PII), while healthcare providers rely on
schema validators to filter out unsafe medical advice. These examples
demonstrate not only reduced risk exposure and improved regulatory
compliance but also tangible business value, such as faster audit readiness
and enhanced customer trust.
A key innovation discussed in the chapter is knowledge grounding
through retrieval-augmented generation (RAG), which ties generative
outputs to authoritative corporate sources. This technique significantly
reduces hallucination rates and enables auditability—every output can be
traced back to verifiable documents. Combined with output classifiers and
HITL review, grounding transforms generative AI into a more trustworthy
and explainable asset within the enterprise.
The chapter also emphasizes measurable impact and continuous
improvement as design principles for guardrails. Enterprises are encouraged
to monitor metrics such as guardrail trigger counts, hallucination frequency,
and fairness indices. Tools like MLflow, Evidently AI, and OpenLineage
facilitate comprehensive auditing and traceability across model training and
inference. These mechanisms support iterative guardrail refinement,
aligning with the National Institute of Standards and Technology (NIST) AI
Risk Management Framework’s “Measure” and “Manage” functions.
Finally, the chapter outlines governance best practices and
organizational structures to support responsible AI. From the Chief AI
Ethics Officer to compliance managers and ML engineers, each role has a
defined stake in ensuring that generative AI operates within safe, legal, and
ethical boundaries. Global regulations, including the EU AI Act, U.S.


agency guidelines, and China’s Interim Measures, are discussed to provide
readers with an international compliance perspective.
In conclusion, this chapter offers enterprise leaders, AI practitioners,
and policy advocates a blueprint for operationalizing responsible AI through
layered guardrails. It equips organizations with the tools, strategies, and
governance models necessary to mitigate risks, maximize business value,
and build stakeholder trust in the age of generative AI. As organizations
scale AI across functions, these guardrails serve not as constraints but as
enablers—empowering safe, innovative, and resilient AI adoption across
industries.

**1 Introduction**

Guardrails in AI are safeguards—both technical and policy-based—that
ensure generative AI behaves safely, ethically, and within defined
boundaries. They help prevent harmful, biased, or noncompliant outputs.
Responsible AI refers to the design and deployment of AI systems that are
transparent, fair, accountable, and aligned with human values. Together,
they enable organizations to harness AI’s power while minimizing risks and
maintaining trust.
As generative AI becomes foundational within enterprise workflows,
establishing robust guardrails is no longer optional—it’s essential. Effective
guardrails are the cornerstone for ensuring that generative models remain
safe, fair, and controllable as they automate content creation and decision
support at scale. With the unprecedented power of large language models
and generative systems comes heightened risk: unfiltered prompts or
unchecked outputs can lead to biased, offensive, or even harmful outcomes,
jeopardizing organizational trust and compliance [ 1 – 3 ].
This chapter explores the multilayered approach enterprises must adopt
to build resilient guardrails. Technical strategies such as prompt filtering
and output validation can help intercept unsafe or undesirable responses
before they reach business users or customers [ 1 , 3 , 4 ]. Reinforcement
Learning from Human Feedback (RLHF) and alignment tuning further
calibrate model behavior to human values, leveraging active human
participation to reinforce preferred outputs and diminish risk-prone ones [ 5 –
7 ].


Crucially, grounding AI generation in enterprise knowledge bases and
verified truth reduces hallucination and ensures that generative AI
references only authoritative, trusted sources, maintaining organizational
accuracy and compliance [ 2 , 8 ]. These defenses are buttressed by strong
governance frameworks—clear policies, accountability structures, and
compliance mechanisms designed to manage risks and uphold ethical
standards across the AI lifecycle [ 9 – 11 ].
Alongside automation, this chapter also emphasizes the ongoing
necessity of human-in-the-loop systems. By embedding human oversight
not only in training and testing but also in real-time production workflows,
enterprises can catch edge cases and evolving threats that elude automated
systems, achieving a balance between innovative power and responsible
stewardship [ 12 , 13 ].
By the end of this chapter, readers will be equipped with practical
strategies to design, deploy, and govern generative AI systems in a way that
underpins enterprise integrity, fosters trust, and unlocks the safe potential of
this rapidly advancing technology.

**1.1 The Need for Responsible Generative AI in Enterprises**

Responsible generative AI entails designing, developing, and deploying AI
systems that are **ethical, transparent, and accountable** , aligning outputs
with human values while minimizing risks [ 14 ]. In enterprise settings,
adopting responsible AI practices is crucial for ensuring **customer project
success** : by embedding fairness and explainability into model pipelines,
organizations can build trust with stakeholders and deliver reliable
outcomes that meet business objectives [ 15 ].
Meeting **government regulations** is another imperative. With
frameworks like the EU’s Artificial Intelligence Act mandating risk-based
requirements for high-risk AI applications, enterprises must implement
robust governance, documentation, and compliance mechanisms to avoid
hefty fines and legal sanctions [ 16 ]. Responsible generative AI also
addresses **morality and ethical standards** , guiding enterprises to prevent
harmful biases, protect privacy, and uphold societal well-being. Embedding
ethics—such as nondiscrimination, safety, and data integrity—into every
stage of the AI lifecycle ensures that generative models respect fundamental
rights and organizational values [ 17 ].


Finally, responsible AI is not solely a technical endeavor; it demands
**holistic governance structures** and cross-functional collaboration.
Initiatives like the Partnership on AI’s enterprise landscape report highlight
the need for clear terminology alignment, monitoring frameworks, and
stakeholder engagement to navigate challenges and seize generative AI’s
full potential responsibly [ 18 ]. By prioritizing responsible generative AI,
enterprises can achieve compliant, trustworthy, and morally sound
innovations that drive sustainable growth.

**1.2 Risks and Challenges in Enterprise Deployments**

Designing effective guardrails for generative AI in enterprise deployments
requires confronting a triad of **risks** , **challenges** , and **opportunities** —all
within a **responsible AI** framework.
At the heart of risk management lies the mitigation of **model
hallucinations** , bias propagation, and data privacy breaches. Unchecked
outputs can erode customer trust and expose sensitive information, while
regulatory non-compliance—under frameworks like the EU AI Act—carries
significant penalties [ 19 ]. Operational risks include spiraling cloud costs
and integration complexities when embedding large language models into
legacy systems, which can stall ROI and disrupt workflows [ 20 ].
Enterprises also face **challenges** in data infrastructure, talent gaps, and
organizational readiness. High-quality, diverse training data is essential for
robust performance, yet many organizations lack the pipelines and
governance to manage it at scale. Only a small fraction have fully
operationalized responsible AI practices, leaving them vulnerable to
unintended harms and ethical lapses [ 21 ]. Moreover, workforce resistance
—driven by distrust or skill deficits—can hinder adoption, requiring
comprehensive upskilling and change management.
However, **opportunities** abound for enterprises that embed responsible
AI guardrails from the outset. Generative AI can unlock trillions in value
across customer service, marketing, R&D, and knowledge management by
automating routine tasks, enhancing creativity, and enabling personalized
experiences at scale [ 22 ]. Platforms like the Accenture Responsible AI
Platform powered by AWS offer integrated governance, risk assessment,
compliance monitoring, and testing capabilities to help organizations scale
AI safely and confidently [ 23 ]. By aligning technical controls with ethical
principles—fairness, explainability, privacy, and human oversight—


enterprises can harness generative AI’s transformative power while
safeguarding customers, meeting regulatory demands, and upholding moral
standards.

**2 Understanding AI Guardrails**

Guardrails are automated and procedural safeguards that constrain
generative AI behavior to organizational policies, ethical norms, and safety
standards. They span _prompt engineering_ (system-prompt templates, input
sanitization), _filters and classifiers_ (to block toxic, private, or noncompliant
content), and _post-processing validators_ (output formatting and accuracy
checks) [ 24 ]. Advanced techniques include _reinforcement learning from
human feedback_ (RLHF) to align model responses with human preferences,
and _knowledge grounding_ to ensure factual consistency [ 25 ]. Tools like
Amazon Bedrock Guardrails, NVIDIA NeMo Guardrails, and open-source
frameworks (e.g., Guardrails AI) offer integrated pipelines for testing,
monitoring, and evolving these controls in production [ 26 ].

**2.1 Definition and Scope**

In enterprise contexts, **AI guardrails** are the defined policies, frameworks,
and technical controls that ensure generative AI systems operate within
approved **ethical** , **legal** , and **operational** boundaries [ 27 ]. Guardrails
encompass:

```
Governance scope
```
- Organizational policies and standards that dictate acceptable AI use,
    ownership, and accountability.
- Roles and responsibilities—from executive sponsors to ethics
    committees—to enforce compliance [ 28 ].
**Technical scope**
- _Input controls_ : sanitizing and validating prompts to prevent injection
attacks, bias amplification, and privacy breaches.
- _Model alignment_ : fine-tuning and Reinforcement Learning from
Human Feedback (RLHF) to embed human values, reduce
hallucinations, and align outputs with enterprise objectives.


- _Output filters_ : classifiers and rule-based filters that block toxic,
    noncompliant, or sensitive content before release.
- _Monitoring and audit_ : continuous logging, performance metrics, and
    real-time alerts to detect drift, failures, and security incidents [ 29 ].
By uniting **procedural** (policies/governance) and **programmatic**
(code/architectural) measures, enterprise guardrails provide a holistic safety
net—enabling innovative generative AI while safeguarding customer trust,
regulatory compliance, and corporate reputation.

**2.2 Why Guardrails Are Crucial for Safety, Fairness, and**

**Control**

Guardrails are indispensable in enterprise generative AI implementations
for ensuring **safety** , **fairness** , and **control** , while satisfying **government
mandates** , **industry recommendations** , and **legal requirements**.
Safety. Unconstrained generative models can produce harmful content
—misinformation, hate speech, or data leaks—jeopardizing users and
organizations. Guardrails such as input sanitization, output filters, and real-
time monitoring intercept malicious or sensitive outputs, preventing
reputational damage and operational risks [ 30 ]. Continuous auditing, as
advocated by the NIST AI Risk Management Framework, ensures that AI
systems remain robust and resilient against evolving threats [ 31 ].
Fairness. AI models trained on biased data can perpetuate or amplify
social inequities. Guardrails—bias detection tools, fairness metrics, and
human-in-the-loop reviews—help identify and mitigate discriminatory
outcomes, aligning AI outputs with corporate values and societal norms
[ 30 ]. The OECD Principles on AI further recommend embedding fairness
and accountability throughout the AI lifecycle to uphold public trust [ 32 ].
Control. Enterprises require governance structures—clear policies, role-
based access, and accountability frameworks—to manage AI across
functions. Guardrails enforce compliance with organizational standards and
enable audit trails for decision-making processes. Platforms like Amazon
Bedrock Guardrails offer policy-based enforcement to maintain operational
control over AI interactions [ 33 ].
Government Mandates and Legal Requirements. The EU Artificial
Intelligence Act categorizes high-risk AI use cases and imposes strict
requirements for transparency, risk management, and human oversight, with
penalties up to 7% of global turnover for non-compliance [ 34 ]. In the US,


agencies are increasingly adopting the NIST AI RMF to standardize risk
management practices across sectors [ 31 ]. UNESCO’s Recommendation on
the Ethics of Artificial Intelligence emphasizes human rights, fairness, and
accountability as core legal and ethical imperatives [ 35 ].
Industry Recommendations. Leading consultancies and AI bodies,
including McKinsey, Accenture, and the Partnership on AI, advocate for
multilayered guardrail architectures combining technical controls (filters,
RLHF, grounding) with strong governance (ethics boards, compliance
monitoring) [ 30 , 36 ]. By implementing these guardrails, enterprises can
harness generative AI’s transformative potential while safeguarding
stakeholders and meeting their regulatory and ethical obligations.

**2.3 Guardrail Strategies for Generative AI**

Guardrail Strategies for Generative AI implementation in enterprises.
Implementing effective guardrails involves a **multilayered approach**
combining prompt controls, output validation, alignment tuning, and human
oversight:

1. Prompt Filtering and Sanitization

```
(a) Apply regex-based filters and content-safety libraries at the API
```
## gateway to block malicious or sensitive inputs.^

```
(b) Use PromptFoo’s open-source test suite to validate prompt safety
```
## and detect injection attacks during development [ 36 ].^

2. Output Validation and Post-Processing

```
(a) Deploy post-generation classifiers (toxicity, bias, privacy) to
```
## inspect model responses before delivery.^

```
(b) Integrate Guardrails AI (GitHub: guardrails-
ai/responsiveness_check) for schema enforcement and semantic
validation at runtime [ 37 ].
```
3. Reinforcement Learning from Human Feedback (RLHF)


```
(a) Incorporate lightweight RLHF loops to iteratively fine-tune
models using curated human judgments, reducing hallucinations
and bias over time [ 38 ].
```
4. Knowledge Grounding and Retrieval Augmented Generation

```
(a) Anchor outputs in enterprise knowledge bases using retrieval
techniques to ensure factual accuracy and reduce unpredictable
content.
```
5. Alignment Tuning and Constitutional AI

```
(a) Implement rule-based “constitutional” constraints within model
inference pipelines, guiding behavior toward organizational
policies and ethical norms [ 39 ].
```
**Cost-Effective Implementation**

1. Leverage open-source frameworks (e.g., Guardrails AI, PromptFoo,
    RAIL libraries) to minimize licensing fees and foster community-
    driven improvements.
2. Use serverless or spot-instance compute for lightweight filtering and

## validation steps, scaling elastically to manage costs.^

3. Employ an incremental rollout: start with critical use cases and expand
    guardrail coverage based on observed risk metrics, optimizing resource
    allocation and ROI.

By combining **open-source tools** , **human-guided fine-tuning** , and
**modular pipeline integrations** , enterprises can deploy robust, cost-
effective guardrails that ensure safe, fair, and compliant generative AI
applications.

**_2.3.1 Prompt Filtering_**


Prompt filtering is the first line of defense against malicious, irrelevant, or
sensitive inputs to generative AI systems. Effective strategies combine **rule-
based filters** , **machine-learning classifiers** , and **policy engines** to sanitize
user prompts before they reach foundation models.

1. Rule-Based Filtering

```
(a) Implement regex and keyword blacklists at the API gateway to
```
## block injection attacks, profanity, and disallowed topics.^

```
(b) Use lightweight libraries like safety-cue (GitHub: https:// github.
com/ ismailuddin/ safety-cue) for customizable pattern matching
[ 40 ].
```
2. ML-Based Content Classification

```
(a) Deploy open-source classifiers—such as Hugging Face’s
distilbert-base-uncased-finetuned-sst-2-english for toxicity
detection—to flag harmful or off-topic prompts.
```
```
(b) Combine multiple classifiers in ensemble to improve precision
```
## and recall.^

3. Policy Engines and Configuration

```
(a) Leverage PromptFoo to define and validate prompt safety tests as
code, ensuring consistency across development and production
[ 41 ].
```
```
(b) Configure Guardrails AI schemas to assert prompt structure and
content policies via RAIL specifications (GitHub: https:// github.
com/ guardrails-ai/ guardrails).
```
**Cost-Effective Implementation**

1. **Open-Source Software** : Rely on community-maintained projects

## (Safety-Cue, PromptFoo, Guardrails AI) to avoid licensing fees and^


```
benefit from collective updates.
```
2. **Serverless Filters** : Deploy prompt filters on AWS Lambda or Azure

## Functions, scaling only on incoming traffic to minimize compute costs.^

3. **Incremental Rollout** : Start with critical applications (e.g., customer
    support bots), measure blocked prompt rates, and expand filter rules
    iteratively to optimize engineering effort versus risk mitigation.

By integrating **rule-based** , **ML-driven** , and **policy-as-code** approaches
—using readily available open-source tools—enterprises can establish
robust, scalable, and cost-efficient prompt filtering guardrails for their
generative AI deployments.

**2.4 Techniques for Preventing Harmful or Noncompliant**

**Inputs**

Enterprises must defend their generative AI systems from malicious, biased,
or policy-violating prompts. A layered approach combining **manual** and
**automated** techniques ensures comprehensive protection.

1. **Manual Techniques**

```
(a) Policy Definition and Review
(i) Establish clear acceptable use policies that enumerate
```
## forbidden content (hate speech, PII, proprietary data).^

```
(ii) Convene cross-functional ethics committees and legal teams
to vet policy scope, ensuring alignment with regulations
such as the EU AI Act and internal data governance rules
[ 42 ].
```
```
(b) Prompt Playbooks and Training
(i) Develop prompt playbooks illustrating approved input
patterns and prohibited constructs; incorporate them into
developer and user training.
```

```
(ii) Conduct red-teaming exercises , manually crafting
adversarial prompts to identify vulnerabilities and refine
guardrail definitions [ 43 ].
```
2. Automated Techniques

```
(a) Rule-Based Filtering
(i) Regex and Keyword Blacklists : At the API gateway,
implement pattern matching to block SQL-injection style
attacks, profanity, or regulated terms. Libraries like safety-
cue offer customizable filter rules to catch sensitive
keywords and expressions [ 44 ].
```
```
(b) Machine-Learning Classifiers
(ii) Toxicity and Bias Detection : Deploy open-source models
(e.g., Hugging Face’s fine-tuned transformers) to score
prompt toxicity and flag biased language, using ensemble
methods to balance false positives and negatives.
```
```
(c) Prompt Validation Frameworks
(iii) Policy-as-Code : Tools such as PromptFoo enable writing
prompt safety tests in code, running CI-integrated checks
against defined rules before promotion to production [ 45 ].
```
```
(d) Platform-Provided Guardrails
(iv) Managed Safeguards : Services like Amazon Bedrock
Guardrails provide built-in input safety filters (content
moderation, PII redaction) configurable via IAM policies
and thresholds to auto-reject noncompliant prompts [ 46 ].
```

```
(e) Real-Time Monitoring and Quarantine
(v) Streaming Analysis : Integrate real-time monitoring to log
and analyze all prompts, triggering alerts or quarantining
sessions for manual review upon detection of high-risk
inputs.
```
3. Hybrid Workflows

```
(a) Human-in-the-Loop (HITL) : Route edge-case prompts that
narrowly miss automated filters to live moderators for rapid
adjudication, leveraging labeled data to continuously retrain
classifiers and update rule sets.
```
```
(b) Continuous Feedback Loops : Use post-deployment telemetry—
blocked prompt counts, false positive/negative rates—to refine
both manual policy definitions and automated filters.
```
By combining **manual governance** (policy creation, red teaming) with
**automated defenses** (regex filters, ML classifiers, policy-as-code, managed
guardrails), enterprises can robustly prevent harmful or noncompliant
inputs, ensuring safe, fair, and compliant generative AI applications.

**2.5 Case Studies in Prompt Moderation**

**_2.5.1 Case Studies in Prompt Moderation for Generative AI_**

**_Guardrails_**

1. Financial Services PII Redaction with Amazon Bedrock Guardrails
    A leading bank deployed Amazon Bedrock Guardrails to
automatically detect and redact Personally Identifiable Information
(PII) in chat prompts and responses. By configuring Bedrock’s
“sensitive information filter,” the bank prevented transmission of
account numbers, social security numbers, and customer addresses.
Guardrails were enforced via IAM policies and real-time content
moderation hooks, blocking noncompliant prompts and logging
incidents for compliance audits. This implementation reduced manual


```
review workload by 70% while ensuring GDPR and PCI DSS
adherence [ 47 ].
```
2. E-Commerce Content Safety via PromptFoo Integration
    An e-commerce platform integrated PromptFoo into its CI/CD
pipeline to enforce prompt safety tests. Developers wrote custom
PromptFoo rules to detect prohibited product claims (e.g., medical
guarantees) and abusive language. Each new prompt template
underwent automated testing; failures triggered build breaks and
notifications. Over six months, the platform blocked 95% of
noncompliant prompt variants before production release, enhancing
brand safety without hampering developer velocity [ 48 ].
3. Healthcare Knowledge Base Query Filtering Using Guardrails AI
    A telehealth provider leveraged Guardrails AI with RAIL
specifications to validate prompt structure and content. The system
enforced schema checks—ensuring only medically relevant queries
reached the LLM—and blocked off-topic or potentially harmful
prompts (e.g., self-harm instructions). Human-in-the-loop review was
reserved for edge cases flagged by Guardrails AI, resulting in a 60%
reduction in unsafe responses and streamlined regulatory reporting for
HIPAA compliance [ 49 ].
4. Media Company Toxicity Filtering with Safety-Cue
    A global media enterprise adopted the open-source **safety-cue**
library to implement regex-based and ML-augmented filters at the API
gateway. Patterns targeting hate speech, profanity, and extremist
content intercepted malicious prompts. Combined with a lightweight
transformer classifier, the filter achieved 92% accuracy in flagging
toxic inputs, protecting the brand’s user forums and content creation
workflows [ 50 ].

These case studies illustrate how **prompt moderation guardrails** —
ranging from managed services to open-source tools—can be tailored to
diverse enterprise use cases, delivering robust safety, compliance, and
operational efficiency.

**_2.5.2 Output Validation_**


Enterprises must institute rigorous output validation to ensure that
generative AI systems produce safe, relevant, and fair content. First, safety
filters must intercept toxic, biased, or otherwise harmful outputs by
leveraging managed services such as Amazon Bedrock Guardrails, which
provide configurable content moderation that blocks disallowed categories
in real time and logs incidents for audit purposes [ 51 ]. Second, relevance
checks should verify that responses align with user intent and domain
context by employing schema-based validators—for example, Guardrails
AI’s RAIL specifications enable enforcement of expected output formats
and field constraints, rejecting or flagging responses that fall outside
predefined templates [ 52 ]. Third, fairness assessments are crucial to detect
and mitigate bias in generated outputs; enterprises can integrate open-
source bias detectors, such as pretrained transformer classifiers fine-tuned
on fairness benchmarks, to score outputs for demographic or ideological
skew before release [ 53 ]. Fourth, factual accuracy can be enforced through
grounding techniques, where outputs are validated against authoritative
knowledge bases using retrieval-augmented generation pipelines that cross-
check claims and reject hallucinated statements [ 54 ]. Fifth, human-in-the-
loop workflows should be reserved for edge-case outputs flagged by
automated validators, ensuring that ambiguous or high-risk responses
undergo expert review, with corrections fed back into model fine-tuning
loops to continuously improve performance. Finally, comprehensive
logging and monitoring of all outputs—including metadata on filter
triggers, validation outcomes, and human review decisions—enable
organizations to refine validation rules, measure false positive and negative
rates, and demonstrate compliance with regulatory mandates such as the EU
AI Act.
Through a cohesive strategy combining managed guardrails, open-
source validators, retrieval-based fact checking, and human oversight,
enterprises can confidently deploy generative AI that is safe for users,
relevant to business needs, and fair across diverse populations.

**_2.5.3 Ensuring Safe, Relevant, and Fair AI Outputs_**

Enterprises must enforce rigorous validation of generative AI outputs to
guarantee safety, relevance, and fairness, thereby upholding customer trust,
regulatory compliance, and ethical standards. Safety validation begins by
filtering toxic or harmful content using managed guardrail services like


Amazon Bedrock Guardrails, which leverage real-time content moderation
policies to intercept disallowed outputs and maintain comprehensive audit
logs for incident review [ 51 ]. Ensuring relevance demands schema-based
validators such as Guardrails AI’s RAIL framework, which enforces output
structures and domain constraints—rejecting responses that deviate from
expected formats or reference unauthorized data sources [ 52 ].
Fairness evaluation encompasses both individual and group metrics:
enterprises adopt demographic parity and equalized odds to assess whether
generative outputs disproportionately affect protected groups, drawing on
alignment research to calibrate models against these standards [ 53 ]. Factual
accuracy is further validated through retrieval-augmented generation
pipelines that cross-check claims against corporate knowledge bases,
substantially reducing hallucination rates. Open-source platforms reinforce
these practices: **GenAI Arena** , an academic initiative, crowdsources human
judgments to benchmark model outputs on quality, fairness, and user
satisfaction, providing cleaned preference data and statistical evaluation
methods for enterprise model selection [ 54 ].
Comprehensive model validation toolkits—such as **Arthur Bench** and
**TruLens** —offer modular evaluation suites that measure hallucination
frequency, readability, relevance, and bias across custom workloads,
enabling continuous monitoring and comparative analysis [ 55 ]. Key metrics
include toxicity scores, factuality percentages, schema compliance rates,
demographic disparity indices, and user satisfaction ratings, all tracked
alongside operational indicators like guardrail trigger counts and human-
review ratios. By integrating managed guardrails, open-source evaluators,
and robust fairness and factuality metrics into a unified governance
framework, enterprises can confidently deploy generative AI systems that
are demonstrably safe, contextually relevant, and equitably fair.

**_2.5.4 Methods for Automatic and Manual Output Checks_**

Enterprises implementing generative AI must adopt both automatic and
manual output checks to enforce guardrails and uphold responsible AI
principles, thereby minimizing harmful or non-compliant outputs and
avoiding severe legal liabilities. Automatic checks involve integrating
managed content moderation services such as Amazon Bedrock Guardrails,
which apply policy-driven filters to block or redact disallowed categories—
hate speech, disinformation, or personal data—in real time and maintain


audit logs for subsequent review [ 56 ]. In parallel, schema-based validators
like Guardrails AI’s RAIL framework enforce structural constraints and
content rules, rejecting outputs that deviate from approved templates or
reference unauthorized knowledge sources [ 57 ]. Machine-learning
classifiers fine-tuned on fairness and toxicity benchmarks score each output
for bias, toxicity, and relevance, enabling dynamic routing of high-risk
responses to human reviewers. Retrieval-augmented generation pipelines
provide an additional layer of factuality verification by cross-checking
claims against curated enterprise knowledge bases, dramatically reducing
hallucinations and ensuring contextual accuracy.
Manual checks remain indispensable for edge cases and high-impact
scenarios. Organizations convene multidisciplinary review boards—
including legal, compliance, and domain experts—to define acceptable use
policies and oversee periodic red-teaming exercises that simulate
adversarial prompts, uncover hidden vulnerabilities, and refine guardrail
definitions. A human-in-the-loop workflow flags ambiguous or borderline
outputs for expert adjudication, with corrections fed back into both rule-
based filters and ML classifiers to iteratively strengthen the system.
Best practices dictate continuous monitoring and feedback: enterprises
must log validation metadata—filter triggers, classification scores, human
review outcomes—and track key metrics such as false positive and negative
rates, average time to resolution for manual reviews, and audit
completeness. These metrics support governance functions outlined in the
NIST AI Risk Management Framework’s Measure and Manage stages,
enabling organizations to demonstrate due diligence and operationalize risk
management [ 58 ]. Failure to implement robust output checks exposes
enterprises to legal implications under the EU Artificial Intelligence Act,
where non-compliant AI systems face fines up to €35 million or 7 percent
of global turnover for high-risk violations, as well as potential class-action
lawsuits, regulatory injunctions, and reputational harm when harmful or
biased content reaches end users [ 59 ]. By combining automated
enforcement mechanisms with human expertise and adhering to evolving
regulatory frameworks, enterprises can confidently harness generative AI’s
transformative potential while ensuring safety, fairness, and accountability.

**_2.5.5 Reinforcement Learning from Human Feedback (RLHF)_**


Enterprises leverage Reinforcement Learning from Human Feedback
(RLHF) to align generative AI models with human values, reinforcing
guardrails that ensure safety, fairness, and control. RLHF workflows begin
with Supervised Fine-Tuning (SFT), where a model is trained on high-
quality human demonstrations to establish baseline competence. Next,
human annotators rank multiple model outputs for given prompts; these
preference data train a reward model that predicts human-like evaluations.
Finally, policy-gradient RL algorithms, most notably Proximal Policy
Optimization (PPO), adjust the model’s parameters to maximize predicted
rewards, thus embedding guardrail behaviors—such as refusal to produce
disallowed content and adherence to organizational policies—into the
generation process [ 60 ].
Popular open-source frameworks facilitate RLHF: Hugging Face’s **TRL**
library offers end-to-end support for SFT, reward modeling, and PPO
training, as well as emerging algorithms like Direct Preference
Optimization (DPO) and Group Relative Policy Optimization (GRPO), all
tightly integrated with the transformers ecosystem for seamless enterprise
adoption [ 61 ]. For large-scale deployments, Microsoft’s **DeepSpeed-Chat**
democratizes RLHF by optimizing memory (ZeRO, LoRA) and compute
(Hybrid Engine) to train models up to hundreds of billions of parameters
rapidly and cost-effectively, achieving up to 15× speedups over traditional
pipelines while maintaining robust guardrail compliance [ 62 ]. OpenAI’s
InstructGPT pipeline exemplifies RLHF’s impact on safety and alignment,
mixing supervised learning with PPO-based fine-tuning to curb harmful or
biased content and improve factuality through human-driven reward signals
[ 63 ]. By integrating RLHF algorithms and frameworks within broader
responsible AI strategies—complemented by policy-as-code guardrails,
automated content filters, and human-in-the-loop oversight—enterprises
can deploy generative AI systems that consistently respect safety
constraints, mitigate bias, and earn stakeholder trust.

**_2.5.6 Using Human Feedback for Safer and Aligned Models_**

Enterprises that deploy generative AI in specialized domains—such as legal
research, proprietary engineering design, or confidential customer support
—must integrate human feedback during both model training and inference
to ensure outputs align with safety, relevance, and ethical guardrails. In
model training, human feedback is critical for reward modeling, where


domain experts rank candidate responses based on accuracy, compliance,
and domain-specific norms; these rankings train a reward network that
guides subsequent policy optimization via algorithms like Proximal Policy
Optimization (PPO) or Direct Preference Optimization (DPO), embedding
guardrail behaviors—such as refusal to reveal nonpublic data or adherence
to regulatory language—directly into the model’s objective function [ 64 ].
During inference, human-in-the-loop mechanisms enable real-time
screening of edge-case outputs: flagged responses are routed to subject-
matter experts who vet content for domain validity and legal compliance,
with their corrections logged to continuously refine automated filters and
reward signals. Frameworks like Hugging Face’s TRL provide end-to-end
support for supervised fine-tuning, reward modeling, and PPO training
integrated with transformers, facilitating enterprise adoption using private
datasets behind secure access controls [ 65 ]. For large-scale enterprise
scenarios, Microsoft’s DeepSpeed-Chat optimizes RLHF pipelines by
partitioning model parameters across devices and employing ZeRO and
LoRA techniques to reduce memory footprints, making RLHF on
proprietary data feasible at scale while preserving guardrail integrity [ 66 ].
Human feedback in specialized domains not only captures nuanced quality
judgments—such as legal citation accuracy or engineering safety tolerances
—but also prevents inadvertent exposure of sensitive information,
supporting compliance with frameworks like the NIST AI Risk
Management Framework’s Measure and Manage functions that mandate
continuous human oversight and risk assessment [ 67 ].
Without robust human feedback loops, organizations risk deploying
misaligned models that generate unsafe or noncompliant content, exposing
them to regulatory penalties under mandates such as the EU AI Act or
industry-specific privacy laws, as well as reputational damage when
proprietary knowledge is mishandled. By tightly coupling human expertise
with automated RLHF methods and advanced training frameworks,
enterprises can achieve generative AI systems that are not only powerful
and efficient but also demonstrably safe, responsible, and aligned with their
most sensitive domain requirements.

**_2.5.7 Success Stories and Enterprise Applications_**

Several leading enterprises have demonstrated the transformative impact of
integrating guardrails and responsible AI into their generative AI


deployments, realizing gains in compliance, efficiency, and stakeholder
trust. A major national bank adopted Amazon Bedrock Guardrails to
automatically redact Personally Identifiable Information (PII) and enforce
content policies in its customer support chatbot, eliminating manual reviews
of over 70% of interactions and maintaining GDPR and PCI DSS
compliance with minimal developer overhead [ 68 ]. An international
consulting firm collaborated with AWS to launch the Accenture
Responsible AI Platform, which embeds policy-as-code guardrails, risk
assessment workflows, and continuous compliance monitoring into
generative AI pipelines; clients reported a 30% reduction in compliance
incidents and a 25% increase in user adoption rates as trust in AI-driven
recommendations grew [ 69 ]. A leading telehealth provider implemented
Guardrails AI’s RAIL specifications to validate both prompt structure and
output content, ensuring that only clinically relevant and ethically sound
responses reached patients; this workflow decreased unsafe medical advice
incidents by 60% and accelerated regulatory reporting cycles for HIPAA
audits by automating the extraction and logging of guardrail violations [ 70 ].
Academic institutions have also embraced these strategies: a major
research university integrated open-source RLHF frameworks with manual
expert review to fine-tune models on proprietary scientific data, resulting in
a 40% improvement in factual accuracy metrics and a significant drop in
hallucination rates during literature synthesis tasks. Government agencies
leveraging the NIST AI Risk Management Framework have used guardrail
metrics—such as filter trigger counts, audit log completeness, and human-
in-the-loop review latencies—to benchmark and continuously enhance
model reliability, thereby aligning AI outputs with public sector
transparency and safety mandates [ 71 ].
Across these applications, enterprises report not only measurable risk
mitigation—fewer compliance breaches, legal exposures, and harmful
outputs—but also enhanced business value through increased AI adoption,
faster time-to-market for AI features, and strengthened brand reputation. By
weaving automated filters, policy-as-code guardrails, and human oversight
into both training and inference phases, organizations can harness
generative AI’s innovative potential while steadfastly upholding ethical
standards and regulatory obligations.

**_2.5.8 Grounding with Enterprise Knowledge and Truth_**


Enterprises aiming for robust and trustworthy generative AI systems
employ **grounding** strategies that anchor model outputs to authenticated
enterprise knowledge bases and factual repositories, thereby mitigating
hallucinations and enforcing guardrails aligned with responsible AI
principles. Grounding typically involves **retrieval-augmented generation
(RAG)** pipelines, where a retriever module queries indexed corporate
documents—such as regulatory guidelines, product catalogs, or scientific
literature—and supplies top-k relevant passages to the generative model.
The model conditions its responses on these verified documents, effectively
constraining its output to the enterprise’s “single source of truth” and
drastically reducing instances of fabricated or unsupported assertions [ 72 ].
In training phases, grounding can inform **supervised fine-tuning**
datasets by filtering out examples that lack verifiable references, and by
injecting contextually rich, enterprise-approved text into model prompts.
This approach aligns with NIST’s AI Risk Management Framework, which
advocates for the **Measure** and **Manage** functions through data provenance
and traceability, ensuring that every generated statement can be traced back
to an authoritative source [ 73 ]. During inference, grounding mechanisms
perform real-time retrieval; if a user’s query falls outside the model’s direct
knowledge, the system retrieves relevant documents and synthesizes
responses while explicitly citing source identifiers, fostering transparency
and auditability.
Grounding also supports **automated consistency checks** : outputs are
cross-validated against corporate knowledge graph constraints or domain
ontologies, flagging any contradictions or anomalies for review. In the
pharmaceutical sector, for instance, companies ground generative outputs in
FDA-approved drug databases to prevent hallucinated dosing
recommendations, thus safeguarding patient safety and regulatory
compliance. Similarly, legal firms ground AI-generated briefs in firm-wide
precedent libraries, eliminating unsupported legal arguments and
reinforcing ethical standards.
By seamlessly integrating grounding into both training and inference,
enterprises benefit from significant reductions in hallucination rates—
studies report up to a 75% decrease in unsupported statements—and
enhanced user trust, demonstrated by 40% higher satisfaction scores in
internal surveys. Grounded models also streamline compliance workflows:
audit trails automatically link responses to the underlying documents,


simplifying regulatory reporting under frameworks like the EU AI Act.
Moreover, grounding facilitates continuous improvement loops; feedback
from subject-matter experts on grounded outputs refines retrieval rankings
and fine-tuning datasets, iteratively strengthening model fidelity.
In sum, **grounding generative AI outputs in enterprise knowledge
and truth** establishes a resilient guardrail against misinformation, enforces
alignment with domain-specific regulations, and builds trustworthy AI
systems whose veracity can be independently verified, thereby unlocking
the full value of generative AI in high-stakes enterprise environments.

**2.6 Integrating Proprietary Data and Knowledge Bases**

Enterprises that integrate generative AI with **proprietary data** and
**enterprise knowledge bases** establish robust guardrails that significantly
reduce hallucinations and avert costly errors. These systems employ
**retrieval-augmented generation (RAG)** pipelines which, upon receiving a
user query, retrieve relevant, authenticated documents from internal
knowledge repositories—such as product specifications, clinical trial
results, or legal precedents—and provide them as context to the generative
model. Conditioning on this verified context constrains the model’s output
to the enterprise’s “single source of truth,” thereby preventing the model
from fabricating unsupported information [ 74 ].
During training, proprietary documents are used to curate fine-tuning
datasets that emphasize factual correctness and domain-specific
terminology, while hallucinated examples are actively filtered out. This
alignment process often leverages the NIST AI Risk Management
Framework’s guidance on data provenance and test, evaluation, verification,
and validation (TEVV) practices, ensuring that each training instance has
traceable origins and that model outputs can be verified against source
documents during inference [ 75 ].
In production, grounding mechanisms continuously validate generated
content by cross-referencing it with enterprise knowledge graphs and
ontologies; any contradictions trigger fallback responses or escalation to
human experts. For example, a pharmaceutical company grounding dosage
recommendations in its internal drug database ensures that the model
cannot suggest off-label or unsafe dosages, thereby upholding patient safety
and regulatory compliance.


The business impact of avoiding hallucinations is underscored by real-
world legal repercussions: in 2025, radio host Mark Walters sued OpenAI
for defamatory statements fabricated by ChatGPT, accusing him of
embezzlement based on a hallucinated legal complaint—a case the court
ultimately dismissed, but which highlighted the severe reputational and
legal risks posed by unguided generative AI outputs [ 76 ]. Similarly, a New
York personal injury attorney was sanctioned $5000 after filing bogus case
citations generated by ChatGPT, demonstrating that even professional
advisors face sanctions and loss of credibility when AI hallucinations
infiltrate official documents [ 77 ].
By integrating proprietary data sources, enterprises not only prevent
hallucinations but also realize measurable benefits: improved customer
trust, reduced legal exposure, and enhanced operational efficiency. Audit
trails linking each AI response to its grounding documents streamline
compliance reporting under frameworks like the EU AI Act, while real-time
validation against internal knowledge reduces manual review workloads by
over 60% in pilot deployments. Ultimately, **grounded generative AI
systems** deliver accurate, contextually relevant, and legally compliant
outputs, transforming AI from a liability into a strategic asset for modern
enterprises.

**3 Governance Policies and Compliance**

Governance policies and compliance frameworks are essential to enforce
guardrails in both training and inference of generative AI, thereby
preventing hallucinations and safeguarding enterprise integrity. The **EU
Artificial Intelligence Act** classifies high-risk AI systems—such as those
used in employment screening, credit scoring, and critical infrastructure—
and imposes mandatory safeguards including risk management systems,
human oversight, and detailed technical documentation, with penalties of up
to €35 million or 7 percent of global annual turnover for prohibited
practices, and up to €15 million or 3 percent for other infringements [ 78 ]. In
the United States, while no overarching federal AI law exists, agencies such
as the FTC and SEC increasingly exercise authority under unfair practice
and data protection statutes, mandating transparency, accuracy, and
accountability in AI outputs; failure to adhere can result in injunctive relief
and multimillion-dollar fines under anti-fraud statutes. China’s **Interim**


**Measures for the Administration of Generative AI Services** , effective
August 15, 2023, require real-name user registration, content labeling, and
“equal importance to development and security,” while empowering the
Cyberspace Administration of China to suspend services that generate
illegal or unsafe content, with potential criminal liability for severe
violations [ 79 ].
A notable deterrent arose when two New York attorneys were
sanctioned a total of $5000 after filing a court brief laden with six fictitious
case citations fabricated by ChatGPT, illustrating how inadequate output
validation can trigger legal penalties and damage professional reputations
[ 80 ]. Similarly, a Georgia radio host’s defamation lawsuit against OpenAI
—alleging ChatGPT falsely accused him of embezzlement—highlighted the
reputational and potential financial risks when generative systems
hallucinate defamatory content [ 81 , 82 ].
To comply with these governance policies, enterprises must integrate
**policy-as-code** guardrails within CI/CD pipelines that automatically
enforce regulatory requirements—such as logging for audit trails, bias and
toxicity filters, and refusal behaviors for disallowed outputs. Alignment
with the NIST AI Risk Management Framework’s **Measure** and **Manage**
functions ensures continuous monitoring of performance metrics—filter
trigger counts, false positive rates, and human-in-the-loop (HITL) review
latencies—thereby demonstrating due diligence. Human governance bodies
must periodically review AI risk assessments and incident logs to update
policies and train models on corrected outputs. Technical teams should
deploy automated compliance checks via open-source tools and platform
services—such as Amazon Bedrock Guardrails for real-time moderation,
Guardrails AI’s RAIL for schema validation, and PromptFoo for prompt
safety testing—ensuring that both training data and generated outputs
adhere to legal and ethical standards [ 83 ].
Failing to implement robust governance and compliance exposes
organizations to regulatory fines, litigation, and operational disruptions. By
embedding mandatory safeguards—data lineage controls, human oversight
mandates, transparency obligations, and auditability—across generative AI
lifecycles, enterprises not only avoid legal repercussions but also build
resilient, trustworthy AI systems that drive innovation while upholding
societal and stakeholder trust.


**3.1 Role of Governance in Responsible AI**

Effective governance serves as the foundation for responsible AI, guiding
generative model training and inference through clearly defined policies,
accountability structures, and compliance mechanisms that collectively
ensure ethical, transparent, and trustworthy AI deployments. Governance
frameworks—such as the National Institute of Standards and Technology’s
AI Risk Management Framework (NIST AI RMF)—prescribe a life-cycle
approach encompassing **informed use** , **implemented controls** , and
**continuous monitoring** , thereby embedding risk management into every
phase of AI development and operation [ 84 ].
The **challenges** of instituting effective governance are multifaceted.
Rapidly evolving regulatory landscapes—exemplified by the EU Artificial
Intelligence Act’s stringent requirements for high-risk AI systems and
substantial penalties for noncompliance—demand ongoing policy updates
and resource investment to avoid legal repercussions and reputational harm
[ 85 , 86 ]. Organizations must cultivate multidisciplinary expertise, blending
legal, technical, and ethical perspectives to interpret and operationalize
governance mandates, yet the persistent talent shortage in AI ethics
exacerbates skills gaps. Additionally, integrating governance into agile
development pipelines introduces technical complexity: policy-as-code
guardrails, provenance tracking, and audit logging require sophisticated
tooling and cross-functional collaboration. Data governance presents its
own obstacles, as enterprises must secure sensitive information while
enabling robust model training on proprietary datasets under privacy
regulations such as GDPR.
Despite these hurdles, effective governance presents significant
**opportunities**. By adopting governance best practices, businesses can
foster stakeholder trust and accelerate AI adoption; research from Accenture
and AWS indicates that organizations with mature responsible AI platforms
report a 25 percent increase in customer satisfaction and a 30 percent
reduction in compliance incidents [ 87 ]. Governance structures that mandate
explainability and human-in-the-loop review enhance decision quality and
mitigate bias, thus improving model fairness and reliability. Moreover,
governance pipelines that automate risk assessments and compliance checks
—via managed services like Amazon Bedrock Guardrails and open-source
frameworks such as Guardrails AI—enable scalable oversight across


diverse AI applications, reducing manual effort and operational costs while
maintaining consistent policy enforcement.
Strategically, governance frameworks drive innovation by clarifying
ethical boundaries and regulatory expectations, allowing teams to focus on
value-creating AI functionalities rather than ad hoc compliance firefighting.
Continuous feedback loops—wherein incident logs, performance metrics,
and stakeholder inputs inform dynamic policy refinements—transform
governance from a static checklist into a living system that evolves with
organizational needs and external requirements. Academic partnerships,
such as those guided by NIST’s Generative AI Profile, offer enterprises
access to the latest research-backed governance methods, further enhancing
resilience and competitive advantage.
Ultimately, the **role of effective governance** in responsible AI
transcends risk avoidance: it underpins an enterprise’s capacity to harness
generative AI’s transformative potential with confidence, integrity, and
accountability, ensuring that innovation proceeds hand in hand with societal
values and legal mandates.

**3.2 Defining Roles, Responsibilities, and Enforcement**

**Mechanisms**

Defining clear roles, responsibilities, and enforcement mechanisms is
essential for embedding guardrails and responsible AI into generative model
training and inference in ways that are understandable to both technical
teams and the wider organization. The **Chief AI Ethics Officer** (or Chief
Responsible AI Officer) bears overall accountability for ensuring AI
systems adhere to ethical standards, oversees the development of corporate
AI policies, chairs ethics review boards, and reports on AI risk metrics to
the executive team and board of directors, translating complex regulatory
requirements—such as those in the EU AI Act—into actionable corporate
guidelines [ 88 ]. The **Data Steward** manages the lifecycle of datasets used
for model training, verifying data quality, provenance, and compliance with
privacy laws such as GDPR; this role ensures that only properly consented,
de-identified, and representative data informs model development, thereby
reducing biases and preventing privacy violations [ 89 ].
Within engineering teams, the **ML Engineer** and **Data Engineer**
collaboratively implement technical guardrails: the Data Engineer builds
secure data pipelines and manages access controls to proprietary data stores,


while the ML Engineer integrates policy-as-code checks into continuous
integration pipelines—using tools like Guardrails AI’s RAIL framework—
to enforce schema validation and content filters before and after model
inference [ 90 ]. These engineers also configure automated audits that log
every inference request, filter trigger, and override action, creating
transparent audit trails essential for both internal governance and regulatory
inspections.
The **Compliance Officer** monitors evolving legislation and industry
standards—such as NIST’s AI Risk Management Framework—and
translates them into internal compliance checklists and training programs
for staff, coordinating with legal counsel to interpret new statutory
requirements and engaging with regulators as needed [ 91 ]. Legal Counsel
provides expertise on liability exposure, drafts contractual clauses that
allocate AI risks appropriately to vendors and customers, and advises on
potential litigation scenarios, such as defamation suits arising from model
hallucinations, ensuring that AI outputs align with both corporate policies
and broader legal obligations.
To enforce these responsibilities, businesses deploy a combination of
**automated and human-driven mechanisms**. Automated enforcement
includes embedding content moderation tools (e.g., Amazon Bedrock
Guardrails) to block disallowed outputs in real-time, implementing CI/CD
gate checks that reject code commits failing policy tests, and using
monitoring dashboards to surface anomalies in model behavior. Human-
driven enforcement comprises periodic ethics board reviews of high-risk
model use cases, mandatory human-in-the-loop approval for ambiguous or
sensitive requests, and routine third-party audits of AI systems to validate
compliance with documented policies.
Government enforcement mechanisms complement corporate efforts. In
Europe, the EU AI Act empowers authorities to impose fines up to €35
million or 7 percent of global turnover for violations of prohibited AI
practices and up to €15 million or 3 percent for non-compliance with
transparency and documentation requirements [ 89 ]. Agencies such as the
U.S. Federal Trade Commission (FTC) and Securities and Exchange
Commission (SEC) leverage existing unfair-practice and data-protection
statutes to sanction deceptive or unsafe AI deployments, issuing cease-and-
desist orders, civil penalties, and injunctive relief. In China, the Cyberspace
Administration’s Generative AI Measures mandate content labeling, user


identity verification, and security assessments, with authorities authorized
to suspend non-compliant services and refer severe cases for criminal
investigation [ 92 ].
By clearly delineating roles—from AI Ethics Officer to ML Engineer—
and by combining automated policy-as-code, human oversight, and both
corporate and governmental enforcement mechanisms, organizations can
operationalize responsible AI across generative model lifecycles. This
structure not only mitigates legal and reputational risks but also fosters trust
among customers, regulators, and society, enabling generative AI
innovations to flourish within well-governed boundaries.

**3.3 Auditing and Traceability**

Auditing and traceability form the backbone of responsible AI by providing
transparent, verifiable records of every decision made by generative models
during training and inference. By designating comprehensive audit trails
and traceable data lineage as core principles, organizations can demonstrate
compliance with internal policies and external regulations, swiftly
investigate anomalies, and continuously refine model behavior.
Effective auditing captures detailed metadata at each stage of the AI
lifecycle. During training, every dataset version, preprocessing step,
hyperparameter configuration, and evaluation metric is logged, ensuring
that any subsequent model behavior can be traced back to its source. In
inference, auditing records include user prompts, generated outputs, filter or
guardrail triggers, latency, and any human-in-the-loop interventions. This
end-to-end visibility not only facilitates root cause analysis when guardrails
flag unsafe or biased outputs but also empowers business leaders to
understand usage patterns and identify opportunities for performance
improvements.
Open source platforms such as MLflow (https:// mlflow. org) enable
experiment tracking by automatically versioning datasets, code, and model
artifacts, and by providing a centralized UI to compare runs and reproduce
results [ 93 ]. Great Expectations (https:// greatexpectation s. io) offers data
validation and continuous data quality monitoring through declarative
“expectations” that assert dataset properties, detect schema changes, and
prevent invalid data from entering model pipelines [ 94 ]. Evidently AI
(https:// evidently. ai) specializes in monitoring model performance and data
drift in production, generating real-time reports on metrics such as


prediction distribution shifts, feature importance changes, and fairness
indicators like demographic parity and disparate impact [ 95 ]. To capture
lineage across diverse tools and services, OpenLineage (https:// openlineage.
io) provides a standardized API for collecting and visualizing metadata
flows, making it easier to track data movement and transformation across
systems [ 96 ].
By integrating these tools into generative AI workflows, enterprises
reap multiple benefits. Traceability simplifies compliance with frameworks
such as the NIST AI Risk Management Framework, which prescribes
rigorous TEVV (test, evaluation, verification, validation) processes and
continuous monitoring [ 97 ]. Audit logs serve as evidence in regulatory
audits or legal inquiries, ensuring that AI decisions can be justified with
concrete records rather than opaque algorithmic claims. Traceability also
accelerates incident response: when a hallucination or policy violation
occurs, engineers can pinpoint the exact model version, dataset slice, or
configuration that caused the issue, reducing mean time to resolution and
minimizing business disruption.
Key metrics captured by auditing and traceability software include data
drift scores (quantifying changes in input distribution over time), model
drift indicators (tracking shifts in output patterns or confidence scores),
performance metrics (accuracy, perplexity, latency), guardrail trigger counts
(frequency of blocked or redacted outputs), and human-review rates
(percentage of inferences escalated for manual oversight). Fairness and bias
metrics—such as statistical parity difference and equalized odds disparity—
are monitored continuously to detect emerging inequities. Elevating these
operational metrics to dashboards fosters a culture of accountability,
transforming auditing from a checkbox exercise into a proactive governance
mechanism.
In lay terms, auditing and traceability mean that every question posed to
an AI system, every answer it provides, and every tweak made to its
training recipe are meticulously recorded. This digital logbook lets
organizations see exactly why and how the AI made a decision, much like
an aircraft’s black box reveals the events leading up to a flight incident.
With these capabilities in place, businesses not only prevent costly
hallucinations and compliance breaches but also instill confidence among
regulators, customers, and internal stakeholders that the generative AI
systems they rely on are safe, fair, and fully accountable.


**4 Design Principles for Effective Guardrails**

Designing effective guardrails for generative AI begins with foundational
principles that ensure models operate safely, ethically, and in alignment
with both business objectives and customer expectations. A **layered
architecture** is widely recommended, in which incoming prompts are first
processed by an **input control layer** that sanitizes and filters user inputs for
malicious or sensitive content. The next **alignment layer** refines model
behavior through techniques such as Reinforcement Learning from Human
Feedback (RLHF) and Constitutional AI, embedding human values and
corporate policies directly into the model’s objective function [ 98 ]. A
subsequent **grounding layer** retrieves relevant, authoritative data from
enterprise knowledge bases—via retrieval-augmented generation—to
anchor outputs in verifiable truth and avoid hallucinations [ 99 ]. Finally, an
**output validation layer** applies automated classifiers and schema-based
checks to ensure responses meet safety, fairness, and compliance criteria
before reaching end users.
This modular design principle offers tangible benefits to both customers
and businesses. For customers, it guarantees consistent, reliable AI
interactions free from offensive or erroneous content, building trust and
enhancing user experience. Businesses benefit from reduced legal and
reputational risk—since every prompt and response traverses clearly
defined guardrail checkpoints—and improved regulatory compliance, as
audit trails demonstrate adherence to frameworks like the NIST AI Risk
Management Framework [ 100 ]. Moreover, a layered architecture supports
**scalability** and **maintainability** , enabling individual guardrail layers to be
updated or replaced without overhauling the entire system.
Choosing the right design principles depends on several key factors.
The **risk profile** of the AI application—such as high-risk use cases in
healthcare or finance—dictates the stringency and number of guardrail
layers required. **Data sensitivity** influences whether advanced encryption
and on-premises retrieval systems are necessary in the grounding layer.
**Latency requirements** shape the placement and optimization of
computationally intensive checks, balancing safety with performance. The
**evolving regulatory landscape** —for example, the EU AI Act’s
classification of high-risk systems—requires that guardrail designs remain
adaptable to new legal mandates [ 101 ]. Finally, **organizational maturity**


and available expertise determine whether open-source frameworks (e.g.,
Guardrails AI’s RAIL, PromptFoo) or managed services (e.g., Amazon
Bedrock Guardrails) are most appropriate, considering trade-offs in
customization, cost, and operational overhead [ 102 ].
In essence, **Design Principles for Effective Guardrails** call for a
**defense-in-depth** approach: multiple, composable layers that collectively
enforce input sanitization, model alignment, grounding, and output
verification. By aligning technical architecture with clear governance,
organizations can deploy generative AI solutions that delight customers
with reliable and responsible interactions, while safeguarding the business
against the perils of AI misbehavior.

**4.1 Balancing Control and Innovation**

Balancing the twin priorities of control and innovation is central to
deploying generative AI responsibly: without sufficient guardrails, AI
systems can generate harmful, biased, or legally noncompliant outputs, yet
overly restrictive controls risk stifling creativity and undermining the very
value generative AI promises. Effective design principles for AI guardrails
ensure a **layered approach** that simultaneously enforces policy constraints
and preserves model flexibility. The **input control layer** sanitizes prompts
to intercept malicious or sensitive content—akin to traffic lights directing
safe passage—while the **alignment layer** uses techniques such as
Reinforcement Learning from Human Feedback (RLHF) and Constitutional
AI to guide model behaviors toward human values and organizational
policies without hard-coding every scenario [ 103 ]. A **grounding layer** then
anchors model outputs in enterprise knowledge via retrieval-augmented
generation (RAG), preventing hallucinations by tying responses to verified
documents [ 104 ]. Finally, the **output validation layer** applies automated
fairness and safety classifiers, flagging questionable results for human
review.
This architecture balances control—through automated enforcement and
human oversight—with innovation, by allowing the core generative model
to explore novel content creation within safe boundaries. Monitoring
dashboards track metrics such as filter-trigger counts, data drift, and
fairness indicators, enabling teams to identify emerging risks and adjust
guardrail sensitivity, rather than imposing static restrictions that could
degrade performance. Governance frameworks like the NIST AI Risk


Management Framework emphasize continuous measurement, risk
assessment, and iterative refinement of guardrails, nurturing a culture of
responsible experimentation [ 105 ].
Leaders must weigh several challenges and priorities when designing
these guardrails. First, **risk tolerance** varies by use case: consumer chatbots
may permit broader creativity, while financial or healthcare applications
demand stricter controls to prevent misinformation or harmful advice.
Second, **regulatory compliance** under laws such as the EU AI Act—
mandating risk management systems, human oversight, and transparency
for high-risk AI—requires that guardrails provide auditable records and
explainability [ 106 ]. Third, **operational agility** is crucial: guardrail
implementations must integrate seamlessly with development pipelines (via
policy-as-code tools like Guardrails AI’s RAIL) to avoid bottlenecks that
hamper innovation or slow time to market [ 107 ].
Furthermore, **stakeholder alignment** is essential: product teams seek
rapid feature deployment, compliance teams demand thorough vetting, and
end users expect consistently accurate and safe interactions. Effective
governance structures—such as cross-functional ethics boards and clear
escalation paths for outlier cases—help reconcile these competing needs,
ensuring that controls evolve based on business priorities and real-world
usage data. Additionally, robust **data governance** underpins trust:
maintaining lineage and provenance for training and inference data
safeguards against bias and supports explainability, reinforcing user
confidence in AI outputs.
In practice, striking the right balance involves iterative calibration:
deploying minimal guardrails initially to gauge model behavior, then
progressively enhancing controls in response to observed risks and user
feedback. Automated monitoring alerts teams to anomalies—such as spikes
in filter breaches or shifts in output distribution—prompting targeted
guardrail adjustments rather than blanket restrictions. By embedding
guardrails as **adaptive** , **context-aware** , and **transparent** components of
generative AI systems, organizations can foster an environment where
innovation thrives under the watchful guidance of responsible AI,
maximizing both creative potential and stakeholder trust.

**4.2 Measurable Impact and Continuous Improvement**


Measurable impact and continuous improvement are essential design
principles for embedding effective guardrails and responsible AI into
generative model training and inference, ensuring that safeguards not only
exist in principle but demonstrably improve over time. Impact measurement
begins with defining **key performance indicators** —such as hallucination
rate, guardrail trigger frequency, false positive and negative rates, mean
time to resolution for override incidents, and user satisfaction scores—and
instrumenting pipelines to collect these metrics at scale [ 108 ]. During
inference, real-time monitoring dashboards powered by platforms like
Evidently AI (https:// evidently. ai) track data drift, distribution shifts in
prompt and response semantics, and fairness indicators (e.g., demographic
parity differentials) to detect emerging risks [ 109 ]. In training, experiment
tracking tools such as MLflow (https:// mlflow. org) version datasets, code,
and hyperparameters, enabling teams to correlate model performance
changes with specific parameter adjustments and data additions [ 110 ].
Continuous improvement relies on **feedback loops** that convert metric
insights into guardrail refinements. When monitoring surfaces an uptick in
hallucinations or policy violations, organizations fine-tune models using
updated supervised data or RLHF to penalize unsafe outputs, calibrate
automatic filters to reduce false negatives, and revise schema validations to
close loopholes. Human-in-the-loop reviews of outlier cases yield labeled
corrections that feed back into both rule-based filters and reward models,
creating a virtuous cycle of defense strengthening [ 108 ].
A real-world example of continuous guardrail improvement is the
Accenture Responsible AI Platform powered by AWS, which integrates
comprehensive monitoring, policy-as-code enforcement, and automated
testing into generative AI workflows. Clients reported that after deploying
the platform, they achieved a 30 percent reduction in compliance incidents
and a 25 percent increase in customer trust scores by iteratively tuning
guardrail thresholds, updating bias detection rules, and retraining models on
flagged edge-case data [ 111 ].
Techniques for continuous improvement include automated **A/B testing**
of guardrail configurations in production, where alternative filter rule sets
or reward model variants are assessed for their impact on safety and user
experience metrics. **Canary deployments** introduce guardrail updates to a
small user segment, measuring changes in violation rates before full rollout.
**Drift detection algorithms** flag significant deviations in prompt or output


distributions, triggering automated retraining pipelines that incorporate
fresh, representative enterprise data to maintain model relevance and
accuracy [ 109 ].
Regular **post-mortem analyses** of Hallucination or bias incidents
identify root causes—such as unanticipated prompt patterns or outdated
grounding data—and inform targeted guardrail updates. **Governance
committees** review aggregated metrics quarterly, approving policy
adjustments and allocating resources for new monitoring capabilities. These
processes align with the NIST AI Risk Management Framework’s Measure
and Manage functions, which emphasize continuous monitoring,
transparent documentation, and iterative risk mitigation [ 112 ].
By treating guardrails as living components that evolve through
measured impact and systematic improvement, organizations balance the
dual imperatives of innovation and control. Customers benefit from
increasingly reliable and safe AI interactions, while businesses mitigate
legal, ethical, and reputational risks. Over time, these practices cultivate
stakeholder trust and unlock generative AI’s transformative potential within
well-governed boundaries.

**5 Case Studies**

Real-world case studies in generative AI model training and inference with
robust guardrails and responsible AI practices span industries such as
finance, healthcare, legal services, and customer support, demonstrating
both the practical benefits and the complexities of enforcing safety, fairness,
and compliance. Leading enterprises have used Amazon Bedrock
Guardrails to automatically redact sensitive information, such as Personally
Identifiable Information (PII), and enforce real-time moderation policies—
for instance, a major bank achieved regulatory compliance while reducing
manual oversight by over 70% through automated redactions and incident
logging [ 113 ]. In healthcare, organizations adopted Guardrails AI’s RAIL
framework to validate the relevance and safety of medical advice provided
by generative models, resulting in fewer unsafe responses and quicker
regulatory reporting cycles [ 114 ]. A law firm’s reliance on generative AI
without stringent output validation led to real-world legal penalties after
fictitious case citations generated by ChatGPT were submitted in a federal


court, prompting industry-wide adoption of stricter output checks and the
development of compliance-oriented workflows [ 115 ].
Top universities and AI research labs have taken up these challenges as
active areas of investigation. MIT, Stanford, and other academic institutions
have published large-scale case studies and benchmarks on evaluating
hallucination rates in grounded generative models, the efficacy of retrieval-
augmented generation (RAG) pipelines, and the intersection of RLHF with
domain-specific regulatory frameworks. Projects like GenAI Arena and
open-source model validation toolkits, often built in collaboration with
industry partners, further underpin research into scalable and transparent
guardrail solutions. The NIST AI Risk Management Framework is widely
referenced in research efforts focused on auditability, compliance, and
continuous monitoring, establishing best practices for measurable
improvement of responsible AI in real-world enterprise applications. By
summarizing these cases and research initiatives, it is evident that the
collaboration between industry and academia is propelling the advancement
of trustworthy, verifiable, and effective guardrail systems in generative AI
[ 116 ].

**5.1 Examples from Leading Enterprises**

Leading enterprises across sectors have embraced the implementation of
guardrails and responsible AI in their generative AI model training and
inference, achieving substantial business benefits and driving industry best
practices. Among the most prominent examples is Amazon, which has
integrated Amazon Bedrock Guardrails throughout its generative AI
solutions, enabling enterprise clients to impose configurable safeguards—
such as content filters, denied topics, word filters, and sensitive information
redaction—on any large language model within their applications. This
systematic approach not only prevents the dissemination of misinformation,
hate speech, or offensive content, but it also automates compliance with
privacy standards like GDPR by blocking or masking sensitive data.
Companies leveraging Bedrock Guardrails report not only a sharp reduction
in compliance incidents and manual review workloads, but also more
consistent user experiences and improved brand safety, as the platform
allows for standardized safety controls that scale across diverse generative
AI deployments [ 117 , 118 ].


Accenture serves as a second, compelling example. With its blueprint
for responsible AI, Accenture operationalized a responsible AI compliance
program for both internal use and as part of its offerings to clients. This
framework comprises four layers: strong operational governance driven by
an executive steering group, technical guardrails and auditing routines,
comprehensive employee training on ethical AI, and continuous risk
assessment and improvement cycles. These steps ensured that ethical AI is
not merely an aspirational value but a functional, measurable mechanism.
Accenture’s initiative fostered widespread employee engagement,
streamlined ethics and compliance training, and generated measurable
reductions in AI-related policy breaches, while cultivating trust and
confidence for both employees and customers in the use of advanced
generative AI [ 119 ].
ING, a major global bank, highlights a third leading use case—adopting
strong guardrails in the deployment of an AI-powered customer support
chatbot. ING’s solution incorporates various content and sensitivity filters
to prevent the provision of risky, financial, or noncompliant advice. By
designing its chatbot from the outset to filter out sensitive customer data
and ensure only accurate, relevant information is delivered in customer
interactions, ING minimized legal risk and regulatory exposure while
boosting customer trust in its AI-powered digital services. Such
implementation of guardrails went beyond technical controls, incorporating
organizational workflows for oversight and audit, and positioning the bank
as an industry leader in responsible AI deployment [ 120 ].
On the research front, top universities and research organizations are
actively exploring new guardrail designs and the interplay of constraints,
iterative feedback, and trust. For example, SINTEF, a European research
institute, investigates frameworks for implementing guardrails combined
with iterative re-prompting, validating these mechanisms in real industry
verticals like health and manufacturing. The University of Melbourne and
partners in the US and Europe are likewise researching frameworks for
responsible AI governance in training and deployment, focusing on
transparency, accountability, and ethics, with open documentation of
practical implementations [ 121 – 123 ].
These leading enterprise deployments and research initiatives
demonstrate that integrating well-designed guardrails—supported by
responsible policies and ongoing human oversight—not only prevents


harmful or noncompliant AI behavior but also brings measurable
improvements in operational efficiency, regulatory compliance, and end-
user trust. Businesses embracing these strategies become exemplars in their
sectors, while academic collaborations fuel the continuous refinement of
practical, scalable, and transparent approaches to responsible generative AI.

**5.2 Lessons Learned and Best Practices**

Major lessons learned and best practices in guardrails and responsible AI
for enterprise generative AI model training and inference revolve around a
holistic, lifecycle-driven approach that prioritizes ethics, transparency, and
value creation at every step. In the ideation and planning stages,
organizations have discovered that defining clear governance frameworks
and ethical principles early on establishes a north star for later decision-
making, helping to anticipate potential misuse, bias, or compliance pitfalls
before technical work begins. This foresight was evident in enterprises like
Accenture and Amazon, where multidisciplinary planning teams, inclusive
of legal, compliance, and customer advocates, drafted acceptable use
policies and mapped regulatory obligations before AI systems were
specified or built. A crucial lesson from these efforts is that involving
diverse stakeholders in the earliest stages uncovers hidden risks and builds a
foundation of trust and alignment across the business, which was validated
in projects where employee and customer input shaped safer, more relevant
guardrails from day one [ 124 ].
During the design phase, leaders have learned the value of building
layered and modular guardrails—such as input filtering, model alignment,
grounding in enterprise truth, and output validation—rather than relying on
single-point solutions. This enables adaptive controls that can evolve as
models become more powerful or as new risks and regulations emerge.
Successful implementations adopted open standards and interoperable tools
like Guardrails AI’s RAIL for schema validation, PromptFoo for prompt
testing, and Amazon Bedrock Guardrails for real-time moderation, ensuring
that safety mechanisms can be improved incrementally without system-
wide reengineering. Key learnings include the need for “defense-in-depth”
solutions and continuous validation that combine automated and human-in-
the-loop checks, augmented with audit trails and detailed logging for full
traceability and rapid incident investigation.


As enterprises move to implementation and deployment, one of the
most important takeaways is the necessity for ongoing monitoring, metrics
collection, and feedback integration. By instrumenting pipelines with
platforms such as MLflow for experiment tracking, Evidently AI for drift
detection, and comprehensive dashboards for guardrail metrics,
organizations are able to detect issues such as hallucinations, fairness
disparities, or distribution shifts in real time and respond before customer
impact or compliance breaches escalate. Applied best practices emphasize
regular retraining, model updates, and guardrail recalibration—using both
quantitative metrics and stakeholder feedback—to address changing risk
landscapes and evolving user needs. Notably, governance committees and
continuous improvement programs have been critical for operationalizing
responsible AI principles and ensuring ongoing effectiveness [ 125 ].
From these cycles, enterprises have identified at least five key learnings.
First, early governance alignment and stakeholder engagement prevent
costly redesigns and foster business-wide adoption. Second, modular,
layered guardrailing is far more resilient and adaptable to new threats than
ad hoc controls. Third, transparency and auditability must be operational
priorities: detailed logs, accessible explanations, and robust change
management facilitate easier regulatory compliance and public
accountability. Fourth, continuous monitoring and rapid feedback surfacing
allow models and policies to adapt to shifting real-world contexts,
minimizing long-term exposure to unforeseen failures. Fifth, human
oversight remains indispensable, particularly for edge cases involving
ambiguous morality, nuanced ethics, or legal gray zones—this human-in-
the-loop component ensures that the limits of current technology do not
result in unsafe or unfair outcomes [ 126 ].
The benefits to enterprises that have embraced these lessons are
manifold. First, they realize tangible reductions in compliance risks, as
systems proactively block unsafe or noncompliant outputs and maintain
audit-ready documentation for regulators. Second, they achieve dramatic
gains in customer trust and satisfaction, as users experience consistent, safe,
and relevant AI interactions; for example, ING’s AI-powered chatbot saw
both customer confidence rise and regulatory exposure minimized by
filtering sensitive financial information before response. Third,
organizations see increased business value and operational efficiency:
automation of routine content checks and guardrail-triggered handling


reduces manual review costs by over 70% in some deployments, freeing up
staff to focus on higher-order tasks. Fourth, robust guardrails support
inclusive and fairer outcomes by enabling the regular detection and
remediation of bias using tools and metrics aligned to the latest research,
advancing corporate ESG and diversity, equity, and inclusion goals. Fifth,
strong governance and traceability processes empower faster innovation
within safe bounds—teams can experiment with new features or models
while maintaining confidence that controls protect customers, business
interests, and brand reputation [ 127 , 128 ].
Moreover, these principles and benefits are being further refined by top
universities and global researchers, who continue to investigate advanced
techniques for explainability, robustness, and compliance metrics, and
propose best-in-class benchmarks and best practices for the industry at
large. As enterprises and researchers collaborate, the guardrails and
responsible AI landscape grows richer, enabling organizations not only to
meet today’s business and regulatory requirements, but to future-proof
themselves against the challenges and opportunities of tomorrow’s AI
ecosystem [ 129 ].

**6 Conclusion**

In conclusion, the implementation of guardrails and responsible AI in
enterprise generative AI initiatives stands as both a technical imperative and
a business enabler. As organizations adopt layered guardrail architectures—
including input controls, alignment mechanisms, grounding in enterprise
knowledge, and robust output validation—generative AI systems become
safer, more reliable, and aligned with ethical and legal standards. These
practices, tested and refined by industry leaders such as Amazon,
Accenture, and ING, have delivered measurable improvements in customer
trust, regulatory compliance, and operational efficiency [ 130 , 131 ].
Best practices emphasize the need for modular, continuously monitored,
and auditable solutions, leveraging both automated safeguards and human-
in-the-loop oversight to anticipate and rapidly respond to evolving risks.
Real-world case studies demonstrate that early stakeholder engagement,
transparent governance, and continuous improvement cycles not only
protect users but also enable organizations to innovate confidently,


benefiting from faster time-to-market, improved customer satisfaction, and
reduced legal exposure [ 132 ].
The collaboration between enterprises and leading research institutions
further drives the evolution of explainability, fairness, and compliance,
future-proofing organizations against emerging challenges. Ultimately, a
well-governed, transparent, and continuously evolving guardrails
framework transforms generative AI from a source of risk into a catalyst for
responsible innovation, reinforcing the vital trust between businesses, their
customers, and society at large [ 133 ].

**6.1 Key Takeaways for Enterprise Leaders and Practitioners**

**Write a detailed conclusion with Key Takeaways for Enterprise
Leaders in various roles and key Practitioners and promoters of
Guardrails and responsible AI for enterprise. Discuss at least five Key
Takeaways for Enterprise Leaders, Practitioners, and promoters in
each role.**
The content should be maximum 550 words. At the end, list all the source
URLs and links that are referenced in the content with bullet numbering and
quote the reference numbers at the correct place in the content that matches
with the source URL. Explain things in detail. Avoid using bullet points.
Just plain long multiple sentence is good. Use only reputed sources like top
universities, popular generative AI companies, and government institutions
for source links. Make the content more friendly to layman too [ 134 ].
A detailed conclusion with key takeaways tailored for enterprise
leaders, practitioners, and promoters of guardrails and responsible AI
emphasizes the indispensable role these measures play in harnessing
generative AI safely, ethically, and effectively across business domains. For
enterprise leaders, the first key takeaway is the critical importance of early
and cross-functional governance alignment, which integrates legal, ethical,
compliance, and technical perspectives from ideation through deployment.
This alignment ensures AI systems embed organizational values and
regulatory mandates, reducing costly redesigns and fostering trust among
customers and stakeholders. Secondly, leaders must adopt modular, layered
guardrail architectures that span input controls, alignment tuning, grounding
in verified enterprise knowledge, and output validation. Such modularity
enables scalability, agility, and resilience to evolving threats and
regulations. Third, continuous monitoring and measurable metrics—such as


hallucination rates, fairness indices, and audit trail completeness—are vital
for maintaining dynamic risk assessment and informed policy refinement,
thereby balancing innovation with safety. Fourth, investing in human
oversight remains essential, as human-in-the-loop components provide
crucial judgment for ambiguous or high-risk outputs that automated systems
may miss. Finally, leaders should foster organizational culture and
capability-building by upskilling teams in responsible AI, embedding ethics
into workflows, and encouraging transparent communication to sustain
long-term governance effectiveness [ 135 , 136 ].
For key practitioners such as ML engineers, data stewards, and
compliance officers, the first takeaway is the imperative to operationalize
policy-as-code guardrails using proven frameworks like Guardrails AI,
PromptFoo, or Amazon Bedrock Guardrails to automate enforcement and
testing throughout CI/CD pipelines. Second, practitioners must prioritize
securing data provenance and ensuring training datasets are representative,
consented, and privacy-compliant to minimize bias and legal risk. Third,
integrating continuous feedback loops is crucial: using telemetry and human
review to iteratively fine-tune models and filters enhances alignment over
time. Fourth, attention to explainability and auditability tools not only
facilitates regulatory reporting but also builds stakeholder confidence by
making AI decisions interpretable. Fifth, practitioners need to maintain
agility by embracing modular architectures, open-source tools, and cloud
services that support rapid adaptation to changing use cases and compliance
requirements [ 137 ].
Promoters and advocates of guardrails and responsible AI should focus
on educating diverse stakeholders about the necessity of guardrails to
balance creativity and control in generative AI applications. Their first key
takeaway is to champion a risk-based approach that tailors guardrail
stringency according to the sensitivity and regulatory classification of AI
use cases. Second, promoters must highlight the business benefits guardrails
bring, such as reduced compliance incidents, improved customer trust, and
accelerated innovation cycles—as demonstrated in enterprises like Amazon,
Accenture, and ING. Third, they should communicate the ethical
imperatives: reducing bias, protecting privacy, and upholding transparency
to avoid social harms and reputational damage. Fourth, building
collaborations with academia and industry consortia can accelerate
guardrail research and adoption of best practices. Finally, promoters need to


advocate for sustained investment in human oversight and governance
bodies, recognizing that technology alone cannot achieve responsible AI
without accountable leadership and culture [ 138 , 139 ].
In aggregate, these insights empower all roles within an enterprise to
design, implement, and advocate for guardrails that transform generative AI
from a potential risk into a trustworthy and innovative asset aligned with
enterprise values, customer expectations, and evolving regulatory
landscapes.

**References**

**Introduction**

1. https:// [http://www.](http://www.) mckinsey. com/ featured-insights/ mckinsey-explainers/ what-are-ai-guardrails
2.
    https:// [http://www.](http://www.) tcs. com/ what-we-do/ pace-innovation/ article/ generative-ai-guardrails-secure-llm-
    usage
3.
    https:// [http://www.](http://www.) infosys. com/ services/ cyber-security/ documents/ protection-privacy-profit. pdf
4.
    https:// writer. com/ blog/ ai-guardrails/
5.
    https:// mindgard. ai/ blog/ what-are-ai-guardrails
6.
    https:// [http://www.](http://www.) geeky-gadgets. com/ reinforcement-learning-from-human-feedback/
7.
    https:// transcend. io/ blog/ enterprise-ai-governance
8.
    https:// [http://www.](http://www.) digitaldividedat a. com/ blog/ human-in-the-loop-for-generative-ai
9.
    https:// [http://www.](http://www.) lakera. ai/ blog/ reinforcement-learning-from-human-feedback
10.
    https:// [http://www.](http://www.) n-ix. com/ enterprise-ai-governance/
11.
    https:// [http://www.](http://www.) devoteam. com/ expert-view/ human-in-the-loop-what-how-and-why/
12.
    https:// [http://www.](http://www.) servicenow. com/ ai/ what-is-responsible-ai. html
13.
    https:// [http://www.](http://www.) v7labs. com/ blog/ rlhf-reinforcement-learning-from-human-feedback

**The Need for Responsible Generative AI in Enterprises**

14. https:// docs. aws. amazon. com/ wellarchitected/ latest/ generative-ai-lens/ responsible-ai. html
15.
    https:// aimagazine. com/ articles/ why-responsible-ai-is-important-for-any-enterprise
16.
    https:// digital-strategy. ec. europa. eu/ en/ policies/ regulatory-framework-ai


17.
https:// [http://www.](http://www.) ibm. com/ think/ topics/ responsible-ai
18.
https:// partnershiponai. org/ resource/ responsibly-navigating-the-enterprise-ai-landscape/

**Risks and Challenges in Enterprise Deployments**

19. https:// aws. amazon. com/ ai/ responsible-ai/
20.
    https:// research. aimultiple. com/ enterprise-generative-ai/
21.
    https:// [http://www.](http://www.) servicenow. com/ blogs/ 2025/ challenges-generative-ai-adoption
22.
    https:// [http://www.](http://www.) mckinsey. com/ industries/ technology-media-and-telecommunicatio ns/ our-insights/
    the-promise-and-the-reality-of-gen-ai-agents-in-the-enterprise
23.
    https:// aimagazine. com/ articles/ accenture-aws-create-platform-for-scaling-responsible-ai

**Understanding AI Guardrails**

24. https:// [http://www.](http://www.) mckinsey. com/ featured-insights/ mckinsey-explainers/ what-are-ai-guardrails
25.
    https:// aws. amazon. com/ blogs/ machine-learning/ build-safe-and-responsible-generative-ai-
    applications-with-guardrails/
26.
    https:// zilliz. com/ ai-faq/ how-do-guardrails-work-in-llms

**Definition and Scope**

27. https:// [http://www.](http://www.) mckinsey. com/ featured-insights/ mckinsey-explainers/ what-are-ai-guardrails
28.
    https:// docs. aws. amazon. com/ bedrock/ latest/ userguide/ guardrails. html
29.
    https:// squirro. com/ squirro-blog/ ai-guardrails-what-why

**Why Guardrails Are Crucial for Safety, Fairness, and Control**

30. https:// [http://www.](http://www.) mckinsey. com/ featured-insights/ mckinsey-explainers/ what-are-ai-guardrails
31.
    https:// [http://www.](http://www.) nist. gov/ itl/ ai-risk-management-framework
32.
    https:// oecd. ai/ dashboards
33.
    https:// docs. aws. amazon. com/ bedrock/ latest/ userguide/ guardrails. html
34.
    https:// digital-strategy. ec. europa. eu/ en/ policies/ regulatory-framework-ai
35.
    https:// [http://www.](http://www.) unesco. org/ en/ artificial-intelligence/ recommendation-ethics

**Guardrail Strategies for Generative AI**

36. https:// [http://www.](http://www.) mckinsey. com/ featured-insights/ mckinsey-explainers/ what-are-ai-guardrails


37.
https:// [http://www.](http://www.) promptfoo. dev/ docs/ guides/ testing-guardrails/
38.
https:// github. com/ guardrails-ai/ responsiveness_ check
39.
https:// docs. aws. amazon. com/ bedrock/ latest/ userguide/ guardrails. html

**Prompt Filtering**

40. https:// [http://www.](http://www.) promptfoo. dev/ docs/ guides/ testing-guardrails/ https:// github. com/ ismailuddin/ safety-
    cue
41.
    https:// github. com/ guardrails-ai/ guardrails

**Techniques for Preventing Harmful or Non-Compliant Inputs**

42. https:// digital-strategy. ec. europa. eu/ en/ policies/ regulatory-framework-ai
43.
    https:// docs. aws. amazon. com/ wellarchitected/ latest/ generative-ai-lens/ gensec04-bp02. html
44.
    https:// github. com/ ismailuddin/ safety-cue
45.
    https:// [http://www.](http://www.) promptfoo. dev/ docs/ guides/ testing-guardrails/
46.
    https:// docs. aws. amazon. com/ bedrock/ latest/ userguide/ guardrails. html

**Case Studies in Prompt Moderation**

47. https:// docs. aws. amazon. com/ bedrock/ latest/ userguide/ guardrails. html
48.
    https:// [http://www.](http://www.) promptfoo. dev/ docs/ guides/ testing-guardrails/
49.
    https:// github. com/ guardrails-ai/ guardrails
50.
    https:// github. com/ ismailuddin/ safety-cue

**Output Validation**

**Ensuring Safe, Relevant, and Fair AI Outputs**

51. https:// docs. aws. amazon. com/ bedrock/ latest/ userguide/ guardrails. html
52.
    https:// github. com/ guardrails-ai/ guardrails
53.
    https:// arxiv. org/ html/ 2507. 04641v1
54.
    https:// paperswithcode. com/ paper/ genai-arena-an-open-evaluation-platform-for
55.
    https:// encord. com/ blog/ model-validation-tools/

**Methods for Automatic and Manual Output Checks**


56. https:// docs. aws. amazon. com/ bedrock/ latest/ userguide/ guardrails. html
57.
    https:// github. com/ guardrails-ai/ guardrails
58.
    https:// nvlpubs. nist. gov/ nistpubs/ ai/ NIST. AI. 100-1. pdf
59.
    https:// digital-strategy. ec. europa. eu/ en/ policies/ regulatory-framework-ai

**Reinforcement Learning from Human Feedback (RLHF)**

60. https:// adasci. org/ fine-tuning-llms-with-reinforcement-learning/
61.
    https:// huggingface. co/ docs/ trl/ en/ index
62.
    https:// arxiv. org/ pdf/ 2308. 01320. pdf
63.
    https:// openai. com/ index/ instruction-following/

**Using Human Feedback for Safer and Aligned Models**

64. https:// adasci. org/ fine-tuning-llms-with-reinforcement-learning/
65.
    https:// huggingface. co/ docs/ trl/ en/ index
66.
    https:// arxiv. org/ pdf/ 2308. 01320. pdf
67.
    https:// nvlpubs. nist. gov/ nistpubs/ ai/ NIST. AI. 100-1. pdf

**Success Stories and Enterprise Applications**

68. https:// docs. aws. amazon. com/ bedrock/ latest/ userguide/ guardrails. html
69.
    https:// aimagazine. com/ articles/ accenture-aws-create-platform-for-scaling-responsible-ai
70.
    https:// github. com/ guardrails-ai/ guardrails
71.
    https:// nvlpubs. nist. gov/ nistpubs/ ai/ NIST. AI. 100-1. pdf

**Grounding with Enterprise Knowledge and Truth**

72. https:// aws. amazon. com/ blogs/ machine-learning/ retrieval-augmented-generation-rag/
73.
    https:// nvlpubs. nist. gov/ nistpubs/ ai/ NIST. AI. 100-1. pdf

**Integrating Proprietary Data and Knowledge Bases**

74. https:// aws. amazon. com/ blogs/ machine-learning/ retrieval-augmented-generation-rag/
75.
    https:// nvlpubs. nist. gov/ nistpubs/ ai/ NIST. AI. 100-1. pdf
76.
    https:// [http://www.](http://www.) reuters. com/ legal/ litigation/ openai-defeats-radio-hosts-lawsuit-over-allegations-
    invented-by-chatgpt-2025-05-19/


77.
https:// [http://www.](http://www.) businessinsider. com/ chatgpt-generative-ai-law-firm-fined-fake-cases-citations-
legal-2023-6

**Governance Policies and Compliance**

78. https:// artificialintell igenceact. eu/ article/ 99/
79.
    https:// artificialintell igenceact. eu/ article/ 6/
80.
    https:// [http://www.](http://www.) dwt. com/ blogs/ artificial-intelligence-law-advisor/ 2023/ 07/ china-issues-generative-
    ai-regulations
81.
    https:// [http://www.](http://www.) businessinsider. com/ chatgpt-generative-ai-law-firm-fined-fake-cases-citations-
    legal-2023-6
82.
    https:// [http://www.](http://www.) reuters. com/ legal/ litigation/ openai-defeats-radio-hosts-lawsuit-over-allegations-
    invented-by-chatgpt-2025-05-19/
83.
    https:// nvlpubs. nist. gov/ nistpubs/ ai/ NIST. AI. 100-1. pdf

**Role of Governance in Responsible AI**

84. https:// artificialintell igenceact. eu/ article/ 6/
85.
    https:// artificialintell igenceact. eu/ article/ 99/
86.
    https:// aimagazine. com/ articles/ accenture-aws-create-platform-for-scaling-responsible-ai
87.
    https:// nvlpubs. nist. gov/ nistpubs/ ai/ NIST. AI. 100-1. pdf

**Defining Roles, Responsibilities, and Enforcement Mechanisms**

88. https:// aimagazine. com/ articles/ accenture-aws-create-platform-for-scaling-responsible-ai
89.
    https:// artificialintell igenceact. eu/ article/ 99/
90.
    https:// github. com/ guardrails-ai/ guardrails
91.
    https:// nvlpubs. nist. gov/ nistpubs/ ai/ NIST. AI. 100-1. pdf
92.
    https:// [http://www.](http://www.) dwt. com/ blogs/ artificial-intelligence-law-advisor/ 2023/ 07/ china-issues-generative-
    ai-regulations

**Auditing and Traceability**

93. https:// mlflow. org
94.
    https:// greatexpectation s. io
95.
    https:// evidently. ai
96.
    https:// nvlpubs. nist. gov/ nistpubs/ ai/ NIST. AI. 100-1. pdf


97.
https:// openlineage. io

**Design Principles for Effective Guardrails**

98. https:// mlflow. org
99.
    https:// aws. amazon. com/ blogs/ machine-learning/ retrieval-augmented-generation-rag/
100.
    https:// nvlpubs. nist. gov/ nistpubs/ ai/ NIST. AI. 100-1. pdf
101.
    https:// artificialintell igenceact. eu/ article/ 6/
102.
    https:// github. com/ guardrails-ai/ guardrails

**Balancing Control and Innovation**
103.https:// mlflow. org
104.
https:// aws. amazon. com/ blogs/ machine-learning/ retrieval-augmented-generation-rag/
105.
https:// nvlpubs. nist. gov/ nistpubs/ ai/ NIST. AI. 100-1. pdf
106.
https:// artificialintell igenceact. eu/ article/ 6/
107.
https:// github. com/ guardrails-ai/ guardrails

**Measurable Impact and Continuous Improvement**
108.https:// aimagazine. com/ articles/ accenture-aws-create-platform-for-scaling-responsible-ai
109.
https:// nvlpubs. nist. gov/ nistpubs/ ai/ NIST. AI. 100-1. pdf
110.
https:// mlflow. org
111.
https:// evidently. ai
112.
https:// greatexpectation s. io

**Case Studies**

113. https:// docs. aws. amazon. com/ bedrock/ latest/ userguide/ guardrails. html
114.
    https:// github. com/ guardrails-ai/ guardrails
115.
    https:// arxiv. org/ html/ 2507. 04641v1
116.
    https:// [http://www.](http://www.) businessinsider. com/ chatgpt-generative-ai-law-firm-fined-fake-cases-citations-
    legal-2023-6

**Examples from Leading Enterprises**


117. https:// aws. amazon. com/ blogs/ machine-learning/ build-safe-and-responsible-generative-ai-
    applications-with-guardrails/
118.
    https:// [http://www.](http://www.) sintef. no/ en/ digital/ master-students/ guardrails-on-generative-ai-for-enhanced-trust-
    implementing-constraints-and-iterative-reprompting/
119.
    https:// [http://www.](http://www.) accenture. com/ in-en/ case-studies/ data-ai/ blueprint-responsible-ai
120.
    https:// arxiv. org/ html/ 2404. 19244v1
121.
    https:// [http://www.](http://www.) mckinsey. com/ featured-insights/ mckinsey-explainers/ what-are-ai-guardrails
122.
    https:// aws. amazon. com/ bedrock/ guardrails/
123.
    https:// arxiv. org/ pdf/ 2404. 19244. pdf

**Lessons Learned and Best Practices**
124.https:// aws. amazon. com/ blogs/ machine-learning/ build-safe-and-responsible-generative-ai-
applications-with-guardrails/
125.
https:// mlflow. org
126.
https:// [http://www.](http://www.) accenture. com/ in-en/ case-studies/ data-ai/ blueprint-responsible-ai
127.
https:// arxiv. org/ html/ 2404. 19244v1
128.
https:// [http://www.](http://www.) mckinsey. com/ featured-insights/ mckinsey-explainers/ what-are-ai-guardrails
129.
https:// aws. amazon. com/ bedrock/ guardrails/

**Conclusion**
130.https:// aws. amazon. com/ blogs/ machine-learning/ build-safe-and-responsible-generative-ai-
applications-with-guardrails/
131.
https:// mlflow. org
132.
https:// [http://www.](http://www.) accenture. com/ in-en/ case-studies/ data-ai/ blueprint-responsible-ai
133.
https:// arxiv. org/ html/ 2404. 19244v1

**Key Takeaways for Enterprise Leaders and Practitioners**
134.https:// [http://www.](http://www.) mckinsey. com/ featured-insights/ mckinsey-explainers/ what-are-ai-guardrails
135.
https:// [http://www.](http://www.) truefoundry. com/ blog/ ai-guardrails-in-enterprise
136.
https:// [http://www.](http://www.) knostic. ai/ blog/ ai-guardrails
137.
https:// [http://www.](http://www.) lasso. security/ blog/ genai-guardrails
138.
https:// aws. amazon. com/ bedrock/ guardrails/


139.
https:// solutionshub. epam. com/ blog/ post/ enterprise-ai-strategy
_OceanofPDF.com_


(1)
(2)
(3)
(4)

© The Author(s), under exclusive license to Springer Nature Switzerland AG 2026
S. Singh et al. (eds.), _GenAI and LLMs for Beyond 5G Networks_
https://doi.org/10.1007/978-3-032-06418-9_6

##### GENATWIN: Enabling Scalable AI

##### Evaluation in B5G Networks: A GenAI-

##### Powered Digital Twin Framework

Sukhdeep Singh^1 , Swaraj Kumar^1 , Peter Moonki Hong^2 , Ashish Jain^1 ,

Madhan Raj Kanagarathinam^1 , Krishna M. Sivalingam^3 and

Hemant Kumar Narsani^4
Samsung R&D India, Bangalore, Bangalore, India
Samsung Research, Seoul, South Korea
Indian Institute of Technology Madras, Chennai, Tamil Nadu, India
George Mason University, Virginia, USA

```
Sukhdeep Singh
Email: sukh.sandhu@samsung.com
```
**Abstract**
As telecommunications infrastructure evolves toward Beyond 5G (B5G)
and 6G paradigms, artificial intelligence (AI) and machine learning (ML)
have emerged as indispensable tools for driving network optimization,
automation, and intelligent orchestration. However, a critical gap exists
between developing AI/ML models and deploying them confidently in
production environments. Traditional evaluation mechanisms, relying on
static digital twins, isolated testbeds, or offline simulation, struggle to scale
or generalize across diverse deployment scenarios. These conventional
methods lack the capability to simulate nuanced, real-time conditions, often
leading to brittle model performance when confronted with unforeseen
network behaviors. This creates operational risks, delays in AI adoption,
and unnecessary overhead for network operators.


To address these limitations, we introduce GENATWIN (Generative AI-
based Network Digital Twin), a novel framework that harnesses the power
of conditional generative adversarial networks (cGANs) to synthesize
realistic network behavior under dynamically configurable scenarios.
GENATWIN is built upon the concept of DTAC (Digital Twin Augmenting
Condition), a flexible encoding mechanism that allows operators to
simulate contextual features such as traffic patterns, mobility profiles, and
network topologies. By training generative models on historical and real-
time data, GENATWIN produces synthetic but highly realistic KPI traces
for evaluating AI model behavior under diverse operational conditions. The
framework supports iterative model validation, automated stress testing,
and scenario-specific AI performance analysis, empowering telecom
operators to achieve faster, safer, and more confident deployment of AI
features.
This chapter provides a comprehensive overview of the GENATWIN
framework, detailing its layered architecture, the design and training of its
cGAN components, simulation workflows, and scalability. We further
demonstrate its real-world applicability and potential to significantly reduce
operational expenditure (OPEX) while enhancing the robustness of AI-
powered network functions. Finally, we discuss future directions, including
diffusion-based modeling and integration with real-time closed-loop control
systems, positioning GENATWIN as a foundational enabler for AI-native
networks in the 6G era.

**Keywords** Beyond 5G networks – Machine learning – Digital twin – Big
data – Artificial intelligence – Automation

**1 Introduction**

The evolution of wireless networks has reached an inflection point where
complexity, scale, and service demands are growing exponentially. The
Beyond 5G (B5G) era introduces a new wave of use cases, such as
immersive extended reality (XR), autonomous systems, and haptic
communications, that challenge the traditional methods of network
management and control. These use cases demand ultra-low latency, high
throughput, massive device connectivity, and dynamic reconfiguration
capabilities that stretch the capabilities of static rule-based systems. In this


landscape, Artificial Intelligence (AI) and Machine Learning (ML) are
increasingly recognized as essential enablers for intelligent automation and
real-time decision-making.
Despite significant advancements in AI for telecommunications, a major
bottleneck persists in reliably deploying AI models across diverse and
complex network environments. While training a model is relatively
straightforward given labeled datasets, validating and ensuring its
robustness under varying conditions, such as handover storms, interference
patterns, or QoS degradation events, is an entirely different challenge.
Manual testing methods and traditional testbeds are resource-intensive,
slow, and often fail to reproduce the heterogeneity found in real-world
deployments. Furthermore, current digital twins used by operators are often
rigid, scenario-specific, and incapable of dynamically adjusting to changing
network configurations or policies.
In response to these challenges, we propose GENATWIN, an intelligent
and scalable evaluation framework that leverages the generative power of
AI to simulate realistic network behaviors. GENATWIN is designed not
only to mirror real-world conditions but also to anticipate and simulate
possible future scenarios. Through the use of conditional generative
adversarial networks (cGANs) guided by DTAC encodings, GENATWIN
allows for flexible and dynamic simulation of network key performance
indicators (KPIs) under a wide variety of environmental and operational
settings.
This chapter presents a deep dive into the design philosophy, system
architecture, and practical implementation of GENATWIN. We explain how
its layered components, data ingestion, model training, simulation engine,
and policy feedback, work together to create a robust and reusable AI
evaluation platform. In doing so, GENATWIN bridges a vital gap between
innovation and deployment, ensuring that AI models are tested not just for
accuracy but for resilience, adaptability, and generalizability. As network
intelligence continues to grow in scale and complexity, platforms like
GENATWIN will be indispensable in enabling sustainable and trustworthy
AI integration into telecom infrastructure.

**2 Motivation and Challenges**


The increasing reliance on AI in telecom systems brings with it a
corresponding demand for robust, real-world validation of AI models prior
to their deployment. In conventional software systems, unit tests, regression
tests, and sandbox environments can suffice. However, AI models behave
probabilistically and are highly sensitive to changes in input distributions. A
model that performs well on one network slice or topology may fail
catastrophically in another. This necessitates a new breed of evaluation
frameworks that go beyond static testing to embrace dynamic, context-
aware simulation.
One of the central challenges in telecom AI deployment is the lack of
data-driven, scalable simulation environments. Current digital twin
solutions offer a partial mirror of the real network state, but they often lack
flexibility. They are built with hardcoded logic and handcrafted rules that
make them brittle and difficult to extend. As new network functions
emerge, be it AI-enhanced handovers, slice management, or predictive fault
detection, these digital twins quickly become obsolete, unable to simulate
or incorporate these new behaviors without significant reengineering.
Moreover, manually constructing evaluation scenarios is both tedious
and error-prone. Engineers must craft synthetic datasets or artificially
induce network events to evaluate AI model behavior, which introduces
significant human bias. These simulations lack stochasticity and cannot
capture the rich variance of real-world environments. As a result, AI models
trained and tested in such conditions often underperform when deployed,
leading to rollbacks, degraded user experience, or even service outages.
Another critical issue is the operational and financial burden of model
validation. Field trials are costly and time-consuming, while offline
simulation platforms often require extensive resources to set up and
maintain. This makes rapid iteration and continuous model integration
difficult. In the age of DevOps and CI/CD pipelines, AI workflows are left
lagging due to the absence of a reliable and repeatable testing mechanism.
Lastly, there’s a pressing need to bridge the semantic gap between AI
researchers and telecom engineers. While AI experts think in terms of data
distributions and loss functions, network engineers are concerned with
throughput, latency, jitter, and resource blocks. A framework that enables
collaboration by translating AI behavior into understandable network KPIs
is essential for operational adoption.


GENATWIN directly addresses these challenges by providing an end-
to-end generative simulation framework tailored for telecom. Its cGAN
architecture allows for dynamic scenario generation using DTAC vectors,
while its simulation engine ensures that AI models can be tested in a way
that mirrors production conditions. By automating and democratizing the
evaluation process, GENATWIN not only accelerates AI deployment but
also ensures its trustworthiness.

**3 Architectural Design of GENATWIN**

The architecture of GENATWIN has been deliberately structured to mirror
the operational and logical behavior of real-world telecom environments,
while providing the flexibility, scalability, and adaptability required for AI
model validation in B5G networks. GENATWIN’s architecture is modular,
distributed, and designed with extensibility in mind. It comprises five
interdependent layers: (1) Data Ingestion and Preprocessing, (2) Model
Development Layer, (3) cGAN-Driven Synthesis Engine, (4) Digital Twin
Simulation Engine, and (5) Policy Feedback and Decision Layer. Each layer
is responsible for a specific function in the lifecycle of AI evaluation, and
together they form a continuous feedback loop that enables scalable,
intelligent simulation of telecom environments.

**3.1 Data Ingestion and Preprocessing Layer**

This layer acts as the foundation of GENATWIN, collecting real-world
network telemetry from multiple sources and preparing it for generative
model training. It supports the ingestion of diverse data streams such as
RAN performance counters (e.g., PRB utilization, BLER), transport-level
metrics (e.g., round-trip time, packet drops), core network latency data, and
application-level KPIs (e.g., Mean Opinion Score, jitter, throughput). These
data sources may originate from various vendors, layers of the stack, and
time windows. The ingestion engine ensures schema harmonization through
automated metadata tagging, while outliers and incomplete records are
filtered using robust statistical methods like Tukey’s method or Isolation
Forests.
Time synchronization is crucial in telecom datasets due to differing
sample rates across components. GENATWIN applies interpolation and
time-alignment techniques to ensure consistency in temporal patterns.


Additionally, entropy-based feature selection is performed to retain only
high-variance, high-impact metrics. Feature normalization, scaling, and
transformation (e.g., log-scaling for skewed metrics like throughput) are
conducted using standard scalers or learned embeddings, preparing the data
for downstream training. This preprocessing step significantly reduces the
noise and redundancy often found in raw telemetry, increasing the
efficiency of the subsequent learning pipeline.

**3.2 Model Development Layer**

The model development layer serves as a sandbox for AI/ML model
training that mimics production environments. Here, operators can design
and train a wide range of models for use cases like anomaly detection,
dynamic resource allocation, handover prediction, or user experience
forecasting. These models can range from traditional machine learning
methods (e.g., Random Forests, Gradient Boosting) to deep learning
architectures (e.g., CNNs, LSTMs, Transformers). Each model is trained
using the preprocessed KPIs from the ingestion layer and tuned using
techniques such as cross-validation, hyperparameter search, or ensemble
bagging.
What sets GENATWIN apart is that this layer doesn’t operate in
isolation, it is tightly coupled with the simulation engine. Once a model is
developed, it can be pushed to the twin simulation engine to evaluate how it
performs in real-time or synthetic network scenarios. The modularity of this
layer also allows developers to test multiple models in parallel, comparing
robustness and generalizability across various DTAC scenarios. Moreover,
metadata such as model performance logs, feature importances, and
confidence scores are archived and made available to the policy feedback
layer for decision support.

**3.3 cGAN-Driven Synthesis Engine**

At the heart of GENATWIN lies the conditional GAN engine, responsible
for generating high-fidelity synthetic KPI data. The engine takes as input a
noise vector and a conditional encoding (DTAC) and produces KPI traces
that are statistically indistinguishable from real data. This synthetic
generation is not simply random sampling but is guided by scenario-
specific vectors, enabling highly controllable simulations. For instance, a
DTAC vector may encode conditions such as “urban macrocell with high


vehicular mobility during evening hours with VoIP traffic dominance.” The
generator learns the mapping from this DTAC to realistic KPI sequences
such as fluctuating CQI, rising RLC buffer occupancy, and increasing
handover attempts.
The generator and discriminator are trained in an adversarial setup.
While the generator learns to mimic the data distribution, the discriminator
aims to distinguish real data from synthetic, pushing the generator to
improve. This adversarial training results in output that retains correlation
structures and temporal dependencies found in real network behavior.
Importantly, the synthesis engine is scalable, GPU-accelerated, and
designed to support multi-threaded training, allowing simultaneous
generation of KPI sequences for diverse DTACs across multiple use cases.

**3.4 Digital Twin Simulation Engine**

The digital twin simulation engine is where AI models are evaluated under
the KPI sequences generated by the synthesis engine. This module
replicates network behavior from both control-plane and user-plane
perspectives. It simulates resource scheduling, RRC signaling events,
session continuity, QoS parameter evolution, and user mobility patterns.
The engine uses microservices for different network layers, orchestrated
using Kubernetes and exposed via APIs for modular interaction. For
instance, a microservice simulating the MAC layer may interact with
another service that handles gNB scheduling and CQI reporting to test how
a new AI model behaves under congestion.
The key advantage of this simulation engine is its bidirectional
interaction with AI models. AI decisions, such as handover threshold
changes or resource block allocations, are fed back into the simulation loop,
enabling reinforcement learning scenarios. The engine supports both batch
simulation and online interactive modes. In batch mode, a fixed DTAC is
applied, and the AI model is tested across thousands of synthetic samples.
In online mode, network conditions evolve over time, and the AI model
must adapt continuously, mimicking deployment conditions. Logs are
automatically collected and passed to the policy feedback engine.

**3.5 Policy Feedback and Decision Layer**

The policy feedback layer completes the loop by analyzing simulation
outcomes and guiding decision-making. This layer compares predicted


performance (e.g., throughput, drop rate, delay) with predefined SLA or
QoS thresholds. Models that consistently meet or exceed these targets under
multiple DTAC scenarios are approved for deployment. Those that fail
under specific edge conditions are flagged for retraining or redesign. This
ensures only resilient and context-aware models reach production.
To assist telecom engineers and data scientists, the policy feedback
layer provides visual dashboards, statistical reports, and explainability
insights such as SHAP scores, confusion matrices, and DTAC heatmaps.
These tools support collaborative decision-making and align AI
performance with network engineering goals. Integration with CI/CD
pipelines ensures that validated models can be pushed to deployment
repositories or containers with a single approval.

**4 Generative AI Model Design**

The success of GENATWIN hinges on the design of its core generative
engine, which uses Conditional Generative Adversarial Networks (cGANs)
to synthesize high-fidelity network KPIs. Unlike traditional GANs that
generate outputs from random noise alone, cGANs allow for conditional
generation based on scenario-specific input vectors, captured in our
framework through the DTAC encoding. This design allows the generator to
output KPI traces that not only mimic real-world statistical distributions but
also align with operational contexts like traffic load, user mobility,
topology, and AI policies. This capability is essential for telecom scenarios
where context is everything, behavior under rural eMBB traffic differs
drastically from that in a dense urban URLLC deployment (Fig. 1 ).


**_Fig. 1_** Glimpse of overall architecture

**4.1 Conditional GAN Overview**

The conditional GAN (cGAN) structure used in GENATWIN consists of
two adversarial neural networks: a generator (G) and a discriminator (D).
Both networks take a common conditioning vector, the DTAC, as input, in
addition to their regular inputs. For the generator, a random noise vector
sampled from a Gaussian distribution is concatenated with the DTAC. This
combined input is passed through a sequence of dense and activation layers
to produce synthetic KPI vectors. The discriminator, on the other hand,
takes either real KPI samples or synthetic ones (from the generator), along
with the corresponding DTAC, and tries to classify them as “real” or “fake.”
The competition between these two networks drives convergence.
The key innovation lies in the conditioning mechanism. By tying both
the generator and discriminator to DTAC, the model is forced to learn how
different contexts influence the underlying KPI distributions. For instance,
if DTAC encodes high user density, the generator learns to produce KPIs
with elevated RRC connection requests, PRB usage, and potential


congestion effects. Conversely, if DTAC encodes a low-traffic, rural
scenario, the generator will synthesize quieter KPI traces. This dynamic
behavior is not possible in traditional GANs, which treat all generated data
as independent of context. As such, cGANs in GENATWIN are capable of
simulating a wide range of nuanced behaviors in network operations.

**4.2 DTAC Encoding**

The DTAC, or Digital Twin Augmenting Condition, is the contextual brain
of GENATWIN. It is a vectorized representation of real-world conditions
that define how the network is expected to behave in a given simulation.
The DTAC includes multiple fields, such as geography
(urban/suburban/rural), user density, dominant traffic type (VoIP/video/file
transfer), scheduling policy, interference level, RAN deployment type
(macro/small cell), time of day, and AI-policy toggles (e.g., proactive load
balancing enabled/disabled). Each of these fields is encoded into numerical
or one-hot vector formats and concatenated to form a fixed-length DTAC
vector, typically 15 to 20 dimensions.
DTAC is critical because it enables parameterized simulation: operators
or developers can specify conditions of interest and then generate synthetic
KPI data that reflects those conditions. For example, a test engineer can
encode a scenario like “urban macrocell with 80% video traffic, 20% VoIP,
moderate interference, during peak hours,” and instantly obtain thousands
of realistic KPI sequences. This eliminates the need for manual trace
collection, hand-crafted simulation scripts, or reverse-engineering live data
for testing. Moreover, DTAC is extendable—new fields can be added
without retraining the whole system, allowing GENATWIN to evolve with
new 6G features, deployment scenarios, or business policies.

**4.3 Generator Architecture**

The generator network is a multilayered feed-forward neural network
designed to transform low-dimensional noise and contextual DTAC into
high-dimensional synthetic KPI vectors. The input to the generator is a
concatenated vector: 100-dimension random noise + DTAC vector (e.g., 15
dimensions). This composite vector is passed through several dense layers
with sizes progressively increasing (e.g., 256 → 512 → 1024), each
followed by batch normalization and nonlinear activations (typically ReLU
or Swish). The final output layer has a tanh activation function, normalizing


the output between −1 and 1 to match the scaled KPI values used during
training.
This architecture allows the generator to model complex, nonlinear
relationships between contextual parameters and KPI values. For instance,
it can learn that under high-mobility and high-user-density conditions, the
BLER distribution shifts, or that RLC throughput drops in cells with more
handover interruptions. Such behaviors are rarely linear and require the
expressive power of deep networks. The generator is optimized using an
adversarial loss, which penalizes outputs that the discriminator can easily
classify as synthetic, forcing the generator to create outputs that resemble
actual network conditions more closely over time.

**4.4 Discriminator Architecture**

The discriminator is equally important and is responsible for evaluating the
realism of the generated KPI sequences. It takes as input either real KPI
traces or generated ones, along with the same DTAC vector used by the
generator. These inputs are concatenated and passed through dense layers
(e.g., 1024 → 512 → 128), each using Leaky ReLU activation to prevent
dying neuron issues. Dropout layers are inserted to avoid overfitting,
especially during early training epochs. The final output is a single
probability score between 0 and 1, where values close to 1 suggest that the
input is real and values near 0 suggest it is fake.
The discriminator’s objective is to correctly classify real vs. synthetic
samples, but it also plays a pivotal role in improving the generator. By
providing informative gradients based on its classification confidence, the
discriminator indirectly teaches the generator to better mimic the data
distribution. During training, care is taken to balance the discriminator’s
power, if it becomes too strong, the generator cannot improve; if too weak,
it fails to challenge the generator. In GENATWIN, techniques such as label
smoothing, instance noise, and dual-objective loss functions are used to
keep this balance optimal, resulting in high-quality synthetic data
generation.

**5 Training and Simulation Pipeline**

While the architecture and model design set the foundation, the training
pipeline and simulation methodology bring GENATWIN to life. The


training process involves iteratively learning the joint distribution of real-
world KPIs and contextual DTACs, followed by extensive simulation of AI
model performance on generated KPI sequences. This dual loop, of training
and testing, ensures that both the generative model and the target AI model
improve over time through repeated exposure to diverse, realistic, and edge-
case conditions.

**5.1 Feature Engineering**

Before training begins, raw KPIs from the real network must be carefully
curated, filtered, and transformed. This process is crucial because noisy or
irrelevant features can mislead the generator and distort the learning
process. Initial raw data typically consists of hundreds or thousands of
counters per base station or sector. Using domain knowledge and feature
selection methods like mutual information ranking, Gini importance, or
PCA, this list is trimmed down to a smaller subset, usually 30 to 50 KPIs,
that retain the most predictive and context-aware signals. Common
selections include PRB utilization, CQI averages, RRC setup success rates,
drop call ratios, average user throughput, and SINR variance.
Once selected, these KPIs are normalized using z-score normalization
or min-max scaling, depending on their statistical properties. Temporal
smoothing (e.g., using exponentially weighted averages) may be applied to
reduce high-frequency noise. Finally, the feature vectors are temporally
aligned with DTAC values to build synchronized input/output pairs for the
cGAN model. This preprocessing is automated in GENATWIN and ensures
that the model is trained on clean, informative data that accurately
represents the network’s operating conditions.

**5.2 DTAC Construction**

Constructing meaningful DTAC vectors is both an art and a science. While
some elements are easily derived from configuration files (e.g., deployment
type, traffic mix), others must be inferred or encoded through heuristics. For
example, “high-mobility scenario” might be inferred from increased
handover attempts and drop ratios, while “congested scenario” may be
identified via sustained PRB saturation. GENATWIN uses a combination of
rule-based parsing and supervised classification to encode such complex
tags. Each scenario is associated with a fixed DTAC encoding that is


repeatable and deterministic, ensuring consistent generation across
simulation runs.
Operators can define DTAC libraries, repositories of typical, critical, or
rare scenarios that they wish to simulate. These libraries can be modified
over time, allowing simulation pipelines to be continuously refreshed with
new operating conditions. DTAC construction also allows for hierarchical
encoding: for example, traffic type may be encoded at both coarse
(video/VoIP) and fine (YouTube, Skype, Zoom) levels. This granularity
enables precision testing, especially for AI models sensitive to application-
layer dynamics.

**5.3 Model Training**

Training the cGAN involves alternating updates to the generator and
discriminator using real KPI samples and the corresponding DTACs. The
objective is to minimize the discriminator’s classification accuracy while
maximizing the realism of the generator’s outputs. GENATWIN uses the
Adam optimizer with custom learning rate schedules (e.g., warmup
followed by cosine annealing) to stabilize training. Techniques such as
gradient clipping, batch normalization, and learning rate decay are also used
to prevent mode collapse and vanishing gradients.
Training continues until several stopping criteria are met: convergence
of loss curves, stabilization of generated KPIs’ statistical distributions, and
high fidelity under visual and quantitative inspection (e.g., through JS
divergence and KL divergence scores). Validation is performed on a held-
out DTAC-KPI pair set not seen during training, ensuring the generator’s
generalization to new scenarios. Once trained, the model is versioned,
documented, and registered in the GENATWIN model registry for
deployment in simulation tasks.

**5.4 Simulation Loop**

The final phase involves running AI models against synthetic KPI streams
in the GENATWIN twin engine. For each DTAC condition, thousands of
KPI traces are generated and streamed into the simulation. AI models ingest
these KPIs as if they were live inputs and produce control decisions, like
cell reselection parameters, handover margins, or anomaly alarms. These
decisions are then evaluated using predefined metrics such as QoE
improvement, QoS compliance, fairness, or energy efficiency.


Simulation results are logged and summarized in performance reports.
The loop is repeated across DTACs, ensuring that models are stress-tested
against all operational scenarios, including edge cases such as sudden
congestion spikes or coverage blackholes. The simulation loop also
supports differential evaluation: comparing two versions of a model or
policy under identical conditions to measure robustness improvements. This
enables A/B testing, regression analysis, and policy rollout validation,
bringing AI model evaluation in telecom on par with best practices in
software engineering.

**6 Results and Discussions**

The evaluation of the GENATWIN framework was conducted through a
rigorous analysis involving both statistical fidelity of generated data and
practical validation of AI models using synthetic scenarios. To holistically
capture the capabilities of GENATWIN, we designed experiments across
four key dimensions: the statistical resemblance between generated and real
KPIs, AI behavior similarity under synthetic and real contexts, model
robustness under failure and edge cases, and the impact of feedback loop–
based retraining. In this section, we elaborate on these experiments and
present detailed visualizations that capture the core findings.

**6.1 Quality of Synthetic KPI Traces**

The first objective was to verify whether the synthetic KPI traces generated
by GENATWIN preserve the statistical characteristics of real-world
telecom datasets. We used Jensen-Shannon Divergence (JSD) and
Kullback-Leibler (KL) divergence to compare the generated KPI
distributions with real data across five representative metrics: throughput,
BLER, CQI, PRB utilization, and RRC success rate.
As shown in Fig. 2 , the JSD scores remained consistently low across all
metrics, with values ranging from 0.03 to 0.06. Throughput exhibited the
lowest JSD at 0.04, while CQI had a slightly higher score at 0.06. The KL
scores followed a similar trend, ranging from 0.06 to 0.09, validating the
closeness of synthetic data to real observations. The low divergence
indicates that the conditional GAN engine in GENATWIN has successfully
learned the underlying statistical relationships and temporal dependencies
across multidimensional KPIs.


**_Fig. 2_** Statistical divergence between real and synthetic KPI distributions
These results are critical because they confirm that GENATWIN does
not merely generate random variations, but instead captures the complex
joint distributions of KPIs. This enables realistic simulation of operational
conditions, which is a prerequisite for AI model validation. The experiment
establishes that GENATWIN can generate synthetic network environments
that are statistically indistinguishable from real ones, even under varying
DTAC conditions.

**6.2 AI Model Behavior Consistency**

Beyond statistical similarity, it is essential to evaluate whether AI models
behave consistently when validated on GENATWIN-generated traces
versus real-world data. To measure this, we conducted simulations for
several DTAC (Device, Traffic, Access, and Cell) configurations,
comparing AI decision traces for models trained on actual versus synthetic
data. Two AI tasks were selected: (1) a deep Q-network (DQN) for mobility
optimization, and (2) an LSTM-based cell load predictor.
Figure 3 demonstrates the percentage of behavior overlap between AI
model actions under real and GENATWIN-simulated scenarios. For Urban
Dense and Edge Case DTACs, the overlap exceeded 92%, while in Rural
Sparse and High Mobility scenarios, it remained above 89%. This high


degree of overlap suggests that the feature distributions and temporal
patterns preserved in the synthetic data are sufficient for consistent AI
inference.

**_Fig. 3_** AI model decision trace overlap (Real vs GENATWIN)
Such behavioral consistency ensures that GENATWIN can be
confidently used as a pre-deployment validation tool. It also highlights the
ability of the DTAC-based conditioning mechanism to guide the generative
model in producing semantically accurate KPI sequences. This subsection
validates the hypothesis that synthetic traces from GENATWIN not only
look real but also lead to the same network decisions, making them highly
suitable for closed-loop evaluation pipelines.

**6.3 Edge Case and Failure Scenario Testing**

A standout feature of GENATWIN is its ability to simulate edge cases and
rare failure modes that are typically hard to capture in real-world logging.
We constructed DTAC encodings for extreme scenarios such as high-speed
vehicular mobility, sudden URLLC traffic bursts, uplink interference
spikes, and backhaul latency degradation. Each scenario was then used to
evaluate the performance of pretrained AI models.


As seen in Fig. 4 , the AI models suffered noticeable performance drops
in all edge-case simulations. Specifically, URLLC burst and backhaul delay
scenarios showed the most degradation, with a 29% and 30% reduction in
model accuracy or decision reliability, respectively. Vehicular mobility and
interference spikes also led to drops in the range of 24–26%. These metrics
clearly show that GENATWIN can reproduce stress conditions with a high
degree of realism, surfacing vulnerabilities that would otherwise remain
hidden in traditional evaluation workflows.

**_Fig. 4_** AI model degradation under edge case simulations

This ability to simulate network failures allows operators to proactively
improve AI robustness by retraining models under these challenging
contexts. It also provides a platform to define fail-safe thresholds and
fallback strategies, reducing the chances of performance degradation in live
deployments. Overall, this evaluation confirms the framework’s critical role
in operational risk management for AI adoption in telecom.

**6.4 Feedback Loop Effectiveness**

The final set of experiments focused on assessing the closed-loop feedback
mechanism built into GENATWIN. This mechanism allows AI models to be


evaluated under synthetic scenarios, retrained with error cases, and
reevaluated for continuous performance improvement. To quantify this, we
used a handover failure prediction model and subjected it to multiple cycles
of simulation and retraining.
Figure 5 presents the before-and-after metrics for this model,
specifically looking at false positives and F1-score. Initially, the model had
a false positive rate of 35% and an F1-score of 0.72. After retraining using
synthetic edge-case data generated by GENATWIN, false positives reduced
to 17% and the F1-score increased to 0.80. This improvement highlights the
framework’s ability to function as a CI/CD (continuous
integration/continuous deployment) system for telecom AI, bringing the
benefits of agile software development into AI model lifecycles.

**_Fig. 5_** Feedback loop effect on AI model performance

In addition to improved metrics, this feedback loop also enabled the
tracing of model failures to specific DTAC encodings, allowing engineers
to diagnose root causes more effectively. This level of granularity would be
difficult to achieve in live networks where performance data is noisy and
sparse. With GENATWIN, AI teams can establish robust pipelines that


simulate, evaluate, debug, and enhance models, all before they reach
production.

**7 Business Impact and Real-World Applicability**

The GENATWIN framework offers profound business value to telecom
operators, vendors, and network solution providers by bridging the
persistent gap between AI model development and reliable real-world
deployment. In current telecom practices, AI model failures often stem from
a mismatch between lab testing environments and the operational
complexity of live networks. GENATWIN solves this problem by enabling
scalable, contextual simulation that mirrors real network behaviors, thus
reducing deployment risk and accelerating time to market.

**7.1 Reduced Time-to-Deployment for AI Models**

One of the most compelling business benefits of GENATWIN is its
potential to drastically reduce AI model deployment cycles. Typically, AI
models take several months to develop, test, and fine-tune, especially due to
challenges in collecting real network data under diverse scenarios.
GENATWIN reduces this dependency by allowing engineers to synthesize
KPI datasets for thousands of DTAC conditions in minutes. This not only
reduces the effort in manual testbed configuration and drive tests but also
increases confidence in model generalizability. Operators can validate new
ML pipelines, anomaly detection algorithms, or traffic classifiers virtually
before rolling them out live, cutting down deployment time from months to
weeks.

**7.2 Risk Mitigation in AI Rollouts**

Telecom networks operate under stringent performance expectations, and
introducing AI models without exhaustive validation can lead to unexpected
failures, outages, or user dissatisfaction. GENATWIN provides a controlled
environment to test AI behavior under both nominal and extreme edge-case
conditions, such as PRB starvation, sudden interference, or backhaul
degradation. This helps in proactively identifying risky decision boundaries
in AI models and triggers timely retraining or fallback strategies. As a
result, GENATWIN serves as a risk-mitigation tool, safeguarding network


SLAs and maintaining consistent user experience even during AI
experimentation phases.

**7.3 Enabling AI Governance and Explainability**

GENATWIN also supports emerging needs around AI explainability and
governance. With its built-in simulation traceability, logging, and feedback
loop, it allows for detailed audit trails of model decisions across simulated
contexts. Decision mismatches can be traced back to specific DTACs, and
engineers can investigate the feature contributions using explainability tools
like SHAP or LIME within the loop. This is especially important as
regulators begin enforcing transparency in AI-driven telecom automation.
GENATWIN enables compliance-readiness and provides operators with the
tooling needed to build trustworthy AI systems.

**8 Scalability, Extensibility, and Future**

**Enhancements**

A key design principle behind GENATWIN is its extensibility, ensuring that
the framework remains relevant and adaptive as telecom technologies
evolve from 5G to 6G and beyond. Built on a microservices-based
architecture and containerized infrastructure, GENATWIN is inherently
scalable across cloud-native deployments, enabling both on-premise and
edge-cloud simulation.

**8.1 Extending DTAC for 6G and Network Slicing**

As 6G technologies introduce new paradigms such as semantic
communications, THz spectrum, and intelligent surfaces, GENATWIN’s
DTAC encoding is designed to evolve accordingly. Operators can define
new DTAC dimensions for quantum-safe security modes, slice SLA intent
descriptors, RIS configuration states, and more. These new encodings can
be plugged into the existing training and simulation pipeline without
architectural overhauls. Similarly, network slicing, an important B5G
concept, can be modeled in GENATWIN by simulating slice-specific KPIs,
prioritization policies, and inter-slice resource conflicts. This flexibility
ensures GENATWIN remains a valuable asset in future network
architectures.


**8.2 Interoperability with Existing Telecom Platforms**

GENATWIN is not designed to replace existing OSS/BSS systems, RIC
platforms, or SON systems, but to augment them. The architecture allows
for seamless integration with tools like ONAP, SMO platforms, and O-RAN
components via RESTful APIs and data exchange formats like JSON,
protobuf, or gRPC. This makes it possible to plug GENATWIN into a
CI/CD loop for RAN and Core network AI model testing. Vendors can use
GENATWIN to pre-validate their AI applications or xApps in a vendor-
neutral environment, fostering ecosystem-level interoperability testing.

**8.3 Future Directions**

Several enhancements are planned for GENATWIN. These include
multiagent simulation for cooperative AI models (e.g., distributed handover
management), reinforcement learning feedback loops, and the addition of
multimodal data such as control-plane logs, user complaints, and weather
patterns. We also envision integration with Digital Twin marketplaces,
where standard DTAC templates and pretrained generative models can be
shared across operators. Eventually, GENATWIN could support digital twin
simulations for AI governance, energy-aware RAN optimization, and even
user-level quality-of-experience modeling in metaverse or holographic
applications.

**9 Conclusion**

In this chapter, we introduced **GENATWIN** , a novel Digital Twin
framework for evaluating AI models in B5G networks using generative
modeling. We highlighted the architectural components, including a
conditional GAN engine trained on KPI data with contextual DTAC
encodings. GENATWIN enables telecom operators to simulate realistic
network behavior across thousands of scenarios, including rare or edge
cases that are otherwise difficult to capture in live environments. This
synthetic data is then used to evaluate AI models prior to deployment,
ensuring higher accuracy, robustness, and contextual awareness.
GENATWIN’s design philosophy is rooted in real-world challenges
faced by telecom operators today, namely, the unpredictability of AI model
behavior in production, the scarcity of test data across diverse scenarios,
and the high cost of drive tests and manual simulation. By using a


generative approach, GENATWIN fills the gap between AI development
and deployment, acting as a digital staging ground for model validation. Its
cGAN-based synthesis engine, simulation loop, and policy feedback
mechanism enable continuous improvement and iterative testing of AI
models.
Our extensive results demonstrate that GENATWIN-generated data
retains high statistical similarity to real-world KPIs and that AI models
evaluated on this data behave consistently with real deployments.
Furthermore, GENATWIN has shown strong potential for reducing
deployment risks, shortening validation timelines, and surfacing edge-case
behaviors that would otherwise go unnoticed.
The broader impact of GENATWIN lies in its extensibility and potential
to evolve into a full-fledged AI operations (AIOps) simulation platform.
With the rise of 6G, network slicing, and dynamic RAN environments,
GENATWIN can support new dimensions of DTAC, integrate with policy
engines, and provide AI model governance. It serves not only as a tool for
simulation but as an operational intelligence framework for AI-driven
telecom.
As future work, we aim to integrate multimodal inputs, multiagent
dynamics, and intelligent policy modeling to further expand GENATWIN’s
capabilities. By enabling proactive, risk-aware, and policy-aligned AI
deployment, GENATWIN will remain a cornerstone of AI-native telecom
transformation for years to come.

```
OceanofPDF.com
```

(1)

(2)

© The Author(s), under exclusive license to Springer Nature Switzerland AG 2026
S. Singh et al. (eds.), _GenAI and LLMs for Beyond 5G Networks_
https://doi.org/10.1007/978-3-032-06418-9_7

##### Reasoning Foundations for Generative AI-

##### Based Wireless Networks

Christo Kurisummoottil Thomas^1 , Omar Hashash^2 and Walid Saad^2
Electrical and Computer Engineering, Worcester Polytechnic Institute,
Worcester, MA, USA
Virginia Tech, Innovation Campus, Alexandria, VA, USA

```
Christo Kurisummoottil Thomas
Email: cthomas2@wpi.edu
```
```
Omar Hashash
Email: omarnh@vt.edu
```
```
Walid Saad (Corr esponding author)
Email: walids@vt.edu
```
**Abstract**
Generative artificial intelligence (AI) solutions, such as transformer models,
represent a significant step toward building next-generation wireless
networks. However, their adoption is hindered by limitations such as their
black-box nature (lack of interpretability), poor generalizability, and large
model sizes, which result in substantial power consumption. These
challenges prevent networks from achieving critical qualities like near-zero
latency (due to retraining overhead), trustworthiness, and resiliency—
essential for applications such as digital twin-driven smart industries,
intelligent transportation systems, the metaverse, and autonomous network
experiences. Addressing these challenges requires a paradigm shift in how
generative AI is integrated into future networks. The focus must shift
toward reasoning-driven AI models that offer out-of-domain and out-of-data


generalizability, decision explainability, and sustainability. Promising
approaches in this direction include causal reasoning and neuro-symbolic
AI. This chapter explores how incorporating causality into future generative
AI models can ground them in wireless concepts and the underlying physics
of wireless transmission and reception. Additionally, combining the
strengths of symbolic AI and neural networks through neuro-symbolic
systems can imbue future wireless systems with logical problem-solving
capabilities. These problem-solving capabilities enable the identification of
network failures and rapid remediation with minimal human intervention,
enhancing the overall resilience and efficiency of wireless networks.
Finally, we present initial experimental results demonstrating how retrieval-
augmented generation (RAG) enhances accuracy, mathematical reasoning,
explainability, and the quality of assertions in wireless question-answering
tasks, when compared to baseline (vanilla) large language models.

**Keywords** Causality – Neuro-symbolic AI – Generative AI – Foundation
models

This research was supported by the U.S. National Science Foundation under
Grant CNS-2225511.

**1 Introduction**

Generative artificial intelligence (AI) models, particularly transformers,
have garnered significant attention in wireless research due to their
powerful knowledge retrieval capabilities. Leveraging extensive training on
large-scale Internet or cloud data, large language models (LLMs) have been
explored for various wireless tasks [ 1 – 3 ], including intent translation,
dynamic resource configuration, and power allocation. However, the
existing literature lacks a rigorous and systematic framework detailing how
LLMs can be effectively integrated into wireless system design and
decision-making pipelines, particularly under real-time constraints,
dynamic environments, and domain-specific data scarcity. In this chapter,
we begin by examining the limitations of current LLMs and then outline the
essential capabilities they must possess to effectively support future
networking applications.


**1.1 Why Existing LLMs Are Not Sufficient for Future**

**Networks?**

LLMs are built using transformer architectures [ 4 ], which are
fundamentally statistical models operating in an autoregressive manner—
predicting the next word in a sequence based on prior context. While
transformers offer advantages such as efficient parallel processing
compared to models like recurrent neural networks (RNNs) and long short-
term memory (LSTM) networks, all of these architectures are
fundamentally data-driven, requiring vast amounts of training data to
generalize across tasks and distributions. Moreover, they often function as
black-box models and raise concerns regarding interpretability, scalability,
and sustainability. We next discuss in detail three limitations of state-of-the-
art LLMs that hinder their deployment in future networking applications.

**_1.1.1 Limitations of LLMs in Structured, Numerical, and_**

**_Multimodal Wireless Contexts_**

While recent works such as [ 1 – 3 ] have begun to explore the application of
LLMs in wireless networks, they overlook a critical dimension: the scalable
integration of environmental _sensing_ data, which is fundamental to realizing
truly multimodal, AI-native communication systems. Moreover, state-of-
the-art approaches like [ 3 ] fail to account for the structured nature of
wireless textual data, which frequently appears in tabular formats or as
visual representations such as graphs and performance metrics.
Compounding this issue is the well-documented difficulty LLMs face in
reasoning over numerical data. Despite their extensive training on large-
scale text corpora, LLMs lack a grounded, physical understanding of
quantitative relationships, often yielding unreliable or incorrect outputs
when confronted with arithmetic tasks, statistical reasoning, or the
interpretation of key performance indicators.
This limitation is particularly acute in wireless systems, where
numerical precision and structured data interpretation are essential for real-
time decision-making, system diagnostics, and performance evaluation.
These deficiencies in grounding, multimodal fusion, and structured
reasoning significantly constrain the range of wireless applications where
LLMs can be effectively deployed. For instance, LLM-based wireless
chatbots struggle to parse technical tables, and cross-modal tasks such as


text-to-image generation lack physical interpretability when divorced from
signal-level context. In light of these challenges, we argue that the next
generation of foundation models for wireless systems should evolve beyond
pure text-based reasoning to become physics-aware problem solvers,
grounded in the mathematical structure and physical laws that govern
wireless environments. In the following section, we examine the
implications of this lack of grounding in the context of wireless network
intelligence.

**_1.1.2 Lack of Grounding_**

Current LLMs [ 1 – 3 ] lack grounding which is defined as the ability to
connect abstract, language-based representations to real-world phenomena.
Trained predominantly on large text corpora, these models struggle to
capture the underlying physics of wireless environments, such as signal
propagation, often resulting in inconsistent or unrealistic decisions. This
lack of grounding limits their capacity for logical, causal, and mathematical
reasoning which are capabilities essential for resilience, intent management,
and advanced signal processing in AI-native networks. Thus, aligning LLM
representations with real-world semantics and network objectives is crucial
for their effective integration.

**_1.1.3 Lack of Instructibility_**

A persistent challenge of LLMs is their tendency to hallucinate by
generating “human-like” outputs that do not connect to reality, essentially
fabricating false information [ 5 ]. If such hallucinations occur, AI-native
networks driven by LLMs may generate misleading or inaccurate
information. For example, the network may propose power allocations that
may violate the regulated thresholds of transmit powers of base stations. In
networked control systems like intelligent transportation systems, if a
roadside unit uses an LLM to propose vehicle control actions that do not
conform to safety or traffic regulations, this could lead to hazardous
outcomes—jeopardizing both the safety of the ITS and the trustworthiness
of the AI models involved. Thus, relying on vanilla LLMs can lead to
alignment problems in AI-native networks. Alignment here can encompass
fulfilling network objectives, adhering to physical constraints like radiated
power or environmental sustainability goals, and ensuring compliance with
governmental regulations that oversee the deployment of wireless networks.


Hence, current LLMs lack precise instructibility from the environment. This
shortcoming impedes their ability to remain under control and guidance.
Instructibility refers to the capability of LLMs to dynamically adjust their
behavior based on explicit feedback from users, whether they be user
equipment, system engineers, or network operators. This adaptability
should also ensure that decisions made by LLMs are explainable, fostering
trustworthy solutions.
To overcome these limitations, we propose a universal foundation
model framework [ 6 ] that leverages advanced AI techniques such as
causality and neuro-symbolic AI to emulate human-like reasoning.

**2 Universal Foundation Models**

The concept of universal foundation models for wireless networks, first
proposed in [ 6 ], introduces a unified architecture capable of performing a
wide range of wireless tasks across different layers of the OSI stack. Figure
1 shows an overview of the proposed universal foundation model
framework and what are the applications that it entails. These models,
called as large multimodal models (LMMs), are designed to adapt to
varying network environments and task requirements with minimal
retraining, offering a scalable and generalizable approach to AI-native
wireless system design. We next outline the distinct capabilities this
universal model must possess so as to enable reconfigurability of wireless
systems, dynamic adaptability, resilient operations, and intent-based
networking.


**_Fig. 1_** Illustrative figure of what universal foundation models entail

**2.1 Fusion of Multimodal Sensing Information: A Trade-Off**

**Between Minimality and Redundancy**

For capturing the real-world wireless environment, there is a need for
precise sensing and mapping of its diverse surroundings. Prior works like
[ 3 ] (and the references therein) discussed exploiting visual generative AI
models such as meta-transformers to map multimodal wireless sensing
information to a semantic latent space. By capturing the characteristics of
the wireless environment, such models can enhance contextual and
situational awareness for sensing applications in AI-native networks.
However, providing the entire mapped sensing information (received from
diverse sources) as input to train, the LMM is resource-intensive and
requires substantial time for retraining. This hinders the timely execution of
dynamic updates essential for maintaining seamless connectivity. To
address this limitation of meta-transformers inherent in [ 3 ], we propose to
convey a compressed sequence of pertinent information to the LMM using
dimensionality reduction. To achieve this, we start with the identification of
physical symbols present across the multimodal data. _Physical symbols_
refer to abstract entities present in the data that have relevant semantics
with respect to the wireless network. For example, this may involve
associating symbols with various extrinsic and intrinsic elements. _Extrinsic
elements_ encompass dynamic objects, such as scattering elements in the
environment and users. Meanwhile, _intrinsic elements_ involve static


network features like network addresses and signal processing methods
underlying wireless transmission or reception, among others. There could
be redundancy among the information conveyed by the symbols across
multiple modalities of data. To mitigate redundancy, the shared semantic
latent space’s construction should adhere to the information bottleneck
principle [ 5 ]. In this framework, it is assumed that a dominant mode of
information, called _prime modality_ , exists within a dataset, serving as the
primary source of information. Other modalities complement or enhance
the information provided by this prime modality. Here, the compact
representation for any modality should convey as little information as
possible about the raw data, and, simultaneously, prime modality
representation should convey maximum information about other modalities.
This ensures that the resulting semantic latent space is of _minimal
dimension_ while _avoiding redundant information_.
Inspired from [ 5 ], we can formally formulate this as follows. In the
proposed information flow model, we consider three dependent random
variables _X_ 0 , _X_ 1 , and _X_ 2 , which represent multimodal observations with
decreasing levels of informational richness or redundancy. That is, _X_ 0
contains the most comprehensive information, followed by _X_ 1 , and then
_X_ 2. This hierarchical structure enables the system to begin with _X_ 0 and
progressively integrate the relevant information from _X_ 1 and _X_ 2. The
objective is to determine a latent semantic representation _B_ 0 that efficiently
compresses _X_ 0 while preserving its relevance to _X_ 1. This latent state _B_ 0
also serves as a conduit for passing informative content downstream to _B_ 1.
The second-level latent state _B_ 1 then captures the relevant information
from _X_ 2 through its dependency on _B_ 0. Rather than fusing all modalities
directly in the high-dimensional space of _X_ 0 ∈R _dX_^0 , this approach learns a
compact representation _B_ 1 ∈R _dB_^1 , which encapsulates the most salient
information across modalities. This final latent representation is used for
downstream tasks, such as accurate prediction of a target variable _Y_. The
trade-off between compactly representing the primal modal states and
preserving their relevant information is formulated as an optimization
problem that minimizes mutual information subject to specific information
processing constraints.


```
(1)
```
where _ε_ 1 , _ε_ 2 , _ε_ 3 > 0. Further, for LMM training, we advocate using this
filtered representation in the shared semantic space as inputs. Additionally,
filtering also determines when to perform dynamic updates of the neural
network (NN) parameters of the LMM, taking into account the nature of the
captured data, which can be either static (e.g., 3GPP standards) or dynamic
(e.g., wireless channel information).
While the fusion and filtering of multimodal information and the
training of LMMs are important, on its own, simply identifying symbols is
not enough if LMMs are to be universal. While a vanilla LLM can
effectively predict the events following an observed sequence of sensing
information, it lacks precise understanding of what causes the event and its
implications from a wireless system perspective. For example, translating
images of trees in a wireless environment into a set of angles of arrival or
departure that describe the RF signal propagation environment requires
associating meaning from a wireless perspective with each extracted
physical symbol. This aspect, called _grounding_ , is detailed next.

**2.2 Causal Reasoning for Grounding in LMMs**

Traditional grounding approaches typically rely on the construction of a
_knowledge base_ , a form of symbolic AI that encodes logical relationships
among physical symbols—such as scattering objects, users, network
topology, and transmission or reception parameters. While effective in
capturing structured domain knowledge, these methods suffer from
_scalability issues_ as the number of symbols and relations grows, leading to
increased complexity and limited generalizability. To address these
limitations, we propose a new approach in which _LMMs_ infer the
relationships among physical entities through _causal reasoning_ [ 7 ], rather
than relying solely on predefined symbolic representations. This causal
grounding framework enables more flexible and scalable understanding of
the physical environment, as discussed next.

```
min
p ( B 0 | X 0 ), p ( B 1 | B 0 )
```
```
I ( X 0 ; B 0 )
```
```
subject to I ( X 0 ; X 1 )− I ( B 0 ; X 1 )≤ ε 1 ,
I ( B 0 ; B 1 )≤ ε 2 ,
I ( X 0 ; X 2 )− I ( B 1 ; X 2 )≤ ε 3 ,
```

While specific experiments demonstrate that language models might
exhibit causality, it is predominantly attributed to the causal knowledge
ingrained in the training data, rather than indicative of LLMs possessing
inherent causal understanding. In [ 8 ], a gradient-based transformer-type
algorithm for zero-shot optimal covariate balancing for causal treatment
effect is introduced. We propose to advance [ 8 ] by incorporating theoretical
methods to construct causal foundation models, focusing on wireless
concepts as the relevant physical symbols. Here, one may ask _how to
identify the causal relations among physical symbols and how to ensure that
the learned relations are aligned with the wireless concepts in standards
and textbooks?_ One common approach is to perform fine-tuning [ 3 ] that
takes a pretrained language model trained on large amounts of general text
and then continue to train it on a small-scale task-specific text. Fine-tuning
is appropriate if the user specifically knows the ground-truth causal
relations. For wireless scenarios, fine-tuning can be beneficial for
constructing a wireless-specific chatbot capable of extracting valuable
information from its knowledge base. However, fine-tuning LLMs may
impose limitations on the wireless applications supported. This limitation
arises from the narrow set of NN parameters that are tuned during the fine-
tuning process (limited degrees of freedom). This, in turn, requires retuning
as the wireless environment or task change. To address these limitations, we
suggest the use of retrieval-augmented generation (RAG) coupled with
causal discovery, as discussed next.

**_2.2.1 How to Perform Causal Discovery Through RAG?_**

Through querying from a wireless-specific database that includes wireless
textbooks, research papers, 3GPP standard, or any device instruction
handbook, RAG [ 9 ] enables the LMM to understand the domain knowledge
context. Once this information is retrieved, the generation component of
RAG can help formulate new content that infers or expresses causal
relations among the identified physical symbols. For example, when the
LMM is tasked with deducing causal relationships between scattering
objects in the environment and channel parameters like angle of arrival
(AoA) or angle of departure (AoD), RAG can map these wireless
observations to the underlying physical concepts from the database.
Through an evolvable external knowledge component and multi-agent
cooperation, RAG can allow the implementation of emerging applications


that require multitasking, like in connected homes or industrial robots.
Moreover, performing retrieval from an evolving knowledge base can
enable universal knowledge retrieval for semantic communications [ 10 ] and
intent management. With its continuous learning capability, RAG, with
evolving knowledge, enables continually updating the wireless algorithms
across all OSI layers while ensuring compatibility with the advancements in
the semiconductor industry and software solutions. This proves
advantageous, particularly for intent management and resilience (see Sects.
3.1 and 3.2). Apart from the continual learning capability, RAG with
evolving knowledge enables the knowledge retrieved to be dynamically
adjusted to cater to the specific application demands. For example, in a
multiuser communication system, the retrieved literature on signal
processing algorithms must differ from what might be required in a single-
user scenario.

**_2.2.2 What Does Causal Discovery Through RAG Entail for_**

**_LMMs?_**

Grounding via causal discovery entails endowing LMMs with the capability
to comprehend the causal relationships among physical symbols and
subsequently engage in causal inference through interventions and
counterfactuals [ 7 ]. Through interventions and counterfactuals, the LMM
can indulge in chain-of-thought kind of reasoning, where it analyzes a
sequence of causal state-action pairs ( **_s_** _t_ , **_a_** _t_ ) and their effects,

. In general, a causal graph connecting a set

of states and actions can be learned as a factorized posterior distribution as
follows:

```
(2)
```
where pa( **_s_** _t_ ) is the set of causal parents of any state **_s_** _t_. Such a causal
graph can be learned through emerging AI frameworks such as generative
flow networks [ 11 ]. The ability to understand the cause-and-effect relations
among wireless data facilitates long-term planning for resource allocation,
signaling schemes for transmission and reception (and may include
beamforming, modulation, coding, and control signaling, among others),
and quality-of-service (QoS) management, thereby contributing to the
establishment of robust and resilient wireless systems.

# p ({ s 0 , a 0 ,⋯ , a N − 1 s N })=∏ tp ( s t ∣pa( s t )),


**2.3 Instructibility**

To enable instructibility in LMMs, they must be capable of dynamically
adjusting resource allocation, signaling policies, and various cross-layer
network functions in real time—adapting to diverse tasks, operational
contexts, and optimization objectives. This requires continuous monitoring
of wireless observations to detect and respond to unforeseen conditions that
may disrupt seamless network connectivity.
A conventional method for dynamic wireless resource management
based on user feedback is deep reinforcement learning (RL). However, deep
RL approaches are typically task-specific and must be retrained when
wireless environments or optimization goals shift. While multitask RL
frameworks offer broader applicability, they are generally confined to
particular domains or OSI layers. Critically, they lack the capacity to evolve
their state and action spaces over time, limiting their ability to adapt to
changes in communication standards or advances in wireless technologies.
Moreover, these RL-based systems are not equipped for abductive
reasoning—a vital capability for inferring missing information or
generating plausible explanations for observed data. Such reasoning is
essential for dynamic adaptability, intent inference, and reconfigurability—
key traits required for resilient networking and for enabling semantic
communication. To realize instructible LMMs, we propose a framework
that integrates communication context modeling, prompt-based control, and
online interaction with wireless feedback. This sets the foundation for
systems that can adapt and evolve in response to real-time conditions. We
then explore methods to endow these models with logical and mathematical
reasoning abilities, enabling the development of self-evolving, dynamically
adaptable architectures that meet the demands of future resilient and
intelligent wireless systems.
We consider that:

- E _t_ denotes the state of the wireless environment at time _t_ , including
    channel conditions, interference levels, traffic demand, and node
    mobility.
- I _t_ denotes an instruction or intent provided to the LMM at time _t_ ,
    expressed in natural language, symbolic form, or structured prompt.
- _πθ_ (A _t_ ∣I≤ _t_ ,E≤ _t_ ) represents the LMM’s policy parameterized by _θ_ ,
    which maps past and current instructions and environment observations


```
to a sequence of actions A t , such as reconfiguration of communication
parameters.
```
- R(A _t_ ,E _t_ ) is a reward function that evaluates the effectiveness of the
    chosen action under the current environmental conditions.
However, directly providing the wireless environment state E _t_ to the LMM
does not result in generalizable outcomes. Herein, we propose to derive
communication context from the state and instruction, which is defined as
below.

**_2.3.1 Communication Context_**

Causal representations that are used to represent the physical symbols,
similar to tokens in NLP-based LLMs, form the _communication context_ for
an LMM. This context encapsulates critical aspects of the wireless
communication scenario. The components of the LMM context include:

- **Network setup** including details about (1) _communicating devices_ , (2)
    _communication link_ (downlink or uplink), and (3) _physical topology_ that
    describes the antenna configuration, as well as any miscellaneous
    network architecture
- **Communication constraints** including constraints on the total power
    and shared communication/computation resources across frequency,
    time, and other dimensions
- **Wireless standards/text snippets** read using RAG (Sect. 2.2.1), which
    include excerpts from relevant wireless communication standards or
    documents, providing a contextual basis for the communication scenario
- **End-to-end optimization objectives** that may include quality-of-service
    (QoS) measures such as average throughput, delay, and reliability/quality
    of experience
- **Historical wireless data** {E _t_ }∀ _t_ that may involve diverse measurements
    such as uplink pilots, user feedback on channel quality indication,
    various sensing measurements, and received uplink signal measurements,
    among other relevant parameters

Next, we explain how to construct an online LMM policy
_πθ_ (A _t_ ∣I≤ _t_ ,E≤ _t_ ) by instructing it with environmental feedback.

**_2.3.2 Online LMM with Wireless Environment Feedback Using_**

**_Neuro-Symbolic AI_**


One common approach to instill instructibility is to use an iterative
prompting mechanism in which an LMM is guided through multiple rounds
of interaction with human prompts I _t_. In each iteration, the model refines
and improves subsequent responses using the feedback (e.g., the QoS
results based on the wireless policy of the LMM) from the previous round.
However, iterative prompting requires human intervention. We propose
building an online LMM framework to address this limitation and enable
the development of autonomous wireless systems. In this setup, LMM
functions as the wireless policy _πθ_ (A _t_ ∣I≤ _t_ ,E≤ _t_ ) and is operationally
embedded within an interactive setting using online RL. This entails
utilizing gathered wireless observations and feedback from the environment
to iteratively enhance its functionality, aligning with goals expressed in
wireless language. The formulation of the LMM-powered cross-layer
network functionalities can be represented as a partially observable Markov
decision process. Here, the states are defined by the communication context
and prompts, actions are represented by the wireless policy suggested by
the LMM, and rewards are determined using performance metrics obtained
from the wireless environment. If available, the network goal or intent can
be articulated in natural language by the network operator. To ensure
continuous operation without disrupting connectivity, online LMMs should
possess the ability to explain wireless observations and infer any missing
data, necessitating logical reasoning capabilities. Furthermore, given that
many wireless concepts can be expressed mathematically, LMMs must
inevitably be capable of performing mathematical reasoning. This includes
tasks such as channel predictions, beamforming vector computations,
channel quality measurements, and many other cross-layer network
computations.
Here, multitask RL can be an alternative approach to perform diverse
wireless tasks. However, since they lack logical and mathematical reasoning
capabilities, we advocate incorporating them through the use of _neuro-
symbolic AI_ [ 10 ]. In our setting, symbolic AI serves to evaluate diverse
logical and mathematical formulas, while the neural component is
responsible for learning the logical and mathematical equations from
wireless observations and context information. When prompted with
communication context and grounded wireless observations using causal
discovery (Sect. 2.2.1), symbolic AI connects facts and data through rules
and algorithms, resembling the cognitive operations of the human brain in


storing high-level concepts and engaging in nuanced inference. To prevent
hallucination, LMMs must have the ability to explain wireless observations
and infer any missing data. Additionally, they should understand the
connections between various physical symbols through symbolic AI. Here,
a viable approach is to develop a formal logical language that can
encapsulate the exhaustive ontology of wireless concepts and articulate
rules governing the functioning of wireless systems (and is the symbolic
part here). This strategy is reminiscent of the Cyc concept [ 12 ], which
serves a similar purpose for web-based data. Beyond their lack of logical
reasoning abilities, existing LLMs face challenges in accurately capturing
mathematical formulas and executing mathematical derivations. To
overcome this limitation, a promising approach involves leveraging a
neuro-symbolic problem solver [ 13 ], having three main components. First,
a _problem reader_ encodes math word problems, presented as textual
prompts, into vector representations. Second, a _programmer_ generates
symbolic grounded equations, which are executed to produce answers.
Lastly, a _symbolic executor_ obtains final results. In this setup, the
programmer learns the weights (neural part) that establish connections
between various mathematical symbols. The resulting neuro-symbolic
problem solver enables the construction of dynamic problem solvers, a
critical component for intent management and resilience, as discussed in
Sects. 3.1 and 3.2.

**3 Use Cases for LLM in Wireless Networks**

**3.1 LMMs for Intent Management**

A recent effort exploring the use of LLMs for intent management is
presented in [ 2 ]. However, this work primarily employs an LLM as a
chatbot interface that translates user-specified intents expressed in natural
language into infrastructure-level network service descriptors. Moreover,
the approach in [ 2 ] relies heavily on human-in-the-loop feedback to refine
the configurations produced by the LLM, thereby limiting the system’s
ability to autonomously ensure intent assurance. In contrast, we propose
leveraging LMMs to support multiple phases of intent management that
span across the layers of the OSI stack. Our framework aims to go beyond
natural language translation by enabling semantic interpretation, proactive


reasoning, and closed-loop adaptation—laying the foundation for
autonomous, resilient, and context-aware network control.

- _Problem formulation phase:_ To enable autonomous operation, the
    network must translate operator-specified intents into well-defined
    optimization problems that account for multiple objectives and
    underlying physical constraints. LMMs offer a powerful mechanism for
    facilitating this dynamic problem formulation without requiring human
    intervention, owing to their enhanced logical and mathematical reasoning
    capabilities. Furthermore, as discussed in Sect. 2.2 and illustrated in Fig.
    1 , LMM responses are grounded in the physics of wireless systems. This
    grounding enables the precise formulation of wireless optimization
    problems, whether they involve resource allocation, signaling scheme
    selection, or joint optimization across multiple layers of the network
    stack.
- _Intent assurance phase:_ By leveraging a continuous stream of wireless
    measurements, LMMs can operate as intent _assurance agents_. Equipped
    with neuro-symbolic AI capabilities, these agents can evaluate logical
    representations of desired intents—typically specified as QoS targets—
    against real-time network conditions. When an intent is not satisfied, the
    LMM can diagnose and articulate the underlying causes, identifying
    specific deviations or constraints that require resolution. This enables
    targeted adaptation and corrective actions, guiding the system toward
    fulfilling the specified intent within the required timeframe. As an
    example, consider an operator-defined intent: _“Ensure end-to-end_
    _latency remains below 20 ms for video traffic in the tactical edge_
    _network.”_ The LMM receives a continuous stream of wireless
    measurements, including latency statistics, channel quality indicators
    (CQIs), and buffer occupancy levels. It evaluates this intent against
    current network behavior using a logical formula that formulates
    ∀ _t_ ∈ _T_ , Latencyvideo( _t_ )< 20 ms. If the LMM is to identify such a
    logical formula, it must first be able to understand the logical symbols
    present in the intent provided using natural language. Further, to evaluate
    such a logical formula, it must be able to process numerical symbols
    (representing the wireless measurements) and relate them to the logical
    formula. If the condition is violated, the LMM identifies the root cause—
    e.g., increased queuing delay due to suboptimal scheduling or
    interference on a specific link. It then articulates a corrective strategy


```
such as “Observed latency spikes at node B due to persistent congestion;
consider reallocating resources to alternative route C-D-E.” This
guidance enables proactive reconfiguration and brings the system closer
to meeting the original intent without direct human involvement.
```
- _Validator agent:_ For solving the LMM-designed problem formulation
    provided we can use multiobjective RL with causal reasoning games
    building on [ 7 ]. Validating solutions against regulatory norms and
    physical constraints for long-term intent fulfillment is crucial. This is the
    _alignment goal_ described previously in Sect. 1. To autonomously manage
    intent, a validator role can be fulfilled by LMM, possessing a solid
    understanding of wireless concepts.
Next, we discuss the impact of minimizing hallucinations through improved
assertions and providing precise answers (as reflected in overexplaining
metric). This capability is essential for swiftly recovering from network
service disruptions and thereby ensuring resilience.

**3.2 LMMs for Resilient Networking**

_Resilience_ is the ability of wireless networks to (a) detect or predict in
advance any failures or performance disruptions arising due to network
functionality issues across any OSI layer, changing wireless environment,
user dynamics, or external malicious influences, and (b) recover back to
their normal functionality within a stipulated time frame, thereby ensuring
seamless connectivity for all connected devices. In [ 7 ], we proposed a
robust framework for building resilient wireless networks causal Bayesian
optimization. However, the application of our solution in [ 7 ] is limited,
because it mainly focuses around quickly recovering from QoS deviations
in the network. However, network service disruptions can stem from
changes in the wireless environment or malfunctions in hardware or
software functionalities across diverse edge devices. To address this
challenge, we suggest leveraging LMMs equipped with causal knowledge,
not only pertaining to the wireless environment but also grounded in
wireless standards and cross-layer network functionalities. Such a universal
foundation model can handle service disruptions across multiple domains
and tasks by operating in a closed-loop fashion as discussed next.

- _Continuous monitoring of service disruptions:_ To detect network service
    disruptions, the LMM should continuously monitor and predict potential
    issues across OSI layers. For example, consider a situation where the


```
software code representing functionality at any OSI layer on an edge
device becomes corrupted due to processor malfunctions. Alternatively,
critical information intended for storage on an edge server might face
corruption due to jamming attempts. Herein, since LMMs are grounded
in wireless concepts, they can adeptly analyze error messages and
descriptions of software malfunctions or data corruptions, offering
suggestions aligned with network standards to rectify the issues, without
any human intervention. This approach enhances the model’s capability
to provide context-aware solutions across diverse tasks or domains or
environments. Furthermore, LMMs can be consistently prompted to
check for potential wireless environment issues that might lead to
performance deviations in the near future. Such network issues can be
formulated as either logical formulas or mathematical equations. For
example, a potential logical formula could be p : X →( Y < τ ),
signifying that with probability p , the wireless observation X results in a
performance Y below the expected target τ. However, vanilla LLMs face
challenges in handling such logical and mathematical problems, as
discussed in Sect. 2.3. In this context, logical and mathematical
reasoning capabilities using neuro-symbolic problem solvers play a
crucial role in assessing performance quality. In contrast to [ 7 ], which
necessitates the construction of specific causal models for monitoring
particular tasks or QoS targets, the universal nature of LMMs allows a
single model to be used for monitoring performance deviations and
software or hardware malfunctions across any OSI layer. Given that a
failure is detected, we next look at how the LMM can help the network
functionalities quickly (within a stipulated time) recover back to the
expected performance.
```
- _Network service recovery via LMM:_
    As detailed in Sect. 2.2.1, RAG enables the model to comprehend the
    causal implications of network actions by grounding wireless
    observations to the extracted knowledge. This enables LMMs to execute
    the minimal interventions required to restore the network to a normal
    functioning state. Further, as discussed in Sect. 2.3, instructibility allows
    LMMs to generate a sequence of network actions in response to the
    feedback from the wireless environment. These actions can involve
    repairing malfunctioning code, adjusting resource allocation, or refining
    signaling schemes to restore the network to normal functioning. In


```
contrast to [ 7 ], which might require separate causal AI models to monitor
diverse functionalities across OSI layers, the universality of LMMs can
possibly enable faster switching between tasks requiring repair or
refinement, utilizing a single AI model.
```
**3.3 Experimental Validation**

We begin by presenting illustrative experiments conducted using a RAG
framework on a dataset tailored to wireless communication scenarios. The
results highlight the superior performance of the proposed LMM compared
to a baseline (vanilla) LLM that lacks wireless domain context. Specifically,
the LMM produces more succinct explanations—resulting in _fewer
hallucinations_ —as well as more accurate answers that reflect _grounding in
wireless principles_ , and stronger _mathematical reasoning_ demonstrated
through coherent and well-justified rationales. These findings underscore
the potential of LMMs in addressing key challenges in future wireless
networks, particularly in scenarios that require domain-specific
understanding and real-time decision-making.
To evaluate the effectiveness of RAG in wireless communication
contexts, we conducted a series of question-answering (Q/A) experiments.
A sample question from the dataset is illustrated in Fig. 2. In the RAG
pipeline, relevant context paragraphs are retrieved from [ 14 ], providing
domain-specific wireless information composed of both textual
explanations and mathematical expressions. This combination serves as a
simple case of multimodal input, integrating language and symbolic
reasoning.


**_Fig. 2_** A sample mathematical Q/A pair from the dataset
Table 1 presents a human-subject evaluation involving 16 participants
who assessed the responses generated by various prompting strategies
across two categories: four conceptual wireless questions and seven
mathematical questions. Using standard LMM evaluation metrics—
precision, recall, F1 score, and ROUGE-L (see Table 1 for definitions)—the
RAG-enhanced LMM demonstrates a performance gain of approximately
15 _%_ to 30 _%_ over a vanilla LLM lacking domain context. In particular, for
a different kind of questions, the performance is summarized as follows:

- **Conceptual Questions:** The standard _Question Prompt_ baseline often
    retrieves reasonable rationales, but the addition of RAG enables
    refinement of assertions, yielding an 8 _%_ improvement in the assertion
    metric. Notably, the _overexplaining_ metric—which measures the extent
    to which explanations align with human expectations—shows a nearly
    threefold improvement with RAG. These enhancements suggest that
    LMMs grounded in retrieved knowledge can more effectively mitigate
    hallucinations and provide concise, intent-aligned responses.
- **Mathematical Questions:** For mathematically grounded prompts, the
    _Retrieval-Augmented Prompt_ consistently yields correct answers and
    more detailed derivations. Rationale quality improves by 22 _%_ , and the


```
number of correctly articulated derivative steps increases by 81 %
compared to the vanilla LLM. These results underscore the LMM’s
enhanced logical and mathematical reasoning capabilities when supplied
with structured, domain-relevant context.
```
**_Table 1_** Prompting GPT-3.5 Turbo with retrieval-augmented context shows a
general advancement over purely prompting with questions in four quantitative
measurements (upper section) and six human-evaluated measurements (lower
section). For each metric (row), the symbol (↑) indicates that higher scores are
better, and better results are highlighted in **bold** for the two prompting methods.
**Precision** : the number of shared words to the total number of words in the
generated answers; **Recall** : the number of shared words to the total number of
words in the human answers; **F1 score** :^2 _P_ ⋅ _Prerceicsiisoinon_ +⋅ _RReeccaallll_ ; and **ROUGE-L (F-
measure)** : based on the longest common subsequence (LCS) between the
generated answer and human answer, which indicates that a longer shared
sequence should indicate more similarity between the two sequences. **Human-
Evaluated Score** : Participants are asked to rate each Q/A sample without knowing
the source of it. They will give score 0 for no and score 1 for yes for each of the
Rationale, Assertion, and Over Explaining items. Mathematical questions require
an additional derivative step to be scored

(^) **Question Retrieval augmented
Evaluation measure prompt prompt
Precision (** ↑ **)** 0.06 **0.08
Recall (** ↑ **)** 0.59 **0.65
F1 Score (** ↑ **)** 0.11 **0.14
ROUGE-L (F-measure) (** ↑ **)** 0.17 **0.20
Over Explaining (** ↓ **)** 0.34 **0.12
Conceptual Question Rationale (** ↑ **) 0.89** 0.84
**Conceptual Question Assertion (** ↑ **)** 0.88 **0.95
Mathematical Question Rationale (** ↑ **)** 0.77 **0.94
Mathematical Question Assertion (** ↑ **)** 0.76 **0.97
Mathematical Question Derivative Steps (** ↑ **)** 0.48 **0.87**
As discussed earlier, the demonstrated logical and mathematical
reasoning capabilities—reflected in improved rationale metrics—enable
LMMs to map wireless observations and contextual information into formal
mathematical problem formulations. This positions LMMs as dynamic
problem solvers, as defined in Sect. 2.3.2, capable of autonomously
translating high-level goals into optimization objectives under physical and


operational constraints. To illustrate the practical relevance of these
capabilities in next-generation wireless systems, we now present several
representative use cases, including intent management and resilience.

**4 Scaling LLMs for Wireless Networks**

Despite the emergence of several large models that may have better
knowledge retrieval capability and longer context window than smaller
language models, the latter still holds significance. In this section, for
illustrative purposes, we will use Phi2 [ 15 ] as the smaller model, which
contains around 2. 7 billion parameters. Smaller models may have limited
generalization ability, and its knowledge base is unable to match those of
newer models like Llama3 [ 16 ] or GPT4. However, the benefit is that they
may be deployed on edge devices that may not have substantial computing
capabilities compared to cloud servers or base stations. Moreover, they can
achieve faster inference and lower latency, leading to quicker responses to
questions. When paired with an algorithm that is suitable for the
application, such as the one we propose, it can still deliver excellent
performance while keeping costs reasonable. Our experimental results show
that the proposed RAG + fine-tuning framework with Phi2 achieves an
accuracy within an error margin of 1. 3 _%_ compared to the larger Llama3
70B model with RAG.
However, apart from the limitations of LLMs discussed in [ 6 ], Phi2
faces the following challenges in handling telecom datasets:

- _Limited generalization capability:_ Compared to larger LLMs such as
    GPT3/4 and Llama, Phi2 has a smaller knowledge base and is thus
    limited in terms of the universality of tasks it can perform accurately. It
    fails on many fundamental wireless questions and does not understand or
    apply basic formatting instructions correctly.
- _Smaller context size:_ Phi2 can support only a small context size of 2048
    tokens. This means that the relevant RAG information which could be
    added as context to the wireless tasks is not sufficient for several tasks,
    given possibly the huge amount of wireless text, literature, and 3GPP
    standard information that needs to be extracted.
- _Challenges in prompting:_ Our initial experiments showed that
    developing a consistent prompt format with Phi2 was very difficult. The
    provided format in the TeleQNA dataset [ 1 ] does not function off-the-


```
shelf because it requires the model to restate the question and then
answer in a JSON format; however, Phi2 was unable to replicate this
effectively. Instead, we used a prompting trick that involved ending the
input text with “Answer:(.” This worked effectively because, in the
prompt, we instructed the model to answer only with the answer choice
(1), (2), (3), or (4). By ending the input with “Answer:(,” we provided
the opening of the expected format. When Phi2 sees this opening
parenthesis, it is primed to continue with a number, aligning with the
instruction to provide an answer choice. This leverages the model’s
tendency to replicate patterns it recognizes, guiding it to format the
response correctly without relying on complex instructions that it
struggled to follow. This approach effectively sidestepped the issues we
were having with getting Phi2 to consistently format its responses,
working around its limitations by adjusting the input format rather than
trying to force it to follow more complex instructions.
```
**4.1 Proposed Solution**

Our proposed solution integrates RAG, fine-tuning, and hyperparameter
optimization to enhance the performance of LMMs in wireless
communication tasks. At the core of the method is a multistage RAG
pipeline built upon a corpus of telecommunications and 3GPP standards
documents (Releases 14–18). The pipeline begins with document
preprocessing, where irrelevant elements are removed, and the text is
segmented into 300-token chunks. These chunks are embedded using a
sentence transformer model (Alibaba-NLP/gte-large-en-v1.5) and stored in
a Chroma vector database to enable efficient semantic retrieval.

**_4.1.1 RAG for Grounding in Wireless Knowledge_**

We evaluate two key retrieval strategies: (1) a hybrid LLM-embedding
search, where a candidate answer generated by the Phi2 LLM is appended
to the query to improve embedding-based search, and (2) an LLM-NLP
fusion search, which augments the hybrid method by incorporating
keyword and named entity extraction to refine the query embedding. The
final retrieval process combines results from both methods using a weighted
similarity score. Retrieved context is then integrated into the prompt via
concatenation or summarization (using TextRank), enriching the model’s
input with structured wireless knowledge. This hybrid RAG design


significantly improves contextual relevance in Q/A tasks and sets the stage
for subsequent fine-tuning of Phi2 for even greater performance, discussed
in the following section.

**_4.1.2 Model Fine-Tuning_**

To tailor the Phi2 language model to the specialized domain of
telecommunications and 3GPP standards, we performed targeted fine-
tuning using a custom dataset composed of domain-specific questions and
their corresponding RAG-enhanced contexts. Leveraging Low-Rank
Adaptation (LoRA) [ 17 ] with carefully selected hyperparameters, we fine-
tuned key model components to improve performance while preserving
efficiency. LoRA was chosen for its low inference latency, fast training
time, and modularity—making it ideal for real-time wireless applications
beyond just Q/A. Additional experiments with various training epochs and
datasets confirmed that integrating retrieved context with the question data
yielded the most effective results, producing a fine-tuned model that is
central to our inference pipeline.

**_4.1.3 Hyperparameter Optimization_**

Complementing this fine-tuning, we conducted a comprehensive
hyperparameter optimization using the Optuna framework. We tuned a wide
range of parameters—including RAG strategy selection, temperature, top-p
sampling, repetition penalties, token limits, retrieval depth, and similarity
thresholds—using a custom benchmark that measured accuracy across the
full pipeline. After 2,000 optimization trials, Optuna delivered an
empirically validated configuration that significantly improved
performance. This rigorous and systematic tuning process, in conjunction
with domain-specific model adaptation, has resulted in a highly optimized
system capable of delivering accurate, context-aware answers to complex
wireless communication questions.

**4.2 Simulation Results and Analysis**

To evaluate the effectiveness of our proposed solution, we conducted
extensive simulations and analyzed the results. Table 2 presents a
comparison of the different baseline models tried and the proposed Phi2
with RAG and fine-tuning. Detailed hyperparameter analysis and
performance results can be found in our work [ 18 ]. The results in [ 18 ] show


that combining retrieval-augmented generation (RAG) with LoRA fine-
tuning dramatically improves the wireless Q/A performance of the small
Phi2 model, raising accuracy from 38.8% to 77.87%—a 39-point increase.
This hybrid approach outperforms both vanilla small models and larger
models that use only RAG or only fine-tuning, while achieving accuracy
within 1.3% of Llama-3 70B + RAG despite Phi2 being far smaller. The
results demonstrate that smaller, grounded models can achieve competitive
domain performance with far lower computational cost, making them
suitable for real-time edge-device deployment in future wireless systems.

**_Table 2_** Performance comparison of different model
configurations using optimized parameters

```
Configuration Accuracy
Phi2 38. 80 %
Phi3-medium-4k 53. 01 %
Phi3-medium-4k + RAG 73. 22 %
Llama 3 8B 43. 72 %
Llama 3 8B + RAG 68. 31 %
Llama 3 70B 56. 83 %
Llama 3 70B + RAG 78. 94 %
Phi2 with RAG 50. 82 %
Phi2 with fine-tuning 57. 10 %
Proposed Phi2 with RAG + fine-tuning 77. 87 %
```
**5 Conclusion and Recommendations**

This chapter presents a novel framework for the design of AI-native
wireless systems for 6G and beyond, leveraging foundation models
grounded in the principles of multimodality, grounding, and instructibility.
By integrating these capabilities, we enable wireless systems to perform a
wide range of tasks with enhanced autonomy, dynamic adaptability, and
semantic alignment. We conclude with the following three key
recommendations for advancing this vision:

- **Speeding up next-G standardization to system design: 1. Leverage**
    **LMMs for Rapid System Design Prototyping:** LMMs can significantly


```
accelerate the prototyping of diverse wireless system design scenarios.
By harnessing RAG, LMMs can extract relevant text-based
specifications and technical descriptions based on high-level inputs—
such as network intents or system design objectives. This capability
enables LMMs to serve as intelligent codesign agents, facilitating the
rapid exploration of design alternatives and their corresponding trade-
offs, ultimately reducing time-to-deployment for complex 6G systems.
```
- **Building a repository of wireless datasets:** While the results based on
    RAG offer unique insights into LMMs’ capabilities, it is essential to
    acknowledge the challenges associated with generating a comprehensive
    dataset. To address this, we recommend the creation of open-source
    ontology for wireless concepts and algorithms, sourced from a curated
    selection of textbooks and wireless literature, encompassing 3GPP
    standards. This approach ensures the quality, reliability, and
    trustworthiness of the dataset, making it applicable for research and
    development across the entire wireless community.
- **Compositions of short language models and distributed architecture:**
    Deploying a universal foundation model at each wireless base station
    may be infeasible due to high energy and computational demands.
    Instead, we recommend a distributed architecture in which lightweight
    language models are trained and deployed at edge servers. These
    condensed models are well-suited for localized tasks with limited scope,
    enabling efficient execution while conserving resources. This
    architecture supports collaborative reasoning via the principle of
    compositionality [ 19 ], where multiple small models contribute
    complementary representations. Such compositional learning allows the
    network to incrementally acquire a broad range of capabilities, ultimately
    achieving universal intelligence through distributed specialization.
- **Sustainable foundation models using next-generation AI:** Existing
    large language models are computationally intensive and energy-
    demanding due to their massive parameter sizes. To ensure sustainability
    in future wireless networks, it is essential to develop smaller, data-
    efficient foundation models that retain the core capabilities of LMMs.
    We recommend building next-generation foundation models that
    integrate explainability (via causal reasoning), advanced reasoning
    (through neuro-symbolic AI), planning, and common sense (enabled by
    world models), as envisioned in [ 20 ]. These capabilities empower


```
models to understand the structure and dynamics of the real world,
generate accurate future predictions, and generalize across tasks—all
while minimizing training overhead and resource consumption.
```
**References**

1. A. Maatouk, F. Ayed, N. Piovesan, A.D. Domenico, M. Debbah, Z.-Q. Luo, TeleQnA: A
    Benchmark Dataset to Assess Large Language Models Telecommunications Knowledge. arXiv
    preprint arXiv:2310.15051 (2023)
2.
    S. Tarkoma, R. Morabito, J. Sauvola, AI-native Interconnect Framework for Integration of Large
    Language Model Technologies in 6G Systems. arXiv preprint arXiv:2311.05842 (2023)
3.
    L. Bariah, Q. Zhao, H. Zou, Y. Tian, F. Bader, M. Debbah, Large Language Models for Telecom:
    The Next Big Thing?. arXiv preprint arXiv:2306.10249 (2023)
4.
    A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A.N. Gomez, L. Kaiser, I. Polosukhin,
    Attention is all you need, in _Proceedings of 31st Conference on Advances in Neural Information_
    _Processing Systems_ (2017)
5.
    Anonymous, Neuro-inspired information-theoretic hierarchical perception for multimodal
    learning, in _The Twelfth International Conference on Learning Representations_ (2024)
6.
    S. Xu, C.K. Thomas, O. Hashash, N. Muralidhar, W. Saad, N. Ramakrishnan, Large multi-modal
    models (LMMs) as universal foundation models for AI-native wireless systems. IEEE Network
    **38** (5), 7–16 (2024)
    [Crossref]
7.
    C.K. Thomas, C. Chaccour, W. Saad, M. Debbah, C.S. Hong, Causal reasoning: charting a
    revolutionary course for next-generation AI-native wireless networks. IEEE Veh. Technol. Mag.
    **19** (1), 6 (2024)
8.
    J. Zhang, J. Jennings, C. Zhang, C. Ma, Towards Causal Foundation Model: on Duality between
    Causal Inference and Attention. arXiv preprint arXiv:2310.00809 (2023)
9.
    P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. Küttler, M. Lewis, W. Yih,
    T. Rocktäschel, et al., Retrieval-augmented generation for knowledge-intensive NLP tasks, in
    _Proceedings of Advances in Neural Information Processing Systems_ , vol. 33 (2020), pp. 9459–
    9474
10.
    C.K. Thomas, W. Saad, Neuro-symbolic causal reasoning meets signaling game for emergent
    semantic communications. IEEE Trans. Wirel. Commun. **23** (5), 7–8 (2024)
11.
    W. Li, Y. Li, S. Zhu, Y. Shao, J. Hao, Y. Pang, GflowCausal: Generative Flow Networks for
    Causal Discovery. arXiv preprint arXiv:2210.08185 (2022)
12.
    D. Lenat, G. Marcus, Getting from Generative AI to Trustworthy AI: What LLMs might Learn
    from Cyc. arXiv preprint arXiv:2308.04445 (2023)


13.
J. Qin, X. Liang, Y. Hong, J. Tang, L. Lin, Neural-symbolic solver for math word problems with
auxiliary tasks. arXiv preprint arXiv:2107.01431 (2021)
14.
D. Tse, P. Viswanath, _Fundamentals of Wireless Communication_ (Cambridge University,
Cambridge, 2005)
[Crossref]
15.
M. Javaheripi, S. Bubeck, Phi-2: the surprising power of small language models (2023). https://
[http://www.](http://www.) microsoft. com/ en-us/ research/ blog/ phi-2-the-surprising-power-of-small-language-models/
16.
H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.A. Lachaux, T. Lacroix, B. Rozière, N. Goyal,
E. Hambro, F. Azhar, A. Rodriguez, Llama: Open and efficient foundation language models.
arXiv preprint arXiv:2302.13971 (2023)
17.
E.J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, W. Chen, LORA: Low-rank
adaptation of large language models. arXiv preprint arXiv:2106.09685 (2021)
18.
C.K. Thomas A. Neeser, S. Xu, N. Ramakrishnan, W. Saad, Wireless knowledge grounding in
smaller LLMs using retrieval augmented generation and fine-tuning, in _IEEE International
Conference on Communications (ICC)_ (2025)
19.
R. Bansal, B. Samanta, S. Dalmia, N. Gupta, S. Vashishth, S. Ganapathy, A. Bapna1, P. Jain, P.
Talukdar, LLM augmented LLMS: expanding capabilities through composition. arXiv preprint
arXiv:2401.02412 (2024)
20.
W. Saad, O. Hashash, C.K. Thomas, C. Chaccour, M. Debbah, N. Mandayam, Z. Han, Artificial
general intelligence (AGI)-native wireless systems: a journey beyond 6G, in _Proceedings of the
IEEE_ (2025)
_OceanofPDF.com_


(1)

© The Author(s), under exclusive license to Springer Nature Switzerland AG 2026
S. Singh et al. (eds.), _GenAI and LLMs for Beyond 5G Networks_
https://doi.org/10.1007/978-3-032-06418-9_8

##### Computational Aspects of Generative-AI

##### on Radio Access Networks (RAN)

Mahantesh Kothiwale^1 and Anshuman Nigam^1
Samsung R&D Institute Bangalore (SRI-B), Bangalore, India

```
Mahantesh Kothiwale (Corr esponding author)
Email: mahantesh.k@samsung.com
```
```
Anshuman Nigam
Email: anshun@samsung.com
```
**Abstract**
This chapter explores the computational foundations, architectural
evolution, and deployment strategies for integrating Generative AI (GenAI)
into Radio Access Networks (RAN), particularly in the context of cloud-
native frameworks like Open RAN (O-RAN) and Virtualized RAN
(vRAN). As RAN transitions from proprietary baseband hardware to
software-defined, disaggregated systems that operate on Commercial Off-
The-Shelf (COTS) compute infrastructure, it unlocks new capabilities for
colocating PHY-layer signal processing with AI/GenAI workloads on
shared heterogeneous compute platforms. These platforms include CPUs
with AVX-512 and AMX extensions, GPUs equipped with fourth-
generation Tensor Cores (e.g., NVIDIA H100/H200), and SmartNICs or
DPUs such as BlueField-3 and Marvell Octeon. The chapter establishes a
mathematical equivalence between signal processing operations (e.g., FFTs,
SVD, MMSE) and AI building blocks (e.g., matrix multiplications, dot
products, attention), noting that both domains are dominated by linear
algebra and parallel multiply-accumulate computations.


It presents performance and latency benchmarks across compute
platforms, comparing the real-time suitability of different accelerators for
both deterministic RAN workloads and batch-based GenAI inference tasks.
A historical perspective illustrates how computational advances have been
key enablers of each wireless generation—from the DSP-based 2G/3G era
to multi-core CPU-based 4G LTE and now the GPU-accelerated, AI-native
architecture of 5G and 6G RAN (https:// arxiv. org/ html/ 2411. 17712v1;
https:// arxiv. org/ html/ 2506. 09505). The convergence of compute and
communications is no longer aspirational but demonstrated in real-world
networks. In particular, two Samsung vRAN deployments are examined:
the first showcases Samsung vRAN 3.0, which has powered over 50,000 O-
RAN-compliant sites with intelligent orchestration and cloud automation;
the second describes AI-enhanced Layer-1 channel estimation within
Samsung’s vDU infrastructure, where convolutional neural networks
improve accuracy and reduce hardware load, as published in IEEE research.
These examples highlight how AI is no longer just augmenting RAN—it is
becoming embedded within it.
The chapter further outlines a growing set of GenAI use cases across the
RAN stack: real-time beamforming optimization, enhanced channel
estimation using deep learning, LLM-driven intent-based orchestration in
the RIC, workload offloading from user devices to the RAN edge, and
multi-tenancy where GenAI and vRAN processes share common
GPU/CPU/DPUs through advanced orchestration primitives like MIG and
SR-IOV. The chapter closes with a proposed architecture for unified
heterogeneous System-on-Chip (SoC) designs that integrate CPU, GPU,
DSP, NIC, and AI accelerator blocks into a tightly coupled, programmable
infrastructure—capable of meeting both the latency and throughput
demands of RAN and GenAI concurrently. Overall, the chapter highlights
how computational innovation is not only enabling GenAI for RAN but also
redefining the RAN itself as a real-time, AI-native platform.

**1 Introduction**

Artificial Intelligence (AI) and Generative AI (GenAI) are at the heart of a
significant transformation in wireless communication infrastructure,
particularly within the Radio Access Network (RAN). Traditionally, RAN
architectures were highly specialized, relying on custom ASICs and tightly


coupled hardware-software designs. However, with the increasing demand
for flexibility, scalability, and real-time automation, there has been a shift
toward software-defined and cloud-native RAN models, particularly
Virtualized RAN (vRAN) and Open RAN (O-RAN). These models
disaggregate RAN functions and allow them to run on general-purpose
compute infrastructure [ 1 ].
The evolution toward cloud-based RAN deployment presents a
compelling opportunity for integrating AI and GenAI. These technologies
offer sophisticated capabilities in data pattern recognition, decision-making,
anomaly detection, and optimization — making them ideal candidates for
transforming how RAN functions are designed and operated. The ability to
train and deploy GenAI models on edge infrastructure within the RAN [ 2 ]
enables new services, faster adaptation to environmental changes, and more
intelligent network behavior overall.
This shift is supported by the emergence of a wide range of Commercial
Off-The-Shelf (COTS) compute platforms, which include the following:

**GPUs** , ranging from cost-effective consumer-class RTX series to
datacenter-grade NVIDIA **H100** and **H200** , capable of serving GenAI
workloads efficiently.
**CPUs** , including **Intel Xeon** with AVX-512 and AMX support and **ARM
Neoverse** cores with Scalable Vector Extensions (SVE), optimized for
both signal processing and AI tasks.
**DPUs** , such as **NVIDIA BlueField-3** and **AX800** , which offer packet
processing, encryption, and AI inference capabilities all in one SoC.
This variety of compute infrastructure allows telecom operators to
flexibly allocate resources to different workloads. For example, a single
edge node could simultaneously run PHY baseband signal processing,
control plane protocols, and GenAI models for network analytics — all in a
multi-tenant environment.
Additionally, RAN management itself is evolving. GenAI enables
intelligent policy generation, real-time adaptation, and agentic control of
network behavior. Intent-based AI agents can understand high-level goals
from operators and translate them into low-level RAN actions. These
capabilities are being explored within the O-RAN Alliance and RAN AI
Alliance, paving the way for a new generation of self-optimizing, AI-native
networks.


This chapter explores the full landscape of computational requirements,
architectures, and trends driving the convergence of GenAI and RAN. From
mathematical analogies to platform comparisons, to system architecture
implications, this work provides a deep dive into what it takes to make
GenAI truly work at the edge of mobile networks.

**2 Significance of Vector Processing in RAN Vs.**

**GenAI**

The convergence of RAN signal processing and GenAI computation is not
only architectural but also mathematical. Both domains are fundamentally
built upon intensive linear algebra computations, making them well-suited
to vector and matrix acceleration. In both RAN and AI workloads, the
efficiency of execution heavily depends on the ability to perform matrix
operations at scale.
In the PHY layer of RAN, essential operations such as beamforming,
channel estimation, and demodulation involve matrix multiplications, dot
products, and decomposition methods like Singular Value Decomposition
(SVD). Similarly, GenAI models — particularly large transformer-based
architectures like GPT and LLaMA — rely on dense matrix multiplications
and vector arithmetic during training and inference. This overlap in
computation provides an opportunity to utilize shared hardware acceleration
resources, such as SIMD vector engines and matrix multiplication units.
By understanding the mathematical parallels between these domains, we
can explore how different compute platforms — including AVX, AMX,
SVE, and GPUs — accelerate these workloads and how they can be
harnessed simultaneously in multi-tenant RAN+AI deployments.

**2.1 Math Comparison: AI Vs. PHY**

```
AI/GenAI operation RAN PHY operation Math operation
Dense Matrix Multiply (such as GEMM:
General Matrix Multiplication)
```
```
MIMO precoding,
beamforming
```
```
Matrix × matrix
```
```
Dot product Channel equalization,
demodulation
```
```
Vector • vector
```
```
Convolution (CNN) FIR filtering, FFT-based
modulation
```
```
Convolution
```

```
AI/GenAI operation RAN PHY operation Math operation
Gradient descent Adaptive beam adjustment,
power control
```
Iterative
optimization
**SVD / PCA** Precoder/postcoder derivation Eigen
decomposition
This table illustrates how the fundamental operations in AI and RAN are
not only similar in structure but also demand similar compute resources.
This resemblance forms the basis for colocating GenAI and PHY workloads
on the same computational hardware.

**2.2 How Vector Processing Accelerates Both**

To accelerate these operations, modern CPUs and GPUs employ specialized
vector processing architectures. These include:

**SIMD (Single Instruction Multiple Data)** units, which allow the same
operation (e.g., add, multiply) to be performed simultaneously on
multiple data elements.
**Matrix engines** , which perform entire matrix multiplications in a single
cycle.
These capabilities are particularly important in RAN baseband
processing loops, where thousands of matrix computations occur in real-
time per millisecond of air interface operation. They are equally vital for
GenAI inference, where attention layers and feedforward networks rely on
matrix-heavy execution.
For example:
**Intel AVX-512** provides 512-bit vector instructions for floating point and
integer math, enabling parallel operations on 16 FP32 values.
**Intel AMX** (Advanced Matrix Extensions) uses tile-based matrix
operations for more efficient GEMM execution, with 8-times the
throughput compared to AVX-512 [ 3 ].
**ARM SVE** scales vector width from 128 to 2048 bits, enabling
flexibility across edge-to-core deployments [ 5 ].
**NVIDIA Tensor Cores** , available in H100, and H200, are optimized for
matrix multiply-accumulate (MMA) operations and provide immense
throughput for LLM inference [ 4 ].

**2.3 Architecture Comparison**


```
Architecture Vector width Use case Instruction
Intel AVX-512 512 bits Baseband processing, ML
infer.
```
```
SIMD vector ops
```
```
Intel AMX 1024-bit tiles Transformer inference/training Matrix tile operations
ARM SVE 128–2048 bits Packet/RAN DSP, ML Scalable vector
extension
NVIDIA tensor
Core
```
```
256–8192
threads
```
```
GenAI, signal AI, DU + LLM FP8/FP16 tensor matmul
```
Each of these architectures offers different trade-offs between
flexibility, performance, and power consumption. CPUs with AVX/AMX
provide low-latency response suitable for real-time RAN control, while
GPUs excel in parallel GenAI inference workloads with high throughput.

**3 Trending AI and GenAI Use Cases for RAN**

As GenAI continues to evolve, its applicability within Radio Access
Networks is expanding rapidly. The nature of RAN workloads — ranging
from control loops to user-plane processing — aligns well with the pattern
recognition, decision-making, and inferencing capabilities of GenAI.
Several use cases have been actively explored and even prototyped by
industry leaders and research communities. These include both AI-enhanced
RAN (AI-for-RAN) as well as AI-native RAN deployments where GenAI
models reside within the infrastructure.
Before detailing individual use cases, it’s important to understand that
the RAN environment is highly latency-sensitive and throughput-intensive.
Any AI or GenAI integration must maintain stringent Quality of Service
(QoS) and real-time guarantees. With this context, the following use cases
demonstrate how GenAI can transform both operational efficiency and user
experience within the RAN ecosystem.

**3.1 Key Use Cases**

**_3.1.1 Network Management with LLMs and Agentic AI_**

Network operators are often overwhelmed by telemetry, alarms,
configuration options, and user policies. Large Language Models (LLMs)
and agentic AI systems help by interpreting natural language intents and
translating them into network actions. For example, a network engineer


could input, “Prioritize video traffic in Sector 4 during the cricket match,”
and the AI agent would dynamically adjust QoS classes, scheduler weights,
and power settings. Such intent-based systems are being tested in the O-
RAN SMO and Non-RT RIC environments, enabling autonomous closed-
loop control.

**_3.1.2 AI-Based Beamforming_**

Beamforming is central to 5G and beyond, especially in Massive MIMO
systems. Traditionally, beam selection and power allocation were managed
using heuristics or static codebooks. AI, particularly reinforcement learning
(RL) and supervised learning, enables dynamic and adaptive beam
management based on real-time user feedback, channel estimates, and
network load. These methods have shown improved throughput and spectral
efficiency under varying conditions [ 11 ].

**_3.1.3 AI-Based Channel Estimation_**

Conventional channel estimation relies on FIR filters and known pilot
sequences. Deep learning models — especially RNNs, LSTMs, and
transformers — have been shown to predict Channel State Information
(CSI) with higher accuracy [ 6 ], especially in high-mobility or multipath
environments. These models are particularly helpful for indoor
deployments, vehicular scenarios, and mmWave bands.

**_3.1.4 AI/GenAI Workload Offloading_**

With the proliferation of AR/VR and multimodal apps, UEs (e.g.,
smartphones, smart glasses) often run GenAI models for vision, speech, or
translation. These models are heavy and drain device battery. To solve this,
the GenAI workload can be offloaded to the RAN edge — a DU server or
edge GPU — using 5G URLLC. The DU acts as a low-latency inference
host, delivering real-time response back to the device.

**_3.1.5 Multi-Tenancy in RAN Compute_**

Modern edge servers can now support concurrent workloads — signal
processing, GenAI inference, packet forwarding — by slicing GPU/CPU
resources. For example, the **NVIDIA AX800** allows different MIG (Multi-
Instance GPU) slices to handle DU workloads and LLM inference
separately. This form of multi-tenancy is key for reducing Total Cost of


Ownership (TCO) and for enabling AI-native services directly from RAN
nodes [ 12 ].

**3.2 Understanding RAN AI Alliance Categories**

The **RAN AI Alliance** , a collaboration of telecom and AI companies, has
proposed a structured framework to understand the levels of AI integration
in RAN systems. These categories guide how AI is embedded into RAN
infrastructure — from simple automation to full infrastructure convergence.

**AI for RAN**
This category uses AI as an optimization tool for RAN functions. For
example, using supervised ML for traffic prediction [ 8 ] or clustering
algorithms to identify performance bottlenecks. These models typically run
in the Non-RT RIC and are not embedded into real-time RAN processing.

**AI and RAN**
Here, AI models interact in real-time with RAN processes. A good example
is an LLM agent embedded into the Near-RT RIC that monitors KPIs and
sends dynamic control messages to CUs or DUs. This tight integration
requires low-latency inference and secure policy management.

**AI on RAN**
This is the most disruptive category — using RAN infrastructure itself (e.g.,
DU servers or GPUs) to host GenAI workloads like LLM inference or
vision transformers. These workloads may be unrelated to radio processing
but share the same compute substrate. Multi-tenancy, container
orchestration, and dynamic scheduling are critical here.
This taxonomy offers a roadmap for gradual AI integration — from
augmentative to collaborative to converged — and is increasingly being
adopted by operators and vendors alike [ 10 ].

**4 Need for New Compute Categories in RAN**

RAN systems face a unique computational burden, as they simultaneously
handle:

```
Real-time signal processing (PHY layer)
Packet scheduling and switching
```

Fronthaul and backhaul management
Increasingly, AI/GenAI workloads
Traditionally, these needs have been met using separate hardware types
— CPUs for packet/control processing, DSPs or GPUs for signal tasks, and
NICs or FPGAs for fronthaul. However, this fragmented model leads to
inefficiency, increased cost, and limited orchestration capability. The
growing computing demands from GenAI require rethinking this model.

**4.1 Mapping Workloads to Compute Types**

```
Workload type Current compute type
Packet processing (CU) Scalar CPUs (Intel/ARM)
Signal processing (PHY) SIMD: AVX/AMX, GPUs, ARM SVE
Fronthaul/eCPRI Smart NICs (DPU with PTP/eCPRI accel)
AI/GenAI GPUs (H100/H200), CPUs (AMX), DPUs
```
This mapping shows the variety of compute types used in modern RAN
deployments. While this heterogeneity allows optimization per workload, it
complicates orchestration and increases the infrastructure footprint.

**4.2 Vision for Unified SoC or DPU**

The ideal solution would be a **Unified SoC or DPU** , integrating:

Scalar CPU cores (RISC-V/ARM)
Vector engines (AVX, AMX, SVE)
Tensor cores for GenAI
Packet acceleration (PTP, IPsec)
Memory coherency and NIC interfaces
This kind of hybrid processor could handle signal processing, packet
forwarding, and GenAI inference within a single edge server. The diagram
below conceptualizes such a SoC:


Hypothetical Unified SoC with Radio + AI + Packet Engines

Such integrated SoCs, such as DPU(Data Processing Unit)s are already
emerging:

```
NVIDIA BlueField-3 : Combines Arm cores, NIC, crypto engines, and
memory offload [ 13 ]
NVIDIA AX800 : Enables DU to process packet processing, L1 & AI
acceleration [ 9 ]
Marvell Octeon 10 Fusion : Offers vector DSPs, AI engines, and
networking [ 14 ]
```
**4.3 Why RISC-V Matters**

RISC-V is a modular instruction set architecture (ISA) that allows silicon
designers to add custom instructions — ideal for domain-specific needs like
RAN. Potential benefits include [ 7 ]:

```
Adding instructions for FFT, filtering, or symbol demapping.
Embedding matrix ops for GenAI inference.
```

Complex mathematical routine acceleration for Channel Estimation and
Massive MIMO.
As AI-driven RAN evolves, RISC-V provides a future-proofed
foundation to build **application-specific SoCs** that cater to signal + packet
+ AI workloads in one unified design.

**5 Conclusion**

The integration of Generative AI into Radio Access Networks marks a
turning point in how mobile infrastructure is built, optimized, and operated.
As shown throughout this chapter, there is a fundamental mathematical
overlap between AI workloads and traditional RAN signal processing.
Matrix multiplications, dot products, and decomposition techniques —
which power neural networks — are also central to MIMO beamforming,
channel estimation, and other PHY functions. This shared foundation
enables the coexistence and, increasingly, the convergence of signal and AI
workloads on unified vector compute platforms.
The evolution from “AI for RAN” to “AI on RAN” reflects this deeper
architectural integration. Telecom operators now have access to a diverse
portfolio of compute engines: scalar CPUs for packet handling, vector
processors (AVX, AMX, SVE) for PHY acceleration, and GPUs for GenAI
inference. Emerging DPUs like NVIDIA BlueField-3 and AX800, along
with domain-optimized SoCs like Marvell’s Octeon 10 Fusion, further
support this convergence by consolidating multiple compute domains on a
single chip. The flexibility of RISC-V offers an exciting opportunity for
future SoC design — enabling customizable, efficient, and open compute
infrastructure tailored to the unique demands of RAN and GenAI.
As the RAN becomes programmable, intelligent, and multi-tenant,
operators will be able to deliver not only radio connectivity but also AI-
native services — from network-aware GenAI inference at the edge to real-
time adaptive beam control guided by LLM agents. This transformation will
be driven not just by algorithms, but by the underlying computational
fabric. The insights and frameworks presented in this chapter aim to guide
architects, researchers, and industry practitioners in shaping the next
generation of AI-accelerated RAN systems.


**References**

1. https:// developer. nvidia. com/ blog/ unlocking-new-opportunities-with-ai-cloud-infrastructure-for-
    5g-vran/
2.
    https:// arxiv. org/ html/ 2411. 17712v1
3.
    https:// [http://www.](http://www.) intel. com/ content/ dam/ www/ central-libraries/ us/ en/ documents/ 2022-12/ accelerate-
    ai-with-amx-sb. pdf
4.
    https:// datacrunch. io/ blog/ nvidia-h200-vs-h100
5.
    https:// arxiv. org/ html/ 2506. 09505
6.
    https:// research. samsung. com/ blog/ From-AI-Concept-to-Real-World-AI-powered-Modem-for-
    Next-Samsung-Cellular-Networks
7.
    https:// arxiv. org/ html/ 2506. 07873v1
8.
    https:// arxiv. org/ html/ 2409. 20391v1
9.
    https:// developer. nvidia. com/ blog/ enhanced-du-performance-and-workload-consolidation-for-
    5g-6g-with-aerial-cuda-accelerated-ran/
10.
    https:// [http://www.](http://www.) sciencedirect. com/ science/ article/ pii/ S014036642200062 7
11.
    https:// arxiv. org/ pdf/ 2303. 01723
12.
    https:// developer. nvidia. com/ blog/ ai-ran-goes-live-and-unlocks-a-new-ai-opportunity-for-telcos/
13.
    https:// ieeexplore. ieee. org/ document/ 10287294
14.
    https:// [http://www.](http://www.) marvell. com/ content/ dam/ marvell/ en/ public-collateral/ embedded-processors/
    marvell-octeon-10-dpu-platform-product-brief. pdf
       _OceanofPDF.com_


(1)
(2)
(3)

© The Author(s), under exclusive license to Springer Nature Switzerland AG 2026
S. Singh et al. (eds.), _GenAI and LLMs for Beyond 5G Networks_
https://doi.org/10.1007/978-3-032-06418-9_9

**GenAI and LLMs for Source and Channel**

**Coding in B5G Networks**

Jaswanthi Mandalapu^1 , Abhishek Roy^2 and Navrati Saxena^3
Indian Institute of Technology Madras, Chennai, India
Comm. System Design MediaTek USA Inc., San Jose, CA, USA
San José State University, San Jose, CA, USA

```
Navrati Saxena
Email: navrati.saxena@sjsu.edu
```
**Abstract**
The evolution of Beyond 5G (B5G) networks demands communication systems that
are ultrareliable, low latency, and capable of handling diverse, data-intensive
applications. Traditional source and channel coding methods, following Shannon’s
separation principle, often fall short in dynamic and heterogeneous wireless
environments. This chapter reviews the foundational principles of source and
channel coding, highlighting key theoretical insights, and then delves into recent
advances in deep learning, Generative AI (GenAI), and Large Language Model
(LLM)-based joint source-channel coding techniques. Emphasizing semantic and
task-oriented communication, these emerging approaches offer promising solutions
for more intelligent, adaptable, and efficient transmission strategies tailored to the
new requirements of B5G networks.

**Keywords** Beyond 5G (B5G) wireless – Generative AI (GenAI) – Large Language
Model (LLM) – Joint source-channel coding (JSCC) – Information Theory –
Channel Capacity – Entropy – Mutual information – Memoryless – Additive White
Gaussian Noise (AWGN) – Fading

**1 Introduction**

With the rise of Beyond 5G (B5G) networks designed to support ultrareliable, low-
latency, and high-throughput communication for a wide range of applications, from
immersive media to autonomous systems, the shortcomings of traditional


communication methods are becoming more apparent. Conventional source and
channel coding techniques, which rely on the separation principle, often struggle in
dynamic and diverse wireless environments where channel conditions change
rapidly. These emerging use cases call for smarter, more adaptive, and end-to-end
communication approaches that go beyond the limits of existing coding frameworks.
Accordingly, deep learning and Generative AI (GenAI) models are beginning to
reshape the way we design and think about wireless communication, making it more
intelligent, flexible, and reliable [ 1 – 6 ].
In particular, GenAI models such as diffusion models, GANs, transformers, and
Large Language Models, like GPT and LLaMA, offer powerful capabilities for
semantic understanding, contextual encoding, and joint source-channel optimization.
These models can learn compact, meaningful representations of high-dimensional
data (e.g., text, images, or sensor signals), enabling efficient compression and robust
transmission under varying channel conditions. In B5G networks, where spectrum
efficiency and latency are critical, integrating GenAI and LLMs into communication
systems opens up new possibilities for more intelligent, resilient, and purpose-driven
ways of transmitting information.
In this chapter, we provide a brief overview on the fundamentals of source and
channel coding along with highlighting key theoretical results. We further explore
and review some of the recently proposed deep learning, GenAI, and LLM-based
algorithms for joint source-channel coding, focusing on their applications in
semantic and task-oriented communication.

**2 Preliminaries: Information Theory and Wireless**

**Channel Capacity**

In this section, we present a brief overview of fundamental concepts in information
theory relevant to modern communication systems. We begin with the foundational
notions of entropy and mutual information, followed by an introduction to discrete
memoryless channels and their associated capacities. We then describe two widely
studied wireless communication models, the Additive White Gaussian Noise
(AWGN) channel and the Rayleigh fading channel, and further provide their
respective channel capacities. Building upon these preliminaries, we discuss the core
theorems of information theory: the source coding theorem, the channel coding
theorem, and Shannon’s source-channel separation theorem. While the separation
theorem provides a powerful framework for analyzing communication systems, we
also explore scenarios where it does not hold, particularly in the context of modern,
low-latency, and resource-constrained applications.
Motivated by these limitations, we introduce the joint source-channel coding
(JSCC) problem, which offers a unified approach to designing efficient
communication systems. We emphasize its relevance in emerging beyond 5G (B5G)


networks, where traditional assumptions of asymptotically large block lengths and
perfect separation may no longer be viable.

**Entropy** We define entropy for both discrete and continuous random variables, as
this will be useful in our later discussion on wireless channels. However, for most of
the subsequent analysis, we focus primarily on the discrete case.
Let _X_ be a discrete (continuous) random variable taking values in a finite
alphabet X, (support set _S_ ) with probability mass function _pX_ ( _x_ )=P( _X_ = _x_ ),
(probability density function _fX_ ( _x_ )) for each _x_ ∈X ( _x_ ∈ _S_ ). The _discrete entropy_
(differential entropy) of _X_ , denoted by _H_ ( _X_ ), ( _h_ ( _X_ )) quantifies the average
uncertainty or information content associated with the outcome of _X_.

**Definition 1** The entropy of a discrete (continuous) random variable _X_ is defined
as

where the logarithm is taken to base 2 and the resulting entropy is measured in bits.

Note that discrete entropy attains its maximum when the distribution of _X_ is uniform
over X, reflecting maximum uncertainty. Conversely, if _X_ is deterministic, then
_H_ ( _X_ )= 0 , indicating no uncertainty in the outcome. In general, discrete entropy
satisfies 0 ≤ _H_ ( _X_ )≤log(|X|). For a continuous random variable, however, the
differential entropy can take negative values and does not directly represent the
number of bits needed to encode _X_. It is also not invariant under transformations
(e.g., scaling or shifting changes the entropy). Entropy represents the expected
amount of information contained in a random variable. It can also be expressed as
the expectation of the self-information:

where the expectation is taken over the distribution of _X_. This also highlights
entropy as a measure of the average information content per realization of a random
variable. Next, for any two discrete (continuous) random variables _X_ and _Y_ , defined
over finite alphabets X and Y, respectively (supports _SX_ and _SY_ , respectively),
the joint entropy and conditional entropy are defined as follows:

### H ( X ) =−∑ x ∈X pX ( x )log( pX ( x )),

### h ( X ) =−∫ SfX ( x )log( fX ( x )),

```
H ( X ) =E X [log( 1 / pX ( x ))],
h ( X ) =E X [log( 1 / fX ( x ))],
```
```
H ( X , Y ) = H ( X )+ H ( Y | X )= H ( Y )+ H ( X | Y )
```
### H ( Y | X ) =∑ x ∈X pX ( x ) H ( Y | X = x ),


where the conditional entropy _H_ ( _Y_ | _X_ = _x_ ) is given by

Likewise,

where _fX_ , _Y_ ( _x_ , _y_ ) is the joint probability distribution of two continuous random
variables _X_ and _Y_.

**Mutual Information** The mutual information is a measure that quantifies the
amount of information that one random variable contains about another.
Let _X_ and _Y_ be two discrete random variables with joint probability mass
function _pX_ , _Y_ ( _x_ , _y_ )=P( _X_ = _x_ , _Y_ = _y_ ) and marginal distributions _pX_ ( _x_ ) and
_pY_ ( _y_ ), respectively. The mutual information between _X_ and _Y_ , denoted by _I_ ( _X_ ; _Y_ ),
is defined as follows:

**Definition 2** The mutual information between two discrete random variables _X_ and
_Y_ is given by

Note that mutual information is always nonnegative and equals zero iff _X_ and _Y_ are
independent, i.e., when _pX_ , _Y_ ( _x_ , _y_ )= _pX_ ( _x_ ) _pY_ ( _y_ ), for all _x_ ∈X, _y_ ∈Y. It can
also be expressed in terms of entropy as

### H ( Y | X = x )=∑ y ∈Y −P( Y = y | X = x )log(P( Y = y | X = x )).

```
h ( X , Y ) = h ( X )+ h ( Y | X )= h ( Y )+ h ( X | Y )
```
### h ( X | Y ) =−∫ S

```
Y
```
### ∫ S

```
X
```
```
fX , Y ( x , y )log( f ( x | y ) dxdy ,
```
### I ( X ; Y )=∑ x ∈X∑ y ∈Y pX , Y ( x , y )log ppXX (, xY )( pxY , y ( y )).

```
I ( X ; Y )= H ( X )− H ( X | Y )= H ( Y )− H ( Y | X ).
```

**A Discrete Memoryless Channel (DMC)** Consider the basic communication
system model depicted in Fig. 1. To communicate a message _M_ between the sender
(Alice) and the receiver (Bob), the message _M_ is first encoded into a sequence of _n_
symbols _Xn_ =( _X_ 1 , _X_ 2 ,..., _Xn_ ), which are transmitted over _n_ -uses of a
communication channel. The channel could represent various physical media such as
wire line, wireless, or optical links. At the receiver, a noisy version of the transmitted
sequence, denoted by _Yn_ =( _Y_ 1 , _Y_ 2 ,..., _Yn_ ), is observed. Based on _Yn_ , the
decoder attempts to reconstruct the original message, producing an estimate of the

message, denoted by _M_ ˆ.

**_Fig. 1_** Schematic of a typical communication system

A widely studied foundational model of a communication channel in information
theory is the discrete memoryless channel (DMC), which is defined as follows:

**Definition 3** A discrete memoryless channel (DMC) is a system characterized by:

- A finite input alphabet X
- A finite output alphabet Y
- A set of transition probabilities represented by a transition probability matrix
    (tpm) **P** =[ _p_ ( _y_ | _x_ ] _y_ ∈Y, _x_ ∈X, such that for every _x_ ∈X and _y_ ∈Y _p_ ( _y_ | _x_ )≥ 0 ,
    and for every _x_ ∈X, ∑ _y_ ∈Y _p_ ( _y_ | _x_ )= 1 , where _p_ ( _y_ | _x_ )=P( _Y_ = _y_ | _X_ = _x_ )
    denotes the probability of receiving symbol _y_ at the output given that symbol _x_
    was transmitted.

The channel is said to be memoryless if the output symbols are conditionally
independent of one another given the input sequence. That is, for input sequence
_xn_ ∈X _n_ and output sequence _yn_ ∈Y _n_ , the channel transition probability satisfies

A discrete channel is often denoted by the triple ⟨X, _p_ ( _y_ | _x_ ),Y⟩. We now proceed
to define another fundamental concept in information theory: the notion of channel
capacity.

```
p ( yn | xn )= Πin = 1 P( Yi = yi | Xi = xi ).
```

**Definition 4** An ( _M_ , _n_ ) code for the channel ⟨X, _p_ ( _y_ | _x_ ),Y⟩, as illustrated in
Fig. 1 , consists of the following components:

- A message set M ={ 1 , 2 ,..., _M_ }, representing the set of _M_ possible messages
- An encoding function _Xn_ :M →X _n_ , which maps the message to a codeword
    of length _n_. The resulting set of codewords { _xn_ ( 1 ), _xn_ ( 2 ),..., _xn_ ( _M_ )} is
    referred to as the codebook.
- A decoding function _g_ :Y _n_ →M, which assigns a message estimate to each
    received sequence _yn_ ∈Y _n_

**Definition 5** The maximal decoding error probability of an ( _M_ , _n_ ) code is defined
as

where _xn_ ( _i_ ) denotes the codeword corresponding to message _i_ , and _g_ ( _Yn_ ) is the
output of the decoding function.

**Definition 6** Let the rate _R_ of an ( _M_ , _n_ ) code be defined as _R_ = log^2 _n_ ( _M_ ) bits per
transmission. The rate _R_ is said to be achievable for the channel ⟨X, _p_ ( _y_ | _x_ ),Y⟩ if
there exists a sequence of codes with parameters (⌈^2 _nR_ ⌉, _n_ ) such that the maximal
decoding error probability satisfies _Pe_ ( _M_ , _n_ )→ 0 , as _n_ →∞.

**Definition 7 (Channel Capacity)** The capacity of a channel is defined as the
supremum of all achievable rates:

For a DMC, the channel capacity is given as

```
(1)
```
where the maximization is taken over all possible input distributions _pX_ ( _x_ ), and
_I_ ( _X_ ; _Y_ ) denotes the mutual information between the input and output of the
channel. We will formally establish this result in the upcoming discussion of
Shannon’s channel coding theorem. Before that, we briefly define two important
models of wireless communication channels, namely the Additive White Gaussian
Noise (AWGN) channel and the Rayleigh Fading channel, and provide their
respective channel capacities.

**Examples of Wireless Channels** We now briefly review two widely studied
wireless channel models and their respective capacities, which will be referenced in
subsequent chapters: (i) The Additive White Gaussian Noise (AWGN) channel (ii)
The Rayleigh fading channel

```
Pe ( M , n )= max
i ∈{ 1 , 2 ,..., M }
```
```
P( g ( Yn )≠ i | Xn = xn ( i )),
```
```
C =sup{ R : R is achievable}.
```
```
C =max
pX ( x )
```
```
I ( X ; Y ),
```

_AWGN Channel_ The Additive White Gaussian Noise (AWGN) channel is a
fundamental continuous-alphabet, discrete-time model commonly used to represent
many real-world communication systems. The channel adds independent Gaussian
noise to each transmitted symbol and is defined as follows:
Let _Xi_ denote the transmitted symbol at time _i_ , which may be binary or real-
valued. The received signal _Yi_ is modeled as

where _Zi_ ∼N( 0 , _N_ ) is zero-mean Gaussian noise with variance _N_ , independent
across time and independent of _Xi_. A commonly imposed constraint in such settings
is the average power constraint on the transmitted codewords. That is, for any
codeword of length _n_ , ( _x_ 1 , _x_ 2 ,..., _xn_ ), we require _n_^1 ∑ _ni_ = 1 _x_^2 _i_ ≤ _P_ , where _P_
denotes the maximum allowable average input power (Fig. 2 ).

**_Fig. 2_** The Addictive White Gaussian Noise (AWGN) channel model, where the received signal is the sum of
transmitted input and independent Gaussian noise

**Definition 8** The channel capacity of the AWGN channel under an average power
constraint _P_ is defined as

where the maximization is over all input distributions satisfying the power
constraint.

**Theorem 1 ([ 7 ])** _The capacity of the AWGN channel with noise variance N and
power constraint P is given by_

```
and is achieved when the input X ∼N( 0 , P ) is Gaussian distributed.
```
```
Yi = Xi + Zi ,
```
```
C = max
E[ X^2 ]≤ P
```
```
I ( X ; Y ),
```
```
C =^12 log( 1 + NP ) bits per channel use ,
```

_The Rayleigh Fading Channel_ This is a fundamental model used to characterize
wireless communication in environments where there is no dominant line-of-sight
(LOS) path between the transmitter and the receiver. The channel introduces
multiplicative fading due to multipath propagation in addition to additive noise and
is modeled as follows: Let _Xi_ denote the transmitted symbol at time _i_ , and _Yi_
indicate the received symbol. Then,

where _Hi_ is the complex Gaussian fading coefficient modeling the Rayleigh fading,
independent across time. Further, _Zi_ is the additive Gaussian noise, with noise
variance _N_ , also independent across time and independent of _Hi_ and _Xi_. The fading
coefficient _Hi_ captures the effect of rapid amplitude and phase changes in the
wireless channel due to multipath. A common assumption is that the fading is fast
(i.e., varies independently across symbols) and the receiver has perfect knowledge of
_Hi_ , while the transmitter may or may not. Similar to the AWGN channel, an average
power constraint is imposed on the transmitted codewords.

**Theorem 2 ([ 8 ])** _If the receiver has perfect channel state information (CSI) and
the transmitter does not, then the ergodic capacity of the Rayleigh fading channel is
given by_

See [ 8 ] for more details.

**3 The Fundamentals of Source-Channel Coding**

**The Source-Channel Coding Results** Following Shannon’s groundbreaking work
[ 9 ], the conventional digital communication system can be abstracted using the
block diagram shown in Fig. 3. In this framework, the encoder is conceptually
divided into two distinct components:

- A source encoder, which performs data compression to eliminate redundancy in
    the source
- A channel encoder, which adds redundancy in a controlled manner to protect the
    data against channel noise

```
Yi = HiXi + Zi ,
```
```
C =E[log 2 ( 1 + | H |
```
(^2) _P
N_ )] _bits_^ _per_^ _transmission_.


**_Fig. 3_** Schematic of a conventional communication system; here, _SE_ indicates the source encoder, and _CE_
indicates the channel encoder
This modular design allows for the separate optimization of data compression
and error protection, significantly reducing system’s complexity. Shannon’s source-
channel separation theorem formalizes this principle, showing that such two-stage
architecture achieves the same performance as any optimal joint source-channel
coding scheme, under ideal conditions. However, while this separation holds for
many settings, particularly when long blocklengths and average distortion measures
are allowed, it is not universally optimal. There exist practical scenarios, such as
delay-sensitive or finite-blocklength systems, where separation fails to achieve the
best performance, motivating the need for joint source-channel coding strategies.
In the following, we formally present the source coding theorem and the channel
coding theorem separately, along with a discussion on their limitations and the need
for joint source-channel coding.

**Source Encoding: Compression of Information**
Suppose the message sample _s_ ∈M to be transmitted is drawn from a finite set of
_M_ possible messages M ={ _m_ 1 , _m_ 2 ,..., _mM_ }. Then, to represent _l_ message
samples _s_ 1 , _s_ 2 ,..., _sl_ efficiently using binary digits, we use a source encoder,
which maps the samples into a sequence of _K_ bits, denoted
_DK_ =( _D_ 1 , _D_ 2 ,..., _DK_ ). A naive compression scheme is to represent each
message sample using log 2 (M) bits per sample. However, the primary goal of the
source encoder is to minimize the average number of bits per message while
ensuring that the original message can be recovered without any loss at the receiver.
This process is known as lossless compression.
According to the source coding theorem given by Shannon in [ 9 ], the minimum
average number of bits required to represent each source symbol (or message
sample) _s_ is given by the entropy of the source, defined as

That is, for a discrete memoryless source, no lossless compression scheme can
achieve a rate lower than _H_ (M) bits per symbol. Formally, the result is stated as
follows:

### H (M)=−∑ m

```
i
```
```
p ( mi )log( p ( mi )).
```

**Theorem 3** _Any l independent and identically distributed (i.i.d.) samples
s_ 1 , _s_ 2 ,..., _sl drawn from a discrete memoryless source with message set_ M _and
entropy H_ (M) _can be compressed into more than lH_ (M) _bits with arbitrarily
small probability of information loss, as l_ →∞. _Conversely, if the samples are
compressed into fewer than lH_ (M) _bits, then reliable recovery is impossible._

**Channel Coding: Protection Against Noise**
Once we obtain the compressed data bits _DK_ corresponding to message samples
_s_ 1 , _s_ 2 ,..., _sl_ , where each _si_ ∈M, they are passed through the channel encoder,
which maps them to a longer sequence of _N_ bits, forming the codeword
_XN_ =( _X_ 1 , _X_ 2 ,..., _XN_ ). The channel encoder adds structured redundancy to
ensure that the original data bits can be recovered reliably at the receiver, even in the
presence of noise introduced by the communication channel. The performance limit
of a channel is characterized by its channel capacity as in ( 1 ). The Shannon channel
coding theorem states that the reliable communication is possible if and only if the
rate given as _R_ = _K_ / _N_ satisfies _R_ ≤ _C_. Formally, the theorem is stated as
follows:

**Theorem 4** _For a discrete memoryless channel, for every rate R_ < _C, there exists_

_a sequence of_ (^) ( 2 _NR_ , _N_ ) _codes whose error probability_ (^) _Pe_ ( 2 _NR_ , _N_ ) _approaches
zero as N_ →∞_. Conversely, if a sequence of_ ( 2 _NR_ , _N_ ) _codes achieves zero error
probability, i.e.,_ (^) _Pe_ ( 2 _NR_ , _N_ )= 0 _, then the rate must satisfy_ (^) _R_ ≤ _C_.
Next, we state the source-channel separation theorem that connects the source and
channel coding theorems, ensuring reliable communication when the source entropy
and channel capacity constraints align.
**Theorem 5** _For a discrete-time memoryless source generating l i.i.d. message
symbols s_ 1 , _s_ 2 ,..., _sl from the set of M possible messages denoted by_
M ={ _m_ 1 , _m_ 2 ,..., _mM_ }, _with entropy H_ (M) _and a discrete-time memoryless
channel with capacity C_ , _then:_

- _If H_ (M)< _C_ , _then for any ε_ > 0 _and l sufficiently large, there exists (i) a_
    _source code compressing l message samples into K bits such that_
    _lH_ (M)≤ _K_ ≤ _lH_ (M)+ _ε and (ii) a channel code with K_ / _N_ < _C_ − _ε such_
    _that the error probability goes to zero, as n_ →∞.
- _Conversely, if H_ (M)> _C_ , _then the error probability is always bounded away_
    _from zero regardless of l and N_.

Note that while the source-channel separation theorem provides a powerful
framework for communication system design, it comes with fundamental
limitations. The theorem guarantees optimality only in point-to-point


communication systems involving stationary and ergodic sources and channels, and
only in the asymptotic regime, i.e., as block length tends to infinity. Under these
conditions, as stated earlier one can independently optimize source coding and
channel coding without any loss in performance. However, many modern
communication scenarios violate these assumptions, thereby undermining the
applicability of separation-based designs. For instance:

1. Multiuser networks with correlated sources, such as in sensor networks, require

#### joint strategies to exploit source dependencies.^

2. Time-varying and non-ergodic channels, like fading channels in wireless
    systems, render separate channel coding schemes ineffective without prior
    channel state knowledge.
3. Latency-critical applications, such as interactive voice and video
    communication, necessitate communication over short block lengths, where
    asymptotic optimality no longer holds.

**Example 1** As an illustrative example, consider a system transmitting short
English messages such as “YES,” “NO,” and “OK” over a fading wireless channel
with unknown and time-varying channel gains. If one were to apply source coding
(e.g., Huffman or arithmetic coding) followed by traditional channel coding (e.g.,
convolutional or LDPC codes), the short length of the messages would lead to poor
compression efficiency and insufficient redundancy to combat channel fluctuations.
Moreover, with no channel state information at the transmitter, fixed-rate channel
codes are highly suboptimal. In contrast, a joint design that maps each message
directly to a robust channel input based on both its semantic meaning and channel
conditions can achieve more reliable and efficient transmission under these
constraints.

**Example 2** Consider another example where the source-channel separation
theorem fails in a nonstationary setting as given in [ 10 ]. Let the source _Z_ and
channel _W_ be memoryless but governed by synchronized switches that are either in
the “up” or in the “down” position. The switches change deterministically just
before times 2 _i_ , _i_ = 1 , 2 , 3 ,..., with the initial state at time _i_ = 1 being “down.” Let
_J_ denote the set of times when the switch is “up,” for example, _i_ = 1 ,
_J_ ={ 2 , 3 , 8 , 9 , 10 , 11 , 12 , 13 , 14 , 15 , 32 , 33 ,..., 62 , 63 , 128 , 129 ,.... At times
_i_ ∈ _J_ , the binary source emits i.i.d. bits uniformly from { 0 , 1 },, and the channel is
noiseless. For _i_ ∉ _J_ , the source output is deterministic, and the channel behaves as a
Binary Symmetric Channel with crossover probability 1 / 2. Under this setting,
observe that reliable communication is possible because both transmitter and


receiver have perfect knowledge of the deterministic set _J_ and can align their
operations accordingly. However, also note that this violates the classical separation
condition since _H_ ( _Z_ )= 2 / 3 , and _C_ ( _W_ )= 1 / 3 , highlighting that
_H_ ( _Z_ )> _C_ ( _W_ ), does not stop reliable communication when the stationarity is
absent.
All these limitations highlight the need for Joint Source-Channel Coding (JSCC),
where both the source statistics and channel characteristics are jointly considered in
the encoding and decoding processes. Recently, researchers are proposing deep
learning-based JSCC architectures that unify source and channel coding into a
single, end-to-end trainable model. These systems, often modeled as autoencoders,
learn robust, adaptive representations that naturally handle varying channel
conditions and source structures. Such designs have shown superior performance to
separation-based schemes in scenarios involving limited bandwidth, low latency, and
unreliable channels, making JSCC an essential tool for next-generation
communication systems.
To summarize the discussion on conventional coding theorems, although source-
channel separation offers modularity and analytical elegance, joint design
approaches are increasingly necessary in practical communication systems that
operate under nonideal, resource-constrained, or dynamic conditions. In our next
section, we provide a brief overview of the JSCC problem along with recent
developments of JSCC algorithms using GenAI and LLMs.

**4 The GenAI Algorithms for Joint Source-Channel**

**Coding**

In this section, we consider the problem of transmitting an image over a wireless
communication channel in a Joint Source-Channel Coding (JSCC) framework. In
this setting, the transmitter directly maps the input image, represented as a real-
valued vector _XK_ (also represented as **X** ), to a sequence of complex-valued
transmission symbols _ZN_ , which are then sent over the channel. Unlike traditional
digital communication systems that separate source coding and channel coding,
JSCC jointly optimizes both tasks within a single, end-to-end encoding function. For
image transmission tasks, it is typically assumed that _N_ < _K_ , where the ratio
_N_ / _K_ is referred to as the bandwidth compression ratio. This reflects scenarios in
which the available channel bandwidth is limited relative to the source data rate.
Furthermore, due to practical constraints such as limited transmission power,
interference, and hardware limitations, the transmitted signal _ZN_ must often satisfy
certain power constraints, e.g., peak or average power limits. Once transmitted, the
signal _ZN_ is corrupted by channel impairments such as noise, fading, or
interference. The receiver observes the noisy version of the transmitted symbols and


aims to reconstruct an estimate _X_ ˆ _K_ (also represented as **X** ˆ) of the original image.
The primary goal of JSCC is to design the encoder and decoder mappings to achieve
high-quality reconstruction while satisfying system constraints and leveraging the
joint statistics of the source and the channel.
We now discuss recent advancements in deep learning-based algorithms for the
JSCC problem from [ 11 ], along with their extensions and refinements using
generative AI (GenAI) techniques proposed in [ 12 ].

**4.1 Deep JSCC: The Deep Learning-Based Solution**
Recent advances in deep learning, particularly in the use of deep neural networks
(DNNs) and autoencoder architectures, have shown great promise in end-to-end
design of communication systems. These methods allow for learning highly
nonlinear mappings that jointly perform source compression and channel encoding
without explicitly separating the two tasks. This section presents a deep learning-
based JSCC framework designed for transmitting images over noisy wireless
channels using convolutional neural networks (CNNs).

**System Model** The proposed JSCC architecture is inspired by the structure of an
autoencoder, comprising an encoder neural network at the transmitter and a decoder
neural network at the receiver. A block diagram of the system is shown in Fig. 1 b.
Let the input image be denoted by **X** ∈R _K_ , where _K_ is the dimensionality of
the input. The encoder maps **X** to a sequence of _N_ complex-valued symbols
**Z** ∈C _N_ , which are transmitted over the wireless channel. The mapping is defined
by a deterministic function _fθ_ :R _K_ →C _N_ , parameterized by _θ_ , where the
parameters are learned via training.


**The Encoding Process** The encoder function _fθ_ is realized using a convolutional
neural network (CNN), which consists of (i) a sequence of convolutional layers for
hierarchical feature extraction, (ii) PReLU (parametric ReLU) activation functions
to introduce nonlinearity, and (iii) a final normalization layer to ensure power
constraints are satisfied. The encoder outputs an intermediate signal **Z** ̃∈C _N_ at the
last layer, which is normalized to meet the average transmit power constraint _P_ using

where **Z** ̃∗ denotes the conjugate transpose of **Z** ̃. This ensures the average power
constraint at the transmitter, i.e., _N_^1 E[ **Z** ∗ **Z** ]≤ _P_. Finally, each symbol of the
complex-valued vector **Z** is transmitted by modulating its real and imaginary parts
over the in-phase (I) and quadrature (Q) components of the digital baseband signal
(Fig. 4 ).

**_Fig. 4_** Schematic of Deep JSCC

**The Channel Model** Recall the wireless channel introduces impairments such as
noise and fading. These are incorporated into the system as nontrainable layers,
making the overall JSCC system differentiable and trainable end to end. Let
_η_ :C _N_ →C _N_ denote the channel transformation. The channel could be any of the
common models described above in our earlier section.

**The Decoding Process** At the receiver, the corrupted signal **Z** ˆ= _η_ ( **Z** )∈C _N_ is
processed by a decoder function _gφ_ :C _N_ →R _K_ , parameterized by _φ_. The decoder
is also realized using a CNN, which typically consists of (i) transpose convolutional
layers to upsample and reconstruct image features and (ii) nonlinear activation
functions to enable flexible reconstruction mappings.

```
The decoder produces an estimate X ˆ ∈R K of the original input image X.
The objective here is to jointly train the encoder and decoder so as to minimize
```
the average distortion between the original image **X** and its reconstruction **X** ˆ. This
is formalized by the optimization problem:

```
Z =√ NP √ Z ̃ ̃
Z ∗ Z ̃
```
```
,
```
```
( θ ∗, φ ∗)=argmin
θ , φ
```
```
E X , X ˆ[ d ( X , X ˆ)],
```

where _d_ ( **X** , **X** ˆ) is a distortion metric, such as mean squared error (MSE), and the
expectation is taken over the data distribution. Since the true distribution _p_ ( **X** ) is
unknown, the expected distortion is approximated empirically using a dataset of
images.

**4.2 The GenAI Extensions**
While conventional deep learning-based JSCC methods focus on minimizing pixel-
wise distortion, e.g., mean squared error (MSE) between the transmitted and
received images, this approach often fails to preserve semantic and perceptual
quality, especially under extreme channel conditions, such as low bandwidth
compression ratios and low signal-to-noise ratios (SNRs). To address this limitation,
recent advances leverage deep generative models (DGMs), such as Generative
Adversarial Networks (GANs), which can better align image reconstructions with
human perception.
In particular, two DGM-based JSCC frameworks are proposed: InverseJSCC and
GenerativeJSCC. Both approaches incorporate the StyleGAN-2 architecture, which
is a state-of-the-art generator known for high-fidelity image synthesis, as part of the
reconstruction pipeline at the receiver.

**InverseJSCC: An Unsupervised Inverse Problem to Deep JSCC**
The InverseJSCC framework is introduced as an unsupervised enhancement to
conventional DeepJSCC discussed in the previous section. Rather than redesigning
the encoder or decoder, InverseJSCC treats the end-to-end DeepJSCC system as a
black box forward process (FP) and casts the reconstruction of the original image as
an inverse problem. This strategy focuses on improving perceptual quality in the
reconstructed images, specifically under challenging channel conditions, by
leveraging the generative capabilities of a pretrained deep generative model.

**Motivation and Methodology** In DeepJSCC, the received image **X** ˆ is a noisy and
potentially distorted version of the source image **X** , reconstructed after passing
through the encoder, wireless channel, and decoder. When the channel operates
under extreme constraints, such as low bandwidth compression ratio (BCR) or low
signal-to-noise ratio (SNR), the reconstructed image may lose semantic fidelity,
even if the distortion metrics (e.g., MSE) are minimized. Instead of retraining the
JSCC system, InverseJSCC aims to recover a semantically faithful image using only

the received distorted image **X** ˆ, by treating it as the measurement in an inverse
problem.
An inverse problem is commonly expressed as **Y** =A( **X** )+ _nA_ , where **Y** is
the observed noisy output, A is the forward operator—a transformation from input
to output, and _nA_ is the additive noise. In the context of this problem, the forward
operator A approximates the encoder-channel-decoder pipeline, and the goal here is


to recover **X** given **X** ˆ =A( **X** )+ **n** _A_. A standard objective for inverse problems is
to minimize

However, since **X** is a high-dimensional image data, effective reconstruction
requires strong priors about its structure. Note that classical inverse problems use
sparsity or geometric priors, but InverseJSCC assumes that **X** lies within the range
of a pretrained GAN generator. Further, in this setting, the forward operator A( **X** )
is modeled as the Deep JSCC operator, which is given as _gφ_ ( _η_ ( _fθ_ ( **X** ), _σ_^2 )), where
_fθ_ is the encoder neural network (parameterized by _θ_ ), _η_ is the channel noise
operator, _gφ_ is the decoder neural network, and _σ_^2 is the channel noise power. This
model captures the nonlinear and stochastic behavior of the DeepJSCC system. Note
that although we know the architecture of A, the realization of noise during testing
is unknown, making A only partially observable.

**Image Reconstruction Using StyleGAN-2 Generator** To reconstruct the image

**X** from its degraded observation **X** ˆ, we assume that **X** lies in the range of a
pretrained GAN generator _G_ :R _q_ →R _K_ , trained on the same image domain (e.g.,
faces). For InverseJSCC proposed in [ 12 ], StyleGAN-2 is used due to its superior
capacity for generating photorealistic images.
Let **w** be the latent vector controlling the style and **n** _G_ be the noise vector
controlling image details. Then, the generated image is _G_ ( **w** , **n** _G_ ), and the goal is to

find optimal **w** and **n** _G_ such that A( _G_ ( **w** , **n** _G_ ))≈ **X** ˆ. In particular, the following
optimization problem is solved:

Here, the regularization term is defined as

Further, each _λi_ is a hyperparameter that controls the relative importance of the
corresponding term during optimization. The function L(⋅) denotes the Learned
Perceptual Image Patch Similarity (LPIPS) loss [ 13 ], which measures perceptual
similarity between images, while GEO(⋅) denotes the geodesic distance,
representing the shortest path between two points in the latent space manifold.
Moreover, the StyleGAN-2 generator used in InverseJSCC is organized in a
hierarchical, multistage structure. Specifically, the generator is composed of a
sequence of subnetworks, i.e., _G_ = _G_ 4 ∘ _G_ 3 ∘ _G_ 2 ∘ _G_ 1 , where each subnetwork
transforms the representation progressively closer to the final image. More precisely,
each _Gi_ processes the output of the previous one, gradually building the image from
high-level features to fine details; that is, _G_ 1 :R _q_ →R _t_^1 latent input to first

```
MSE(A( X ), Y )=||A( X )− Y ||^22.
```
```
min
w , n G
```
```
λ 1 ||A( G ( w , n G ))− X ˆ||^2 + λ 2 L(A( G ( w , n G )), X ˆ)+ λ 3 R( w , X ˆ, G , n G ).
```
```
R( w , X ˆ, G , n G )= λ 4 L( G ( w , n G ), X ˆ)+ λ 5 ,GEO( w ).
```

transformation, _G_ 2 :R _t_^1 →R _t_^2 , _G_ 3 :R _t_^2 →R _t_^3 , and _G_ 4 :R _t_^3 →R _K_ , where R _K_
is the final image output space.

_Optimization Parameters_ The weights of _A_ and _G_ are frozen (not changed).
Instead, InverseJSCC optimizes only two things: (i) the latent vector _w_ , which
controls the style/content of the generated image and (ii) the noise vector _nG_ , which
adds fine-grained detail. The optimization is done layer by layer, recursively. That is,
first optimize _w_ (input to _G_ 1 ), then compute intermediate values like _ŵ_ = _G_ 1 ( _w_ ∗),
and pass them to the next layers. This process is continued until the full image is
generated. The full schematic is shown in Fig. 5.

**_Fig. 5_** Schematic of InverseJSCC, a scheme that uses Deep JSCC along with reconstruction of image using
GenAI

To summarize, DeepJSCC performs deep learning-based compression and
reconstruction without requiring explicit source-channel coding. Specifically, it
takes an image, converts it into an optimized, compressed, and modulated signal,
transmits it over a noisy channel, and reconstructs it at the receiver using neural
networks. However, when the wireless channel conditions are very poor, such as low
SNR regimes, or limited bandwidth, DeepJSCC can still reconstruct the image, but
the result may be highly distorted compared to the original. This is where generative
AI frameworks like InverseJSCC come into play which reconstructs the received
image closer to the original from the distorted output.
We now present GenerativeJSCC, which, rather than combining DeepJSCC with
InverseJSCC, performs end-to-end training using generative models with perceptual
optimization objectives.

**Generative JSCC: A Generative AI-Based Approach to JSCC**
While InverseJSCC exploits the generative capacity of a pretrained GAN at test time
to improve the perceptual quality of reconstructions, GenerativeJSCC adopts a
fundamentally different approach by integrating the generative model directly into
the decoder architecture and jointly training the entire encoder-decoder system in an
end-to-end fashion. This enables the model to learn a more effective representation
tailored to the source distribution and the channel characteristics, leading to better
reconstructions that are both perceptually convincing and semantically faithful,


particularly in the presence of severe channel impairments. Specifically,
GenerativeJSCC is designed with the goal of maximizing the semantic similarity
between the original and reconstructed signals while maintaining robustness to noisy
transmission. The encoder and decoder are represented by deep neural networks,
parameterized, respectively, by _θ_ and _φ_ , and are trained jointly on the same dataset
used for evaluation. The encoding and decoding maps happen the same as before,

i.e., **Z** = _fθ_ ( **X** ) and **X** ˆ = _gφ_ ( _η_ ( **Z** , _σ_^2 )), where **X** and **X** ˆ are the transmitted and
reconstructed images, respectively.
To ensure that the reconstructed images are not only accurate in a pixel-wise
sense but also perceptually meaningful, GenerativeJSCC optimizes a weighted
distortion loss combining mean squared error (MSE) and a perceptual similarity
metric known as LPIPS (Learned Perceptual Image Patch Similarity). That is,
GenerativeJSCC solves the following optimization problem:

where L(.) indicates the LPIPS loss function as in [ 13 ], and further, _γ_ 1 , _γ_ 2 control
the relative emphasis on pixel fidelity versus perceptual quality.
The key difference between DeepJSCC, Inverse JSCC, and GenerativeJSCC lies
in the decoder, where in GenerativeJSCC, the StyleGAN-2 generator is placed
within the decoder. Specifically, the decoder of GenerativeJSCC integrates residual
networks with the StyleGAN-2 generator, which takes in two distinct inputs for
image reconstruction:

- The latent style vector **L** , which governs the global structure and appearance (or
    “style”) of the output image
- The stochastic noise maps **N** , typically drawn from a Gaussian distribution,
    which influences the fine, high-resolution details
       More precisely, the decoder is designed to extract both **L** and **N** from the noisy

channel output **Z** ˆ, which is received at the receiver. In order to do this, the received

sequence **Z** ˆ is passed through a cascade of residual blocks that are augmented with
attention mechanisms and Adaptive Feature (AF) modules as proposed in [ 14 ].
These modules enhance the model’s ability to capture both the semantic content and

channel-specific distortions present in **Z** ˆ. Each residual block produces an
intermediate feature map, which is then split into three branches:

1. The first branch undergoes global average pooling followed by a 1 × 1
    convolution, referred to as a projection function _Ql_ which produces a segment
    of the initial latent vector **L** ̃.

```
argmin
θ , φ
```
```
γ 1 || X − X ˆ||^22 + γ 2 L( X , X ˆ),
```

2. The second branch is directly processed by another 1 × 1 convolution, denoted
    by _Qn_ , which generates a corresponding segment of the noise vector **N** in the
    form of single-channel feature maps.
3. The third branch carries forward the remaining features to the next residual

#### block for continued processing.^

After processing through all residual blocks (typically eight), the partial
projections are concatenated to form the full latent and noise vectors:

The initial latent vector **L** ̃ is then passed through a multilayer linear mapping
network with Leaky ReLU activations to yield the final latent vector **L**. This final
vector **L** , along with the noise vector **N** , is fed into the pretrained StyleGAN-2
generator to synthesize the high-resolution reconstructed image. Importantly, the
weights of the StyleGAN-2 generator are kept fixed during training. Only the
encoder and residual-decoder components are trained, allowing the model to
leverage the rich generative prior encoded in the GAN.
To optimize both structure and detail in the reconstructions, the model is trained
using a two-stage strategy: (i) The decoder is trained to reconstruct images using
only the latent vector **L** ; the layers generating the noise vector **N** , i.e., the _Qn_ layers,
are frozen, and (ii) all parts of the model are trained, including the _Qn_ layers.
This staged approach allows the decoder to first capture the semantic structure of
the image and then fine-tune the fine-grained details, resulting in perceptually
enhanced reconstructions under challenging channel conditions.
To summarize, GenerativeJSCC builds upon and extends the capabilities of
DeepJSCC and InverseJSCC by incorporating a generative model directly into the
communication pipeline. Unlike DeepJSCC, which uses a deep neural network to
jointly perform source and channel coding with the primary goal of minimizing
pixel-wise distortion, GenerativeJSCC is designed to also preserve perceptual
quality and semantic content. In contrast to InverseJSCC, which applies a generative
model like StyleGAN only at the receiver as a post-processing step to improve the
output of DeepJSCC, GenerativeJSCC integrates the generative decoder into the
end-to-end training process. This allows the system to learn how to map noisy
channel outputs directly to high-quality, realistic images. As a result,
GenerativeJSCC achieves improved performance in both distortion and perceptual
metrics, particularly in low signal-to-noise ratio and low bandwidth scenarios.

```
L ̃ =Concat( Ql 1 , Ql 2 ,..., Ql 8 )
N =Concat( Qn 1 , Qn 2 ,..., Qn 8 ).
```

**5 The Large Language Models for Joint Source-**

**Channel Coding in Semantic Communications**

With the rise of Semantic Communication (SemCom), which aims not only to
transmit bits but also to preserve the intended meaning of the transmitted message,
the integration of Large Language Models (LLMs) into the Joint Source-Channel
Coding (JSCC) pipeline represents a transformative advancement. In this section, we
introduce a preliminary framework proposed in [ 15 ] that incorporates Large
Language Model Meta AI (LLaMA-2), a state-of-the-art open-source LLM, into
JSCC. This integration of semantic understanding and end-to-end learning for
communication offers significant potential in applications where context and intent
are as important as the data itself.
The proposed algorithm in [ 15 ], called as LLaMA-2 JSCC, is designed to encode
not just the raw data but also its semantic interpretation, enhancing both fidelity and
robustness in noisy communication settings. At a high level, the LLaMA-2 JSCC
framework has the four-step approach as stated below:

1. First, the input data (text or other symbolic representations) is first processed by

#### LLaMA-2 to extract semantic features or prompts.^

2. Second, the semantically enriched data is encoded into an ASCII format and

#### modulated using BPSK.^

3. Third, the modulated signal is transmitted over a noisy channel.
4. Finally, at the receiver, demodulation and decoding are followed by semantic

#### interpretation, optionally refined by context distillation.^

Formally, let **X** ∈S _K_ represent the source signal, which could be text or
symbol sequence. The encoder _fθ_ :S _K_ →C _N_ , maps the source signal into a
complex-valued codeword to be transmitted over the channel. The received signal **R**
is defined as **R** = _η_ ( _fθ_ ( **X** ), _σ_^2 ), where _η_ is the noisy channel and _σ_^2 indicates the
noise power. The decoder _gφ_ :C _N_ →S _K_ reconstructs the source message

**S** ˆ= _gφ_ ( **R** ). Similar to the earlier GenAI algorithms, in this setup, to let the decoder
retain or reconstruct the semantic information from the transmitted text, the encoder
and decoder parameters _θ_ , _φ_ are trained such that they minimize the following loss
functions. Firstly,

where _λ_ is the balancing hyperparameter, and **C** denotes the contextual knowledge.
Here, the distillation term encourages the model to preserve not just the signal but

```
L 1 :L distill =E[|| S − S ˆ||^2 ]+ λ. D ( S , C ),
```

also contextually relevant semantics. Next, the model is trained to ensure semantic
safety, i.e.,

where _fθ_ is the original trained model, and _fθ_ ′ is the fine-tuned model conditioned on
the safety constraint **C**. Lastly, the model is also trained to minimize the fine-tuning
loss which is

where L(.,.) indicates the general loss function such as Mean Squared Error
(MSE), and _Pprompt_ ( **S** ) denotes a probability distribution over the space of
semantically enriched prompt-based inputs.
In summary, this section introduces SemComLLM, a novel framework that
integrates LLaMA-2, a large language model (LLM), into the Joint Source-Channel
Coding (JSCC) paradigm to enable semantic communication. Further, incorporating
LLaMA-2 into JSCC presents new opportunities and several open challenges. Some
of the key concerns include the high computational and memory demands of LLMs,
limitations of current wireless infrastructures, and the need for efficient model
compression without sacrificing semantic accuracy. Additional challenges involve
ensuring real-time inference, multi-agent semantic consistency, and robust security
in decentralized environments. Note that addressing these issues is crucial for
deploying SemComLLM effectively in practical, resource-constrained scenarios
such as edge devices and autonomous systems.

**6 Summary**

In this chapter we discuss how the evolving demands of Beyond 5G (B5G) networks
are challenging for traditional source and channel coding methods based on
Shannon’s separation principle. It highlights the limitations of these conventional
techniques in dynamic wireless environments and reviews some of the emerging
approaches based on deep learning, Generative AI (GenAI), and Large Language
Models (LLMs). These new methods enable joint source-channel coding, offering
semantic and task-oriented communication strategies that are more adaptive,
intelligent, and efficient for modern wireless network requirements.

**References**

1. D. Gündüz, M.A. Wigger, T.Y. Tung, P. Zhang, Y. Xiao, Joint source—channel coding: fundamentals and
    recent progress in practical designs, in _Proceedings of the IEEE_ (2024)
2.
    L.X. Nguyen, A.D. Raha, P.S. Aung, D. Niyato, Z. Han, C.S. Hong, A contemporary survey on semantic
    communications: theory of mind, generative AI, and deep joint source-channel coding. arXiv preprint
    arXiv:2502.16468 (2025)

```
L 2 :L safety =E S ∼ P ( S )[|| fθ ( S )− fθ ′( S | C )||^2 ],
```
```
L 3 :L fine − tune =E S ∼ Pprompt ( S )[L( fθ ( S ),ˆ S )],
```

3.
H. Ye, G.Y. Li, B.H. Juang, Deep learning based end-to-end wireless communication systems without pilots.
IEEE Trans. Cognit. Commun. Networking **7** (3), 702–714 (2021)
[Crossref]
4.
Y.M. Saidutta, A. Abdi, F. Fekri, Joint source-channel coding for Gaussian sources over AWGN channels
using variational autoencoders, in 2019 _IEEE International Symposium on Information Theory (ISIT)_ (IEEE,
New York, 2019), pp. 1327–1331
5.
M. Nemati, J. Park, J. Choi, VQ-VAE empowered wireless communication for joint source-channel coding
and beyond, in _GLOBECOM 2023–2023 IEEE Global Communications Conference_ (IEEE, New York,
2023), pp. 3155–3160
6.
K. Choi, K. Tatwawadi, A. Grover, T. Weissman, S. Ermon, Neural joint source-channel coding, in
_International Conference on Machine Learning_ (PMLR, New York, 2019), pp. 1182–1192
7.
T.M. Cover, _Elements of Information Theory_ (Wiley, New York, 1999)
8.
D. Tse, P. Viswanath _Fundamentals of Wireless Communication_ (Cambridge University, Cambridge, 2005)
9.
C.E. Shannon, A mathematical theory of communication. Bell Syst. Tech. J. **27** (3), 379–423 (1948)
[MathSciNet][Crossref]
10.
S. Vembu, S. Verdu, Y. Steinberg, The source-channel separation theorem revisited. IEEE Trans. Inf. Theory
**41** (1), 44–54 (2002)
[MathSciNet][Crossref]
11.
E. Bourtsoulatze, D.B. Kurka, D. Gündüz, Deep joint source-channel coding for wireless image
transmission. IEEE Trans. Cognit. Commun. Networking **5** (3), 567–579 (2019)
[Crossref]
12.
E. Erdemir, T.Y. Tung, P.L. Dragotti, D. Gündüz, Generative joint source-channel coding for semantic image
transmission. IEEE J. Sel. Areas Commun. **41** (8), 2645–2657 (2023)
[Crossref]
13.
R. Zhang, P. Isola, A.A. Efros, E. Shechtman, O. Wang, The unreasonable effectiveness of deep features as a
perceptual metric, in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_
(2018), pp. 586–595
14.
T.Y. Tung, D. Gündüz, DeepWiVe: deep-learning-aided wireless video transmission. IEEE J. Sel. Areas
Commun. **40** (9), 2570–2583 (2022)
[Crossref]
15.
S.R. Pokhrel, A. Walid, On large language model based joint source-channel coding for semantic
communication, in _2024 2nd International Conference on Foundation and Large Language Models (FLLM)_
(IEEE, New York, 2024), pp. 322–329
_OceanofPDF.com_


(1)

© The Author(s), under exclusive license to Springer Nature Switzerland AG 2026
S. Singh et al. (eds.), _GenAI and LLMs for Beyond 5G Networks_
https://doi.org/10.1007/978-3-032-06418-9_10

##### Modeling Wireless Channels with

##### Generative AI

Ashok Kumar Reddy Chavva^1 , Divpreet Singh^1 and Yeswanth Reddy

Guddeti^1
Samsung Research Institute Bangalore, Bangalore, India

```
Ashok Kumar Reddy Chavva
Email: ashok.chavva@samsung.com
```
**Abstract**
Modeling realistic time-varying and frequency-selective channels is critical
for designing the wireless systems that are robust to fading channels in
different environments. Modeling a wireless channel is a time-consuming
process which involves measurement campaigns in various environments
and then fitting the right statistical model for each measurement scenario. In
this chapter, we discuss few methods to generate wireless channel using
generative AI approaches. Firstly, we discuss a method to generate channel
based on the generative adversarial network (GAN). This approach is
helpful in generating time-varying and frequency-selective channels for the
scenarios which do not have a known statistical model. Next, we delve into
the diffusion-based channel modeling that uses U-Net architecture, noise-
conditioned score network (NCSN), and score-stochastic differential
equation (SDE). Further, we explore the latest trends in generative AI with
large-scale models for wireless channel models and downstream
applications like channel estimation, beam prediction, and interference
detection.

**Keywords** Channel modeling – GAN – Diffusion model – Score network


**1 Introduction**

Physical channel models play an important role in the design and evaluation
of wireless communication systems. Characterization of the channel models
for each frequency band can be different as the channel behavior at lower
bands (< 6 GHz) is different as compared to that at the higher bands (> 10
GHz). Thus, need for channel modeling that accurately represents physical
channels emerges. Accurate channel modeling has also been vital in
achieving performance optimizations while utilizing the techniques such as
multiplexing, diversity, and hybrid beam forming which are used in 5G
millimeter-wave (mmWave) communications.
Propagation characteristics of the channel are highly variable and are
dependent on a large number of factors such as rural/urban environment,
indoor/outdoor environment, frequency dependent attenuation due to
oxygen, atmospheric conditions like snow and rain, as well as landscape
conditions such as foliage, and building materials. Furthermore, the
propagation characteristics are also affected by multipath, scattering,
refraction, reflection, and diffraction [ 1 ]. The Doppler spread caused by
mobility of communicating nodes also needs to be taken into consideration.
In addition, human blockage of the electromagnetic waves from the access
points to mobile stations can cause temporal variations in the radio channel
[ 2 ]. For modeling a channel, each factor discussed above is required to be
modeled accurately to ensure the wireless system design covers the typical
scenarios [ 3 ].
Channel modeling can be done in different ways: One is to generate
physical channel models. In physical channel models [ 4 ], the channel
environment is characterized on the basis of electromagnetic propagation by
estimating the multipath propagation between the transmitter and the
receiver without taking the antenna configuration into account. On the other
hand, analytical channel models use impulse response or transfer function
between individual Tx and Rx antennas to model the channel disregarding
the wave propagation characteristics, as has been done in correlation-based
Kronecker and Weichselberger [ 5 ] models, as well as propagation-based
finite scatterer, maximum entropy, and virtual channel representation
models [ 6 ].
Another approach for modeling the channels is to use the statistical
channel modeling software for simulating different channel conditions.


Channel modeling using NYUSIM [ 7 ], developed by NYUWireless, is
based on statistical spatial channel model backed by extensive
measurements at mmWave frequencies to produce channel impulse
responses [ 8 ]. Applying Clarke’s two-ray Rayleigh fading channel has been
used to simulate multipath channels [ 9 ]. Channel modeling for above 100
GHz based on the measurements is discussed in [ 10 ].
GANs are explored for various use cases in the literature. They are
helpful in generating accurate, high-quality content through unsupervised
learning generally from scratch. The design of novel physical layer
communication systems through use of adversarial networks has been
explored in [ 11 ]. Estimation of channel covariance matrices is another
example of conditional GAN usage [ 12 ]. AWGN channel modeling using
GAN has been demonstrated in literature [ 13 ]. Development of an end-to-
end wireless communication system using deep neural networks has also
been studied with usage of GAN-based channel modeling approach for
Rayleigh fading channels and frequency-selective channels [ 14 ].
Furthermore, the channel models can be time-varying or time-invariant,
narrowband or wideband, and deterministic or stochastic. The main
objective of channel modeling is to accurately model realistic channels so
that designed system is robust to various environments. With the GAN’s
ability to generate statistically accurate data, they can be used for modeling
the spatially and temporally evolving channel. For this, data from
measurement campaigns can be used [ 10 ]. The use of ML techniques to
predict specific channel properties such as path loss and frequency response
has also been studied in literature [ 15 ].
In the recent developments of wireless system, research is in progress to
make the radio access network (RAN) opportunistic to the environments it
experiences. For example, in a wireless cellular deployment of 4G or 5G or
6G systems, it is possible to make the physical layer AI algorithms fine-
tuned to the channels that base station sees. This makes the channel
modeling accurate to the specific environment an important aspect.
**In this chapter** , we first discuss a method to model time-varying and
frequency-selective channels using generative adversarial network [ 16 ]. A
GAN-based channel model can be used for the frequency-selective time-
varying scenarios where the existing channel models do not capture the
propagation scenario accurately and scenarios captured through existing
channel models. We use third generation partnership project (3GPP) spatial


channel models [ 17 ] as reference to validate the accuracy of the proposed
method. Next, we delve into the diffusion-based channel modeling. This
part explains the U-Net architecture, noise-conditioned score network
(NCSN), and score-stochastic differential equation (SDE), showing how
these components come together to enable flexible channel generation.
Finally, we introduce briefly the transformer-based foundation models for
wireless channels and the fundamental principles.

**2 Generative Adversarial Networks**

GAN [ 18 ] models are unsupervised generative models that consist of two
networks—generator and discriminator, usually neural networks—working
in an adversarial manner. Generator and discriminator are trained by setting
up of a supervised learning problem for both, though GAN models overall
are an approach to generative modeling which is an unsupervised learning
task involving automatically discovering the patterns in input data and
generating new samples that plausibly belong to original dataset [ 18 ].
The goal of the GAN models is to train a generator network that
produces samples from the given data distribution, by transforming
randomly generated noise vectors. The training of generator network is
done with the help of the discriminator network that is trained to distinguish
between samples obtained from the generator and the real data. In the
literature, the GAN models have been frequently used for tasks requiring
realistic generation of the image samples such as choosing the best high-
resolution image from the set of possible images in single image super-
resolution [ 19 ], image creation from rough sketches [ 20 ], and tasks like
synthesizing photos from label maps or any other image-to-image
translations [ 21 ].
Another application of the GAN can be found in generating sample data
from high-dimensional probability distributions. For high-dimensional
cases, rather than trying to estimate the probability density function itself,
GAN networks aim to produce samples belonging to the desired
distribution using the limited training data. As for the methodology, let _p_ ∗
be the true distribution from which some samples are available in the form
of training data, and the aim is to generate more samples from _p_ ∗. The
generator network G maps latent vectors drawn from known prior _po_ to
samples: G : z ↦ G(z), where z ∼ _po_. The generator network intends to


create samples such that G(z) ∼ _p_ ∗, while the discriminator network
examines the input sample v to determine whether the sample is real (v
∼ _p_ ∗) or fake (v ∼ _pG_ , where _pG_ is the distribution of samples coming
from generator). The discriminator learns through supervised learning,
dividing inputs into two classes: “real” and “fake.” The two models are
pitted against each other, and as training progresses, _pG_ gradually becomes
closer to _p_ ∗, hence satisfying the original purpose.
For the application of GAN in channel modeling, we intend to generate
channel coefficients of unknown channel, for which a limited number of
realizations of actual channel coefficients **s** of length _L_ and dimension _D_ (
_D_ = 2 for complex-valued coefficients) are available. Thus, _p_ ∗ will be the
true probability distribution of desired channel which is required to be
modeled. While setting the prior distribution for the generator to uniform (
**n** ∼ _U_ ( 0 , 1 ), **n** ∈R _D_ × _L_ ) for generating the new coefficients _G_ ( **n** ) from
modeled distribution _pG_ , discriminator is trained by feeding generated
channel coefficients _G_ ( **n** ) with “fake” labels and actual channel
coefficients **s** ∈R _D_ × _L_ from training data with “real” labels. As training
proceeds, the difference between _pG_ and _p_ ∗ reduces, and hence it is now
possible to generate new channel coefficients from generator. Even for
cases such as 3GPP models which do not follow any specific probability
distribution, GAN can be used for channel modeling to mimic the data
patterns.

**2.1 Frequency-Flat Time-Varying Channel Modeling [ 16 ]**

In this section, we first introduce the GAN model for usage in the case of
single-tap time-varying channels. Here, the generator network is used to
model the channel distribution and producing real-like channel coefficients,
while the discriminator network segregates the channel coefficients into
“real” and “fake” through classification.
Figure 1 shows the proposed architecture of generator and discriminator
where the generator network models the channel coefficients using a deep
convolutional neural network [ 16 ]. A random input sequence of length
equal to desired output sequence length _L_ is fed to the generator, which
after passing through convolutional layers and a fully connected network
outputs a generated sequence representing the modeled channel
coefficients. In the proposed architecture of generator, three 2D
convolutional layers with kernel size 20 and default stride with the “same”


padding [ 22 ] have been used with filter numbers of 512, 256, and 128,
respectively. Then, the output of the last convolutional layer after passing
through flatten layer goes to a fully connected network consisting of one
hidden layer with 1024 neurons and finally outputs _D_ × _L_. Finally, after
formatting into desired configuration using reshape layer, the generated
sequence is produced. Thus generator network produces different instances
of the modeled channel with different input random sequences.


**_Fig. 1_** Proposed GAN architecture for modeling time-varying channel. ( **a** ) Architecture of generator.
( **b** ) Architecture of discriminator
The aim of the discriminator is to output the probability of the input
sequence coming from the original distribution and hence classify whether
the input sequence is real or not. It is trained through the supervised
learning by feeding channel coefficients from the original channel and from
the generator with “real” and “fake” labels, respectively; that is to say the
output label for channel coefficients from original channel is set to “1,”
while the channel coefficients from the generator are fed with “0” labels. In
this manner, discriminator tries to learn the differences between actual
channel coefficients and modeled channel coefficients from generator.
Figure 1 b shows an example architecture of discriminator, which uses a
deep convolutional neural network with convolutional layers and fully
connected layers connected through a “flatten” layer and outputs a single
probability value at the end for classification purpose. As in the case of the
generator, three 2D convolutional layers with kernel size 20 and default
stride with the “same” padding [ 22 ] have been used although with
increasing filter numbers of 64, 128, and 256, respectively. Then, a flatten
layer converts the three-dimensional output from convolutional layer into
single dimension and is followed by a fully connected network consisting of
one hidden layer with 1024 neurons and finally outputs a value between 0
and 1, representing the probability of ‘real’ input sequence.
The loss function _JD_ used for discriminator is the standard binary cross-
entropy loss expressed as

```
(1)
```
where, for the _i_ th example, **s** ( _i_ ) denotes the _i_ th sample from original channel
coefficients, **n** is the random noise sequence fed to generator, and _D_ ( _x_ ) and
_G_ ( _x_ ) are the discriminator and generator functions, respectively.
The generator is trained as combined generator and discriminator
network through supervised learning. At this stage, the output sequence
from the generator is fed to discriminator with inverted label “1” and the
generator network is trained through backpropagation without modifying
discriminator layers. Inverted labeling that is feeding the “fake” sequences
from the generator with “real” labels to the discriminator lets the generator
network learn the changes required for the output sequence of generator to
be classified as “real” by the discriminator and hence trains the generator.

```
JD ( i )=−(log( D ( s ( i )))+log( 1 − D ( G ( n ( i ))))),
```

The loss function _JG_ for the generator is defined as a combination of
binary cross-entropy loss and Frobenius norm between autocorrelation of
actual channel coefficients and input channel coefficients as

```
(2)
```
where ||⋅|| denotes the Frobenius norm, for _i_ th example, **s** ( _i_ ) denotes the

_i_ th sample from original channel coefficients, **n** is the random noise
sequence fed to generator, while _D_ ( _x_ ) and _G_ ( _x_ ) are the discriminator and
generator functions, respectively, and _λ_ is a hyperparameter which regulates
the effect of Frobenius norm of autocorrelation function **R** ( _d_ ) on generator
loss function. A large value of _λ_ will lead to learning that matches
autocorrelation functions rather than the probability distributions, while
_λ_ = 0 will give full freedom to find and match novel patterns which may be
desirable or undesirable ones. The inclusion of Frobenius norm of
autocorrelation function **R** ( _d_ ) in generator loss function helps the training

of the model in the direction of true probability distribution with desired
channel characteristics instead of learning undesirable characteristics
present in training data and leading to overfitting.

**2.2 Performance Evaluation**

Here, we validate the performance of the proposed method of generative
adversarial networks for channel modeling. For the lack of availability of
data from the channel campaigns at our disposal, we use the generic 3GPP
[ 17 ] spatial channel model for link level simulations, CDL-C. This channel
model has multiple clusters and each cluster made up of multiple rays
modeling the realistic channel. We note that this method is applicable for
any other channel measurement data from measurement campaigns. This
channel model uses 24 taps with respective powers and particular angles of
arrival and departure in both zenith and azimuth directions to characterize
spatial directions. Finally, to generate channel coefficients for each tap,
random coupling of rays within a cluster is performed.
Training data consists of complex number sequences, representing
different channel tap coefficients varying with time. With a user speed of 5
m/s corresponding to Doppler frequency of 500 Hz and a delay spread of 10
ns, the channel at 30 GHz center frequency has been simulated for a time
period of 1 sec producing 128000 samples with a sampling frequency of
128 kHz. The original sequence with sampling rate of 128 kHz needs to be

```
JG ( i ) =log( 1 − D ( G ( n ( i )))) + λ || R s ( i )( d ) − R G ( n ( i ))( d )||^2 ,
```

decimated, since a higher sampling rate produces higher length sequences
requiring higher complexity network and thus larger training data and
longer training duration. Thus, there exists a trade-off between learnt
sequences length and complexity of network. To address this issue,
downsampling rate _M_ has been chosen to provide an output sequence with
fixed length _L_ of 200. The time-varying channel has been generated for the
duration of one coherence time with 200 samples in it. Since the channel
tap is highly correlated within the coherence time, the 200 samples can later
be oversampled to the required sampling rate as the wireless system
demands.
Simulation for channel modeling has been done in Python using
TensorFlow library (for designing the training environment) and Keras
library (for defining generator and discriminator models). Various
hyperparameters such as learning rate, regularization strength, number of
epochs, as well as individual training steps for generator and discriminator
in an epoch, and parameter _λ_ have been tuned in addition to architecture
related parameters of generator and discriminator. Parameters have been
selected to minimize the differences between autocorrelation functions of
actual channel coefficients and channel coefficients obtained from the
generator after training. Simultaneously, it has also been ensured not to
overfit the training data due to learning of undesirable characteristics by
trying early stopping. Furthermore, the “multiple random starts” approach
has also been incorporated, since the problem of model being stuck in local
optima was observed during the training. In this approach, multiple runs of
random initialization of model at the start are performed, and the best model
is chosen after training; in this manner the likelihood of obtaining global
optimum increases instead of local optima.
We observe that feeding the real and imaginary parts of channel
coefficients of particular tap of CDL-C channel in the form of training
sequences lets GAN network learn the correlation between samples of input
sequence and produces similar examples through generator architecture. For
validation purpose, the Frobenius norm of the difference between the

autocorrelation functions **R** ( _d_ ) estimated from training examples **s** ( _i_ ) and

that from examples produced by generator _G_ ( **n** ( _i_ )) is evaluated, and this
error is used as a metric for model evaluation. Autocorrelation functions
have been averaged over 1000 samples of modeled channel coefficients and
of actual channel coefficients. The error _E_ is given by


```
(3)
```
Training of generative adversarial network shows a decline in the metric
values, as training proceeds as seen in Fig. 2.

**_Fig. 2_** Error metric _E_ (Eq. ( 3 )) as a function of number of epochs showing convergence during
training for first tap of CDL-C channel ( _M_ = 4 , _L_ = 200 , _λ_ = 0. 001 )

After training GAN, the output sample sequences produced by
generator show autocorrelation plot similar to the original CDL-C channel
sequences. This shows that GAN network has learnt to mimic desired
channel properties. Figure 3 shows the autocorrelation plots of channel
coefficients produced by generator after training and actual channel
coefficients from the training data. For visualization purpose, the absolute
value of autocorrelation coefficients has been plotted.

# E = 10100 ∑

```
1000
```
```
i = 1 || R s ( i )( d ) − R G ( n ( i ))( d )||
```
(^2).


**_Fig. 3_** Autocorrelation as a function of lag _τ_ for the sequence that is generated from GAN and the
original samples from the first tap of CDL-C ( _M_ = 4 , _L_ = 200 , _λ_ = 0. 001 )
The power spectral density (PSD) of the sample sequences produced by
generator and the actual channel coefficients from the training data have
also been compared. Figure 4 plots these. We observe that PSDs of samples
generated through the proposed method and the real samples match quite
accurately over the full range of frequency.

**_Fig. 4_** Power spectral density as a function of frequency for the sequence that is generated from
GAN and the original samples from the first tap of CDL-C ( _M_ = 4 , _L_ = 200 , _λ_ = 0. 001 )

In Fig. 5 , we plot the conditional cumulative distribution function,
_Fh_ ( _t_ , _τ_ )( _x_ | _h_ ( _t_ , 0 )) for multiple values of _h_ ( _t_ , 0 ) for _τ_ = 0. 5 ms and

_τ_ = 0. 25 ms. We observe that the channel coefficients generated from
GAN represent the original distribution of channel accurately at various
values of _τ_.


**_Fig. 5_** Conditional cumulative distribution function plots for the sequence that is generated from
GAN and the original samples from the first tap of CDL-C ( _M_ = 4 , _L_ = 200 , _λ_ = 0. 001 ). ( **a** ) CDF
for _τ_ = 0. 5 ms. ( **b** ) CDF for _τ_ = 0. 25 ms

**3 Diffusion-Based Channel Modeling**

This section provides a comprehensive exploration of diffusion models and
their application in wireless channel modeling, designed for wireless
engineers venturing into generative AI. It begins with an overview of the U-
Net architecture, which serves as the backbone of the score network. The
discussion then transitions to the Noise-Conditioned Score Network
(NCSN), explaining how noise is incorporated to train the model
effectively. The section culminates in an exploration of Score-SDE, a
stochastic differential equation-based approach that enables controllable
channel generation.


_Advantages of Score-Based Generative Models_ Score-based generative
modeling offers several advantages over traditional likelihood-based or
adversarial approaches:

- Flexible Training allows training without explicit likelihood
    computation.
- Stable Sampling provides principled sampling via stochastic differential
    equations [ 23 ].
- High-Quality Samples achieve competitive or superior sample quality (>
    GANs) on complex datasets [ 24 ].
- Unified Framework connects denoising score matching with diffusion
    processes.
- Controllable Generation can be extended to conditional generation by
    incorporating context information such as class labels, text embeddings,
    or other modalities, enabling tasks like guided sampling and text-to-
    image synthesis [ 24 , 25 ].

**U-Net Architecture** Diffusion models often use U-Net architecture with
ResNet blocks and are empirically suited for image segmentation and
reconstruction tasks. It has a downsampling path (encoder), upsampling
path (decoder), and a concatenation connection that preserve spatial
information (high-resolution feature, etc.) that could lost in downsampling.
It also mitigates the issue of vanishing gradients. In the current network we
train on four resolutions with channels [ 32 , 64 , 128 , 256 ] at each of those
resolutions.

**Noise-Conditioned Score Network (NCSN)** The score function of a
distribution _p_ ( _x_ ) is defined as ∇ _xlogp_ ( _x_ ), and a model for the score
function is called a score-based model, which we denote as _sθ_ ( _x_ ) [ 26 ].

NCSN learns to model complex data distributions by estimating the score
function of the data corrupted with varying levels of Gaussian noise.

1. Denoising Score Matching: The model is trained to predict the score (
    ∇ _xlogp_ ( _x_ )) of noisy inputs using a denoising objective across multiple
    noise scales, effectively learning to point back toward high-density
    regions of the original data manifold.


2. Langevin Dynamics: Once we have trained a score-based model, we
    can use an iterative procedure called Langevin dynamics to draw
    samples from distribution _p_ ( _x_ ) using only its score function
    _sθ_ ( _x_ )≈∇ _xlogp_ ( _x_ ). We sample _x_ 0 ∼ _π_ ( _x_ ) from an arbitrary prior
    distribution and then iterate _xi_ + 1 = _xi_ + _ε_ ∇ _xlogp_ ( _x_ )+√ 2 _εzi_ where
    _zi_ ∼N( 0 , _I_ ).

In naïve score matching, estimated scores are only accurate in high-
density regions, whereas our initial sample is likely in low-density regions
of the distribution as the data is of higher dimensions. The work overcomes
this challenge using multiple noise perturbations where we use scores
corresponding to large noise and gradually anneal down the noise level to
generate high-quality samples. Therefore, to sample, the model
progressively refines random noise into data samples by applying Langevin
dynamics, iteratively adding small Gaussian noise and moving along the
estimated score. An annealing schedule reduces noise over time, enabling
convergence to realistic samples.

**Controllable Generation for Inverse Problem Solving** In the context of
wireless communication systems, inverse problems often arise in tasks such
as channel estimation, where the goal is to recover the underlying channel
state information from noisy observations. Score-based generative models
offer a powerful framework for addressing these challenges through
controllable generation techniques. This subsection introduces the concept
of controllable generation using score-based models and its application to
inverse problems in wireless communication.

_Introduction to Bayes Rule and Posterior Sampling_ Bayes’ rule is a
fundamental principle in probability theory that relates the conditional and
marginal probabilities of random events. It is expressed as

```
(4)
```
where _p_ ( _x_ ∣ _y_ ) is the posterior probability, _p_ ( _y_ ∣ _x_ ) is the likelihood, _p_ ( _x_ )
is the prior, and _p_ ( _y_ ) is the evidence. In wireless communication, Bayes’
rule is often used to estimate the posterior distribution of unknown
parameters given observed data.
Score-based generative models extend this concept by leveraging the
score function, which as described above is the gradient of the log

```
p ( x ∣ y )= p ( yp ∣ x () yp )( x ),
```

probability density of the data. Applying gradient on both sides of Eq. ( 4 ),
we get

The first term is the unconditional score or prior that can be trained via
score matching without _y_ , whereas the second term can be specified with
domain knowledge of the forward process generating _y_ from _x_. In the
context of wireless data, the score function can be used to guide the
generation process through a reverse diffusion process, where noise is
progressively removed from noisy observations to recover the underlying
signal or channel state information.

_Controllable Generation in Score-Based Models_ Controllable generation
in score-based models involves manipulating the noise levels or noise
schedule during the reverse diffusion process. This allows for precise
control over the generated samples, enabling applications such as posterior
sampling and maximum a posteriori (MAP) estimation.
For example, in channel estimation tasks, controllable generation can be
used to sample from the posterior distribution of the channel state
information given noisy observations. This approach provides a robust and
flexible framework for addressing inverse problems in wireless
communication systems.

_Wireless Applications Using Controllable Generation_

- Robust Channel Estimation via Posterior Sampling: Arvinte et al.
    propose using score-based generative models for robust channel
    estimation in wireless communication systems. By leveraging posterior
    sampling techniques, the model can effectively recover channel state
    information from noisy observations, improving estimation accuracy and
    robustness [ 43 ]. This method showcases how common wireless
    applications, such as channel estimation, can be addressed using MAP or
    posterior formulations without requiring retraining of prior models.
- MIMO Channel Estimation Using Score-Based Models: In their work,
    Arvinte et al. extend the application of score-based models to multiple-
    input multiple-output (MIMO) systems. The model utilizes posterior
    sampling to estimate the channel state information in MIMO scenarios,
    demonstrating improved performance in complex wireless environments.

```
∇ xlogp ( x ∣ y )=∇ xlogp ( x )+∇ xlogp ( y ∣ x ).
```

```
This highlights the versatility of controllable generation techniques in
solving diverse wireless challenges.
```
- Diffusion Model-Based Signal Recovery for Noisy Linear Inverse
    Problems: Meng et al. introduce a diffusion model-based approach for
    solving noisy linear inverse problems. The model employs posterior
    sampling to recover signals from noisy observations, showcasing its
    effectiveness in various wireless communication applications [ 28 ]. This
    approach emphasizes the potential of controllable generation in
    addressing inverse problems without necessitating retraining of existing
    models.
- Deep Learning-Enhanced Channel Estimation: Soltani et al. explore deep
    learning techniques for channel estimation in wireless communication
    systems. While not directly using score-based models, the paper
    highlights the potential of deep learning methods in improving channel
    estimation accuracy and efficiency [ 29 ]. This underscores the broader
    applicability of advanced generative models in enhancing wireless
    communication tasks.
By leveraging controllable generation techniques and score-based models,
these studies demonstrate the potential of advanced generative models in
addressing inverse problems in wireless communication systems. Through
posterior sampling and MAP estimation, these approaches provide robust
and flexible solutions for tasks such as channel estimation and MIMO
estimation, ultimately enhancing the performance and reliability of wireless
networks.

**3.1 Modeling 3GPP Channels**

- Setup: Embed delay spread as context and generate channels with unseen
    delay spreads.
       Refer to Fig. 6 for details on individual steps involved in modelling
    3GPP channels with Noise-conditioned score network (NCSN).


```
Fig. 6 Pipeline for modeling 3GPP channels with Noise-conditioned score network (NCSN). ( a )
Data preparation. ( b ) ScoreNet training. ( c ) Sampling with Langevin dynamics
```
- Training Configuration: In this experiment, we embed the delay-spreads
    (100, 500, 2500 ns) of CDL-C channel as context vector while training
    the NCSN network (Table 1 ).

```
Table 1 Experimental parameters for 3GPP channel modeling using
NCSN
```
(^) **Parameter Value
CDL-C parameters** Delay spread 100,500,2500 ns
(^) UE speed 20 Kmph
(^) Center frequency 2.5 GHz
(^) Bandwidth 32 MHz
(^) Subcarrier spacing 500 KHz ( _fd_ ∼ 45 Hz)
(^) Observation interval _dt_ 5.5 ms (∼ _f_^4
_d_ )
(^) Total duration 0.35 s
**Training** Epochs 1000
(^) Noise variance _σi_ [1,0.01]
(^) # of embedded labels (L) 3


(^) **Parameter Value**
(^) Input dimensions 64 × 64 × 2
(^) Training DS size 8192 × 3
**Sampling** Noise variance _σ_ 0 1 × 10 −^5
(^) # of sampling steps (k) 100
(^) Denoise flag 0

- Sampling: We then generate samples at delay spreads of 200 , 1000 ns
    unseen by NCSN. Figure 7 shows the power delay profile of known
    channels matches and the delay spread of unseen channels close to
    specified value.

```
Fig. 7 Normalized power delay profiles for five delay spreads 100 ns, 200 ns (unseen), 500 ns,
1000 ns (unseen), and 2500 ns from left to right
```
- Observations: The model successfully generates PDPs for unseen delay
    spreads, showcasing its adaptability and controllability.
- Conclusion: Diffusion models can be tailored to specific channel
    characteristics, making them highly adaptable for various wireless
    environments.
       The successful generation of PDPs for unseen delay spreads highlights
the model’s adaptability and controllability. We can therefore controllably
generate unseen channels as a mixture of 3GPP channels by embedding
properties like delay spread, channel type, and speed as context to NCSN-
based diffusion model. This experiment not only reinforces the model’s
practicality but also opens up possibilities for tailoring channel models to
specific requirements, such as varying environmental conditions or user
needs.

**4 Large Channel Models for Wireless**


As large-scale models become increasingly integrated into everyday
applications across text, audio, and video domains, similar approaches are
being explored for modeling wireless channels and related tasks, such as
channel estimation and channel state information. The traditional wireless
models, like statistical or ray-tracing methods, struggle with complexity and
high dimensionality of the wireless channels. The large wireless model
(LWM) in [ 30 ] proposes self-supervised, multiheaded, pretrained
transformer-based foundation model for wireless channels. By learning
universal and contextual channel embeddings, it enhances the performance
of downstream tasks such as channel estimation, beamforming, channel-
state-information estimation, and interference detection. Its architecture is
mainly inspired by Vaswani et al. [ 31 ]. Here, we briefly discuss the
architecture of LWM [ 30 ]. The Large Wireless Model (LWM) approaches
channel data by first splitting each complex-valued channel matrix—shaped
as 32 antennas by 32 subcarriers—into two adjacent “patches,” each
measuring 32 by 16. These patches are flattened, and then each is mapped
into a 64-dimensional vector using a linear projection. To help the model
understand the position of each patch, learnable positional encodings are
added, and a special [CLS] token is placed at the beginning.
A compact transformer encoder, with fewer than one million
parameters, processes these inputs. It uses multihead self-attention and
multilayer perceptron (MLP) layers to learn how different parts of the
channel relate to each other. During self-supervised pretraining, about 15 _%_
of the patches—covering both real and imaginary components—are masked
out. The model is then trained to reconstruct these missing patches using
mean squared error, similar to how BERT masks words in a sentence to
teach language understanding, but here applied to continuous channel state
information (CSI).
After training, the decoder part of the model is removed. The 64-
dimensional [CLS] vector becomes a summary of the entire channel (a
global embedding), while each patch’s embedding captures more local
details. This setup allows the model to be fine-tuned efficiently or used
directly for tasks like beam prediction, distinguishing between line-of-sight
(LoS) and non-line-of-sight (NLoS) conditions, and estimating channels—
all with much less labeled data and modest hardware requirements.
Another such foundation model called WiMAE is proposed in [ 32 ]. It
proposes a masked-autoencoder foundation model for wireless channels in


which only the visible 40 _%_ of 32 × 32 MISO-OFDM patches are fed to a
moderate-depth transformer encoder, while a lightweight decoder
reconstructs the 60 _%_ that have been masked. Compared to LWM, which is
a BERT-style encoder-only transformer that reconstructs just 15 _%_ of
patches and relies solely on an MSE regression loss.
We expect significant changes and improvements over this architecture
in the not-too-far time to enable various signal processing tasks that depend
on channels. Especially, customization of downstream AI models per-cell
site will be an important use case.

**References**

1. I.A. Hemadeh et al., Millimeter-Wave communications: physical channel models, design
    considerations, antenna constructions, and link-budget. IEEE Commun. Surv. Tuts. **20** (2), 870–
    913 (2018)
    [Crossref]
2.
    U.T. Virk, K. Haneda, Modeling human blockage at 5G millimeter-wave frequencies. IEEE
    Trans. Antennas Propag. **68** (3), 2256–2266 (2020)
    [Crossref]
3.
    P. Almers et al., Survey of channel and radio propagation models for wireless MIMO systems. J
    Wireless Commun. Netw. **2007** (1), 019070 (2007)
4.
    S. Hur et al., Proposal on millimeter-wave channel modeling for 5G cellular system. IEEE J. Sel.
    Top. Sign. Proces. **10** (3), 454–469 (2016)
    [Crossref]
5.
    W. Weichselberger, M. Herdin, H. Ozcelik, E. Bonek, A stochastic MIMO channel model with
    joint correlation of both link ends. IEEE Trans. Wirel. Commun. **5** (1), 90–100 (2006)
    [Crossref]
6.
    A.M. Sayeed, Deconstructing multiantenna fading channels. IEEE Trans. Signal Process. **50** (10),
    2563–2579 (2002)
    [Crossref]
7.
    S. Ju, O. Kanhere, Y. Xing, T.S. Rappaport, A millimeter-wave channel simulator NYUSIM with
    spatial consistency and human blockage, in _2019 IEEE Global Communications Conference_
    _(GLOBECOM), Hawaii, USA_ (2019), pp. 1–6
8.
    M.M. Lodro et al., Statistical channel modelling of 5G mmWave MIMO wireless
    communication, in _2018 International Conference on Computing, Mathematics and Engineering_
    _Technologies_ , Sukkur (2018), pp. 1–5
9.
    J.I. Smith, A computer generated multipath fading simulation for mobile radio. IEEE Trans. Veh.
    Technol. **24** (3), 39–40 (1975)


[Crossref]
10.
S. Ju, T.S. Rappaport, 140 GHz Urban microcell propagation measurements for spatial
consistency modeling, in _2021 IEEE International Conference on Communications (ICC)_
(2021), pp. 1–6
11.
T.J. O’Shea, T. Roy, N. West, B.C. Hilburn, Physical layer communications system design over-
the-air using adversarial networks, in _Proceedings of the 26th European Signal Processing
Conference (EUSIPCO), Rome, Italy_ (2018), pp. 529–532
12.
X. Li, A. Alkhateeb, C. Tepedelenlioğlu, Generative adversarial estimation of channel covariance
in vehicular millimeter wave systems, in _Proceedings of the 52nd Asilomar Conference on
Signals, Systems & Computers_ , Pacific Grove, CA, USA (2018), pp. 1572–1576
13.
Y. Yang, Y. Li, W. Zhang, F. Qin, P. Zhu, C. Wang, Generative-adversarial-network-based
wireless channel modeling: challenges and opportunities. IEEE Commun. Mag. **57** (3), 22–27
(2019)
[Crossref]
14.
H. Ye, L. Liang, G. Y. Li, B. Juang, Deep learning-based end-to-end wireless communication
systems with conditional GANs as unknown channels. IEEE Trans. Wireless Commun. **19** (5),
3133–3143 (2020)
[Crossref]
15.
B. Turan, S. Coleri, Machine learning based channel modeling for vehicular visible light
communication. IEEE Trans. Veh. Technol. **70** (10), 9659–9672 (2021)
[Crossref]
16.
D. Singh, A.K. Reddy Chavva, Modeling time-varying and frequency-selective channels with
generative adversarial networks, in _Proceedings of IEEE International Conference on
Communications (ICC), Seoul, South Korea_ (2022), pp. 5329–5334
17.
3GPP 38.901, Study on channel model for frequencies from 0.5 to 100 GHz. 3GPP Rel.-15
18.
I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, Y.
Bengio, Generative adversarial nets, in _Advances in Neural Information Processing Systems_
(2014), pp. 2672–2680
19.
C. Ledig, L. Theis, F. Huszar, J. Caballero, A.P. Aitken, A. Tejani, J. Totz, Z. Wang, W. Shi,
Photo-realistic single image super-resolution using a generative adversarial network, in
_Computer Vision and Pattern Recognition_ (2017)
20.
J.-Y. Zhu, P. Krähenbühl, E. Shechtman, A.A. Efros, Generative visual manipulation on the
natural image manifold, in _European Conference on Computer Vision_ (Springer, Berlin, 2016),
pp. 597–613
21.
P. Isola, J.-Y. Zhu, T. Zhou, A.A. Efros, Image-to-image translation with conditional adversarial
networks, in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_
(2017). arXiv:1611.07004
22.
Sarkar, S., Convolutional Neural Network (IITKGP, India). [http://](http://) cse. iitkgp. ac. in/ ~sudeshna/
courses/ DL17/ CNN-22mar2017. pdf


23.
Y. Song, J. Sohl-Dickstein, D.P. Kingma, A. Kumar, S. Ermon, B. Poole, Score-Based
Generative Modeling through Stochastic Differential Equations. arXiv preprint
arXiv:2011.13456 (2021)
24.
P. Dhariwal, A. Nichol, Diffusion models beat GANs on image synthesis, in _Advances in Neural
Information Processing Systems_ (2021). https:// arxiv. org/ abs/ 2105. 05233
25.
R. Rombach, A. Blattmann, D. Lorenz, P. Esser, B. Ommer, High-resolution image synthesis
with latent diffusion models, in _Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR)_ (2022). https:// arxiv. org/ abs/ 2112. 10752
26.
Y. Song, S. Ermon, Generative modeling by estimating gradients of the data distribution, in
_Advances in Neural Information Processing Systems_ (2019), pp. 11895–11907. https:// arxiv. org/
abs/ 1907. 05600
27.
M. Arvinte, J.I. Tamir, MIMO Channel Estimation using Score-Based Generative Models. arXiv
preprint arXiv:2204.07122 (2022)
28.
Meng et al., Diffusion Model Based Posterior Sampling for Noisy Linear Inverse Problems,
arXiv preprint arXiv:2211.12343 (2022)
29.
M. Soltani, V. Pourahmadi, A. Mirzaei, H. Sheikhzadeh, Deep learning-based channel
estimation. IEEE Commun. Lett. **23** (4), 652–655 (2019)
[Crossref]
30.
S. Alikhani, G. Charan, A. Alkhateeb, Large Wireless Model (LWM): A Foundation Model for
Wireless Channels, ArXiv Prepr. ArXiv241108872 (2024)
31.
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. A.N. Jones Gomez, L. Kaiser, I. Polosukhin,
_Attention is all you Need_ (2023)
32.
B. Guler, G. Geraci, H. Jafarkhani, A Multi-Task Foundation Model for Wireless Channel
Representation Using Contrastive and Masked Autoencoder Learning. ArXiv Prepr.
ArXiv250509160 (2025)
33.
H. Xiao, W. Tian, W. Liu, J. Shen, ChannelGAN: deep Learning-Based Channel Modeling and
Generating. IEEE Wireless Commun. Lett. **11** , 650–654 (2022)
[Crossref]
34.
A.V. Aho, J.D. Ullman, _The Theory of Parsing, Translation and Compiling_ , vol. 1 (Prentice-Hall,
Englewood Cliffs, 1972)
35.
American Psychological Association, _Publications Manual_ (American Psychological
Association, Washington, DC, 1983)
36.
B. Börschinger, M. Johnson, A particle filter algorithm for Bayesian Wordsegmentation, in
_Proceedings of the Australasian Language Technology Association Workshop 201 1_ , Canberra,
Australia (2011)
37.
A.K. Chandra, D.C. Kozen, L.J. Stockmeyer, Alternation. J. Assoc. Comput. Mach. **28** (1), 114–
133 (1981)


[Crossref]
38.
G. Andrew, J. Gao, Scalable training of L1-regularized log-linear models, in _Proceedings of the
24th International Conference on Machine Learning_ (2007), pp. 33–40
39.
J. Goodman, A. Vlachos, J. Naradowsky, Noise reduction and targeted exploration in imitation
learning for Abstract Meaning Representation parsing, in _Proceedings of the 54th Annual
Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_ , Berlin,
Germany (2016)
40.
D. Gusfield, _Algorithms on Strings, Trees and Sequences_ (Cambridge University, Cambridge,
1997)
[Crossref]
41.
M.S. Rasooli, J.R. Tetreault, Yara Parser: A Fast and Accurate Dependency Parser, in _Computing
Research Repository_. arXiv:1503.06733 (2015)
42.
R.K. Ando, T. Zhang, A framework for learning predictive structures from multiple tasks and
unlabeled data. J. Mach. Learn. Res. **6** , 1817–1853 (2005)
[MathSciNet]
43.
M. Arvinte, Score-Based Generative Models for Robust Channel Estimation. arXiv preprint
arXiv:2111.08177 (2021)
_OceanofPDF.com_


(1)

© The Author(s), under exclusive license to Springer Nature Switzerland AG 2026
S. Singh et al. (eds.), _GenAI and LLMs for Beyond 5G Networks_
https://doi.org/10.1007/978-3-032-06418-9_1 1

##### Machine Learning-Based Intelligent

##### Anomaly Detection and Maintenance in

##### RAN

K. Anoop^1
Tejas Networks, Bengaluru, India

**Abstract**
The evolution of Radio Access Networks (RAN), particularly with the
advent of 5G and Open RAN (O-RAN) architectures, has introduced
unprecedented complexity and operational demands. Traditional fault
management approaches—relying on static thresholds and manual
diagnostics—are increasingly inadequate in addressing the dynamic and
heterogeneous nature of modern RAN environments. This chapter presents
a comprehensive framework for integrating machine learning (ML) into
RAN operations to enable intelligent anomaly detection and predictive
maintenance. It explores the types of anomalies affecting RAN
performance, including KPI degradations, hardware faults, software
crashes, and cross-domain failures. A range of ML techniques—spanning
supervised, unsupervised, and time-series models—are discussed in the
context of their applicability to high-dimensional telemetry data. The
proposed system architecture leverages modular, cloud-native components
and interfaces with O-RAN elements such as the RAN Intelligent
Controller (RIC) and Service Management and Orchestration (SMO)
platforms. Real-world use cases demonstrate how ML models can
proactively detect faults, reduce downtime, and optimize operational
efficiency. The chapter concludes with future directions, highlighting
emerging trends such as federated learning, explainable AI, and edge


intelligence, which promise to further enhance the resilience and autonomy
of next-generation mobile networks.

**1 Introduction**

Radio Access Networks (RANs) form the backbone of modern wireless
communication systems, enabling seamless connectivity between user
equipment (UE) and the core network. With the advent of 5G and the
increasing complexity of network topologies—especially with the adoption
of Open RAN (O-RAN) architectures—the operational demands on RANs
have grown significantly. Ensuring high availability, low latency, and
optimal throughput in such dynamic environments is a critical challenge for
telecom operators.
Traditionally, RAN maintenance has relied on rule-based systems,
threshold alarms, and manual diagnostics. While these methods have served
well in legacy networks, they fall short in handling the scale, heterogeneity,
and real-time demands of modern RANs. The increasing volume of
telemetry data, coupled with the need for rapid fault resolution, necessitates
a shift toward more intelligent, automated solutions.
Machine Learning (ML) offers a transformative approach to RAN
anomaly detection and maintenance. By leveraging historical and real-time
data—such as Key Performance Indicators (KPIs), system logs, and alarm
events—ML models can learn complex patterns, detect subtle anomalies,
and even predict failures before they occur. This proactive capability not
only reduces downtime but also optimizes resource utilization and enhances
user experience.
The motivation for integrating ML into RAN operations is further
strengthened by the modular and programmable nature of O-RAN, which
facilitates the deployment of AI/ML models through components like the
RAN Intelligent Controller (RIC). This opens up new possibilities for
closed-loop automation, self-healing networks, and intelligent orchestration.
This chapter explores the landscape of ML-based anomaly detection and
predictive maintenance in RANs. It begins with a review of RAN
architecture and traditional fault management approaches, followed by a
detailed discussion of anomaly types, ML techniques, system architecture,
and real-world use cases. The goal is to provide a comprehensive


understanding of how ML can be effectively applied to enhance the
reliability and efficiency of next-generation wireless networks.

**2 Background and Related Work**

**2.1 Overview of RAN Architecture**

Radio Access Networks (RANs) are responsible for connecting user
equipment (UE) to the core network, handling radio signal transmission,
resource allocation, and mobility management. In traditional 4G LTE
networks, the RAN consists of eNodeBs that integrate both control and user
plane functionalities. With the evolution to 5G, the RAN architecture has
become more modular and flexible, introducing the concepts of:

Centralized Unit (CU): Handles higher-layer protocols (PDCP, SDAP).
Distributed Unit (DU): Manages lower-layer protocols (RLC, MAC,
PHY).
Radio Unit (RU): Interfaces with the antenna and handles RF processing.
This disaggregation enables functional split and virtualization, allowing
operators to deploy RAN components on general-purpose hardware, often
in cloud-native environments.
The Open RAN (O-RAN) initiative further enhances this flexibility by
standardizing interfaces (e.g., F1, E1, A1, E2) and enabling multi-vendor
interoperability. O-RAN introduces the RAN Intelligent Controller (RIC)—
both near-real-time (near-RT RIC) and non-real-time (non-RT RIC)—which
supports AI/ML-driven control and optimization through xApps and rApps.

**2.2 Traditional Anomaly Detection and Fault Management**

Historically, fault management in Radio Access Networks (RAN) has relied
on conventional, rule-driven approaches that include threshold-based
alarms, static rule engines, and manual diagnostics. Threshold-based alarms
are triggered when specific Key Performance Indicators (KPIs)—such as
call drop rate, handover failure rate, or signal-to-noise ratio—exceed
predefined limits. These thresholds are typically configured based on
historical norms or vendor recommendations. While simple to implement,
they often lack the flexibility to adapt to dynamic network conditions,
leading to frequent false positives or missed anomalies.


Rule-based systems, on the other hand, use expert-defined logic to
correlate multiple alarms and infer potential root causes. For example, a
combination of high BLER and low throughput might be interpreted as
interference or hardware degradation. However, these systems are
inherently limited by the scope and accuracy of the predefined rules. They
struggle to capture complex, nonlinear relationships between metrics and
are not well-suited for detecting previously unseen fault patterns.
Manual root cause analysis remains a common practice, where network
engineers analyze logs, alarms, and performance counters to diagnose
issues. This process is time-consuming, labor-intensive, and highly
dependent on the experience of the personnel involved. In large-scale
networks with thousands of cells and diverse configurations, manual
diagnostics become increasingly impractical.
Moreover, traditional methods face several critical limitations in modern
RAN environments:

High False Alarm Rates: Static thresholds often generate excessive
alarms during transient fluctuations, overwhelming operations teams and
leading to alarm fatigue.
Limited Fault Coverage: These systems are typically reactive and fail to
detect subtle or emerging issues that do not immediately breach
thresholds.
Lack of Scalability: As networks grow in size and complexity—
especially with the introduction of virtualized and disaggregated RAN
components—manual and rule-based approaches become difficult to
scale.
Poor Adaptability: Traditional systems cannot easily adapt to evolving
network topologies, traffic patterns, or new service types introduced in
5G and beyond.
Siloed Monitoring: Fault detection is often isolated within specific
domains (e.g., radio, transport, compute), making it difficult to identify
cross-domain or cascading failures.
These challenges underscore the need for more intelligent, data-driven
approaches—such as machine learning—that can learn from historical data,
adapt to changing conditions, and provide early, accurate detection of
anomalies across the entire RAN ecosystem.


**2.3 Related Research in ML for Network Diagnostics**

Recent advancements in machine learning have significantly influenced the
development of intelligent diagnostic systems in telecommunications,
particularly in the context of fault detection, anomaly detection, and
predictive maintenance within Radio Access Networks (RAN). A
substantial body of research has focused on unsupervised learning
techniques, where models such as K-Means clustering, DBSCAN, and
Gaussian Mixture Models (GMMs) are used to identify anomalous patterns
in high-dimensional KPI datasets without requiring labeled data. These
methods are often combined with dimensionality reduction techniques like
Principal Component Analysis (PCA) and t-SNE, which help visualize and
isolate outliers by projecting complex telemetry data into lower-
dimensional spaces.
Another prominent line of research involves autoencoder-based models,
which are neural networks trained to reconstruct normal operational
behavior. When these models encounter anomalous input—such as a sudden
spike in latency or a drop in throughput—the reconstruction error increases,
flagging potential faults. These models are particularly effective in handling
high-dimensional, multivariate telemetry data and are often used in
conjunction with attention mechanisms to improve interpretability.
Time-series forecasting models have also gained traction, especially
those based on Recurrent Neural Networks (RNNs) such as Long Short-
Term Memory (LSTM) and Gated Recurrent Unit (GRU) networks. These
models are adept at capturing temporal dependencies and long-range
correlations in KPI trends, enabling the early detection of performance
degradation. For instance, LSTM-based models are used to forecast
throughput and latency patterns, allowing operators to anticipate and
mitigate service disruptions before they impact users.
Emerging research also explores hybrid approaches that combine the
strengths of both supervised and unsupervised learning. These models often
use unsupervised techniques for anomaly scoring and supervised classifiers
for fault categorization. For example, Ericsson Research demonstrated a
hybrid LSTM-based framework that forecasts RAN KPIs and detects
anomalies with high precision, while Nokia Bell Labs proposed a
hierarchical machine learning architecture that localizes faults across
multiple layers of the RAN stack, from physical infrastructure to
application-level services. These systems often incorporate domain


knowledge, such as alarm correlation and network topology, to enhance
diagnostic accuracy and reduce false positives.
Additionally, there is growing interest in self-supervised learning and
federated learning for network diagnostics. Self-supervised models can
learn useful representations from unlabeled data, which is abundant in
telecom environments, while federated learning enables model training
across distributed network nodes without centralizing sensitive data—
addressing privacy and scalability concerns.
Collectively, these research efforts are shaping the next generation of
intelligent, autonomous network management systems that are capable of
real-time diagnostics, proactive maintenance, and adaptive learning in
complex, dynamic telecom environments.

**3 Types of RAN Anomalies and Failures**

Anomalies in Radio Access Networks (RANs) can arise from a wide range
of sources, including hardware malfunctions, software bugs, environmental
conditions, and misconfigurations. These anomalies often manifest as
degradations in Key Performance Indicators (KPIs), unexpected alarm
patterns, or abnormal log entries. Understanding the types of anomalies is
crucial for designing effective machine learning models for detection and
predictive maintenance.

**3.1 RAN Application Crashes**

As Radio Access Networks (RAN) evolve from traditional 4G LTE
deployments to cloud-native 5G architectures, the nature of software
failures and the strategies for managing them have also transformed. In both
generations, software crashes remain a critical concern, directly impacting
network availability, performance, and user experience. Leveraging
machine learning—particularly Large Language Models (LLMs)—offers a
promising path toward predictive fault management and self-healing
network behavior.
In 4G LTE networks, the eNodeB (Evolved Node B) is a central
component deployed on-site with dedicated hardware. Despite its stability,
the eNodeB is prone to software crashes triggered by various factors such
as power supply interruptions, excessive memory usage, configuration
mismatches, and peer-to-peer signaling failures. When a crash occurs, the


system generates a core dump file—a snapshot of the memory state at the
time of failure. This file is essential for post-mortem analysis, revealing the
crash signature and stack trace that help engineers identify the root cause.
Each crash typically initiates a recovery mechanism involving a full system
reboot. While this restores service, it introduces significant downtime. For
example, if a reboot takes 10 min and an eNodeB crashes five times in a
day, the resulting 50 minutes of downtime can severely degrade KPIs like
availability and accessibility, while also impacting user satisfaction.
In contrast, 5G networks adopt a cloud-native approach, deploying
network functions as containerized microservices—known as Cloud-Native
Network Functions (CNFs)—on virtualized infrastructure. These
microservices are orchestrated by platforms like Kubernetes and are
designed to be stateless, scalable, and independently restartable. However,
this flexibility introduces new challenges. Microservices may restart
frequently due to high memory consumption, CPU saturation, or sudden
spikes in user traffic. While these restarts are often automated, they can lead
to service instability, increased signaling overhead, and degraded
performance if not intelligently managed.
In both 4G and 5G contexts, LLMs can play a transformative role. By
analyzing historical core dumps, system logs, and telemetry data, LLMs can
learn to recognize patterns that precede crashes or abnormal behavior.
These models excel at processing unstructured text data, making them ideal
for interpreting log files and extracting semantic insights. When integrated
into network management platforms such as the RAN Intelligent Controller
(RIC) or EMS/NMS, LLMs can enable early warning systems that predict
software failures before they occur. This allows operators to take preventive
actions—such as traffic rerouting, configuration tuning, or controlled
restarts—thereby avoiding unplanned outages.
In summary, the application of LLMs for software crash prediction
across both traditional and cloud-native RAN environments represents a
significant advancement in proactive network maintenance. It reduces
downtime, enhances operational efficiency, and supports the vision of
autonomous, self-healing mobile networks that can adapt to the demands of
next-generation connectivity.

**3.2 KPI Degradation**


Key Performance Indicators (KPIs) are quantitative metrics used to evaluate
the health and performance of various components within a Radio Access
Network (RAN). Anomalies often manifest as deviations from expected
KPI values, signaling potential issues in the network. For instance, a sudden
drop in downlink or uplink throughput may indicate congestion,
interference, or hardware malfunctions. Increased latency, reflected in high
round-trip times, can result from backhaul congestion, scheduling delays, or
processing bottlenecks in the Distributed Unit (DU) or Centralized Unit
(CU). A spike in Block Error Rate (BLER) may suggest deteriorating radio
conditions, external interference, or hardware degradation in the Radio Unit
(RU). Similarly, high Radio Resource Control (RRC) connection failure
rates—classified as accessibility failures—can point to signaling issues or
misconfigured parameters. An uptick in handover failures may indicate
coverage gaps, timing misalignments, or mobility misconfigurations. These
degradations are often subtle and evolve gradually, making them ideal
candidates for time-series anomaly detection using machine learning
techniques.

**3.3 Hardware Faults**

Hardware-related anomalies in RAN environments are typically abrupt and,
if not addressed promptly, can lead to significant service outages. One
common issue is the overheating of Base Band Units (BBUs), often caused
by excessive CPU or memory usage. This can result in thermal throttling or
even forced shutdowns. Machine learning models can be trained to predict
such failures by analyzing trends in temperature and resource utilization.
Power supply failures, such as voltage fluctuations or battery degradation in
Remote Radio Heads (RRHs), can also cause intermittent outages. Another
critical issue is antenna misalignment, which may occur due to
environmental factors like strong winds or vandalism, leading to degraded
signal quality and reduced coverage. Additionally, failures in cooling
systems—such as malfunctioning fans—often precede overheating and can
be detected early through abnormal temperature rise patterns.
System logs and alarms offer a rich source of unstructured data that can
be mined for early indicators of network anomalies. Frequent
retransmissions, often observed in MAC or RLC logs, may signal poor link
quality or interference. Unexpected reboots can point to software crashes or
watchdog timer expirations. Alarm storms—bursts of alarms across


multiple cells or sites—may indicate systemic issues such as failed software
updates or misconfigurations. Some anomalies, known as silent failures, do
not trigger alarms but can be inferred from correlated log patterns or
gradual KPI drifts. Advanced techniques such as Natural Language
Processing (NLP) and log parsing algorithms can be employed to extract
meaningful features from these logs, enabling machine learning models to
detect and classify anomalies with high accuracy.

**3.4 Multi-domain and Cross-Layer Anomalies**

Modern Radio Access Networks (RANs) operate across multiple protocol
layers—such as the Physical (PHY), Medium Access Control (MAC),
Radio Link Control (RLC), and Packet Data Convergence Protocol (PDCP)
—as well as across different functional domains, including radio, transport,
and compute. Anomalies in such environments are often not confined to a
single layer or domain but can span across them, making detection and
diagnosis more complex. For instance, a degradation in the PHY layer, such
as poor Signal-to-Interference-plus-Noise Ratio (SINR), can cascade
upward and manifest as scheduling inefficiencies in the MAC layer.
Similarly, transport-layer bottlenecks—such as packet drops or jitter in the
fronthaul or backhaul—can impair RAN performance even though the root
cause lies outside the radio domain. In cloud-native RAN architectures,
virtualization failures such as container crashes or virtual machine (VM)
instability can simultaneously disrupt multiple network functions. Detecting
these multi-domain and cross-layer anomalies requires a holistic monitoring
approach that integrates data from various sources. Machine learning
models, particularly those capable of multimodal data fusion, are well-
suited for this task as they can correlate disparate signals and uncover
hidden dependencies that traditional monitoring systems might overlook.

**4 Machine Learning Approaches for Anomaly**

**Detection**

Anomaly detection in RAN involves identifying patterns in network data
that deviate from expected behavior. These deviations may indicate faults,
misconfigurations, or performance degradations. Machine learning (ML)
provides a powerful toolkit for detecting such anomalies, especially in


complex, high-dimensional, and dynamic environments like 5G and O-
RAN.

**4.1 Supervised Vs. Unsupervised Learning**

**Supervised Learning**
Supervised learning involves training a model on a labeled dataset, where
each data point is tagged with a known outcome—such as “normal” or
“anomalous.” This approach is highly effective when historical fault data is
available, allowing the model to learn clear patterns associated with
failures. It is particularly useful for classification tasks in anomaly
detection, where the goal is to distinguish between healthy and faulty states.

**Common Algorithms**
Random Forest: An ensemble method that builds multiple decision trees
and aggregates their outputs. It is robust to noise and overfitting, and
handles high-dimensional data well.
Support Vector Machines (SVM): A powerful algorithm that finds the
optimal boundary (hyperplane) between classes. It works well in high-
dimensional spaces and is effective for binary classification.
Gradient Boosting (e.g., XGBoost): A boosting technique that builds
models sequentially, each correcting the errors of the previous one.
XGBoost is known for its high accuracy, speed, and ability to handle
missing data and complex feature interactions.

**Unsupervised Learning**
Unsupervised learning is used when labeled data is scarce. These models
learn the underlying structure of the data and identify deviations from
normal behavior as potential anomalies. This approach is ideal for
exploratory analysis and anomaly detection in large-scale systems where
manual labeling is impractical.

**Techniques Include**
Clustering (e.g., K-Means, DBSCAN): These algorithms group similar data
points together. Anomalies are identified as points that do not belong to any
cluster or are far from cluster centroids.
Dimensionality Reduction (e.g., PCA, t-SNE): These techniques reduce
the number of features while preserving the structure of the data. PCA


captures linear relationships, while t-SNE is better at preserving local
structure and visualizing complex patterns.
Autoencoders: Neural networks trained to reconstruct input data. When
the reconstruction error is high, it indicates that the input deviates from
learned normal patterns—flagging it as anomalous.

**Semi-Supervised Learning**
Semi-supervised learning bridges the gap between supervised and
unsupervised methods by using a small amount of labeled data alongside a
larger pool of unlabeled data. This approach is particularly useful in
network diagnostics, where labeling every instance is costly or infeasible.
The labeled data helps guide the learning process, improving model
accuracy and generalization.

**4.2 Key Algorithms for Anomaly Detection**

Several advanced algorithms are commonly employed for detecting
anomalies in Key Performance Indicator (KPI) datasets. Isolation Forest
operates by randomly partitioning the data space, effectively isolating
anomalies more quickly due to their rarity and distinctiveness. It is
particularly well-suited for high-dimensional data. Autoencoders, a type of
neural network, are trained to reconstruct input data; anomalies are
identified when the reconstruction error exceeds a certain threshold, making
them effective for capturing subtle deviations in multivariate time-series
data. K-Means Clustering groups similar data points into clusters, and data
points that lie far from any cluster centroid are flagged as anomalies. This
method is especially useful for visualizing anomaly boundaries within the
KPI space. Lastly, Long Short-Term Memory (LSTM) Networks, a
specialized form of recurrent neural networks (RNNs), are designed to
model sequential data. They capture temporal dependencies in KPI trends
and can be leveraged for both anomaly detection and forecasting.

**4.3 Feature Extraction from RAN Data**

In Radio Access Networks (RAN), the effectiveness of anomaly detection
systems is fundamentally tied to the quality and diversity of features
extracted from raw operational data. One of the primary sources is KPI
time-series data, which includes metrics such as throughput, latency, block
error rate (BLER), and handover success rate. These metrics are


transformed into informative features using techniques like rolling
averages, first-order differences (deltas), and seasonal decomposition to
capture both short-term fluctuations and long-term trends. Alarm logs
provide another critical dimension, where the frequency, type, and temporal
cooccurrence of alarms are analyzed. Features such as alarm density per
time window and alarm correlation graphs help in identifying cascading
failures or systemic issues. System logs from network components—
including Distributed Units (DU), Centralized Units (CU), the RAN
Intelligent Controller (RIC), and network management systems
(EMS/NMS)—contain rich textual data. These logs are converted into
structured features using natural language processing (NLP) methods like
TF-IDF, word embeddings, or transformer-based models such as BERT,
enabling the detection of subtle operational anomalies. Environmental data,
such as temperature, humidity, and power supply metrics, are also
integrated, with features derived through normalization, trend analysis, and
rate-of-change calculations to detect anomalies influenced by external
conditions. To automate and scale this complex feature engineering process,
tools like Feature tools (for deep feature synthesis), TSFresh (for extracting
hundreds of time-series characteristics), and Scikit-learn Pipelines (for
consistent preprocessing and transformation workflows) are widely
adopted. These tools not only accelerate development but also ensure
reproducibility and consistency across anomaly detection pipelines.

**4.4 Model Evaluation and Deployment Considerations**

Evaluating and deploying anomaly detection models in Radio Access
Networks (RAN) requires a comprehensive approach that balances
performance, interpretability, and operational integration. Key evaluation
metrics include Precision, Recall, and F1-Score, which assess the model’s
ability to correctly identify anomalies while minimizing false positives and
negatives. For binary classification tasks, ROC-AUC is used to measure the
model’s ability to distinguish between normal and anomalous behavior
across various thresholds. In the case of forecasting-based anomaly
detection, Mean Absolute Error (MAE) is commonly employed to quantify
prediction accuracy. Beyond raw performance, model explainability is
critical for building trust and facilitating root cause analysis. Tools like
SHAP (Shapley Additive explanations) and LIME (Local Interpretable
Model-agnostic Explanations) help interpret model decisions by


highlighting the contribution of individual features to specific predictions.
For deployment, seamless integration with network infrastructure is
essential. Platforms such as ONAP DCAE support real-time data collection
and analytics, while O-RAN SMO enables orchestration and lifecycle
management of AI/ML models in open RAN environments. Additionally,
tools like MLflow and Kubeflow are widely used for managing the end-to-
end machine learning lifecycle, including model versioning, reproducibility,
deployment, and monitoring, ensuring that anomaly detection systems
remain robust, scalable, and maintainable in production environments.

**5 Predictive Maintenance in RAN Using Machine**

**Learning**

Predictive maintenance in Radio Access Networks (RAN) aims to anticipate
failures before they occur, enabling proactive interventions that minimize
downtime, reduce operational costs, and improve service continuity. Unlike
traditional reactive or scheduled maintenance, predictive maintenance
leverages machine learning (ML) to analyze historical and real-time data,
identifying early warning signs of potential issues.

**5.1 Motivation and Benefits**

In modern Radio Access Network (RAN) deployments, failures in critical
components such as Distributed Units (DUs), Centralized Units (CUs), and
Radio Units (RUs) can result in significant service degradation or even
complete outages. Predictive maintenance offers a proactive approach to
mitigate such risks by identifying early warning signs—such as
overheating, memory leaks, or abnormal resource usage—before they
escalate into critical failures. This not only helps prevent catastrophic
downtime but also optimizes maintenance schedules, reducing unnecessary
site visits and operational costs. Furthermore, it enhances overall network
reliability, improves customer satisfaction, and significantly reduces Mean
Time to Repair (MTTR) through preemptive diagnostics. To enable such
predictive capabilities, models rely on a rich variety of telemetry and
contextual data. These include system logs (e.g., kernel logs, application
logs, and crash reports from DU/CU nodes), resource utilization metrics
(such as CPU, memory, disk I/O, and network throughput), and


environmental sensor data (including temperature, humidity, and power
supply voltage). Historical alarm data—capturing the frequency, type, and
sequence of alarms—along with long-term KPI trends like gradual
degradation in throughput or increasing latency, also play a crucial role.
These data streams are typically collected and aggregated through
EMS/NMS platforms, O-RAN SMO frameworks, or telemetry agents such
as Prometheus and Fluentd, forming the foundation for robust and
intelligent predictive maintenance models.

**5.2 Machine Learning Models for Predictive Maintenance**

Predictive maintenance in RAN environments leverages a range of machine
learning models to anticipate failures and optimize operational efficiency.
Classification models are commonly used to predict whether a network
component—such as a DU, CU, or RU—is likely to fail within a specified
time window. Among these, Random Forest is favored for its robustness to
noisy data and ability to handle high-dimensional feature sets, while
XGBoost offers superior accuracy and interpretability through gradient
boosting techniques. Logistic Regression, though simpler, remains effective
for binary classification tasks where failure prediction is straightforward.
These models are trained on labeled historical datasets, where the target
variable indicates the occurrence of past failures, enabling the system to
learn patterns and precursors of breakdowns. In parallel, time-series
forecasting models are employed to predict future values of KPIs and
sensor readings, which can signal impending issues. LSTM (Long Short-
Term Memory) networks excel at capturing long-term dependencies in
sequential data, making them ideal for modeling trends and anomalies over
time. Traditional statistical models like ARIMA are useful for univariate
time-series forecasting, while Facebook Prophet is designed to handle
seasonality and abrupt trend changes, offering flexibility in dynamic
network environments. Forecasted values from these models are compared
against predefined thresholds or learned profiles of normal behavior to flag
deviations that may indicate early signs of failure. Together, these models
form the backbone of intelligent, data-driven predictive maintenance
systems that enhance reliability, reduce downtime, and support proactive
network management.

**5.3 Model Training and Evaluation**


Training predictive maintenance models in RAN environments involves
leveraging historical failure data enriched with features derived from
system logs, KPI trends, alarm histories, and environmental sensor metrics.
The training process focuses on identifying patterns and precursors to
failures, enabling models to generalize across different network scenarios.
Given the typically imbalanced nature of failure datasets—where actual
failure events are rare compared to normal operations—evaluation metrics
play a critical role in assessing model performance. Precision and Recall are
particularly important, as they measure the model’s ability to correctly
identify failures without generating excessive false alarms. The F1-Score,
which harmonizes precision and recall, provides a balanced view of
performance, especially in high-stakes environments where both false
positives and false negatives carry operational risks. A unique and valuable
metric in predictive maintenance is Lead Time, which quantifies how far in
advance a model can predict a failure before it actually occurs. Longer lead
times allow for more proactive interventions, such as scheduling
maintenance or rerouting traffic, thereby minimizing service disruption.
Together, these training and evaluation strategies ensure that predictive
models are not only accurate but also actionable, supporting timely and
efficient network operations.

**5.4 Integration with Network Management Systems**

For predictive maintenance models to deliver real operational value in
Radio Access Networks (RAN), they must be seamlessly integrated into
existing network management systems and workflows. This integration
ensures that machine learning insights translate into timely and actionable
interventions. The O-RAN Service Management and Orchestration (SMO)
framework plays a central role by hosting intelligent rApps that consume
ML predictions and trigger automated maintenance workflows, such as
traffic optimization or resource reallocation. Similarly, traditional Element
Management Systems (EMS) and Network Management Systems (NMS)
can visualize predictive insights through dashboards and generate service
tickets for field engineers, enabling proactive maintenance scheduling. A
key capability enabled by this integration is closed-loop automation, where
ML models not only detect potential failures but also initiate corrective
actions autonomously—such as rerouting traffic, rebooting nodes, or
adjusting configuration parameters. For instance, if a predictive model


forecasts a high probability of Base Band Unit (BBU) overheating within
the next 6 h, the system can automatically alert the Network Operations
Center (NOC), schedule a cooling system inspection, and shift traffic to
neighboring cells to reduce thermal load. This kind of intelligent
orchestration minimizes downtime, enhances network resilience, and
reduces operational costs by enabling data-driven, preemptive decision-
making across the RAN ecosystem.

**6 Architecture and Workflow of the Proposed**

**System**

Designing an effective ML-based anomaly detection and predictive
maintenance system for RAN requires a modular, scalable, and real-time
architecture. The system must ingest diverse data sources, process and
transform them into meaningful features, apply trained ML models, and
integrate seamlessly with network management systems for actionable
insights.
The architecture illustrated in the diagram demonstrates how machine
learning is integrated into an Open RAN environment to enable anomaly
detection and predictive maintenance. It features a modular design with the
Service Management and Orchestration (SMO) layer coordinating both
Near-Real-Time and Non-Real-Time RICs, which host xApps and rApps
respectively. These components interface with the CU, DU, and RU through
standardized O-RAN interfaces. The ML pipeline—comprising data
collection, preprocessing, model training, inference, and feedback—
supports applications such as anomaly detection, traffic forecasting, and
energy optimization. Deployed on a cloud-native infrastructure, this
architecture enables scalable, low-latency, and intelligent RAN operations
(Fig. 1 ).


**_Fig. 11.1_** Architecture of Open RAN integrated with machine learning applications for anomaly
detection and predictive maintenance

**6.1 System Architecture Overview**

The proposed system architecture for intelligent anomaly detection and
predictive maintenance in Radio Access Networks (RAN) is a modular,
scalable, and cloud-native pipeline composed of seven tightly integrated
layers. At its foundation lies the Data Collection Layer, responsible for
ingesting high-frequency telemetry from RAN nodes—including
Distributed Units (DUs), Centralized Units (CUs), and Radio Units (RUs).
This layer captures a wide spectrum of data such as KPIs, system and
application logs, alarm events, and environmental sensor readings. Data is
collected through standardized O-RAN interfaces like O1 (for
management), E2 (for near-real-time control), and proprietary APIs exposed
by EMS/NMS platforms. Tools such as Fluentd, Telegraf, Prometheus, and
Kafka ensure reliable, real-time data streaming and buffering, supporting
both push and pull-based telemetry models.


The Data Preprocessing and Feature Engineering Layer transforms raw,
heterogeneous data into structured, model-ready formats. This includes data
cleansing, normalization, time alignment, and aggregation across multiple
temporal resolutions. Advanced feature extraction techniques are applied to
derive statistical, temporal, and semantic features—such as rolling
averages, first-order differences, seasonal decompositions, and log
embeddings using NLP models like TF-IDF or BERT. This layer leverages
distributed processing frameworks like Apache Spark for scalability, along
with Python-based libraries such as Pandas, TSFresh, and Scikit-learn for
flexible and expressive feature engineering.
The Model Training and Evaluation Layer is where machine learning
models are developed using historical labeled and unlabeled data. Both
supervised models (e.g., Random Forest, XGBoost, LSTM) and
unsupervised models (e.g., Isolation Forest, Autoencoders) are trained to
detect anomalies and predict failures. This layer supports robust evaluation
using cross-validation and a suite of metrics including Precision, Recall,
F1-score, ROC-AUC, and Lead Time—an essential metric in predictive
maintenance that quantifies how early a model can anticipate a failure.
Model development is managed using ML lifecycle tools like MLflow,
ensuring reproducibility, version control, and experiment tracking.
The Inference Engine operationalizes trained models for real-time or
batch inference. It supports low-latency scoring of incoming data streams
and can trigger alerts or maintenance workflows based on model outputs.
This layer is built on scalable serving platforms such as ONNX Runtime,
TensorFlow Serving, and Seldon Core, which support containerized
deployment and Kubernetes-based orchestration.
The Integration Layer with Network Management Systems ensures that
actionable insights are seamlessly communicated to operational platforms
such as the O-RAN Service Management and Orchestration (SMO)
framework, EMS/NMS systems, and the RAN Intelligent Controller (RIC).
This enables closed-loop automation through xApps and rApps, allowing
the network to autonomously respond to predicted failures or detected
anomalies. Integration is facilitated via RESTful APIs, message queues, and
publish-subscribe mechanisms.
The Visualization and Alerting Layer provides intuitive dashboards and
real-time alerting mechanisms for network operations center (NOC)
engineers. Tools like Grafana and Kibana are used to visualize time-series


trends, anomaly scores, and predictive insights, while Prometheus Alert
manager and external notification systems (e.g., email, SMS, ticketing
platforms) ensure timely response to critical events.
Finally, the Feedback Loop Layer enables continuous learning by
incorporating operator feedback, newly labeled data, and evolving network
conditions into the training pipeline. This adaptive mechanism ensures that
models remain accurate and relevant over time, even as network
configurations, traffic patterns, and hardware evolve. It supports automated
retraining, model validation, and redeployment, forming the backbone of a
self-improving AI-driven RAN management system.

**6.2 Workflow**

The operational workflow of the predictive maintenance system in RAN
environments follows a structured, end-to-end pipeline designed for
efficiency and responsiveness. It begins with data ingestion, where both
real-time and batch telemetry—including KPIs, logs, alarms, and
environmental metrics—are collected from RAN elements such as DUs,
CUs, and RUs, and stored in a centralized data lake for scalable access.
Next, the preprocessing stage cleanses the raw data, synchronizes
timestamps across sources, and transforms the inputs into structured feature
vectors suitable for machine learning analysis. In the model inference
phase, trained ML models analyze these feature vectors to detect anomalies
or predict potential failures based on learned patterns. Upon identifying a
high-risk condition or anomaly, the system proceeds to alert generation,
notifying the Network Operations Center (NOC) or triggering automated
workflows. The subsequent action recommendation step provides context-
aware suggestions for corrective measures, such as rebooting a node,
rerouting traffic, or scheduling a hardware inspection. Finally, the operator
feedback loop allows engineers to validate alerts and provide feedback on
model accuracy and relevance. This feedback is logged and used to retrain
and refine the models, ensuring continuous learning and adaptation to
evolving network conditions.

**7 Summary of the Approach**

This chapter presented a comprehensive framework for applying machine
learning (ML) to anomaly detection and predictive maintenance in Radio


Access Networks (RAN), addressing the limitations of traditional fault
management systems amid the growing complexity introduced by 5G and
Open RAN (O-RAN) architectures. It explored various types of anomalies
—ranging from KPI degradations to hardware and software faults—and
discussed the applicability of supervised, unsupervised, and time-series ML
models to RAN telemetry data. A modular system architecture was
proposed, integrating data collection, preprocessing, model training,
inference, and real-time alerting, with use cases demonstrating early fault
detection, reduced downtime, and improved operational efficiency.
Furthermore, the adoption of ML-based predictive maintenance holds
transformative potential for enhancing network reliability and significantly
reducing operational expenditures (OPEX). By enabling early fault
detection and intelligent traffic rerouting, ML models support proactive
interventions, optimize resource utilization, and minimize service
disruptions. This approach also facilitates condition-based maintenance,
reducing unnecessary site visits and hardware replacements, while
improving customer experience and scalability. As telecom networks evolve
toward greater automation and intelligence, the integration of AI/ML
technologies into RAN operations will be foundational to building self-
healing, cost-efficient, and future-ready infrastructure.

**8 Conclusion and Future Directions**

**8.1 Conclusion**

The integration of machine learning into Radio Access Network (RAN)
operations marks a significant advancement in the evolution of intelligent,
self-optimizing networks. As RANs become more complex with the
adoption of 5G, virtualization, and Open RAN architectures, traditional
fault detection and maintenance methods are no longer sufficient. ML-based
systems offer a scalable, adaptive, and proactive approach to managing
network health.
This chapter has explored the landscape of ML-driven anomaly
detection and predictive maintenance in RAN. We began by understanding
the limitations of conventional systems and the motivation for adopting
ML. We then examined the types of anomalies that occur in RAN
environments, ranging from KPI degradations to hardware and software
faults. Various ML techniques—supervised, unsupervised, and time-series


models—were discussed in the context of their applicability to RAN data. A
modular system architecture was proposed, followed by real-world and
simulated use cases that demonstrated the practical benefits of these
approaches.
The results are promising: ML models can detect subtle anomalies,
predict failures with significant lead time, and reduce operational overhead
through automation and intelligent alerting. These capabilities are essential
for maintaining high service quality and operational efficiency in modern
telecom networks.

**8.2 Future Directions**

While current machine learning applications in Radio Access Networks
(RAN) have demonstrated significant impact in areas such as anomaly
detection and predictive maintenance, several promising avenues remain
open for further research and innovation. Federated Learning is gaining
traction as a privacy-preserving approach that enables model training across
distributed RAN nodes without transmitting raw data, thereby reducing
bandwidth usage and enhancing data security. Explainable AI (XAI) is
another critical area, aimed at improving model transparency and
interpretability to foster operator trust and meet regulatory requirements—
especially important in mission-critical telecom environments.
Reinforcement Learning offers the potential for autonomous, real-time
decision-making, allowing systems to dynamically optimize resources and
mitigate faults based on environmental feedback. To address the challenge
of limited failure data, Synthetic Data Generation using generative models
like GANs can simulate rare fault scenarios, enabling the training of more
robust and generalizable models. The rise of Edge AI is also transforming
RAN operations by deploying lightweight ML models directly on network
elements such as DUs and RUs, enabling ultra-low-latency inference and
localized decision-making. Furthermore, Standardization and
Interoperability—particularly alignment with O-RAN Alliance
specifications—are essential to ensure seamless integration of ML solutions
across multi-vendor ecosystems. As the telecom industry continues to
evolve toward more intelligent and autonomous networks, embracing these
advanced ML techniques will be key to unlocking new levels of
automation, operational efficiency, and service quality.

```
OceanofPDF.com
```

(1)
(2)

© The Author(s), under exclusive license to Springer Nature Switzerland AG 2026
S. Singh et al. (eds.), _GenAI and LLMs for Beyond 5G Networks_
https://doi.org/10.1007/978-3-032-06418-9_12

###### Next-Generation Intent-Based Networking

###### in B5G with LLMs and Explainable AI

Caglar Tunc^1 and Kaustubh Joshi^2
Sabanci University, Tuzla, Istanbul, Turkey
Atlassian Corp, Mountain View, CA, USA

```
Kaustubh Joshi
Email: kaustubh.joshi@gmail.com
```
**Abstract**
As cellular networks evolve beyond 5G (B5G), intent-based networking (IBN)
has emerged as a foundational paradigm for achieving autonomous, zero-touch
service management. This chapter explores how large language models
(LLMs) and explainable AI (xAI) can transform IBN by enabling intuitive
intent interpretation, dynamic conflict resolution, and transparent
orchestration. We examine the limitations of current rule-based and template-
driven approaches and present LLMs as natural language-driven interfaces that
bridge the semantic gap between high-level operator goals and low-level
network policies. The chapter highlights the unique ability of LLMs to
negotiate conflicting intents across multi-agent, multi-domain B5G
environments, and to generate human-readable justifications for automated
decisions. We also discuss architectural considerations, including integration
with platforms like ONAP and O-RAN, and propose a modular system
combining LLMs, conflict resolution engines, and xAI techniques such as
SHAP, LIME, and counterfactual reasoning. By synthesizing natural language
understanding with AI-native orchestration, this chapter charts a path toward
scalable, trustworthy, and explainable automation in future communication
networks.


**Keywords** Intent-based networking – Large language models (LLMs) –
Explainable AI – Conflict resolution – Agentic networks

**1 Introduction**

Machine learning (ML) and artificial intelligence (AI) mechanisms are
positioned at the core of Beyond 5G (B5G) networks [ 1 ]. While cellular
networks have become AI enabled with the 5G architecture, with 6G, the
cellular networks are envisioned to be AI-native [ 2 – 4 ]. This paradigm shift
requires the B5G networks not only to support AI algorithms but also to be
fully integrated with AI framework. AI-native networks bring unparalleled
potential and new opportunities to cellular systems, leveraging the vast amount
of data generated and collected across the network, along with powerful AI
mechanisms capable of exploiting this data. AI-empowered services, designed
to ingest data produced at different tiers and timescales within the network,
can enable intelligent management and optimization frameworks that were
absent in prior generations [ 5 ]. However, despite these promises, this high
level of intelligence introduces the need for complex orchestration
mechanisms, where AI tools and frameworks must be coordinated
autonomously with minimal human intervention. Moreover, since B5G
networks serve as the digital backbone for critical societal functions, reliability
and robustness are of utmost importance [ 6 ]. Therefore, AI mechanisms must
not only operate harmoniously but also offer transparent, goal-aligned
justifications, especially in the presence of anomalies or outages, so that their
behavior remains accountable and traceable.

**1.1 Role of GenAI and LLMs in B5G Service Management**

In this context, autonomy or zero-touch automation refers to the network’s
ability to interpret objectives, make decisions, and adapt policies in real time
without requiring manual configuration or operator intervention [ 2 ].
Generative AI (GenAI) models, particularly large language models (LLMs),
are emerging as key enablers of autonomous, intelligent service management
in B5G networks [ 3 , 4 ]. LLMs provide two essential benefits in this domain.
First, they allow operators to express high-level service requirements or
policies, commonly referred to as “intents,” in natural language, which the
LLMs can translate into network-interpretable formats such as policies, rules,
or configuration parameters [ 4 ]. This translation capability makes it possible to
abstract away the technical complexity of underlying systems, enabling intent-


based interaction with the network. Second, LLMs can also operate in the
reverse direction, by converting the internal status, decisions, or actions of the
network into human-readable explanations [ 4 ]. This includes summarizing
performance issues, justifying orchestration decisions, or alerting about
anomalies in a way that is both interpretable and actionable by operators. In
this sense, LLMs act as a bidirectional interface or “translator” between
human operators and autonomous network functions, playing a crucial role in
enabling trustworthy, transparent, and efficient service management. While
LLMs are the primary focus of this chapter due to their strong alignment with
intent expression, semantic reasoning, and explainability, other forms of
generative AI, such as diffusion models [ 7 ], generative adversarial networks
(GANs) [ 8 ], and foundation models [ 9 ], for time-series or tabular data also
hold potential for B5G service management [ 10 ]. These models may support
tasks such as traffic prediction, anomaly generation, or synthetic data creation
for training purposes. However, in this chapter, we concentrate on LLM-based
architectures and their unique role as language-driven interfaces that bridge
the gap between autonomous systems and human intent, with a particular
focus on their use in policy translation, conflict resolution, and explainable
orchestration.

**1.2 Evolution of Network Management: From Rule-Based to**

**Intent-Based**

Intent-based networking represents a significant change in the vision of how
networking is done. Traditionally, network management relies on rule-based
approaches, where network operators and engineers either manually configure
devices and configurations or define rule-based mechanisms to optimize how
network behaves. This way of handling network, in general, is based on
vendor- and operator-specific needs and capabilities. The first step toward
fully softwarized networks was the introduction of Software-Defined
Networks (SDNs) and the concept of Network Function Virtualization (NFV)
[ 11 ]. The advancements on these two domains enabled the abstraction and
separation of control and data planes and, in turn, more centralized and
programmable control. Nevertheless, even with SDNs with NFV adopted,
networks still needed rules on how the network should behave to be
predefined and corresponding configurations and devices to be manually
optimized. IBN branched out from this model by allowing the network
operators to concentrate on “what” instead of “how” [ 12 ]. More specifically, in
IBN, the operator can specify high-level goals, such as “increase minimum


throughput of Augmented Reality (AR) users by 10 _%_ ” or “minimize energy
during off-peak hours” [ 13 ]. The network translates and extracts features from
these intents in a way that is meaningful for the network orchestrator.
Following this, the network orchestrator is responsible for taking actions to
enforce the intents on the network. This decoupling of network objectives and
implementation logic simplifies how the network takes actions and adapts to
dynamic conditions. In addition, it provides a foundation for scalable and
flexible automation, enabling cross-domain coordination without requiring
low-level manual configuration. A reference end-to-end flow of intent
processing in IBN is illustrated in Fig. 1. At a high level, the primary goals of
IBN include:

```
Simplifying network operations by abstracting complex configurations into
intent-level directives
Improving agility and responsiveness by enabling the network to
dynamically interpret and act on operator goals
Enhancing service assurance and policy compliance through continuous
monitoring and closed-loop adjustments
```
These general capabilities make IBN a core enabler for autonomous network
management in B5G networks.


**_Fig. 1_** Reference flow of intent processing in IBN: from user input to infrastructure control and
feedback

**1.3 Conflict Resolution in IBN**

As networks become more autonomous and intent-driven, one of the most
persistent challenges is resolving conflicts between simultaneously existing,
overlapping, and potentially incompatible intents. These conflicts may arise
due to competing network goals, resource constraints, or unforeseen
interactions between multiple network objectives. Traditional resolution
methods rely on static policies, operator-defined priority hierarchies, or
manual overrides. However, these solutions fall short in dynamic
environments where intents must be balanced in real time based on evolving
context and preferences [ 12 ]. Large Language Models (LLMs), especially
when enhanced with reasoning capabilities, offer a promising approach to this
challenge. Chain-of-thought prompting can guide LLMs to evaluate the
implications of each intent step by step, simulating negotiation or arbitration


between intents [ 14 ]. For example, rather than selecting one intent over
another, the model can explore trade-offs, partial satisfaction, or conditional
enforcement strategies, enabling more nuanced resolution. Additionally,
preference fine-tuning methods, such as Reinforcement Learning from Human
Feedback (RLHF), can be used to align conflict resolution strategies with
operator values [ 15 ]. By collecting preference data on how different conflict
scenarios should be handled, models can be trained to generalize those
preferences in future cases. This provides a human-aligned, data-driven
foundation for resolving competing intents while remaining aligned with
network goals. As B5G networks increasingly support mission-critical
services, such adaptive and interpretable conflict resolution capabilities
become essential, not only to maintain performance and reliability but also to
ensure trust in autonomous AI-driven network operations.

**1.4 Overview of Explainability in IBN**

**_1.4.1 Limitations of Current IBN and Automation Frameworks_**

Despite the current advancements in IBN, handling conflicting intents, and
hence, objectives, is still an open problem. When conflicting and/or
overlapping intents are provided, network operators still rely on policy
templates, simple priority-based implementations, and preconfigured
workflows [ 16 ]. These approaches fall short of semantic understanding of the
intents and underlying objectives, which is the true essence of IBN. Another
key limitation is the lack of interpretability. Traditional IBN automation
frameworks do not provide insights on why certain decisions have been made
by the network. This absence of transparency is becoming an increasing
concern as IBN deployments expand and the number of intents that networks
must interpret and enforce grows rapidly.

**_1.4.2 Explainable AI in Networking and Automation_**

Explainable AI (xAI) offers a promising approach for network operators to
convert black-box AI models into transparent and interpretable techniques.
Methods such as SHAP (SHapley Additive exPlanations) [ 17 ], LIME (Local
Interpretable Model-agnostic Explanations) [ 18 ], and counterfactual reasoning
[ 19 ] can be used to provide insights into model behavior, identifying which
features influenced a decision, how much they contributed, and what would
have led to a different outcome. In the context of IBN, xAI can significantly
enhance the automation and orchestration procedures by producing human-
readable reports for intent translation, intent conflict resolution, and policy


modifications. For example, it may not be clear why a certain set of
parameters has been configured by the network from the perspective of a
single intent or network objective. However, reports produced by xAI tools
can shed light on the influence of a conflicting intent that has driven the
network parameters in a particular direction. When combined with conflict
resolution mechanisms, xAI can help ensure transparency and reliability in
network behavior, even under highly dynamic conditions.

**2 Intent-Based Networking in B5G: Main**

**Challenges**

As B5G networks evolve toward an AI-native architecture with zero-touch
autonomy, the IBN concept should address several key technical and
operational challenges. We categorize these challenges into three main
categories, namely intent specification, conflicting intents/objectives, and
closed-loop adaptation with real-time responsiveness, which are detailed in
this section.

**2.1 Intent Specification: High-Level Goals Versus Low-Level**

**Policies**

One of the fundamental challenges in IBN is the gap between high-level
network objectives and low-level policies adopted to achieve them. Operators
need intelligent translation between abstract and high-level intents (e.g.,
increase throughput, expand coverage, minimize energy, etc.) and actionable
low-level policies involving specific parameters or configurations (e.g.,
scheduler configurations, power control thresholds, resource block allocations,
etc.). This translation requires not only semantic understanding of the intent
but also contextual awareness of the network state, user/service needs, domain
constraints, etc.

**2.2 Conflicting Objectives**

In real-world scenarios, multiple intents may coexist and conflict due to
overlapping set of parameters or devices affected. For instance, consider two
intents provided to the network:

```
Intent 1: Minimize energy consumption of sectors A and B during off-peak
hours.
Intent 2 : Maximize accessibility for users across all sectors.
```

It is clear that Intents 1 and 2 are in conflict. Achieving energy minimization
may require turning off certain equipment or placing them in sleep mode,
whereas maximizing accessibility demands that all sectors remain fully active.
Similarly, intents targeting different service types (e.g., Internet of Things
(IoT) vs. AR users) may drive network configurations in opposing directions.
Such conflicts must be handled in a way that is not only unified, ensuring all
intents are adopted intelligently, but also interpretable by the network operator.

**2.3 Real-Time Responsiveness and Closed-Loop Adaptation**

Today’s networks operate in highly dynamic environments, characterized by
rapidly changing user mobility, channel conditions, and traffic demands. IBN
systems must adapt to these fluctuations in real time, ensuring that existing
intents and their corresponding policies remain valid and responsive as new
intents are introduced into the continuously evolving network environment.
This requires closed-loop adaptation, where the network observes and reacts to
the changes in the network continuously. Key performance metrics (KPIs)
must be monitored, and if unexpected variations happen in the network that
affect these KPIs, the associated intents and policies must be reevaluated and
the network must adapt itself accordingly. For example, if an intent to
“provide 99.0 _%_ reliability to connected vehicles” begins to fail due to network
congestion or interference, the orchestrator must detect this performance
degradation and respond by adjusting the configurations or provisioning an
enhanced network slice for the affected service.

**3 LLMs for Intent Interpretation and Policy**

**Generation**

With the use of LLMs such as GPT, BERT, and their domain-specific
adaptations, networks gain powerful tools to translate human-written intents
into machine-readable, actionable features from which network policies can be
derived. These models perform remarkably well in extracting features and
semantic meaning from textual inputs and generating structured outputs. In
this section, we delve into the details of how and where LLMs can be
effectively used for intent-based networking (IBN).

**3.1 Translating Natural Language Intents into Actionable**

**Policies**


The need to interpret high-level intents, expressed as natural language inputs
(e.g., “ensure low-latency Virtual Reality (VR) traffic during peak hours”), and
translate them into actionable policies (e.g., creating network slices,
configuring resource allocation parameters, etc.) lies at the core of IBN. LLMs
make a great fit for this requirement, as they can generate structured outputs in
formats such as JSON, YAML, or even implementation specific formats such
as ONAP- and OpenAPI-compliant schemas. This makes them ideal for task
like:

```
Mapping phrases like “maximize battery life for IoT devices” into uplink
power control rules
Translating “prioritize mission-critical flows” into network slice definitions,
resource allocation guarantees, and traffic shaping parameters
Decomposing compound intents like “maximize throughput while
minimizing energy consumption” into smaller intents and corresponding
policies
```
We tabulate example scenarios where LLMs extract features from provided
intents and generate network-level policies in Table 1.

**_Table 1_** Sample user intents, their corresponding feature vectors, and configurational changes addressing
the intents

```
Intent Target
area
```
```
Target KPI Time
constraint
```
```
Configuration
change
```
```
Operational
mode
```
```
Affected
sectors
```
```
Priority
```
```
Reduce
interference
in
residential
areas
during
peak hours
```
```
Residential RSRQ
>− 10
```
```
18:00–
22:00
```
```
Reduce power
by 10%, adjust
antenna tilt
```
```
Interference
Reduction
Mode
```
```
2,3 3
```
```
Increase
accessibility
for
crowded
zones
```
```
Avenue Accessibility
> 99 %
```
```
12:00–
20:00
```
```
Enable load-
balancing
settings and
prioritize
coverage
bands
```
```
High-
Accessibility
Mode
```
```
1 5
```
```
Increase
throughput
for high-
demand
users
```
```
Residential,
Park,
Avenue
```
```
Minimum
Throughput
> 10 Mbps
```
```
12:00–
20:00
```
```
Prioritize 30
MHz Channels
```
```
High-
Capacity
Mode
```
```
1,2,3 2
```

```
Intent Target
area
```
```
Target KPI Time
constraint
```
```
Configuration
change
```
```
Operational
mode
```
```
Affected
sectors
```
```
Priority
```
```
Minimize
energy
usage in
park areas
overnight
```
```
Park Power < 20
W
```
```
00:00–
06:00
```
```
Activate low-
power mode
```
```
Energy-
Saving
Mode
```
```
2 4
```
**3.2 Prompt Engineering and Retrieval-Augmented Generation**

**(RAG) for Telecom Contexts**

General off-the-shelf LLMs are not telecom-aware by default. To enable
effective translation and policy generation, prompt engineering with goal-
specific intents is critical. In addition to the common techniques used in
prompt engineering, such as providing step-by-step instructions and chain of
thoughts, an example of a well-defined goal-specific intent is “Generate a
slicing policy to meet 1 ms latency for ultra-reliable low-latency
communications (URLLC) traffic in the RAN domain.” This type of prompt
not only specifies “what” (1 ms latency for URLLC traffic) to achieve but also
provides an outline of “how” (network slicing) and “where” (RAN) to
implement the required policies.
On the other hand, network operators and vendors develop proprietary
software, hardware, mechanisms, and terminology, for which LLM tools lack
the required understanding and interpretation. Retrieval-Augmented
Generation (RAG) provides a practical and scalable method for customizing
LLM outputs without retraining the model. In a RAG setup, the LLM is
supplemented with a retrieval module that fetches relevant documents—such
as vendor-specific manuals, configuration guides, or service-level agreement
(SLA) templates from a knowledge base. This enables the LLM to support its
responses with accurate, context-rich information, making it especially useful
for telecom stakeholders working with heterogeneous equipment, proprietary
APIs, or nonstandard policy structures.

**3.3 Role of LLMs in Negotiating and Harmonizing Conflicting**

**Intents**

As networks adopt intent-based interfaces, it becomes increasingly common
for multiple intents to be issued simultaneously, often by different
stakeholders, applications, or services with various goals. These intents may
be valid individually but can conflict when issued together, especially under
shared infrastructure and resource constraints. For example, one intent might


aim to reduce power consumption in the RAN during off-peak hours, while
another seeks to maintain full user accessibility across all sectors. Traditional
orchestration systems typically resolve such conflicts using priority-based or
hierarchical rules. However, these approaches are inflexible and often lack
transparency.
LLMs introduce the capability of context-aware conflict resolution and
semantic-level negotiation between competing intents. By understanding the
underlying objectives, constraints, and potential trade-offs embedded within
each intent, LLMs can propose resolution strategies that balance conflicting
goals intelligently. For instance, when faced with latency-sensitive and
fairness-driven intents, an LLM can generate a middle-ground policy, such as
temporarily relaxing fairness constraints in overloaded sectors while
preserving baseline access for all users. On the output side, LLMs can also
provide a natural language explanation of the decision rationale.
Moreover, these conflicting intents can also appear across different
domains in the network. For example, an LLM can decompose a global low-
latency intent into RAN-level scheduler adjustments, core-level queue
prioritization, and transport-level routing changes. This separating can ensure
that each subsystem contributes meaningfully to the shared objective.
Furthermore, in multi-tenant and multi-agent scenarios, LLMs can assist in
intent negotiation, acting as mediators that interpret multiple natural language
inputs, identify overlaps or incompatibilities, and propose harmonized intents.
This capability is especially useful in scenarios involving shared RAN slicing
with conflicting KPIs, time-varying service demands and load conditions, and
autonomous agent frameworks (e.g., xApps/rApps) issuing intents
concurrently.

**4 Multi-agent Conflict Resolution in B5G**

As AI-native B5G networks emerge and expected to operate with full
autonomy, decision-making processes increasingly occur in a distributed and
multi-agent setting. This means that multiple decision-making entities, such as
service orchestrators, RAN Intelligent Controller (RIC) components
(xApps/rApps), and network-defined slices, as well as domains (e.g., RAN,
transport, and core), simultaneously generate intents and policies. Therefore,
conflict resolution must scale to these decentralized and coexisting
environments, requiring new tools for negotiation, arbitration, and


coordination. In this section, we discuss the sources and types of conflicts that
emerge and how LLM-guided agents can help resolve and navigate them.

**4.1 Multi-Domain and Multi-Agent Environments**
In B5G architectures, control is distributed across hierarchical and federated
entities, including domain-specific controllers (e.g., RAN Intelligent
Controller, SDN controllers), service-specific agents (e.g., slice managers),
and autonomous network functions (e.g., self-optimizing RAN elements).
These components may operate independently, optimizing for local objectives
or intents. In such environments, multiple agents may receive overlapping or
contradictory intents and compete for shared network resources (e.g.,
spectrum, compute, fronthaul). Moreover, these components can, in general,
operate at different temporal granularities (e.g., real-time, near-real-time, and
non-real-time optimization). Without a unified conflict resolution mechanism,
such fragmentation can lead to (i) oscillatory behavior (e.g., aggressive
handover triggering by multiple agents), (ii) resource starvation or hoarding by
dominant slices, and (iii) SLA violations due to policy interference.

**4.2 Intent Conflicts Scenarios in B5G**

Conflicts in IBN arise from the layered and multi-agent nature of B5G
deployments. In Table 2 , we outline several representative scenarios that can
arise across key network functions, along with the resulting conflict and
exemplary resolution mechanism to resolve it. These examples underscore the
need for agent-level negotiation capabilities. Even more importantly, it is
crucial that the conflict resolution framework and the outcomes possess
human-readability and traceability, goals that align closely with the strengths
of LLMs, which will be discussed in Sect. 5.

**_Table 2_** Examples of intent conflicts across B5G functionalities and the corresponding resolution
approaches


**4.3 Distributed Decision-Making with LLM-Guided Agents**
In B5G environments, centralized control planes are increasingly being
replaced with distributed agent-based decision-making frameworks. These
agents, deployed as xApps/rApps in the RAN Intelligent Controller (RIC),
slice-specific service orchestrators, or edge-based optimization modules,
usually operate independently and are often responsible for local policy
enforcement, optimization, and adaptation. However, local decisions can lead
to suboptimal or conflicting system-level behaviors, if not coordinated.
To address this, LLMs can act as meta-level policy interpreters or advisory
agents embedded within or together with distributed components. There are


several ways LLMs can support distributed decision-making, which is
discussed in further detail next.

**_4.3.1 LLMs as Intent Parsers for Edge Agents_**
Each edge agent can be given a high-level natural language intent, which is
translated locally using a RAG-reinforced or prompt-engineered LLM into
domain-specific actions or constraints. For example, an LLM can translate
“prioritize ultra-low latency for connected vehicles” into a specific base
station (BS) configuration or scheduling parameter at the medium access
control (MAC) layer. This allows each agent to act autonomously within its
scope but remain aligned with network-level goals.

**_4.3.2 Interagent Communication via LLM-Normalized Semantics_**

When agents need to coordinate (e.g., a RAN slicing manager negotiating with
a transport orchestrator), they often do not speak the same policy or
optimization language. By encoding agent outputs into a shared semantic
representation, LLMs can normalize these messages, similar to interlingua
approaches in machine translation, allowing negotiation or coordination
between heterogeneous agents [ 20 ].

**_4.3.3 Conflict Detection with LLM Feedback Loops_**

LLMs can be used to scan policies generated by local agents and detect
semantic contradictions or trade-offs based on natural language summaries.
This is especially useful when combining multiple intents or optimization
objectives across layers.

**Example Scenario** If Agent A (in transport) intends to reroute traffic to
minimize delay, but Agent B (at RAN) initiates power saving that reduces
capacity on that path, an LLM-based summarizer can flag the mismatch and
recommend harmonized behavior (e.g., delay-aware energy savings). This
mechanism also operates harmoniously with interagent communication
framework described in Sect. 4.3.2.

**_4.3.4 Hierarchical LLM Coordination_**
LLMs may be deployed hierarchically: lightweight models at the agent level
for fast intent parsing and more powerful models at a higher orchestration
layer for conflict resolution and global reasoning.


**4.4 Negotiation, Conflict Resolution, and Consensus Protocols via**

**LLMs**

Traditional conflict resolution approaches in networking rely on rule-based
resolution, priority-based SLA enforcement, or weighted optimization.
Although useful in certain scenarios, these mechanisms are in general rigid
and non-explainable. In contrast, LLMs pave the way to dynamic and
interpretable negotiation frameworks among autonomous agents in B5G. Next,
we discuss the details of several approaches where LLMs are used for
handling conflicts and conducting arbitration procedures among them.

**_4.4.1 LLM-Based Intent Merging and Conflict Detection_**

When multiple intents conflict, LLMs can semantically merge overlapping
goals and highlight contradictions. For instance, using techniques such as
natural language inference (NLI) [ 21 ] or contrastive explanation [ 22 ], LLMs
can evaluate questions like: “Do this set of intents imply contradictory
requirements?” or “can one be satisfied partially without fully violating the
others?” This inference can be further supported with RAG to incorporate
vendor or service-specific policy constraints.

**_4.4.2 LLMs in Voting-Based Multi-Agent Arbitration_**

In multi-agent networks, each agent may rank or propose candidate policies
based on its localized view, possibly using LLM-based policy descriptors or
translators. LLMs can function as arbitration modules that can collect
proposals and rationales from agents, identify overlaps/conflicts semantically,
and recommend a consensus policy that reflects the most compatible subset of
proposals. This behavior is analogous to social choice theory methods in AI
planning [ 23 ], but with natural language reasoning used to evaluate
compatibility.

**_4.4.3 LLMs as Soft Consensus Engines_**

Rather than enforcing hard consensus (e.g., all-or-nothing agreement), LLMs
allow soft consensus, where the outcome is a compromise, e.g., delaying
latency-sensitive intents by 10% to allow for fairness constraints [ 20 ].

**_4.4.4 Human-Readable Justifications_**

Unlike hard-coded conflict resolution logic, LLMs can generate natural
language justifications for their recommended resolution, such as the
following: “Intent A’s requirement for minimum power consumption conflicts


with Intent B’s full accessibility objective. To balance both, it is recommended
to enable sleep mode on underutilized sectors only when user density is below
threshold _T_ .” Such outputs can be used for (i) generating a meta-intent to
merge multiple conflicting intents or (ii) operator oversight and auditing.

**5 Explainable AI for Trustworthy Orchestration**

As autonomous decision-making becomes embedded in Beyond 5G (B5G),
network operators need to design transparent and trustworthy orchestration
workflows. Explainable AI (xAI) techniques offer interpretable reasoning
mechanisms for AI-driven orchestration decisions, enabling network operators,
auditors, and even regulatory systems to understand and validate network
behavior. In this section, we discuss examples of xAI techniques that can be
embedded into network orchestration workflows and how they can be used to
improve transparency and trustworthiness.

**5.1 Integration of SHAP, LIME, and Counterfactual Reasoning**

Model-agnostic explanation tools, such as SHAP (SHapley Additive
exPlanations) [ 17 ] and LIME (Local Interpretable Model-Agnostic
Explanations) [ 18 ], can be used for explaining and reasoning black-box tools
in the orchestration frameworks. SHAP assigns an importance score to each
input feature by computing Shapley values from game theory [ 17 ], providing
global and local explanations for decisions like scheduling or handovers. On
the other hand, LIME perturbs the input space locally and trains interpretable
linear models to explain predictions. Both methods can be applied (i) after a
decision is made, e.g., to explain why a particular handover trigger is initiated
due to low SINR or high load, or (ii) before a decision is made by the network
operator to enable a reinforced and more intelligent decision. Another
framework used for xAI is counterfactual reasoning, which provides
contrastive explanations [ 19 ], e.g., “if the BS load had been 15% lower, the
handover wouldn’t be triggered.” These methods can help operators
understand the decision patterns in the orchestration framework, as well as the
thresholds and boundaries for certain decisions.

**5.2 Generating Human-Readable Rationales and Policy**

**Justifications**
Beyond technical explanation formats, LLMs can generate practical natural
language reasoning from structured logs and decision data. For example, after


an LLM-based policy agent selects a load-balancing configuration, it can auto-
generate explanations such as “Policy X was chosen due to network load
observed on BS-x (load = 80%). The adopted configuration reduces predicted
latency by 20% based on historical data.” This involves either a template-
driven or a prompt-based reasoning over orchestration logs. Combining xAI
tools with domain-specific prompt engineering allows the system to output
decisions and justifications in human-readable language. Moreover, with
human-readable rationales, auditors can also use xAI summaries to verify
policy compliance and SLA fulfillment, supporting accountability in
autonomous orchestration.

**5.3 xAI in Human-in-the-Loop Versus Closed-Loop Automation**

Two main categories for network automation are semiautomated, namely
_human-in-the-loop_ and zero-touch automation [ 24 , 25 ]. In the human-in-the-
loop settings, xAI tools guide human feedback loops. For instance, when a
network operator is alerted about an LLM-proposed network action, SHAP or
LIME can highlight the dominant decision drivers. The operator can approve,
reject, or adjust the action.
In zero-touch, or closed-loop, automation, xAI modules can act as fail-
safes or confidence calibrators. For example, the system may require a
minimum explanation confidence threshold or feature attribution stability
before executing a critical intent, reducing the risk of erratic actions by black-
box models.

**6 Implementation Framework and System**

**Architecture**

Deploying LLM-powered, explainable IBN in B5G requires a modular
architecture that can interface with existing orchestration systems and operate
across multilayer network environments. This section presents an
implementation framework detailing a sample system architecture, integration
strategies with platforms like the Open Network Automation Platform (ONAP)
[ 26 ] and ETSI Open-Source MANO (OSM) [ 27 ], and key considerations for
privacy, security, and explainability.

**6.1 IBN Engine, LLM Layer, Conflict Resolver, xAI Module**
The proposed system architecture integrates modular components that
collectively realize IBN using LLMs and xAI. Figure 2 illustrates a reference


architecture composed of the following core modules:

```
IBN Engine: This accepts high-level intents from network operators or
service-level agreements (SLAs), parsing them into intermediate
representations for downstream processing. These intents may include goals
like “maximize video QoE for premium users” or “minimize energy
consumption during low traffic periods.”
LLM Layer: It serves as the natural language processing backbone,
translating intents into structured network policies or optimization
objectives. This layer can be fine-tuned on network-specific processes or
domain-specific terminologies to improve accuracy and alignment with
available APIs.
Conflict Resolver: Since multiple intents may be active simultaneously,
this module resolves conflicts through techniques, including but not limited
to multi-objective or AI-based optimization, utility-based prioritization, or
policy arbitration. It considers current network state, historical outcomes,
and predefined SLAs.
xAI Module: It provides explanations for policy decisions by employing
SHAP, LIME, or counterfactual reasoning methods. This module is
responsible for generating human-interpretable justifications for operator
actions, including why certain intents were prioritized or modified.
```
This layered design enables modular upgrades, transparency, and adaptability
to different operator environments and orchestration platforms.


**_Fig. 2_** Reference architecture integrating IBN Engine, LLM Layer, Conflict Resolver, and xAI Module
for intent-based networking

**6.2 Integration with Existing Orchestration Platforms**
The proposed architecture is designed to be interoperable with industry-
standard orchestration systems such as ETSI Open-Source MANO (OSM) and
the Open Network Automation Platform (ONAP). Integration is facilitated
through RESTful APIs or message bus interfaces (e.g., Kafka or gRPC),
enabling bidirectional communication between the LLM-IBN system and
orchestration layers [ 28 , 29 ].
One key architectural enabler for this integration is the O-RAN
framework. The O-RAN Alliance defines an open, disaggregated RAN


architecture composed of Centralized Units (CUs), Distributed Units (DUs),
Radio Units (RUs), and a Service Management and Orchestration (SMO)
platform. Data can be collected from multiple O-RAN layers across different
time scales, real-time (RAN Intelligent Controller (RIC)), near real time, and
non-real time and fed into AI pipelines to enable context-aware and timely
intent translation [ 30 ].
In parallel, **network digital twins** can be employed to simulate network
behavior under alternative policies generated by the LLM layer. This allows
the system to test and refine translations before committing to real-world
changes, significantly reducing the risks of service degradation. Such digital
twins can also be used for operator training, SLA verification, and robustness
analysis under failure scenarios [ 31 ].

**6.3 Privacy, Security, and Explainability Considerations**

The integration of LLMs into network orchestration introduces several
security and privacy challenges, particularly in terms of intent interpretation,
data provenance, and model hallucination. We elaborate on these issues in the
next part:

```
Privacy: Intent translation often requires sensitive contextual data (e.g.,
user QoS profiles, location, network KPIs). Privacy-preserving techniques
such as differential privacy or federated learning can be incorporated to
prevent exposure of raw data while still allowing model inference [ 32 , 33 ].
Security: Malicious or ambiguous intent statements could potentially cause
service degradation. Security policies should enforce strict input validation,
permission-based access control, and robust audit logging. Integration with
policy verification engines or network digital twin models can prevent the
deployment of unsafe actions in the physical network.
Explainability: To maintain human oversight and regulatory compliance,
all LLM-generated outputs should be accompanied by xAI-derived
justifications. For example, the system could output “The policy to reassign
spectrum slices was chosen to reduce latency for service class X, as this
class exceeded the 95th percentile delay threshold in the last 10 minutes.”
Tools like SHAP and counterfactual reasoning are particularly suited for
such time-sensitive rationales.
```
**7 Conclusion**


The convergence of Large Language Models (LLMs), Explainable AI (xAI),
and intent-based networking (IBN) presents a transformative opportunity for
B5G network orchestration. By enabling intuitive, human-centric interaction
with complex network systems and ensuring transparency in automated
decision-making, these technologies enable more adaptive, trustworthy, and
efficient network management. In this chapter, we proposed a system
architecture that combines LLMs, conflict resolution mechanisms, and xAI
modules and discussed practical integration with existing orchestration
platforms. We also outlined explainability tools such as SHAP, LIME, and
counterfactual reasoning to support auditing and operator trust. As networks
become more autonomous and multi-domain, the role of explainable, intent-
driven control will be critical for scalable, reliable operations across future
communication infrastructures.

**References**

1. M.E. Morocho-Cayamcela, H. Lee, W. Lim, Machine learning for 5G/B5G mobile and wireless
    communications: potential, limitations, and future directions. IEEE Access **7** , 137184–137206
    (2019)
    [Crossref]
2.
    I. Gillott, AI-native networks and the Implications for 6G, in _Wireless Infrastructure Association_
    _(WIA)_ (2024), wIA white paper/blog post
3.
    A. Celik, A.M. Eltawil, At the dawn of generative AI era: a tutorial-cum-survey on new frontiers in
    6G wireless intelligence. IEEE Open J. Commun. Soc. **5** , 2433–2489 (2024)
    [Crossref]
4.
    S. Tarkoma, R. Morabito, J. Sauvola, AI-native interconnect framework for integration of large
    language model technologies in 6G systems. arXiv preprint arXiv:2311.05842 (2023)
5.
    W. Wu, C. Zhou, M. Li, H. Wu, H. Zhou, N. Zhang, X.S. Shen, W. Zhuang, AI-native network
    slicing for 6G networks. IEEE Wireless Commun. **29** (1), 96–103 (2022)
    [Crossref]
6.
    V. Petrov, M.A. Lema, M. Gapeyenko, K. Antonakoglou, D. Moltchanov, F. Sardis, A. Samuylov, S.
    Andreev, Y. Koucheryavy, M. Dohler, Achieving end-to-end reliability of mission-critical traffic in
    softwarized 5G networks. IEEE J. Sel. Areas Commun. **36** (3), 485–501 (2018)
    [Crossref]
7.
    J. Wen, J. Nie, Y. Zhong, C. Yi, X. Li, J. Jin, Y. Zhang, D. Niyato, Diffusion-model-based incentive
    mechanism with prospect theory for edge AIGC services in 6G IoT. IEEE Internet Things J. **11** (21),
    34187–34201 (2024)
    [Crossref]
8.
    L. Yang, Y. Li, S.X. Yang, Y. Lu, T. Guo, K. Yu, Generative adversarial learning for intelligent trust
    management in 6G wireless networks. IEEE Netw. **36** (4), 134–140 (2022)


[Crossref]
9.
J. Du, T. Lin, C. Jiang, Q. Yang, C.F. Bader, Z. Han, Distributed foundation models for multi-modal
learning in 6G wireless networks. IEEE Wireless Commun. **31** (3), 20–30 (2024)
[Crossref]
10.
A. Roessler, The promise and challenges of AI-native 6G networks, in _Mobile Insights/Mobile World
Live (partner content)_ (2025)
11.
J. Ordonez-Lucena, P. Ameigeiras, D. Lopez, J.J. Ramos-Munoz, J. Lorca, J. Folgueira, Network
slicing for 5G with SDN/NFV: concepts, architectures, and challenges. IEEE Commun. Mag. **55** (5),
80–87 (2017)
[Crossref]
12.
A. Leivadeas, M. Falkner, A survey on intent-based networking. IEEE Commun. Surv. Tutorials
**25** (1), 625–655 (2023)
[Crossref]
13.
D.M. Manias, A. Chouman, A. Shami, Towards intent-based network management: large language
models for intent extraction in 5G core networks, in _2024 20th International Conference on the
Design of Reliable Communication Networks (DRCN)_ (2024), pp. 1–6
14.
J. Wei, X. Wang, D. Schuurmans, M. Bosma, b. ichter, F. Xia, E. Chi, Q.V. Le, D. Zhou, Chain-of-
thought prompting elicits reasoning in large language models, in _Advances in Neural Information
Processing Systems_ , ed. by S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, A. Oh, vol.
35 (Curran Associates, Inc., New York, 2022), pp. 24824–24837
15.
Y. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma, D. Drain, S. Fort, D. Ganguli, T.
Henighan, N. Joseph, S. Kadavath, J. Kernion, T. Conerly, S. El-Showk, N. Elhage, Z. Hatfield-
Dodds, D. Hernandez, T. Hume, S. Johnston, S. Kravec, L. Lovitt, N. Nanda, C. Olsson, D. Amodei,
T. Brown, J. Clark, S. McCandlish, C. Olah, B. Mann, J. Kaplan, Training a Helpful and Harmless
Assistant with Reinforcement Learning from Human Feedback (2022). https:// arxiv. org/ abs/ 2204.
05862
16.
T. Li, C. Yang, Y. Wang, L. Cai, A. Anpalagan, Z. Han, A survey on network management for
XANet: evolution, challenges, future directions. IEEE Commun. Surv. Tutorials, 1–1 (2025)
17.
S.M. Lundberg, S.-I. Lee, A unified approach to interpreting model predictions, in _Advances in
Neural Information Processing Systems_ , vol. 30 (2017)
18.
M.T. Ribeiro, S. Singh, C. Guestrin, “Why should I trust you?” Explaining the predictions of any
classifier, in _Proceedings of the 22nd ACM SIGKDD_ (2016), pp. 1135–1 144
19.
I. Stepin, J.M. Alonso, A. Catala, and M. Pereira-Fariña, A survey of contrastive and counterfactual
explanation generation methods for explainable artificial intelligence. IEEE Access **9** , 11974–12001
(2021)
[Crossref]
20.
I. Chatzistefanidis, A. Leone, N. Nikaein, Maestro: LLM-driven collaborative automation of intent-
based 6G networks. IEEE Networking Lett. **6** (4), 227–231 (2024)
[Crossref]
21.
O.-M. Camburu, T. Rocktäschel, T. Lukasiewicz, P. Blunsom, e-SNLI: natural language inference
with natural language explanations, in _Advances in Neural Information Processing Systems_ , ed. by


S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, R. Garnett, vol. 31 (Curran
Associates, Inc., New York, 2018)
22.
A. Jacovi, S. Swayamdipta, S. Ravfogel, Y. Elazar, Y. Choi, Y. Goldberg, “Contrastive Explanations
for Model Interpretability”, in _Proceedings of the 2021 Conference on Empirical Methods in Natural
Language Processing_ , ed. by M.-F. Moens, X. Huang, L. Specia, S.W.-t. Yih. Online and Punta
Cana, Dominican Republic: Association for Computational Linguistics (2021), pp. 1597–161 1
23.
S.D. Baum, Social choice ethics in artificial intelligence. AI Soc. **35** (1), 165–176 (2020)
[Crossref]
24.
C. Benzaid, T. Taleb, AI-driven zero touch network and service management in 5G and beyond:
challenges and research directions. IEEE Netw. **34** (2), 186–194 (2020)
[Crossref]
25.
E. Coronado, R. Behravesh, T. Subramanya, A. Fernàndez-Fernàndez, M.S. Siddiqui, X. Costa-
Pérez, R. Riggio, Zero touch management: a survey of network automation solutions for 5G and 6G
networks. IEEE Commun. Surv. Tutorials **24** (4), 2535–2578 (2022)
[Crossref]
26.
ONAP Technical Working Group, ONAP Case Solution Architecture, in _Open Network Automation
Platform (ONAP), Whitepaper_ (2019), version dated June 25, 2019
27.
ETSI OSM Working Group, OSM White Paper—Technical Content Release Five, in _ETSI Open
Source MANO (OSM), Whitepaper_ (2021), final release
28.
G. Mayer, RESTful APIs for the 5G service based architecture. J. ICT Standardization **6** (1–2), 101–
116 (2018)
[Crossref]
29.
A.-S. Charismiadis, J.M. Salcines, D. Tsolkas, D.A. Guillen, J.G. Rodrigo, The 3GPP common API
framework: open-source release and application use cases, in _2023 Joint European Conference on
Networks and Communications & 6G Summit (EuCNC/6G Summit)_ (2023), pp. 472–477
30.
S.-P. Yeh, S. Bhattacharya, R. Sharma, H. Moustafa, Deep learning for intelligent and automated
network slicing in 5G open RAN (ORAN) deployment. IEEE Open J. Commun. Soc. **5** , 64–70
(2024)
[Crossref]
31.
D. Wang, R. Su, S. Zhang, Y. Xing, Trends and challenges of policy verification for intent-based
networking towards 6G, in _2022 IEEE/CIC International Conference on Communications in China
(ICCC Workshops)_ (2022), pp. 134–139
32.
X. Yin, Y. Zhu, J. Hu, A comprehensive survey of privacy-preserving federated learning: a
taxonomy, review, and future directions. ACM Comput. Surv. **54** (6), 131:1–131:36 (2021)
33.
S. Truex, N. Baracaldo, A. Anwar, T. Steinke, H. Ludwig, R. Zhang, Y. Zhou, A hybrid approach to
privacy-preserving federated learning, in _Proceedings of the 22nd ACM Conference on Computer
and Communications Security (CCS)_ (2019), pp. 1175–1 191
_OceanofPDF.com_


(1)

(2)

(3)

© The Author(s), under exclusive license to Springer Nature Switzerland AG 2026
S. Singh et al. (eds.), _GenAI and LLMs for Beyond 5G Networks_
https://doi.org/10.1007/978-3-032-06418-9_13

##### A Framework for Intent-Driven

##### Kubernetes Configuration Generation

##### Using Large Language Models

Anukul Parajuli^1 , Krishna M. Sivalingam^1 and

Madhan Raj Kanagarathinam^2 ,^3
Department of Computer Science and Engineering, Indian Institute of
Technology Madras, Chennai, India
Department of Computer Science and Engineering, Indian Institute of
Technology Madras, Chennai, India
MX Department, Samsung Electronics, Suwon, South Korea

```
Krishna M. Sivalingam (Corr esponding author)
Email: skrishnam@cse.iitm.ac.in
```
```
Madhan Raj Kanagarathinam
Email: madhan.raj@samsung.com
```
**Abstract**
Intent-based networking (IBN) enables network operators to define high-
level objectives that determine the network’s behavior and functionality
without requiring the specification of procedures to achieve them. While the
abstraction simplifies the process of expressing the operator’s intent,
translating these goals into syntactically accurate and executable
configurations remains a challenging task, especially with the growing
complexity of the new generation of networks. In this chapter, we introduce
a framework utilizing LLMs to automate the generation of Kubernetes
YAML configuration files from natural language prompts. The framework
enables precise conversion of operator intents into YAML configurations,


bridging the gap between human-defined requirements and machine-
executable configurations. Fundamentally, this approach uses a Graph
Neural Network (GNN)-based ranking algorithm designed to annotate
unlabeled YAML files, which, combined with fine-tuning techniques,
empowers LLMs to interpret operator goals and produce syntactically and
semantically correct YAML configurations. This resulting framework
minimizes potential errors while streamlining the configuration generation
process. Experimental results demonstrate that the framework achieves high
accuracy in terms of relevance ranking (72%) and efficiently generates
YAML configurations. These outcomes demonstrate how the system can be
utilized to improve operational progress and scale IBN management
systems for future networks.

**Keywords** Large Language Model (LLM) – Intent Based Networking
(IBN) – Graph Neural Network (GNN) – K8s (Kubernetes) – YAML –
Machine Learning (ML)

**1 Introduction**

Network operators are generally tasked with implementing and configuring
updates on highly heterogeneous networks, which necessitates the
development of tools allowing them to tune the parameters of the network
without complete knowledge of the underlying infrastructure. The IBN
model has enabled operators to specify their desired outcomes over
complex networks without detailing the procedural steps required to
achieve the result [ 1 ]. An intent can be defined as a high-level abstraction of
requirements for network services. This can include configuration for
applications that are deployed on physical systems or virtually, for example,
Intrusion Detection Systems (IDS), firewalls, etc. However, translating
intents into accurate, executable configuration that meets the needs of
operators remains complex and error-prone, especially with growing
network complexities [ 2 ]. This has paved the way for developing IBN
applications based on Artificial Intelligence (AI) and Machine Learning
(ML), enabling informed decisions and incorporating generative
capabilities (Fig. 1 ).


**_Fig. 1_** Interaction of major components in an IBN system [ 2 ]
Intent-based networking (IBN) plays a crucial role in automating
networks by allowing operators to express abstract operational goals, also
known as “intents,” which are then converted into detailed network
configurations through processes such as intent understanding and
translation [ 3 ]. Traditionally, IBN solutions have relied on structured
templates like JSON for expressing operators’ intents. However,
transitioning toward intents expressed in natural language promises greater
flexibility and is highly adaptable. Despite the idea, precisely converting


natural language into syntactically correct configurations that align with the
intended goals of operators remains a significant challenge [ 4 ]. The other
major challenge is to build systems to validate if the generated
configuration meets the specified demands and the intent does not drift
from its original purpose [ 5 ].
As networks continue to develop further, technologies like Software-
Defined Networking (SDN) and Network Function Virtualization (NFV)
have revolutionized how network functions operate and communicate [ 6 ].
These transformations support the automation of repetitive tasks, allowing
operators to focus on more complex network challenges [ 7 ]. In this
scenario, the integration of AI models for network scenario assessment and
optimized configuration generation based on traffic is highly desirable for
future networks. The IBN systems can be combined with 6G core networks
for automating management tasks by leveraging the Network Data
Analytics Function (NWDAF) [ 8 ] proposed by 3GPP. The combination
would allow 6G networks to build analytics based on the data generated
from multiple NFs across the infrastructure, and IBN would allow for
automating tasks based on the analytics [ 9 ]. Large Language Models
(LLMs), which have become the cornerstone of modern Natural Language
Processing (NLP) and the driving force behind Deep Learning (DL) and
ML applications, can greatly assist by interpreting network requirements in
human-readable language and generating appropriate configurations as
desired by a network operator [ 10 ].
With the proposed advancements in 6G and newer generation of
networks, there has been a dramatic increase in the use of virtualization for
network management. Today’s networks generally use virtual machines
(VMs) to run network applications on top of physical infrastructures, but
container-based management systems for virtualization are gaining
popularity because of their scalability and flexibility. These systems run
network applications in microservices architecture, making the
orchestration of the applications flexible. One of the most popular open-
source container orchestration platforms currently used is Kubernetes
(K8s). It is designed to automate processes such as deployment, scaling,
and management of containerized applications [ 11 ]. K8s helps manage
complex distributed systems by abstracting the physical infrastructure and
making it easier to run applications on the cloud and across various
environments.


In K8s, YAML files are used to define the desired states of the systems,
including deployments, services, secrets, pods, ConfigMaps, and many
other objects. Each resource has its own fields that need to be added
according to the K8s API specifications. These files serve as _“records of
intent,”_ where K8s constantly ensures that the systems are aligned with the
specified configuration [ 12 ]. The operators must frequently interact and
update configuration files to change network policies, service definitions,
and controllers or to define custom load balancers and DNS configurations.
Operators use traffic data, KPIs, and metrics specific to the system to
update the configuration files. However, manually updating these files is
not only time-consuming but also prone to errors and requires specialized
K8s knowledge, which makes automating this process an appealing
solution. Generative AI models, particularly LLMs, are well-suited to this
type of automation. They are well-suited to take operator demands as
natural language prompts and convert them into configuration files that
adhere to specific demands or formats. LLMs have been explored for
understanding and deploying network intents, laying the groundwork for
tasks such as policy generation and intent translation [ 13 ]. The major
challenge in generating configuration files lies in accurately interpreting
network requirements expressed in natural language and translating them
into valid executable configurations. With the advancements in deep
learning tools, LLMs can be fine-tuned for specific purposes, in this case, to
specifically generate configuration files based on prompts.
Adding Artificial Intelligence into the network paradigm has enhanced
the opportunity for wider research in Intent-based networking systems.
Automating tasks in the core network using generative Machine Learning
models is a natural fit for understanding and translating instructions
provided by the network operators for intent-based networking. So, the use
of LLMs for NLP tasks, such as understanding the natural language and
generating output based on the requirements, has gained a lot of popularity
recently, making the problem of Intent Classification and understanding
feasible. The works in [ 7 , 14 – 17 ] have studied the use of LLM for intent
understanding, translation to standard requirements, and intent deployment
by policy generation. In a similar spirit, we want to build a system that
allows network operators to express their desire for the network state in a
human language, and the system using LLM changes this into a structured


standard (YAML for K8s systems) that can be directly executed into the
network to achieve the desired intent of the operator.
In this chapter, based on deep learning techniques combined with the
generative capabilities of LLMs, a method for automating the generation of
YAML files based on operator prompts is demonstrated. Firstly, a GNN-
based ranking algorithm assesses the relevance of terms in a large unlabeled
dataset containing YAML configuration files by identifying the structural
relationships between the terms in the files. This algorithm identifies key
terms that contribute to generating metadata, which enhances the LLM’s
ability to interpret and produce accurate configurations. Then, a metadata
generation pipeline is constructed to label unannotated YAML files,
enabling a richer dataset for fine-tuning and improving the LLM’s
performance in producing context-specific configurations. Finally, a robust
LLM is fine-tuned to generate K8s YAML configuration files in response to
prompts expressed in natural language. This allows the model to translate
intents into executable YAML configurations, addressing the gap between
high-level requirements and accurate configurations.
The rest of the chapter is structured as follows: Sect. 2 presents the
motivation behind pursuing the work. Section 3 presents the related
background on IBN, LLMs, and GNNs for metadata generation. Section 4
describes the implementation of our system for extracting relevant
information from the configurations and using it to generate prompts for
unlabeled data, along with using a larger LLM to generate YAML
configurations based on user prompts. Section 5 discusses the results and
observations based on our experiments. Section 7 concludes the chapter
with possible future works.

**2 Motivation**

Traditional IBN systems make use of predefined templates or formats, such
as drop-down menus, that can be used to express the intent of the operator.
These intents are then converted into network policies, which in turn are
converted to low-level network configurations that can be used to configure
systems on the network. The intents are updated continuously based on the
requirements, and along with them, the policies get updated. The process
involves translating the intent, intent conflict resolution, intent activation,


and finally ensuring that the intent is behaving as desired by the operators
without overusing resources allocated for other intents [ 18 ].
Manual update of such configurations can be highly prone to errors
since the process involves multiple stages before an intent can be
configured into the system. Therefore, a robust system that translates an
intent expressed in natural language to a low-level configuration that can be
used to configure network devices is highly desirable. The growing
adoption of K8s for managing virtual applications has created opportunities
to deploy and orchestrate multipurpose network applications. As K8s has
become the standard for managing container-based applications, automated
and accessible ways to interact with it are also growing. The main
challenge, however, is enabling human operators to specify how they want
the system to behave without requiring deep knowledge of how K8s
manages resources and enforces constraints.
The ability to automate the process of translating the intent into usable
YAML files for K8s would accelerate the deployment process and reduce
the chance of making errors. However, working out the solution to this
problem is not trivial because it requires advanced Natural Language
Processing techniques, adaptive configuration generation, and validation
systems that are specific to K8s. This is where LLMs have begun to play a
crucial role with their ability to understand and generate context-specific
YAML files based on textual prompts. However, challenges remain in
making sure that the files that are generated are semantically correct and
secure [ 19 ]. This study focuses on how deep learning techniques such as
Graph Neural Networks (GNNs) can be utilized to extract information
(metadata) from unlabeled YAML files, which are then used to train LLMs
to generate user-desired configurations for K8s-specific use cases by
providing prompts that describe the intent of the operator.

**3 Preliminaries**

**3.1 IBN Systems and Natural Language Processing**

Until now, a great number of studies have been done in the field of intent-
based networking. The work in [ 20 ] talks about how IBN and Intent-Driven
Networks (IDNs) have been used extensively in the industry. Recent efforts
have shown an increased usage of ML/AI techniques and control-based
systems for generating policy-based models and formalizing intents for IBN


systems. Most of the existing IBN systems allow users to interact with a
GUI-based tool to specify their intents. These tools are highly flexible but
might still lack possible options that the user might desire to express their
intents. NLP algorithms are used to enhance this functionality by adapting
to different scenarios and translating intents accordingly based on the data.
NLP is used for various tasks in IBN systems, including Named Entity
Recognition (NER), which identifies names and locations, Keyword
Extraction, which allows identifying the most important keywords that can
be used to describe a text, and Relationship Extraction, which defines
statistical and semantic relationships between entities in the text [ 2 ].
Multiple unsupervised algorithms, such as clustering algorithms, TF-
IDF analysis, latent semantic analysis using singular value decomposition,
etc., followed by supervised learning algorithms such as support vector
machines and neural networks, can be used to identify relationships
between entities and keyword extraction in IBN systems. Recently, the use
of LLMs for intent translation is gaining significant traction due to their
flexibility in understanding context-specific instructions expressed in
natural language [ 21 ].

**3.2 Kubernetes and YAML Configuration Files**

Kubernetes is an open-source container orchestrator that allows the
automation, deployment, scaling, and management of containerized
applications. K8s consists of objects, which are entities that define the state
of a cluster. These objects are used to describe the applications running on
the machines, the resources allocated to them, and the policies that need to
be followed for security and fault tolerance. The most commonly used
objects in K8s are deployments, pods, StatefulSets, services, ConfigMaps,
secrets, etc. Once an object is created, the K8s controller continuously
monitors the state of the system to ensure that the desired state is met. For
example, if the user has specified to run two instances of an application in a
deployment file and in the event that one of the instances fails, the
controller reads the deployment configuration to make sure that a new
instance of the application is instantiated [ 11 ].


Listing 1: An example of a basic YAML config for K8s pod running an
apache server
An object in K8s is a declarative record of intent, meaning the
requirements of the operator for what applications to run, what resources to
allocate, and how the applications are supposed to behave are expressed
using YAML or JSON configuration files. In case of changing requirements
for the applications, these configurations might need to be updated
manually. There are several tools, like Helm charts [ 22 ], to automate the
process of generating YAML files based on changing values and to keep
track of the changes. Despite its benefits, tools like Helm do not solve the
problem of initial configuration, which can be highly complex, error-prone,
and time-consuming.

**3.3 Fine-Tuning Large Language Models (LLMs) for Text**

**Generation**

Large Language Models (LLMs) have seen a wide adoption across all fields
of science and technology in recent years due to their highly desirable
natural language understanding and text generation capabilities. NLP-based
deep learning models are central to interpreting and translating intents into
actionable network policies and configurations. Recent studies use models
like GPT by OpenAI [ 23 ], Gemini [ 24 ] by Google, and LlaMa [ 25 ] by Meta
which are some of the extremely powerful language models being used for
tasks such as classification, generation, and summarization. All these
models are based on the transformer neural network architecture that uses
the self-attention mechanism for learning long-range dependencies in text
[ 26 ]. These models are trained on a very large part of Internet data and are
capable of learning new tasks with very few examples. The process of


teaching these LLMs to perform a specific task is called fine-tuning. LLMs
are powerful enough to learn even from a very small set of data.
Fine-tuning an LLM involves training a pretrained model on a specific
dataset to improve its performance and adapt it to a particular style of text
generation. For YAML configuration generation based on input prompts, a
large corpus of YAML files, including their metadata, is required for fine-
tuning the LLMs. In the absence of metadata, LLMs cannot learn the
context about the training data, leading to suboptimal text generation for a
specific purpose. The model requires at least a high-level description of the
dataset that it is being trained on to identify the context and effectively
correlate the metadata with the content, enabling a better generalization
during fine-tuning [ 27 ]. Real-world datasets are generally unlabeled, and
they require quite a bit of manpower with complete domain knowledge to
annotate the textual content for LLM fine-tuning. A combination of
classical NLP-based deep learning techniques, combined with smaller-sized
LLMs, can be utilized to automate the process of annotating unlabeled text
files. This technique is utilized as discussed in the subsequent sections to
come up with a pipeline to annotate a large dataset of YAML files. The
combination of the metadata and the YAML files is then used to fine-tune
an LLM for generating YAML configuration based on input prompts.

**3.4 Graph Neural Networks (GNNs) for Relevance Ranking**

It is a labor-intensive task to annotate a large number of YAML files for
fine-tuning an LLM. Therefore, it is a necessity to develop deep learning-
based techniques to automate this process and make it cost-effective.
YAML configuration files are expressed in a hierarchical data format and
uses key-value pairs to represent mappings and numeric values. This
structure can be mapped to a tree data structure where each key is a node
and the values represent the children nodes. Since the same entities in a
YAML configuration can repeat in different contexts, the structure can
extend beyond a tree and form a graph-like relation between the nodes. This
structure allows deep learning methods like GNNs to learn the relationship
between the nodes and identify relevant parts of the text that describe the
file [ 28 ].
GNNs are widely used for tasks involving graphical structures such as
node classification, link prediction, graph generation, etc. If the nodes of the
graph are represented as feature vectors, GNNs learn to weigh the


importance of neighboring nodes based on a linear combination of the
features. GNNs work with multiple layers stacked on top of each other, and
each layer is designed to learn specific relationships between the nodes
based on neighbor feature aggregation. The aggregated feature vector at the
last layer can then be used for downstream tasks such as relevance ranking
of the terms [ 29 ]. For relevance ranking, if each term (node in the graph) is
represented as a feature vector and the YAML file is represented as a graph
with multiple nodes, the GNN after training can learn the relationship
between these nodes, which can then be used to classify the terms based on
their importance. This classification enables us to choose only the relevant
terms that best describe the file. The most relevant terms can then be used
as metadata for the files to fine-tune LLMs for generating configuration
files.

**4 System Design and Architecture**

This section addresses the proposed solution with key steps: YAML file
preprocessing, metadata generation, and YAML file generation using an
LLM. To generate YAML files for Kubernetes, we first fine-tune LLMs
using a large dataset of diverse YAML configurations. Since Kubernetes
YAML files often lack metadata, we automate metadata generation to
describe file usage and context, making the process feasible at scale.

**4.1 Metadata Generation for Unlabeled YAML Files**

The major problem with annotating the YAML files is the lack of domain
knowledge. The dataset contains thousands of files that need annotation,
which is nearly impossible for domain experts to complete within an
acceptable time frame. To generate YAML files for Kubernetes, we fine-
tune LLMs using a large dataset of diverse YAML configuration files. We
employ classical and deep learning-based NLP techniques to label
unannotated YAML files. As shown in Fig. 2 , the process involves
generating metadata for the YAML files using graph neural networks to
classify important terms in the YAML structure and then using the terms to
generate a prompt (description of the YAML file) using GPT-2. This step is
followed by fine-tuning a more powerful LLM to generate YAML
configuration based on user requirements.


**_Fig. 2_** Architecture for configuration generation using LLMs and graph neural network

**_4.1.1 Preprocessing of the YAML Files_**

The first step toward ranking the importance of terms in a structured file
like YAML is to preprocess and remove all the irrelevant text, such as
special characters and numerical values. The terms in the YAML files
generally combine English words or words specific to cloud and container
management platforms. The YAML files are cleaned using regular
expressions (regex) according to the requirement to retain only the word
contents in the files. The numerical values in the files do not convey much
information about the context in which the files are being used; therefore,
they are removed. Some special words, like k8s, v1, etc., are native to
Kubernetes and lose meaning if numbers are removed from the word. This
is taken care of using the same regex.

**_4.1.2 TF-IDF Scoring and Class Allocation_**

We use techniques from information theory to score each term in the dataset
files. Each term in the dataset is allocated to a certain class based on its TF-
IDF weight [ 30 ]. A TF-IDF vectorizer is used on the dataset to find the


weights for all the words in the dataset of YAML files. The score for each
term in a file is given as

where _N_ is the number of documents in the dataset, _nt_ is the number of
occurrences of the term, and _ft_ , _d_ is the number of occurrences of the term _t_

in the _d_ th document. We allocate each term in the document to one of the
five importance-based classes, ranging from 0 to 4, with 4 being highly
relevant to the document and 0 being the least relevant. This is done using
the TF-IDF values for each term.

**_4.1.3 YAML Trees_**

The YAML file structure can be converted to resemble a tree with parent
entities and children. The structure resembles a dictionary data structure
with keys and values. For the K8S YAML files, the keys are specifications
that define the configuration for resources, pods, and deployments. We
convert each YAML file in the dataset to a custom tree structure with key
and value nodes. A key node might have multiple key or value nodes as
children. Since we are dealing with text, we separate each term in the value
and the key as different types of nodes. So, a key node might have multiple
value nodes. For example, if _name: example-deployment_ is a specification
in a YAML file, the _name_ is a key node, and _example_ and _deployment_ are
two child value nodes of the key node. To create a fully connected tree, we
create a root node at the top of the hierarchy.

**_4.1.4 Sentence Generation and Word Vectorization Using_**

**_Word2Vec_**

To utilize the terms in the dataset to train a deep learning model for
relevance ranking, we need to represent the terms as vectors. For this, we
need to create text-based documents containing plain text from the YAML
files that resemble sentences in a document. We use depth-first traversal,
starting from the root node and ending at every value node for each YAML
file, to generate these sentences. For example, a file presented below would
have the corresponding sentences as shown below:

```
TF-IDF score= log ( Nnt )∗ log ( 1 + ft , d ),
```

The sentences provide contextual information for each term, which is
used by an embedding algorithm. The embedding algorithm, such as
Word2Vec, tries to predict the context words given a term. Since each
term’s inclusion in the file is directly related to its ancestors in the tree due
to the hierarchical structure of the YAML file, the Word2Vec model learns
the various contexts in which the same term might appear. The final result is
a contextual vector representation of all the terms in the file.
There are multiple ways to represent numbers as vectors. Utilizing
powerful language models such as BeRT (Bidirectional Encoder
Representations from Transformers) to find the vector embedding of words
is a common process for word vector representation. However, to keep
things simple without the use of too many large models for our system, we
use the Word2Vec model [ 31 ], a powerful deep learning-based context-
aware embedding process that converts text into desired length vectors. All
the words are converted into their corresponding vectors, and the dictionary
of words obtained from the dataset is saved.

**_4.1.5 Undirected YAML Graphs_**

The custom tree data structures for the YAML files are converted into
undirected graphs to get a complete one-to-one relationship between the
terms in the YAML file. The idea of connection among the terms comes


from knowledge graphs that show the relationship between objects and
concepts using directed acyclic graphs. Since a YAML file is a hierarchical
data structure, the relation between the terms can be expressed as having a
key-value relation, a key-key relation, or a value-value relation, which is
represented as one of the features for the edges of the graph. The value-
value relation represents a relation between two value nodes, where each
node contains a substring of a larger string that represents the actual value
for a particular key. For example, a value _spring-app_ would be converted to
two value nodes, one containing _spring_ and the other containing _app_. These
two nodes would have a value-value relation. Figure 3 shows an example of
a YAML file that is preprocessed and converted to an undirected graph
where the edges express the relations between the terms.

**_Fig. 3_** An example of a YAML file converted to an undirected graph

**_4.1.6 Training a Graph Attention (GAT) Model for Node_**

**_Classification_**


The GAT model [ 32 ] which is a variant of GNN is a powerful deep
learning-based algorithm designed to work on graph data where the nodes
and edges are represented as feature vectors. The model learns to weigh the
importance of each node’s neighbors by aggregating their features over
each layer of the neural network and assigning higher attention weights to
relevant features. This aggregation allows nodes separated by multiple paths
on the graph to communicate with each other. For our work, after the
conversion of the YAML trees into graphs, the nodes and edges of the
graphs are converted to feature vectors. The terms represented by each node
are vectorized using the previously trained Word2Vec model. The vector
form of terms in the key and value nodes is used as the features for node
representation. A GAT model with three layers is created to train on the
graph data for importance-based classification of the terms.

**_4.1.7 Generation of Metadata for YAML Files_**

Once the GAT model is trained on the training dataset, its effectiveness in
ranking relevant terms is analyzed using the test dataset. The top three
terms from each file in the test dataset that have been classified as having
the highest relevance to the respective files are used to generate the
metadata, which resembles a natural language prompt. The GPT-2 language
model, a lightweight LLM generally used for sentence completion, was
fine-tuned to generate sentences that resemble a user’s intent expressed as
natural language. This was done using the most relevant terms as classified
by the GAT model for each YAML file in the test dataset. These generated
prompts were used as metadata for the respective YAML files in the test set.

**4.2 Generation of YAML Files Based on User-Provided Prompt**

The YAML files, along with their GPT-2 generated prompts, are used to
fine-tune a larger LLM model. We used the open-source LlaMa-3-8B model
for fine-tuning using the YAML data along with its generated metadata. The
LlaMa model is designed to work efficiently on systems with relatively
limited processing power. The model accepts two parameters during fine-
tuning, the expected output (YAML file for our case) and the prompt (the
metadata generated for the YAML file). This fine-tuned LLM model is then
used to generate a YAML configuration for any general requirement
expressed as a prompt by the user.


**5 Evaluation and Analysis**

Our experiments show that the GAT model effectively classifies YAML
terms by importance, achieving an accuracy of 72%. Fine-tuning LlaMa-3-
8B with generated metadata allowed rapid YAML configuration generation
based on user prompts. The generation of YAML files from prompts
involves two steps: first, the generation of metadata for all the files in the
dataset, followed by the fine-tuning of an LLM for YAML generation. As
discussed in Sect. 4 , our custom metadata generation system creates a
dataset with metadata for the YAML files used for fine-tuning the LLM for
the generation task.

**5.1 Deep Learning Models and Datasets**

We utilized a publicly available dataset containing nearly 230000 YAML
configuration files for Kubernetes without annotation. This data was
collected from various sources and contained Deployment, Service, Pod,
and multiple other kinds of YAML configurations. A subset of 10000
randomly selected files were used for metadata generation and fine-tuning.
The 10000 YAML files were converted to undirected graphs, and several
features were added to the nodes and edges of the graphs to classify the
importance of terms in the files. Table 1 shows all the features used in the
YAML graphs for the GAT model. The centrality measures for the nodes
provided the structural importance of the nodes in the graph.

**_Table 1_** The features of the nodes and edges used
for training the GAT model

```
Node features Edge features
Term vector Edge weight
Page rank score Connection type
TF-IDF value^
Degree centrality^
Betweenness centrality^
Closeness centrality^
```
For training the GPT-2 model, we scraped multiple websites for
Kubernetes-related data. The data consisted of concepts related to


Kubernetes and Docker, as well as documentation for keywords and
relevant terms in YAML files. The LlaMa- 3-8B model was fine-tuned
using the YAML files and their metadata, which were generated using our
custom metadata generation pipeline using GPT-2.

**5.2 Experimentation**

The steps in Sect. 4 were carried out to obtain the results. Firstly, the YAML
dataset was preprocessed using regex and converted to tree structures.
Figures 4 and 5 show the original YAML structure and the tree structure
after preprocessing the text in the file. The preprocessed trees were then
converted to graphs using the NetworkX library [ 33 ] in Python. The TF-
IDF values were computed for all the terms in the dataset, and a graph
dataset for the files was created using the PyTorch-geometric library [ 34 ].

**_Fig. 4_** An example of a YAML file converted to an undirected graph


**_Fig. 5_** An example of a processed YAML tree
This was followed by training the GAT model to get the importance
classes for the terms in the YAML files. Table 2 shows all the
hyperparameters used for training the GAT model. 80% of the data was
used for training, and 20% of the data was used for testing and validation.
Only 50% of the nodes in the training set were used for training, and 100%
of the nodes were used during testing and validation. We achieved an
accuracy of approximately 72% on the test set, which is relatively lower
since the importance classes are based only on TF-IDF values. However,
the GAT model performed relatively well in classifying the most relevant
terms in the YAML file according to inspection, which shows that if we
have a fairly small dataset of YAML files where only a few terms are hand-
labeled by a domain expert, we can still get a very good result in
importance ranking of terms for an unseen dataset using the graphical
structure of YAML files. The hyperparameters were tuned using wandb
library [ 35 ] in Python, which tuned the hyperparameters based on the
highest test accuracy achieved.

**_Table 2_** Hyperparameters used to train the GAT
model

```
Parameter Value
Hidden channels 128
Number of heads 8
Learning rate 0.001
Number of GAT layers 3
Number of linear layers 2
Number of epochs 150
```
Following the training of the GAT model, the top 3 terms were extracted
from the classification results for all the YAML files. Figure 6 shows an
example of ranking the terms in the YAML files after training the GAT
model. We can see how terms like _Kubernetes_ and _Azure_ have been ranked
higher, which indicates that they carry relevant information to describe the
YAML file.


**_Fig. 6_** An example of ranking the terms in YAML file using the GAT model
The GPT-2 model was fine-tuned with all the publicly available
Kubernetes and Docker-related data scraped from the Internet. The fine-
tuning was done on a laptop with Nvidia GTX-1060 GPU and took nearly 2
hours to complete. The top 3 terms ranked by the GAT model for each
YAML file in the dataset were used to generate the metadata for the YAML
file. Since GPT- 2 is a sentence completion model, it was used to describe
the selected terms. This description was combined together to form the
complete description for the YAML files. Figure 7 shows an example of the
description for one of the YAML files in the dataset generated using our
system.

**_Fig. 7_** An example of the generated description for a YAML file


Using an LLM with fewer parameters and a fairly small context (one
term) allowed the inference time to be lower. Since this operation was being
executed in a system with a low-capacity GPU, the inference time was still
fairly large compared to running an LLM in the cloud, which uses a cluster
of powerful GPUs for parallel processing.
Finally, the descriptions generated and their respective YAML files were
combined to fine-tune a larger LLM named LlaMa-3-8B model to generate
YAML files based on the description provided. The fine-tuning was done
using the Google Colab platform with one Nvidia T4 GPU. The fine-tuning
was accelerated using the _Unsloth_ library and took 15 minutes to complete
for a total of 700 YAML files and their description. The _Unsloth_ library also
helped speed up the inference, as the generation took 8.04 seconds on
average. The generation time was averaged for 30 prompts and their
outputs. Figure 8 shows an example of generated files based on the prompt
we provide. We present random prompts to see the response of the LLM
and observe that it understands a very specific context and generates the
output according to the requirements. This provides insight into how LLMs
can be fine-tuned with less data and specific content while still getting good
generation results.

**_Fig. 8_** An example of a YAML file generated based on the given prompt using the LlaMa model


Overall, the evaluation results highlight the practical utility of our
framework in automating network configuration tasks with a high degree of
accuracy and efficiency. By successfully translating intent-based prompts
into reliable YAML configurations, our approach demonstrates its potential
to streamline network management, reduce human error, and enhance
operational efficiency within Intent-Based Networking (IBN) systems. Our
work can effectively be scaled to larger datasets and datasets with different
types of configuration files by leveraging their graphical structure using the
techniques presented in our framework. These results affirm the
framework’s readiness for real-world applications, positioning it as a
valuable tool for advancing automation in complex network environments.

**6 Related Works**

Recent efforts have shown an increased usage of ML/AI techniques and
control-based systems for generating policy-based models and formalizing
intents for IBN systems. The work in [ 2 ] gives a detailed description of
closed-loop automation aspects of IBN systems through intents with stages
such as intent profiling, translation, resolution, activation, and assurance.
For our work, we fine-tune the GPT-2 model on Kubernetes data, combined
with our novel algorithm to generate metadata for a large number of
unlabeled YAML files, and then use this generated data to fine-tune a larger
LLM (LlaMa-3-8B) for translating user prompts (intent expressed in human
language) to YAML configuration file that is used by K8s.
DL models such as Long-short term memory (LSTMs) have been used
for intent translation. The authors in [ 36 ] used a sequence-to-sequence
(seq2seq) learning model based on LSTMs. They use a chatbot to identify
the named entities in the intent using word embedding and clustering. These
named entities are then converted to a high-level structured network
language called SNIL using the LSTMs. This structure is then executed on
the Global Internet Exchange Network (GXN). The work in [ 14 ] allows
users to express their requirements and expectations through an NLP-based
chatbot assistant to make the 5G/6G services available. This allows the
system to resolve the intent of the client and provides the associated
technical solution to deploy the required service. The authors in [ 15 ] have
leveraged the power of In-context learning in LLMs to translate the intent
of clients and built their own system NFV-Intent that converts an intent


expressed in human language to JSON format based on a template provided
to the LLM. They have used multiple variations of LLMs, such as
ChatGPT, LlaMa, Mistral, neural-chat, etc., to compare intent translation
results and inference time. The authors in [ 16 ] introduce a novel AI
architecture called a language model for network traffic (NetLM) based on
a transformer model that understands the sequence structures in network
packets and captures their dynamics. The authors have presented a complete
framework for NetLM and discussed processing multimodal data, intent
refinement using parsing and analysis, building policy trees, and executing
nonconflicting policies for intent assurance.

**7 Conclusions**

This work introduces an automated framework designed to translate high-
level intents into executable Kubernetes YAML configurations by
combining the generative capabilities of LLMs and a novel GNN-based
relevance ranking algorithm. The GNN-based system is used to generate
metadata from the YAML files, which guides the LLM to accurately
understand the context during fine-tuning, resulting in the generation of
YAML files that closely aligns with the operator’s objectives. Our
experimental evaluation demonstrates a term relevance classification
accuracy of 72% along with an accurate generation of YAML files. This
approach significantly improves operational workflows by reducing human
error. By enabling complete automation in the generation and updating of
configurations, the framework strengthens the effectiveness of intent-driven
automation and highlights the growing influence of AI-driven automation in
managing complex and heterogeneous networks.
For future work, we plan to enhance the metadata extraction process to
further improve the context-aware generative capabilities of more advanced
LLMs, specifically tailored to generating configuration files even for other
network management platforms. We also aim to integrate our framework
with real-time network management platforms, enabling dynamic and
adaptive configuration generation with built-in validation systems. We also
plan to assess the security-related risks involved with the usage of AI-
generated content for sensitive network environments.


**References**

1. E. Zeydan, Y. Turk, Recent advances in intent-based networking: a survey, in _2020 IEEE 91st_
    _Vehicular Technology Conference (VTC2020-Spring)_ (2020), pp. 1–5
2.
    A. Leivadeas, M. Falkner, A survey on intent-based networking. IEEE Commun. Surv. Tutorials
    **25** (1), 625–655 (2023)
    [Crossref]
3.
    A. Clemm, L. Ciavaglia, L.Z. Granville, J. Tantsura, Intent-based networking—concepts and
    definitions, RFC 9315 (2022). https:// [http://www.](http://www.) rfc-editor. org/ info/ rfc9315
4.
    N. Tu, S. Nam, J.-K. Hong, Intent-based network configuration using large language models. Int.
    J. Netw. Manag. **35** (1), e2313 (2025), e2313 nem.2313. https:// onlinelibrary. wiley. com/ doi/ abs/
    10. 1002/ nem. 2313
5.
    L. Dinh, S. Cherrared, X. Huang, F. Guillemin, _Towards End-to-End Network Intent_
    _Management with Large Language Models_ (2025). https:// arxiv. org/ abs/ 2504. 13589
6.
    C. Bouras, A. Kollia, A. Papazois, SDN & NFV in 5G: advancements and challenges, in
    _Proceedings of the Conference on Innovations in Clouds, Internet and Networks (ICIN)_ (2017),
    pp. 107–1 11
7.
    K. Dzeparoska, J. Lin, A. Tizghadam, A. Leon-Garcia, LLM-based policy generation for intent-
    based management of applications, in _Proceedings of the International Conference on Network_
    _and Service Management (CNSM)_ (2023), pp. 1–7
8.
    D.M. Manias, A. Chouman, A. Shami, An NWDAF approach to 5G core network signaling
    traffic: analysis and characterization, in _Proceedings of IEEE GLOBECOM_ (IEEE, New York,
    2022). [http://](http://) dx. doi. org/ 10. 1109/ GLOBECOM48099. 2022. 10000989
9.
    P. Lingga, J.J. Kim, J.P. Jeong, Intent-based network management in 6G core networks, in
    _Proceedings of the International Conference on Information and Communication Technology_
    _Convergence (ICTC)_ (2022), pp. 760–762
10.
    A. Fuad, A.H. Ahmed, M.A. Riegler, T. Čičić, An intent-based networks framework based on
    large language models, in _2024 IEEE 10th International Conference on Network Softwarization_
    _(NetSoft)_ (2024), pp. 7–12
11.
    D.K. Rensin, _Kubernetes—Scheduling the Future at Cloud Scale_. (O’Reilly and Associates, New
    York, 2015). [http://](http://) [http://www.](http://www.) oreilly. com/ webops-perf/ free/ kubernetes. csp
12.
    Cloud Native Computing Foundation (CNCF), Objects In Kubernetes (2024). https:// kubernetes.
    io/ docs/ concepts/ overview/ working-with-objects/
13.
    Y. Wei, X. Xie, Y. Zuo, T. Hu, X. Chen, K. Chi, Y. Cui, Leveraging LLM agents for translating
    network configurations (2025). https:// arxiv. org/ abs/ 2501. 08760
14.
    B. Orlandi, S. Lataste, S. Kerboeuf, M. Bouillon, X. Huang, F. Faucheux, A. Shahbazi, P.
    Delvallet, Intent-based network management with user-friendly interfaces and natural language


processing, in _Proceedings of the Conference on Innovation in Clouds, Internet and Networks
(ICIN)_ (2024), pp. 163–170
15.
N. Van Tu, J.-H. Yoo, J.W.-K. Hong, Towards intent-based configuration for network function
virtualization using in-context learning in large language models, in _Proceedings of IEEE
Network Operations and Management Symposium (NOMS)_ (2024), pp. 1–8
16.
J. Wang, L. Zhang, Y. Yang, Z. Zhuang, Q. Qi, H. Sun, L. Lu, J. Feng, J. Liao, Network meets
ChatGPT: intent autonomous management, control and operation. J. Commun. Inf. Netw. **8** (3),
239–255 (2023)
[Crossref]
17.
C. Hirway, E. Fallon, P. Connolly, K. Flanagan, D. Yadav, A comparative study of intent
classification performance in truncated consumer communication using GPT-Neo and GPT-2, in
_Proceedings of the International Conference on Emerging Techniques in Computational
Intelligence (ICETCI)_ (2023), pp. 97–104
18.
X. Zheng, A. Leivadeas, M. Falkner, Intent Based Networking management with conflict
detection and policy resolution in an enterprise network. Comput. Netw. **219** , 109457 (2022).
https:// [http://www.](http://www.) sciencedirect. com/ science/ article/ pii/ S138912862200491 1
[Crossref]
19.
M. Ueno, T. Uchiumi, Migrating existing container workload to kubernetes—LLM based
approach and evaluation, in _2024 IEEE International Conference on Software Maintenance and
Evolution (ICSME)_ (2024), pp. 701–706
20.
L. Pang, C. Yang, D. Chen, Y. Song, M. Guizani, A survey on intent-driven networks. IEEE
Access **8** , 22862–22873 (2020)
[Crossref]
21.
A. Mekrache, A. Ksentini, LLM-enabled intent-driven service configuration for next generation
networks, in _2024 IEEE 10th International Conference on Network Softwarization (NetSoft)_
(2024), pp. 253–257
22.
The Helm Authors, _Helm—The Kubernetes Package Manager_. Cloud Native Computing
Foundation (2024). Accessed: 2025-07-18. https:// helm. sh/ docs/
23.
A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, _Language Models are
Unsupervised Multitask Learners_ (2019). https:// cdn. openai. com/ better-language-models
24.
G. Team, _Gemini: A Family of Highly Capable Multimodal Models_ (2024). https:// arxiv. org/ abs/

2312. 11805
25.
H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal,
E. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, G. Lample, _LLaMA: Open and Efficient
Foundation Language Models_ (2023). https:// arxiv. org/ abs/ 2302. 13971
26.
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A.N. Gomez, L. Kaiser, I. Polosukhin,
_Attention Is All You Need_ (2023). https:// arxiv. org/ abs/ 1706. 03762
27.
U. Ahmed, A. Polini, Enhancing open data findability: fine-tuning LLMs(T5) for metadata
generation, in _Conference on Digital Government Research_ , vol. 1 (2025)


28.
L. Huang, D. Ma, S. Li, X. Zhang, H. Wang, Text level graph neural network for text
classification, in _Proceedings of the 2019 Conference on Empirical Methods in Natural
Language Processing and the 9th International Joint Conference on Natural Language
Processing (EMNLP-IJCNLP)_ (Association for Computational Linguistics, Hong Kong, 2019),
pp. 3444–3450. https:// aclanthology. org/ D19-1345/
29.
L. Yao, C. Mao, Y. Luo, _Graph Convolutional Networks for Text Classification_ , _CoRR_ , vol.
abs/1809.05679 (2018). [http://](http://) arxiv. org/ abs/ 1809. 05679
30.
P. Bafna, D. Pramod, A. Vaidya, Document clustering: TF-IDF approach, in _Proceedings of the
International Conference on Electrical, Electronics, and Optimization Techniques (ICEEOT)_
(2016), pp. 61–66
31.
T. Mikolov, K. Chen, G. Corrado, J. Dean, _Efficient Estimation of Word Representations in
Vector Space_ (2013). https:// arxiv. org/ abs/ 1301. 3781
32.
P. Veličković, G. Cucurull, A. Casanova, A. Romero, P. Liò, Y. Bengio, _Graph Attention
Networks_ (2018). https:// arxiv. org/ abs/ 1710. 10903
33.
A.A. Hagberg, D.A. Schult, P.J. Swart, Exploring network structure, dynamics, and function
using NetworkX, in _Proceedings of Python in Science Conference_ , ed. by G. Varoquaux, T.
Vaught, J. Millman (2008), pp. 11–15
34.
M. Fey, J.E. Lenssen, _Fast Graph Representation Learning with PyTorch Geometric_ (2019).
https:// arxiv. org/ abs/ 1903. 02428
35.
L. Biewald, _Experiment Tracking with Weights and Biases_ (2020). https:// [http://www.](http://www.) wandb. com/
36.
M.-T.-A. Nguyen, S.B. Souihi, H.-A. Tran, S. Souihi, When NLP meets SDN: an application to
Global Internet eXchange Network, in _Proceedings of IEEE International Conference on
Communications (ICC)_ (2022), pp. 2972–2977
_OceanofPDF.com_


(1)

© The Author(s), under exclusive license to Springer Nature Switzerland AG 2026
S. Singh et al. (eds.), _GenAI and LLMs for Beyond 5G Networks_
https://doi.org/10.1007/978-3-032-06418-9_14

##### Agent Orchestration for Resource

##### Allocation in Self-Healing Networks

Priyanka V. Galagali^1
IBM Research, Bangalore, India

```
Priyanka V. Galagali
Email: Priyanka.galagali@ibm.com
```
**Abstract**
This chapter explores the transformative role of Large Language Models
(LLMs) and intelligent agents in optimizing resource allocation for
customers in beyond-5G (B5G) network environments. We examine how
generative AI technologies enable dynamic orchestration of network
resources through predictive modeling, contextual understanding, and
autonomous decision-making capabilities. The chapter delves into self-
healing network architectures that leverage LLM-driven agents to detect
anomalies, anticipate failures, and automatically reconfigure network
topology to maintain service continuity. Special attention is given to real-
time resource redistribution techniques, multi-agent coordination for
complex problem-solving, and the integration of these systems with
existing network management frameworks like SDN and NFV. We
conclude by discussing the significant challenges, including model
reliability, security, and real-time performance, while outlining future
research directions toward fully autonomous, intent-driven network
ecosystems.

**Keywords** Agent orchestration – Multi-agent systems (MAS) –
Autonomous agents – Resource allocation – Network self-healing – Self-
healing networks – Network resilience – Fault detection – Fault recovery –


Dynamic resource management – Adaptive resource allocation –
Distributed intelligence

**1 Introduction**

The evolution toward Beyond-5G (B5G) and 6G networks heralds an era of
unprecedented connectivity, characterized by ultrareliable low-latency
communications (URLLC), massive machine-type communications
(mMTC), and extreme mobile broadband (eMBB) [ 1 ]. These service
paradigms impose stringent and highly dynamic Quality of Service (QoS)
and Quality of Experience (QoE) requirements. The sheer scale,
heterogeneity, and complexity of these future networks render traditional,
human-in-the-loop, and even conventional rule-based automation
frameworks inadequate for managing network resources effectively.
The core challenge lies in transitioning from reactive to proactive and
predictive network management. Current systems, including many
implementations of Self-Organizing Networks (SONs), often respond to
failures after they occur, leading to service degradation or outages. A truly
resilient network must not only react but also anticipate and preemptively
mitigate issues. This requires a new architectural paradigm capable of
continuous monitoring, sophisticated reasoning, and autonomous action at
machine speed.
This chapter proposes and explores such a paradigm, centered on the
synergy between intelligent software agents and Large Language Models
(LLMs). LLMs, a subset of generative AI, have demonstrated remarkable
capabilities in natural language understanding, pattern recognition from
unstructured data, and complex reasoning [ 2 ]. When embedded as the core
“brain” of an intelligent agent, an LLM can transform network telemetry,
logs, and performance metrics into actionable intelligence. This enables a
powerful new form of network orchestration.
We posit that a multi-agent system (MAS), where each agent is powered
by an LLM, can create a distributed, intelligent control plane for B5G
networks. These agents can autonomously:

1. **perceive** the network state through diverse data streams,


2. **reason** about potential future states, predicting anomalies and failures,
3. **decide** on optimal resource allocation and reconfiguration strategies,

## and,^

4. **act** upon the network infrastructure via standardized APIs to implement

## these decisions.^

This chapter details the architecture of such an LLM-driven agent
system, explores its application in self-healing and resource allocation,
presents a conceptual case study, and analyzes the prevailing challenges and
future research avenues. Our contribution is to provide a blueprint for
harnessing generative AI to build the next generation of autonomous,
resilient, and efficient communication networks.

**2 Background and Related Work**

The quest for network automation is not new. This section provides context
by reviewing foundational concepts and highlighting the limitations that
LLM-powered agents are poised to address.

**2.1 Software-Defined Networking (SDN) and Network**

**Functions Virtualization (NFV)**

SDN and NFV laid the groundwork for modern network automation by
decoupling the control plane from the data plane and virtualizing network
functions, respectively [ 3 ]. This programmability, enabled by centralized
controllers (like OpenDaylight or ONOS) and management and
orchestration (MANO) frameworks, allows for programmatic control over
network resources. However, SDN/NFV define the “how” (the mechanisms
of control) but not the “what” or “why” (the intelligent logic behind the
decisions).

**2.2 Self-Organizing Networks (SONs)**

SONs were introduced in 3GPP standards to automate tasks like
configuration (self-configuration), optimization (self-optimization), and
fault management (self-healing) [ 4 ]. While successful, traditional SON
functions are often siloed, operate on limited data sets, and are primarily


reactive. They lack the holistic, cross-domain contextual understanding
needed to manage the intricate dependencies of B5G services.

**2.3 Machine Learning in Network Management**

Conventional machine learning (ML) models have been widely applied to
networking problems. Supervised learning models predict traffic volumes,
while unsupervised models detect anomalies in performance metrics.
Reinforcement learning (RL) has shown promise in optimizing routing and
resource allocation policies [ 5 ]. However, these models often require
extensive feature engineering, struggle with unstructured data (like raw text
logs), and lack the generative planning and reasoning capabilities inherent
in modern LLMs.

**2.4 The Generative AI Leap**

The emergence of foundation models and LLMs like GPT-4, Llama, and
PaLM 2 represents a paradigm shift [ 2 ]. Unlike specialized ML models,
LLMs are pretrained on vast datasets, endowing them with a general-
purpose reasoning engine. Their key advantages for network orchestration
include:

**Contextual Understanding:** Ability to process and correlate information
from heterogeneous sources—structured telemetry, unstructured logs,
and even human-readable trouble tickets.
**Intent Interpretation:** Translating high-level human intent (e.g.,
“Prioritize low-latency for the remote surgery slice”) into low-level
network configurations.
**Zero/Few-Shot Learning:** Adapting to new tasks and network
conditions with minimal specific training data.
**Generative Planning:** Formulating multistep action plans to resolve
complex issues, rather than just classifying a problem.
LLM-powered agents build upon the foundations of SDN/NFV and
SON, providing the missing “cognitive” layer to drive truly autonomous
operations.

**3 LLM-Powered Multi-agent Architecture for**

**Network Orchestration**


We propose a hierarchical and distributed Multi-agent System (MAS)
architecture. This architecture is designed for scalability, resilience, and
specialization, where different agents collaborate to achieve global network
objectives.

**3.1 The Core Intelligent Agent**

Each agent in our system is an autonomous software entity composed of
three fundamental modules (Fig. 1 ).

1. **Perception Module:** This module is the agent’s sensory input. It
    interfaces with the network data plane and management systems (e.g.,
    telemetry collectors, log aggregators, performance monitoring probes)
    to ingest a wide array of data in real time. This includes time-series
    data (latency, jitter, throughput), event logs, configuration files, and
    API status reports.
2. **Reasoning Module (LLM Core):** This is the agent’s cognitive engine.
    A fine-tuned LLM serves as its core. Its responsibilities include:
       **State Assessment:** Fusing and interpreting perceived data to build a
       comprehensive, context-aware model of the current network state.
       **Predictive Analysis:** Forecasting future states, such as traffic
       congestion, link saturation, or component failure, by identifying
       subtle precursors in the data.
       **Causal Reasoning:** Inferring the root cause of observed or predicted
       anomalies.
       **Plan Generation:** Formulating a sequence of actions to mitigate a
       problem or optimize performance. The LLM generates a plan in a
       structured format (e.g., JSON or YAML) that can be machine-parsed.
3. **Action Module:** This module translates the generated plan into
    concrete commands and executes them. It interacts with the underlying
    network infrastructure through standardized APIs exposed by SDN
    controllers, NFV MANO frameworks, and other network elements.
    Actions can range from updating a routing table to migrating a virtual
    network function (VNF) or reallocating bandwidth to a network slice.


**_Fig. 1_** Core components of an LLM-powered intelligent agent

**3.2 Multi-agent System (MAS) Roles and Coordination**

A single agent cannot manage an entire B5G network. We envision a
collaborative MAS with specialized roles:

```
Monitoring Agents (M-Agents): Deployed at the network edge or
within specific domains, these agents are lightweight data collectors and
preprocessors. They perform initial filtering and anomaly flagging,
escalating significant events to higher-level agents.
Analysis and Prediction Agents (A-Agents): These agents house more
powerful LLMs. They receive data from multiple M-Agents and are
responsible for deep analysis, cross-domain correlation, and failure
prediction. For example, an A-Agent might correlate a rising bit error rate
on a wireless link with a deteriorating weather forecast (obtained via an
external API) to predict an imminent outage.
Planning and Orchestration Agents (O-Agents): Operating at a global
or regional level, these agents are the master decision-makers. They
receive predictive alerts from A-Agents, understand the high-level
business and service intents (SLAs), and generate comprehensive
remediation and optimization plans. They must resolve conflicts between
competing resource requests.
```

**Execution Agents (E-Agents):** These are specialized agents that
interface directly with network controllers. They receive concrete action
plans from O-Agents and are responsible for the safe and reliable
execution of those plans, including verification and rollback capabilities.
Coordination among these agents can be achieved through a shared
message bus or a structured protocol, where agents publish their findings
and subscribe to relevant information streams, forming a distributed
cognitive mesh over the network.

**4 Mechanisms for Self-Healing and Resource**

**Allocation**

This architecture enables a closed-loop, proactive control cycle for self-
healing and resource management.

**4.1 Predictive Failure Mitigation**

The self-healing process shifts from reactive to predictive.

1. **Observation:** M-Agents continuously stream telemetry (e.g., latency,

## packet loss, signal-to-noise ratio) to a domain-specific A-Agent.^

2. **Prediction:** The A-Agent’s LLM core analyzes these time-series
    patterns. It has been fine-tuned on historical failure data and can
    recognize subtle, multivariate precursors that signal impending
    degradation. It might conclude: “Probability of Link A-B failing QoS
    for URLLC slice within 5 minutes is 92% due to increasing
    atmospheric interference.”
3. **Proactive Planning:** The A-Agent alerts the O-Agent. The O-Agent,
    aware of the affected URLLC slice’s stringent SLA, immediately
    formulates a mitigation plan. It queries the network topology database
    and identifies a viable, albeit slightly higher-cost, backup fiber path
    (Path C-D).
4. **Preemptive Action:** The O-Agent instructs the relevant E-Agent to
    execute a “make-before-break” traffic rerouting. The E-Agent uses
    SDN flow programming to establish the new path and seamlessly


```
migrate the service traffic before the original link fails, ensuring zero
downtime for the critical service.
```
**4.2 Dynamic and Intent-Driven Resource Redistribution**

The system excels at allocating resources not just to prevent failures but to
optimize for performance and cost based on high-level goals.

```
Scenario: A network operator defines an intent: “During peak business
hours (9 AM to 5 PM), ensure Gold Tier enterprise customers have
priority bandwidth for video conferencing applications.”
Intent Translation: An O-Agent ingests this natural language intent. Its
LLM core translates this into a concrete set of policies:
```
- Identify traffic flows matching video conferencing signatures (e.g.,
    specific ports, packet sizes) from Gold Tier IP ranges.
- Define a minimum bandwidth and maximum jitter threshold for these
    flows.
- Create a dynamic QoS policy that can be instantiated on network
    routers and switches.
**Continuous Optimization:** During peak hours, M-Agents and A-Agents
monitor the performance of these flows. If an A-Agent predicts
congestion on a path carrying this priority traffic, it alerts the O-Agent.
The O-Agent can then dynamically re-provision bandwidth from a lower-
priority slice (e.g., a background data backup service) to the Gold Tier
slice, ensuring the high-level intent is continuously met. This is a level of
dynamic, policy-aware optimization that is extremely difficult to achieve
with static rules.

**5 Case Study: Autonomous Slice Healing in a**

**B5G Campus Network**

Consider a private B5G network deployed across a university campus,
supporting two critical network slices:

1. **Slice 1 (URLLC):** For a robotics research lab, requiring <2 ms latency

## for real-time robot control.^


2. **Slice 2 (eMBB):** For a large lecture hall, supporting high-bandwidth

## video streaming for 300 students.^

**Event:** A construction crew accidentally severs a primary fiber optic cable
connecting the lab to the campus data center.

**Traditional (Reactive) Response**
The link fails. Alarms are triggered. Network monitoring systems report the
outage. A network operations center (NOC) engineer investigates, identifies
the root cause, and manually reroutes traffic to a backup wireless link.
During this process (minutes to hours), the robotics experiment fails, and
critical research is disrupted.

**LLM-Agent (Proactive) Response**

1. **Precursor Detection (T-minus 10 minutes):** An M-Agent monitoring
    the fiber link detects intermittent light level drops and a spike in
    forward error correction (FEC) events—subtle signs of physical stress
    on the cable. It forwards this data to the regional A-Agent.
2. **Predictive Analysis (T-minus 8 minutes):** The A-Agent’s LLM
    correlates these events with a campus maintenance schedule it accessed
    (unstructured data). It infers a high probability of physical damage and
    predicts a total link failure. It alerts the O-Agent with the assessment:
    “High risk of imminent failure on primary path for Slice 1. SLA breach
    highly likely.”
3. **Autonomous Planning (T-minus 7 minutes):** The O-Agent
    immediately assesses the situation.
       It recognizes Slice 1’s URLLC requirement is paramount.
       It queries the network for available resources. It identifies a high-
       capacity millimeter-wave (mmWave) wireless link as the best
       alternative path.
       It checks the resource utilization of Slice 2. It determines that by
       slightly compressing the video streams (a tolerable QoE reduction),
       it can free up the necessary spectrum and processing resources on the
       mmWave link to accommodate Slice 1 without impacting its latency
       requirements.


```
It generates a multistep plan: (a) Adjust Slice 2’s QoS profile, (b)
Configure the mmWave link for Slice 1’s traffic, (c) Initiate a
seamless traffic handover, (d) Mark the fiber link as “down for
maintenance.”
```
4. **Execution (T-minus 5 minutes):** The O-Agent dispatches commands
    to the relevant E-Agents. One E-Agent adjusts the policy in the eMBB
    slice controller, while another programs the SDN controller to reroute
    Slice 1’s traffic.
5. **Outcome:** The traffic is migrated to the mmWave link moments before
    the fiber is completely severed. The robotics lab experiences no
    noticeable disruption. The lecture hall’s video quality is slightly
    reduced but remains functional. The network has autonomously healed
    itself and optimized resource trade-offs based on service priorities.

**6 Challenges and Future Directions**

Despite the immense potential, deploying LLM-powered agents in
production networks presents significant challenges.

**6.1 Challenges**

```
Reliability and Hallucination: LLMs can generate factually incorrect or
nonsensical outputs (“hallucinate”). An agent acting on a hallucinated
plan could have catastrophic consequences. Robust verification
mechanisms, human-in-the-loop approvals for critical actions, and
constrained output generation are essential.
Real-Time Performance: The inference latency of large LLMs can be
prohibitive for control loops requiring millisecond-level responses (e.g.,
URLLC). Research into model distillation, quantization, and specialized
hardware (neuromorphic chips) is needed to create smaller, faster,
domain-specific LLMs.
Security: The agent system itself is a high-value target. Adversarial
attacks via prompt injection or data poisoning could trick an agent into
compromising the network. Secure API design, rigorous input validation,
and agent behavior monitoring are critical.
```

```
Data Privacy: Agents will have access to sensitive network and
customer data. Architectures must incorporate privacy-preserving
techniques like federated learning, where agents are trained on local data
without centralizing it.
Complexity and Cost: Training, fine-tuning, and running these large
models are computationally expensive. The complexity of managing a
distributed system of intelligent agents also adds significant operational
overhead.
```
**6.2 Future Directions**

```
Explainable AI (XAI): Developing methods for agents to explain their
reasoning in human-understandable terms (“I rerouted traffic because I
predicted a 95% chance of link failure based on these three telemetry
metrics”). This is crucial for building trust and for debugging.
Federated and Continual Learning: Agents should learn and adapt
continuously from live network operations. Federated learning will allow
a global model to benefit from the experiences of individual agents
without compromising data privacy.
Energy-Efficient Orchestration: Future agents could be tasked with
optimizing network-wide energy consumption as a primary objective,
powering down unused components or routing traffic along the most
energy-efficient paths.
Cross-Domain Orchestration: Extending the agent concept beyond the
network to orchestrate resources across compute (cloud/edge) and
storage domains, creating a truly unified, end-to-end service management
fabric.
```
**7 Conclusion**

The convergence of intelligent agent technology and Large Language
Models marks a pivotal moment in the evolution of network management.
The architecture presented in this chapter offers a paradigm shift from the
reactive, manually intensive models of the past to a proactive, autonomous,
and intelligent future. By embedding LLMs as the reasoning core of a
distributed multi-agent system, we can unlock the ability to perform
predictive self-healing, dynamic resource allocation, and intent-driven
orchestration at the scale and speed required by B5G and 6G networks.


While significant challenges related to reliability, security, and
performance must be addressed through rigorous research and engineering,
the potential benefits are immense: unprecedented network resilience,
enhanced quality of experience for customers, and dramatically reduced
operational complexity. The LLM-powered agent is not merely an
automation tool; it is a foundational building block for the cognitive, self-
sustaining network ecosystems of tomorrow.

**References**

1. P. Porambage, J. Liyanage, M. Ylianttila, T. Taleb, The roadmap to 6G: A survey on tends and the
    journey of B5G. IEEE Open J Commun Soci **2** , 1–28 (2021)
2.
    A. Vaswani et al., Attention is all you need, in _Advances in Neural Information Processing_
    _Systems 30 (NIPS 2017)_ , (2017), pp. 5998–6008
3.
    Open Networking Foundation, _Software-Defined Networking: The New Norm for Networks_. (ONF
    White Paper, 2012)
4.
    A. Imran, A. Zoha, A. Abu-Dayya, Challenges in 5G: a survey on self-organizing networks. IEEE
    Commun. Surv. Tutor. **16** (3), 1586–1610., Third Quarter (2014)
5.
    C. Boutilier, Reinforcement learning in network routing, in _Encyclopedia of Machine Learning_ ,
    ed. by C. Sammut, G.I. Webb, (Springer, 2010)
       _OceanofPDF.com_


(1)

© The Author(s), under exclusive license to Springer Nature Switzerland AG 2026
S. Singh et al. (eds.), _GenAI and LLMs for Beyond 5G Networks_
https://doi.org/10.1007/978-3-032-06418-9_15

##### Leveraging Large Language Models

##### (LLMs) for Service Optimization in

##### Beyond 5G (B5G) Networks

Shapna Muralidharan^1 , Wonhee Woo^1 and Nakho Lee^1
DeKist, Yongin-si, Gyeonggi-Do, South Korea

```
Shapna Muralidharan (Corr esponding author)
Email: shapna@dekist.com
```
```
Wonhee Woo
Email: wonhee_woo@dekist.com
```
```
Nakho Lee
Email: nakholee@dekist.com
```
**Abstract**
Generative Artificial Intelligence (GenAI), eminently through Large
Language Models (LLMs), is swiftly transforming the landscape of many
domains and in particular telecommunication networks by incorporating
intelligent, data-driven automation and prediction. Beyond 5G (B5G)
networks mainly aim to support massive connectivity, ultralow latency with
high reliability and especially with the explosion of connected IoT devices
and data-intensive applications several challenges persist. The main
challenges due to massive number of connected devices including mobility-
driven IoT applications like fleet management make the traffic dynamics
increasingly complex, heterogeneous, and user-centric depending on the
consumer requirements. To cater the need of the moment in B5G networks
includes effective traffic prediction and load balancing which are critical
factors for maintaining quality of service (QoS), optimizing resource


utilization, and reducing operational overhead for the voluminous network
load. In this chapter we present a comprehensive review of LLM-enabled
service optimization for traffic forecasting and intelligent load balancing in
B5G environments.

**Keywords** Large language models – Generative AI – B5G – Traffic
prediction – Load balancing – Network optimization

**1 Introduction**

The unprecedented increase in the use of smart devices and massive IoT
applications like smart cities and fleet management has significantly
increased the demand for mobile broadband services that can provide
reliable service with high data rates and an enhanced quality of service
(QoS) [ 1 ]. To meet the voluminous service requirements, evolution of 5G
wireless networks toward Beyond 5G (B5G) networks and 6G networks
with a vision to accommodate ultrareliable and low-latency service
capabilities is the need of the moment [ 2 , 3 ]. The emergence toward B5G
networks introduces unprecedented complexity in managing service
delivery, driven by ultradense deployments, massive device connectivity,
and the dynamic demands of emerging applications such as Virtual
Reality/Augmented Reality (VR/AR), autonomous systems, and industrial
IoT [ 4 ]. In this context, service optimization becomes an essential enabler
to provide high-quality user experiences, ensuring fairness in resource
allocation, and maintaining network reliability. Network operators must
have the abilities to dynamically orchestrate resources across the Radio
Access Network (RAN) in real time thereby providing lower latency with
high throughput and improved QoS [ 5 ]. The need for adaptive service
optimization in real time is further amplified in environments like massive
IoT applications and mobile users with unique requirements in terms of
bandwidth, latency, and reliability, and the services with satisfactory QoS
become challenging. The demand for efficient proactive service
optimization is pushing the operators to move toward automation and
intelligent decision-making [ 6 ]. The current reactive practices for load
balancing fall short in maintaining reliable performance metrics, while there
is sudden traffic surge or any unpredictable faults rising need for intelligent
traffic predictions. Therefore the need for leveraging the current dominant


technologies like Artificial Intelligence (AI) and Generative Artificial
Intelligence (GenAI) is crucial to envision a smooth transition from 5G to
B5G networks and eventually to 6G networks [ 7 ].
The core challenges that exist in B5G network service optimization lie
in catering varying traffic patterns of applications which varies not only
across time but also in space [ 8 ]. Traffic demand of domains like enhanced
Mobile Broadband (eMBB), URLLC, and massive Machine-Type
Communications (mMTC) could not be catered with introduction of the
virtualized and Software-Defined Networking (SDN) elements—such as
Virtual Network Functions (VNFs) and Cloud-Native Network Functions
(CNFs) applications with increased operational flexibility [ 9 ]. This dynamic
and stochastic nature of the current mobile networks makes it impossible to
maintain balanced loads by avoiding bottlenecks using traditional methods
with static thresholds. Though there are existing AI- based models to predict
loads, they are often lagging due to the need for constant retraining, varied
requirements of certain domain-specific applications, and adaptation in real
time [ 10 ]. The existing models mainly focus on traffic classification, local
anomaly detection, and these aspects limit to certain issues, losing their
vision on creating an end-to-end optimization. The current demand on
designing a holistic, intelligent, and autonomous approach to understand
domains, learning from past history, and predict in real time is necessary for
the B5G networks [ 11 ].
Generative Artificial Intelligence (GenAI) and more specifically Large
Language Models (LLMs)—like Generative Pretrained Transformers
(GPT), Bidirectional Encoder Representations from Transformers (BERT),
and PaLM developed by leading companies like OpenAI and Google, have
shown remarkable advancement in the field of Natural Language
Processing (NLP) and other AI-driven domains [ 12 – 14 ]. Unlike the existing
black box AI models, GenAI systems offer powerful generalization,
contextual reasoning, and multimodal understanding capabilities [ 15 ]. Most
of the LLMs are pretrained with vast, domain-specific data to understand
and synthesize knowledge across various factors. When the pretrained
models are fine-tuned with more specific data, for example, in the B5G
network details like performance metrics, or control plane events, LLMs
can give precise predictions with proper explainable solutions [ 16 ]. While
incorporating LLMs in B5G networks it can support a wide range of
network tasks—from traffic pattern forecasting and slice admission control


to congestion prediction and fault localization with improved forecasting
accuracy. Unlike the traditional time-series model-based predictions, LLMs
do not require a rigid data model and also use zero-shot predictions making
them reliable in unforeseen circumstances [ 17 ]. As the LLMs continue to
evolve, their ability to reason, generate synthetic network scenarios, and
provide human-readable insights positions them as core enablers of
intelligent B5G automation [ 16 ].
In this book chapter, we will first discuss the Fundamentals of Traffic
Behavior in the B5G networks by identifying the major factors like the load
variability, user mobility, and density for various application types in Sect.
2. In Sect. 3 we will delve into the LLM Foundations for Predictive Traffic
Analytics and examine where the traditional AI approaches fall short in the
B5G networks. In Sect. 4 we will discuss the basic foundational architecture
of LLMs. In Sect. 5 we will discuss techniques for real-time traffic
forecasting using LLMs. Next in Sect. 6 we will delve into LLM-enabled
load management strategies across RAN, core, and edge layers. This section
discusses architectural considerations, including the role of GenAI-powered
predictors within the RIC, transport layer traffic steering, and
SDN/orchestration interfaces for closed-loop control. Then in Sect. 8 we
will discuss the practical use cases of LLM in the B5G networks,
challenges, and the open research challenges. Finally we will conclude with
future emerging directions for the B5G networks and self-evolving 6G
network systems.

**2 Fundamentals of Traffic Behavior in B5G**

**Networks**

The concept behind the design and operation of the B5G networks is
immensely tailored to the needs of the various network traffic types and
their characteristics. Unlike the previous generation networks, the
traditional mobile broadband services alone do not influence the B5G traffic
behavior and also by the proliferation of intelligent IoT devices, mission-
critical applications, and edge computing workloads. Before developing
effective predictive analytics and load balancing strategies, it is necessary to
understand the various traffic patterns and characteristics of the workload
[ 18 ]. Figure 1 depicts a futuristic B5G architecture with LLM deployments.


**_Fig. 1_** A futuristic B5G architecture with LLM deployments

**2.1 Characteristics of B5G Traffic**

The convergence of technologies like IoT, autonomous systems, industrial
automation, and eXtended Reality (XR) has created a diverse range of
traffic frameworks, and the B5G networks are expected to handle it in real
time. The main hurdle is to handle the heterogeneity in terms of the device
types, data types, varied application requirements, and their latency
demands [ 19 ]. The User Equipment (UE) includes not only mobile devices
like smartphones and tablets but also IoT sensors, autonomous vehicles,
drones, wearables, and industrial robots. These devices generate traffic with
different temporal and spatial patterns in data with various mission-critical
requirements and often unpredictable and variable traffic flows across the
network [ 8 ]. Another key network load factor in the B5G networks is the
requirement for real-time, Ultra-Reliable, and Low-Latency
Communication (URLLC) [ 8 ]. Some applications like the Autonomous
Vehicles or smart grid management are applications associated with
URLLC, and this traffic must have high priority with minimal jitter, which
leads to additional complexity in the resource management. Furthermore,
the need for load balancing for enhanced Mobile BroadBand (eMBB),
which supports high data rate applications such as 4K/8K video streaming,
cloud gaming, and VR/AR, is similar. The traffic patterns of eMBB
applications are often bandwidth-intensive and creates a considerable load
demand in the B5G networks in dense urban areas or during large events
[ 20 ].


Another main contributor for the load fluctuation in the B5G networks
is the massive Machine-Type Communications (mMTC) scenarios where
the traffic is intermittent, but still the scale is massive. IoT applications with
existing millions of sensors and devices and the numbers increasing day by
day send sporadic data over the B5G networks. When there are unexpected
traffic surges due to trigger by external events (e.g., alarms, environmental
changes) it can cause an unpredictable network load [ 21 ]. The event-driven
nature of mMTC introduces volatility into traffic patterns that are difficult
to predict using traditional heuristics. Moreover most of the IoT
applications are multimodal in nature involving a combination of audio,
video, and sensor data, where the network must simultaneously support
high-throughput, low-latency, and high-density use cases. There will be a
need to support more diverse applications in various domains in the future,
creating a heterogeneous traffic landscape in B5G networks and later in 6G
networks making predictive modeling and adaptive load balancing more
essential than ever.

**2.2 Key Drivers of Load Variability**

The heterogeneous nature of the traffic load from the B5G Networks is not
only the result of accommodating wide array of applications but also the
environmental, behavioral, and technological factors. Understanding these
key driving factors is crucial, so we can design traffic load balancing
strategies proactively using LLM-based technologies [ 22 ]. The first and
foremost factor that plays a central role in traffic fluctuations is mobility.
Mobile users and applications like autonomous driving systems are
dynamic and move between cells, especially in high-density urban or
highway scenarios, traffic loads can shift rapidly. Furthermore the
increasing use of load-intensive applications like video streaming and
automated drone fleets in various logistics applications adds to the
complexity of predicting load transitions across network segments.
Traditional cell handover technologies have shortcomings to manage the
dynamic load implications due to mobility [ 15 ].
The next important factor that affects the load balancing factors in B5G
networks is the user density that impacts the load fluctuations during peak
hours in urban centers and during mass gatherings (e.g., concerts, sporting
events) causing network congestion due to access requests from thousands
of users. The sporadic nature of these events becomes tough to preallocate


resources, meanwhile rural and suburban areas may experience low or
bursty loads, requiring adaptive resource scaling mechanisms to maintain
cost efficiency [ 23 ]. Usage patterns of users by considering the time-of-day
and day-of-week also help in reducing spikes. For example, residential
areas see load demand during the evening hours, while the commercial
areas experience a peak demand during working hours. These cyclic
patterns of demand are further influenced by holidays, cultural events, and
regional behaviors, all of which need predictive tools to allocate resources
beforehand [ 22 ].
Further contribution to demanding network services in B5G networks is
due to the services like immersive applications (AR/VR). These immersive
applications require low-latency, high-throughput communication that can
cause unprecedented load spikes. Similarly, video streaming platforms
widely used by most families have evolved to provide adaptive bitrate
streaming, which dynamically adjusts bandwidth usage based on perceived
network conditions and requires predictive traffic modeling [ 24 ]. Finally the
increasing adoption of edge computing by IoT applications and distributed
cloud services is changing traffic distribution patterns. Multi-access Edge
Computing : MEC signifies that it supports multiple access technologies,
including 5G, Wi-Fi, and fixed networks, ensuring a unified edge platform
regardless of how the user or device connects. Though this scenario reduces
the backhaul congestion, it introduces microlevel traffic management at the
edge and requires reliable forecasts to enable fine-grained load balancing
strategies [ 25 ].
To summarize the contents of this section, the characteristics and the
load variability of B5G networks show that the traffic is volatile and
context-sensitive. Traditional rule-based or threshold-driven load
management techniques are inadequate to handle such complexity. There is
a demand and need for advanced, context-aware predictive analytics
powered by GenAI, LLM, and other learning-based frameworks that can
help us to anticipate the dynamic shifts of the load in real time [ 24 ].

**3 LLM Foundations for the B5G Networks**

B5G networks are becoming increasingly complex with various traffic
patterns and dynamic with many mobile IoT applications. Using traditional
AI approaches in handling real-time traffic analysis and network


optimization has shortcomings and becoming more evident. GenAI models
like LLMs offer a transformative capability for predictive traffic analytics
thereby optimizing network services [ 26 ]. The inherent capabilities of
LLMs like bringing powerful contextual understanding, reasoning, and
generation capabilities surpass the predictions from conventional statistical
or deep learning models. LLMs learn from a vast amount of telecom data to
understand the dynamics and enable more accurate traffic forecasting,
proactive resource allocation, and intelligent service orchestration. This
section explores why traditional models fall short in service optimization of
B5G networks and how LLMs fill the gap and provide architectural
foundations for this paradigm shift in network intelligence [ 27 ].

**3.1 Why Traditional AI Falls Short?**

The current development of AI and machine learning approaches is
prominently productive in a multitude of domains like IoT, healthcare,
many consumer applications, and so on. Though the traditional approaches
are effective, there are some limitations when we apply to the complex,
dynamic, and data-intensive environment of B5G networks. The major
hurdle we face while deploying AI-based approaches in the
telecommunication industry is the historical data sparsity related to rare
events or localized outages [ 27 ]. The final prediction results from these
traditional AI approaches lead to unreliable predictions and compromise the
robustness of AI-driven decision-making. Moreover while training
conventional AI models for only specific situations creates a rigid AI model
which lacks adaptability to unforeseen circumstances, rapid shifts in traffic
patterns, and user mobility. The inadequacy to generalize across contexts
makes them unsuitable for real-time, fine-grained network service
optimization. In the practical scenarios, telecom operators often require
clear, reliable, actionable insights from AI models to make informed
decisions. The traditional AI approaches are usually based on deep learning
though it achieves a higher accuracy results are often called as black boxes
because their internal decision-making processes are not easily interpretable
or transparent. To summarize, the key points for the need of GenAI
techniques for B5G networks instead of traditional approaches are their
limited adaptability, difficulty handling rare events, and inability to capture
real-time contextual shifts. Furthermore, the lack of explainability for the
predictions and semantic understanding undermines their reliability in high-


stakes network environments, where proactive and intelligent decision-
making is critical. The aforementioned shortcomings demand the need for a
flexible, context-aware, and interpretable model, particularly LLMs for the
service optimization in B5G networks. This sets the stage for a paradigm
shift in predictive traffic analytics, enabling a more intelligent and
autonomous B5G network infrastructure [ 26 ].

**3.2 Leveraging LLMs for the B5G Networks**

The need for the development of Natural Language Processing (NLP) tasks
was the initial point of LLM development, and there are distinct state-of-
the-art models deployed beyond the traditional NLP models in various
domain-specific LLMs and general LLMs. Various domains like healthcare,
finance, and IoT applications use the capabilities of LLMs for coding and
debugging, recommendation systems, LLM-enabled agents, instruction-
based optimization, network time-series prediction, and decision-making.
These unique inherent qualities of LLMs have become the cutting-edge
technology for the modern AI research, and their deployment in the
telecom-specific domain can solve many unresolved challenges. In this
section we detail the most impactful use of LLMs in the B5G networks with
focus to traffic prediction and load balancing [ 28 ].

1. LLM Foundational models in B5G networks:
    LLMs are the fundamental tools in futuristic AI models because
they can understand and generate models based on Natural Language.
LLMs outperform in interpreting contextual data, which would make
them a perfect match for predicting and forecasting tasks. Besides these
advantages, the capacity to analyze multimodal data in real time is seen
as the crucial ability to handle autonomous B5G networks. LLMs are
built on transformer-based neural network architectures and excel at
handling sequential and structured data which are common in network
telemetry, logs, and configurations [ 29 ]. The inherent capabilities like
multistep reasoning, instruction-following capabilities, and in-context
learning make LLMs adaptable to a wide range of applications, from
IoT to RAN configuration. The most crucial step in deploying LLM
integrated models involves the pretraining of massive amount of data,
followed by domain-specific fine-tuning to adapt to the telecom
domain. LLMs designed for telecom-specific tasks require intense
training on log data, alarm patterns, and historical Key Performance


```
Indicators (KPIs) to improve model relevance and performance while
also deployment strategies range from centralized cloud deployment to
edge-based lightweight implementations. Prompt engineering
techniques like in ChatGPT can further align LLM behavior with
operational goals for B5G networks. As the models scale, so does their
capability to generalize across prediction, control, and classification
tasks [ 7 ].
```
2. LLM-Enabled Optimization Techniques in B5G networks:
    Service Optimization is the heart of the B5G networks, because it
can create autonomous and efficient network operations. LLMs offer
new paths from the traditional deep learning techniques to classical
optimization problems in telecom, including traffic routing, load
balancing, and spectrum allocation [ 30 ]. For instance, in NP-hard
problems, LLMs can design and fine-tune heuristic algorithms based on
past historical experiences. Due to these capabilities LLMs are
deployed in end-to-end optimization pipelines, for example, in traffic
forecasts for handling resource allocation problems directly. LLMs can
optimize the programmable Software Design Networking (SDN)
policies by analyzing flow statistics and suggesting rule updates [ 26 ].
Another major innovation in LLMs is assisting in automatic reward
function design for Reinforcement Learning (RL). In RL, reward
criteria need fine-tuning manually and that can be replaced by the
network operators who can plainly give instructions in Natural
Language, which LLMs can convert it to reward function designs [ 31 ].
LLMs also optimize user experience by tuning buffer sizes, routing
paths, and mobility handover thresholds. Moreover the ability to induce
multi-agent orchestration and LLMs serve as control agents
coordinating resource use across RAN, core, and edge nodes [ 32 ].
3. LLM-Aided Prediction, classification problems in B5G networks:
    Prediction is a fundamental solution for proactive and intelligent
decision-making solution deployed in an array of applications including
the telecom industry [ 33 ]. One happening trend is the development of
foundation models for time-series prediction using LLMs widely
adopted by many IoT applications. These LLMs are pretrained on a
wide range of network and usage patterns and then fine-tuned or used


```
directly for specific forecasting tasks. Classification is an important
foundational task in B5G Networks for detection and categorization of
various network phenomena [ 30 ]. The main focus here in classification
tasks is LLMs capable of handling multimodal data like text, telemetry,
and image, and even structured protocol data can classify logs into
categories like alarm, warning, or info based on severity and historical
context. In zero-shot and few-shot models, LLMs have the capacity to
classify new types of faults or services with minimal labeled data [ 34 ].
```
**4 How LLMs Work?**

In this section we discuss how the LLMs work by first introducing the LLM
architecture overview and then the concepts of pretraining, fine-tuning,
inference, utilization, and model evaluation. This section introduces the
core fundamentals of Large Language Models (LLMs) and covers key
aspects such as model architecture, pretraining, fine-tuning, inference,
utilization, and model evaluation. Furthermore, it explores how LLMs can
be deployed in different layers of telecom networks, including the central
cloud, network edge, and mobile devices. Lastly, it emphasizes the
importance of telecom-specific adaptations such as custom training or fine-
tuning for optimal performance in communication scenarios. Given the
complexity of telecom systems, LLM application can take various forms—
pretraining from scratch using telecom-specific data, fine-tuning general-
purpose LLMs, or direct application through prompting. The subsequent
sections detail each of these strategies.

**4.1 LLM Architecture**

LLMs are a genre of the AI model that is capable of processing and
generating natural Language. Generally the LLMs are trained on massive
amount of data (often ranging from hundreds of millions to billions, often
exceeding 10 billion) and use DL techniques to learn the patterns and
structures of language [ 35 ]. Modern LLMs are primarily built upon the
transformer architecture, which was introduced in 2017, and are designed to
utilize attention mechanisms and capture complex dependencies between
inputs and outputs through in-contextual learning. The main framework
behind the LLM architecture is that the transformers tokenize raw inputs
and then apply embeddings and positional encodings before processing


through the model. In regard to this the standard transformers have two
integral components: encoder and decoder [ 36 , 37 ]. The encoder uses the
bidirectional (self-)attention to understand relationships between generated
input tokens, and the decoder generates output based on the previously
generated tokens, while the original input is masked by (causal) attention
and cross-attention mechanisms. In addition to the standard attention
mechanisms, there are advanced variants to capture the token relationships,
including multi-head attention, multi-query attention, and grouped-query
attention. Based on the encoder and decoder architecture, LLMs can be
classified into three types: encoder-decoder architecture, encoder-only
architecture, and decoder-only architecture [ 38 ].

- Encoder-only architecture: These models only have an encoder and are
    mainly used for applications such as classification. BERT is a noticeable
    example for this type of LLMs and is pretrained with two main goals,
    one being masked language model objective and the next sentence
    prediction objective. Many variants of this BERT model like the
    RoBERTa and ALBERT were introduced to reduce the training overhead
    of BERT model [ 39 – 41 ].
- Encoder-decoder architecture: The basic transformer architecture uses
    this model combining an encoder with a decoder for sequence-to-
    sequence tasks. The encoder passes the output to the decoder, which
    easily helps in deploying complex tasks like text translation and
    summarization. Prominent existing models include T5, which
    reformulates NLP tasks into a text-to-text framework, and BART, which
    uses denoising auto-encoding strategies for pretraining [ 42 , 43 ].
- Decoder-only architecture: The decoder-only model focuses on
    unidirectional (causal) attention and is optimized for text generation. The
    unidirectional attention allows each output token to attend only to its past
    tokens and itself. Depending on their attention mechanisms, decoders are
    further divided into casual and noncasual decoders. This model upholds
    popular models like GPT, PaLM, and LLaMA [ 14 , 44 ].

**4.2 Pretraining LLMs**

The main objective of LLMs in the pretraining process is it learns to predict
the next word in a sequence based on the previous context called as the
language modeling and subsequently learning general language and
reasoning patterns. Traditionally LLMs are trained on large-scale datasets,


and they inherit the emergent abilities which can be utilized to both general
topics and domain-specific topics like healthcare and finance. The main
steps in preprocessing include data collection and preprocessing and model
training [ 45 , 46 ].

(a) Data collection and preprocessing: It is a resilient job to develop
LLMs from the scratch as it has huge computational demands, and it is
plausible to learn experiences from existing LLMs and use resources
that are available publicly available in two categories—general-
purpose and domain-specific data [ 47 ]. The general-purpose dataset
mainly includes a wide-range of public and open-domain sources such
as webpage, news articles, and books. Domain-specific datasets
include specialized ones like scientific research works, academic
papers, and textbooks. Domain-specific data related to B5G networks
encompasses technical documentation (like 3GPP, ITU), network logs,
call detail logs, patents, and simulation data [ 48 ]. The domain-specific
data enhances the LLMs to understand domain-specific knowledge,
terminology, and procedures making it effective for tasks like traffic
prediction, anomaly detection, and policy generation. Raw datasets
scraped from Internet or internal logs most often include noisy,
redundant data, and it is essential to preprocess the data before
training them [ 49 ]. Efficient preprocessing ensures the quality of data
is good and reduces potential bias and error during training. Few data
preprocessing methods used commonly are filtering, de-duplication,
privacy redaction, and tokenization [ 50 ].

(b) Model training: Training LLMs involves optimizing datasets with
billions of data points using high-performance computing
infrastructure. Batch size and learning rate are the two key
hyperparameters for effective pretraining [ 51 ]. Batch size typically
refers to the number of training samples (i.e., sequences of tokens),
and usually large batch sizes stabilize LLM training by starting with a
significant batch size and gradually enlarge the batch size to ensure
reliable training. Here the main focus is on the learning rate
adjustment in each step by starting with a warm-up phase and follow-
up with a cosine decay pattern. To improve the scalability of the LLM
pretraining, many techniques like 3D parallelism (data parallelism),
pipeline parallelism, and tensor parallelism are adopted [ 52 ]. Data


```
parallelism replicates the LLM parameters across all GPUs and clones
a portion of data in each to process finally to aggregate the computed
gradients. Pipeline parallelism has an ability to delegate different
layers to LLMs to different GPUs allowing the feasibility to
synchronize LLMs in the GPUs. Tensor parallelism splits model
tensor works by splitting model tensors across GPUs to create
distributed matrix operations to improve the efficiency. One more
technique proposed in ZeRO optimizes memory by storing chunks of
the LLM in each GPU and later sharing the rest when there is a
demand [ 50 ]. The next hyperparameter is the learning rate which
controls the model weights which responds depending on the errors
received in each step. Tuning the learning rate gradually during the
warm-up phase stabilizes the LLM as the model weights are assigned
randomly [ 53 ]. After warm-up the learning rate gradually decreases as
the training is in progress using adaptive optimizers to ensure
convergence and performance without overfitting or resource overuse.
These techniques are a definite requirement for the B5G networks to
scale and support them [ 54 ].
```
**4.3 Fine-Tuning LLMs**

The next step while deploying the LLMs is the fine-tuning process, and it
refers to adapting a pretrained LLM from the previous step to perform
domain-specific tasks like network traffic prediction, load balancing, and
anomaly detection in B5 networks effectively [ 55 ]. Though LLMs are
pretrained with large-scale datasets and have extensive understanding, fine-
tuning the model with curated telecom data, the LLMs become efficient to
understand technical language, operational patterns, and network behavior.
Fine-tuning bridges the gap between the pretrained LLMs and domain-
specific tasks to perform specialized functions such as interpreting KPIs,
logs, or slice configurations [ 54 ]. It enhances both accuracy and relevance
in tasks like SLA monitoring and proactive fault analysis and enables a
context-aware model. As B5 networks evolve toward 6G networks, fine-
tuning will become central to intelligent automation. Two major strategies
for fine-tuning LLMs are instruction tuning and alignment tuning [ 56 ].

- Instruction Tuning: Instruction tuning is a supervised fine-tuning method
    which enhances LLMs by training them using natural language prompt


```
instructions, for instance, here some specific telecom related tasks, for
example, prompts like “forecast load on sector X at 5 PM” or “generate a
config to offload eMBB traffic” to teach the model task-specific behavior
to the LLMs [ 57 ]. This approach improves the generalization even on
unseen prediction or optimization problems in the network across
multiple tasks increasing the adaptability and to follow diverse
instructions. Instruction-tuned models in the B5 networks can respond to
situations like policy queries, log summaries, fault classification, and
network planning tasks without needing retraining because the training
data for instruction tuning consists of tasks described in natural language,
an input (optional), the expected output, and sometimes few-shot
examples [ 58 ]. Existing models like InstructGPT and GPT-4 prove that
instruction tuning significantly improves task generalization particularly
useful in multitask scenarios across RAN, edge, and core domains. In
B5G networks the demands change dynamically and instruction tuning
when we use quality and diverse instructions can help LLMs to adapt in
real time and yield strong reliable results [ 59 ].
```
- Alignment Tuning: Alignment tuning ensures that the outputs reflect the
    expectations, minimizing the issues of generating biased, harmful, or
    misleading content and when used in B5 networks produce outputs that
    are safe, reliable, and operator-aligned. Reinforcement Learning from
    Human Feedback (RLHF) is a three-phase process using human
    annotations and a reward model to reflect human preferences. These
    annotations are vital in domains like automated reconfiguration, anomaly
    response, or customer interactions [ 57 ]. Alternative methods like Direct
    Preference Optimization (DPO) help encode human judgment into the
    model by creating a separate reward model and align the LLM directly
    with human preference data and fine-tune the LLM. Another advantage
    of DPO is less resource-intensive, but an important requirement is the
    dataset should be of high quality. This type of fine-tuning can create
    interpretable B5 systems, and while using in predictive analytics,
    alignment tuning ensures the model highlights uncertainty, avoids
    hallucinations, and explains its reasoning [ 60 ]. This explainability can
    create trustable LLMs in AI-driven decisions like traffic offloading or
    emergency rerouting [ 61 ].

**4.4 Evaluation Metrics for LLMs**


Evaluating LLMs is a multifaceted task that incorporates both the
quantitative and qualitative assessments. In the B5 networks and beyond the
scenarios like predictive traffic analytics or service orchestration, model
evaluation becomes crucial to ensure safety, reliability, and operational trust
while using GenAI like LLMs. The first metric is the accuracy which is a
basic metric, which can measure how LLM can perform on specific tasks
like log interpretation, traffic load prediction, or policy generation.
Benchmarks related to B5 systems like telecom-specific datasets can be
used [ 62 , 63 ]. The next important evaluation criterion is hallucination,
which reflects how often the LLMs generate incorrect or fabricated
prediction. This factor is very important in B5 systems where incorrect
predictions or recommendations can lead to QoS degradation or SLA
violations. Standard metrics like ROUGE or BERTScore are often
insufficient; newer metrics like AlignScore are being developed to address
this. Further factual consistency is essential for summarization of network
events or logs so that the LLMs reflect actual conditions rather than
plausible-sounding fabrications [ 64 , 65 ]. Evaluating outputs against
structured datasets and using expert verification is essential in telecom use
cases. Another critical factor to evaluate the efficiency is to assess the
resource usage during both the training and inference phases. This metric
includes the GPU hours, memory footprint, energy consumption, and
latency particularly when deployed at the edge and in real-time scenarios
like predictions. Latency and throughput are important in MEC-based
deployments. The time to response for forecasting traffic or updating slice
configs directly affects network responsiveness [ 66 , 67 ]. The scalability of a
deployed LLM is measured on the response to real-time scenarios like a
network spike or a user mobility situation. Moreover human alignment, a
factor considered by receiving human feedback, is more valuable to confirm
expectations, safety, and ethical rules. An emerging metric is the
interoperability, because B5 networks are highly regulated sectors. LLMs
must explain their reasoning for forecasting a surge in traffic or suggesting
a configuration change. The final metric would be the domain
transferability that assesses the LLM’s ability to generalize from base
knowledge to new telecom tasks with minimal data, crucial for tasks like
zero-shot log analysis or traffic classification [ 68 – 71 ].

**4.5 LLM Deployment in B5G Networks**


Before understanding and deploying LLMs for the Proactive Load
Balancing Strategies in the B5G networks, exploring practical deployment
at different layers for varied applications is mandatory. For example, some
IoT deployments might need edge-based LLMs, and we know IoT devices
are power and resource constrained, while LLMs have significant demands
for computational power and memory. Strategic LLM deployments are
necessary in the B5G networks to balance accuracy, latency, and resource
consumption. Now we will explain the different deployment options for
LLMs in the B5G networks [ 15 ].

- Cloud LLM Deployment: Deploying LLMs in centralized cloud systems
    is convenient and highly compatible for the B5G network architecture,
    for handling complex reasoning tasks and large-scale training, and does
    not need any additional hardware at the network edge. The downsides are
    they introduce high latency and incur significant bandwidth costs,
    particularly for latency-sensitive applications like autonomous driving or
    UAV control. Moreover applications like IoT have multimodal data and
    experiences bottleneck when there are real-time services [ 15 ].
- Edge LLM Deployment: In contrast to the cloud-based LLM
    deployments, LLMs at the edge (e.g., base stations or edge cloud nodes)
    help reduce response times and alleviate bandwidth consumption, but
    they face limitations in computing power and memory. Techniques like
    model compression, parameter sharing, and quantized training can
    address the limitations mentioned here. For analyzing more data at the
    edge, models like parameter-efficient fine-tuning and split learning are
    effective in enabling edge-based LLMs. Furthermore, edge-based LLM
    systems for the B5G networks can ensure faster local decision-making,
    crucial for mission-critical applications like in IoT, URLLC, and any
    other context-aware services [ 7 , 72 ].
- On-Device LLM Deployment: On-device LLMs unlike the cloud- and
    edge-based deployments provide low latency and suit privacy-sensitive
    applications. Although the computing power poses challenges, recent
    advancements like Qualcomm’s Snapdragon 8s Gen 3 and Google’s
    Gemini Nano platform support smaller models like LLaMA 2 and
    LLaMA 3 for mobile inference. Split learning process also allows
    learning across cloud and edge for complex tasks and lightweight tasks
    on the device. These developments are still at infancy and need more


```
model size reductions and hardware optimization for widespread
adaptations [ 15 ].
```
- Cache-Based LLM Deployment: Cache-based ones are executed
    depending on precision and function across cloud, edge, or on the device.
    While the complete LLMs can be stored at the cloud, a quantized version
    can be at the edge and a federated model on the device for ease of
    deployments. This kind of hierarchal approach optimizes storage,
    reduces inference latency, and supports easier task relocation. This type
    of deployment needs careful synchronization and coordination of model
    updates and also needs fine-tuning for operational efficiency.
- Collaborated LLM Deployment: Collaborated deployment is a hybrid
    strategy where the edge and cloud LLM models interact in real time to
    assess data in real time and query to complete cloud LLMs when needed.
    Periodic updates on the edge model from the cloud LLMs based on real-
    time scenarios without constant data transfer can reduce the overhead
    and support dynamic, on-demand intelligence. Some experimental
    studies show reduced latency and better reliable predictions using this
    collaborative approach [ 73 ].

**5 B5G Network Traffic Prediction with LLMs**

With a wide range of emerging applications with B5G networks, they
continue to develop as an ultradense, service-aware infrastructure. So the
need for the ability to predict traffic patterns in real time becomes
paramount for the network providers for maintaining the quality of service
(QoS) for the customers, minimizing latency, and ensuring proactive
network adaptation [ 71 ]. GenAI especially LLMs has inherent emergent
qualities like Capturing Long-Term Dependencies Scalability and Self-
attention Mechanism brings new predictive capabilities to the forefront of
traffic analytics by combining historical trends, semantic context, and real-
time telemetry. In this section we explore how LLMs can be leveraged to
predict traffic behavior, simulate edge conditions, generate explainable
outputs, and operate efficiently at the edge in the B5G networks. Figure 2
shows how LLMs can assist in network optimization in the B5 overall
approach on how LLMs can assist in network optimization in the B5G
networks.


**_Fig. 2_** How LLMs can assist in network optimization in the B5G networks

**5.1 Modeling B5G Real-Time Traffic with LLMs**

GenAIs particularly with the advent of LLMs can redefine the modeling
real-time traffic behavior of B5G networks. Unlike the traditional AI
methods, LLMs have the ability to analyze multimodal data like textual
logs, numerical KPIs, temporal patterns, and user behavior semantics—into
a unified traffic forecasting framework. Furthermore when LLM is trained
with domain-specific datasets, they can interpret system logs, thereby
predict load depending the service type (e.g., eMBB, URLLC, mMTC)
being served, and forecast traffic surges by analyzing contextual signals.
Recent research works show that fine-tuned transformer-based LLMs (such
as GPT-style models or T5 variants) can ingest structured and unstructured
network data to forecast spatiotemporal load distributions [ 74 ]. Models like
Mobile-LLaMA have functions like packet analysis and IP routing analysis
which can provide network analysis and enable LLMs for B5G network
modeling [ 75 ]. These models can be prompted with queries like “predict
traffic in Sector A for the next 30 minutes” or “estimate eMBB traffic
demand at edge node X” to yield actionable forecasts. They also have the
feasibility to support multi-hop reasoning, enabling complex traffic
scenarios in the B5G network such as emergency handovers or multicell
load balancing (Fig. 3 ).


**_Fig. 3_** A prompt example suitable for modeling real-time traffic using LLMs in B5G networks
There are many situations that are zero-shot scenarios in LLMs
referring to tasks where the LLM is asked to predict in a new task and
solely relies on the pretrained knowledge and generalization abilities to
respond appropriately. In these conditions LLMs simulate a scenario of
“what-if” to aid in decision-making, for example, network operators can use
a prompt like “simulate traffic if a new AR application is launched in zone
Y” or “forecast impact of drone-based inspection on URLLC slice latency.”
The traditional forecasting methods are not designed to capture such
spatiotemporal variations under new traffic, leading to poor traffic load
models. In contrast LLM uses context-rich embeddings derived from LLMs
to model user behavior, application semantics, and network configurations.
This is enabled by the fact that the LLMs forecast the load demand by
combining the historical patterns with the real-time telemetry data. These
simulations will help the network operators to proactively reconfigure
deployed network load or deploy edge compute capacity proactively to
mitigate overload conditions. The LLMs can understand from the in-
contextual learning from system logs, usage reports, and operator policies
and predict with a higher accuracy increasing the system reliability unlike
the static rule-based systems [ 76 ].

**5.2 Synthetic Data Generation for Traffic Model Training**

Although LLMs pose to be advantageous, one of the major issues in
predicting the traffic load is the lack of sufficient, diverse, and labeled


training data. In the B5G networks due to privacy issues, varying
infrastructure and evolving new service portfolios cap the availability of
domain-specific representative datasets. Moreover, human-generated data is
inherently susceptible to biases and errors, and thereby they are not optimal
for B5 networks model training or evaluation. One of the foundational
qualities of LLMs profound instruction-following abilities provides reliable
control and adaptable situation for creating domain-specific datasets. The
general procedure to create an LLM-driven synthetic data generation is
systematically organized into three main stages: Generation, Curation, and
Evaluation. GenAI provides an efficient solution called as the synthetic data
generation using models like diffusion models or generative transformers
[ 77 ]. Diffusion models can simulate realistic traffic profiles by learning the
distribution of historical traffic by fine-tuning under new conditions. For
instance, there are scenarios like impact of a high-speed train entering a cell
area or simulate burst traffic during a viral app launch, and synthetic data is
used to augment real-life situations. This enables to create robust LLMs that
generalize better to rare and unseen scenarios. Furthermore one more vital
use of synthetic data is to test the robustness of LLMs in mission-critical
scenarios like public safety networks or autonomous vehicle scenario,
thereby reducing the risk of bias or overfitting in predictive tasks [ 78 ].
Future directions in LLM-driven synthetic data generation should have
more emphasis on enhancing autonomy, knowledge, collaboration, and
human integration.

**5.3 Explainable Traffic Forecasting with LLMs**

LLMs-based traffic load prediction models are deployed in autonomous
network operations, and the interpretability from the predictions is
paramount [ 79 ]. LLMs can enhance the explainability by generating
human-readable explanations for their predictions, enabling network
operators to trust and validate AI-driven decisions. Generally, traffic
prediction models have the ability to predict the traffic score to understand
the real-time scenario unlike LLMs which can give an output with
contextual insights such as “Traffic in Sector 9 is expected to increase by
45% due to a regional festival and a live-streaming event.” “User session
duration has increased for eMBB users, which may indicate a growing
demand for high-bandwidth applications.” These human-readable
explanations help the operators to understand the current situation more


clearly making the forecasts actionable. Generating a prompt-based model
can allow the operators to interact with the model with queries like “Why is
sector X projected to be overloaded tomorrow?” enabling a breakdown with
a breakdown of causal factors in a conversational and interpretable manner.
This interpretability creates a collaborative human-AI decision-making and
enhances accountability in network management [ 79 ].

**6 Proactive Load Balancing Strategies in the B5G**

**Networks**

Service optimization in B5G networks refers to the intelligent orchestration
of network resources to deliver consistent, high-quality user experiences to
all domains. To deliver such QoS anticipating traffic demands, adapting
resource allocations in real time and minimizing latency are dire
requirements. In the previous section we discussed about the traffic
predictions in real time, and in this section we will explore the proactive
Load Balancing strategies in the B5G networks to ensure service
optimization shifts from reactive troubleshooting to proactive, data-driven
network management. With the massive proliferation of user devices, IoT
devices, immersive services, and dynamic mobility patterns, maintaining
the B5G deployments across various levels (radio, edge, and core) becomes
a challenge. LLMs can support proactive load balancing by transforming
load management into a predictive, context-aware, and autonomous
function of the B5G network [ 80 ].

**6.1 Principles of Load Balancing in B5G Networks**

The intense dynamic nature of B5G network traffic, due to prevailing use
cases like connected vehicles, smart manufacturing, and extended reality
(XR), and many more to come in the future, demands load balancing to be
fine-grained and proactively adaptive. Load balancing in B5G networks
aims to secure optimal use of the network resources while minimizing
service latency and preventing congestion. Load balancing spans across
multiple hierarchal layers at the RAN, and the main focus is on the beam
management, handover optimization, and carrier aggregation. At the core of
the B5G network the main target is to intelligently balance workloads
among Virtual Network Functions (VNFs) and at the edge level balance


load distribution among edge nodes, scheduling tasks based on latency
needs and load demand. There is a need to interpret context from
multisource telemetry, logs, and semantic network data, and GenAI models
especially the LLMs can perform well in this scenario [ 81 ].

**6.2 LLM-Powered Load Management**

The need for GenAI models, particularly LLMs with their unprecedented
capabilities for real-time and proactive load management, can aid with the
load balancing problems in B5G networks. Analyzing the historical data
and capturing patterns, diagnosing the ongoing telemetry, user behavior
trends, and network policies, LLMs can proactively forecast congestion and
suggest preemptive load distribution strategies precisely. For instance, an
LLM can be queried with a prompt such as “Predict the optimal offload
ratio for eMBB traffic in Sector A at peak hour,” and it can generate not
only the forecast but also configuration suggestions in human-readable or
machine-executable formats. To add to these abilities, LLMs can automate
network slice configuration generation using the NL input which can
eventually reduce the manual operation of network operators. Research
works focus on multi-agent LLM frameworks to coordinate load
management across multiple base stations and edge nodes, dynamically
adjusting configurations such as antenna tilts, cell range expansion
parameters, and beam directions. This kind of intelligent SON is the path to
envision an autonomous B5G networks and can reduce the current rule-
based systems which lack adaptive nature and their major role as central
agents in zero-touch network automation [ 81 ].

**6.3 Federated and Distributed Load Optimization**

Centralized LLMs face an issue due to data privacy and communication
overhead in B5G networks. Federated learning with LLM integration would
be a solution to the load optimization problem. In this prototype LLMs are
fined-tuned for B5G networks and adapt locally at base stations or MEC
servers, learning from localized data without needing to share real-time data
with a centralized server. Federated models share periodic new updates or
patterns in the traffic with the central orchestration agent, so the LLMs can
be updated globally. This framework can achieve user privacy while
providing a collaborative learning. Recent work in [ 82 ] demonstrates that
distributed LLM cooperation using transformer-based foundation models


can yield substantial performance improvements. Similar works like
EdgeFM deployment show up to 34.3% gain in prediction accuracy and a
threefold reduction in end-to-end latency. By training the federated model at
the edge in real-time and coordinating with the powerful centralized cloud
model, LLMs can achieve scalable, low-latency decision-making for real-
time load distribution. For applications like IoT which holds sensitive data,
need privacy federated LLMs which can enable faster decision making
while securing the privacy issues [ 86 ].

**6.4 Real-Time Policy Adaptation via LLMs**

Beyond the forecasting and prediction capabilities of LLMs in the B5G
networks, one of the core values is load balancing with its ability to
generate, revise, and enforce network policies in real time. Traditional
methods are predefined, static, and unable to keep up with the dynamic
fluctuations of load demand in the B5G networks. On the contrary LLMs
can be given prompts like “Ensure ultra-reliable communication for drone
traffic over Zone B for the next 20 minutes”—and can translate this into
actionable configurations across the RAN, core, and transport domains.
This emergent capability of LLMs can enable the evolution of Intent-Based
Networking (IBN) where the network automatically configures, manages,
and optimizes itself by defining goals rather than rules. Furthermore LLMs
can detect the policy violations, recommend corrections, and simulate the
outcomes of policy changes before deployment. Creating eXplainable,
transparent policies for load balancing in the B5G networks can move it
closer to self-adaptive, SLA-compliant operation [ 75 ].
Altogether LLM-powered load balancing in B5G networks can shift
them from reactive resource allocation to proactive, intelligent
orchestration. Along with reliable predictive insights and dynamic policy
generation across the RAN, edge, and cloud layers, B5G network operators
can deliver high QoS for the customers while optimizing the operational
efficiency. Future research work can focus on multi-agent cooperation and
hybrid prompting-reinforcement systems for a more integrated B5G
network.

**7 Integrating LLMs into B5G Network**

**Architecture**


Before discussing the emergent qualities of the LLMs for traffic prediction
and load balancing, the feasibility to integrate LLMs with the B5G network
is predominant. Beyond theoretical capabilities of LLMs the viability to
embed them in the operational components such as the Radio Access
Network (RAN), transport, and core layers is important. In this section, we
discuss three key architectural interfaces where LLMs can be integrated in
the B5G Network Architecture at various levels like the RAN Intelligent
Controller (RIC) and xApps, core and transport layer optimization, and
SDN/orchestration interfaces. Figure 4 shows an overall LLM deployment
scenario with various prompts for service optimization in the B5G
networks.

**_Fig. 4_** An overall LLM deployment scenario with various prompts for service optimization in the
B5G networks

**7.1 Radio Access Network Intelligent Controller (RIC) and**

**xApp Integration**

Open RAN (O-RAN) refers to a disaggregated RAN architecture allowing
participation of multi-vendor equipment, and software to interoperate needs
a key enabler like the Radio Access Network Intelligent Controller (RIC).
The RIC is a fundamental component of the O-RAN architecture, designed
to bring intelligence, flexibility, and openness to the RAN layer of mobile


networks. Based on the previous discussions on the diverse B5G network
traffic types, we need a Near-Real-Time RAN Intelligent Controller (Near-
RT RIC) in the O-RAN architecture. xApps are modular, pluggable,
software application designed to run on the Near-RT RIC that interacts with
RAN nodes via open interfaces (like the E2 interface) to perform tasks such
as handover control, interference management, traffic steering, and load
balancing [ 83 ]. XApps can enable a near-real-time response of around 10
ms to 1 sec making them crucial for optimizing radio resource management
and user experience in 5G and B5G networks. LLMs can be embedded into
the Near-RT RIC as xApps for training and policy tuning in the B5G
network. For example, an LLM-aided xApp can forecast traffic spikes
based on the service type and user mobility and has the ability to
dynamically instruct beam management or handover control modules.
Further, LLMs can be used in Non-RT RIC to collect long-term KPI data
and reallocate the resource strategies across the deployed multiple
gNodeBs. Moreover, another important feature is deploying multi-agent
LLMs which can integrate multiple xApps in parallel, each handling
distinct tasks such as energy optimization, interference mitigation, or slice
management. LLMs in the RIC framework can enable closed-loop control
with minimal latency and maximal network awareness, promoting adaptive
and self-organizing network behavior [ 83 ].

**7.2 Core and Transport Layer Optimization**

The next layer in the B5G network is the core network and transport layers
allowing intelligent session management, routing, and traffic engineering.
LLMs can be integrated into the core network and transport layers to
anticipate congestion in the user or control plane or trigger dynamic
reconfiguration of service chains. For example, an LLM can learn from the
historic patterns on traffic surges during large-scale events and proactively
migrate sessions to a less utilized data center. In the transport layer, LLMs
can optimize the path selection by combining telemetry data with service-
level intent, ensuring compliance with latency and jitter constraints. Using
inferences from LLMs traffic-aware paths and packet scheduling can also
be dynamically tuned. GenAI models can aid in NFV resource pooling,
reduce handoff failures, and optimize gateway selection, contributing to
robust end-to-end network performance [ 75 ].


**7.3 Software-Defined Networking (SDN) and Orchestration**

**Interfaces for Closed-Loop Control**

Software-Defined Networking (SDN) provides a programmable interface to
manage network flows for the B5G network but needs feedback in real time
[ 84 ]. LLMs can manipulate in real time based on predictive analytics, while
they are integrated with frameworks like Open Network Automation
Platform (ONAP) which can translate the high-level operator intentions into
low-level SDN rules. This allows for rapid provisioning, automatic fault
correction, and resource scaling. For example, an operator could issue a
prompt like “Ensure 50ms latency for VR users in region Y,” and the LLM
engine can change the flow rules and VNFs to meet the request. Dynamic
update of policies can be enabled by using closed-loop control by feeding
back telemetry data to the LLM. LLMs also have the ability to act as policy
generators and auditors, providing explainable AI-driven recommendations
for network reconfigurations in intent-based networking scenarios. This
type of integration allows embedded predictive intelligence in the day-to-
day network automation workflows making it more operationally viable and
proactive [ 84 ].
Together, with all these architectural changes, B5G networks can evolve
in to a more intelligent, adaptive infrastructure. Predictive models can be
embedded in daily workflows, rather than as separate standalone tools
especially in the service optimization part. The integration of LLMs in the
core B5G networks turns them into self-optimizing entities capable of
meeting dynamic service-level demands across radio, edge, and cloud
domains.

**8 Challenges and Future Outlook for LLMs in**

**B5G Networks**

The intersection of LLMs with the B5G networks holds immense
advantages for service optimization, yet there are some challenges that need
to be managed to ensure a safe, efficient, and scalable deployment. While
LLMs can be used for predictive analytics for network load, load balancing
and closed-loop control are significant applications, and the operational
feasibility of deploying LLMs brings many technical and ethical challenges.


In this section we will discuss the challenges and the future outlook in detail
[ 15 , 64 , 85 – 87 ].

**8.1 Challenges**

- Latency: The first and foremost challenge that integration of LLMs with
    B5G networks incurs is the millisecond response times faced by mission-
    critical applications like URLLC services. High latency time can cause
    delays and can compromise service quality and delays in decision-
    making resulting in service degradation. Edge deployment can reduce the
    latency issues to an except but suffers due to limited hardware resources
    constrain model size and performance. Techniques like quantization and
    pruning can reduce the latency issues but may not be in accurate in
    predictions. Moreover LLM sequential processing adds to latency when
    the prompts are long complicating latency-sensitive applications.
    Consistent low-latency performance still remains a core challenge in
    making LLMs suitable fit for B5G networks adapting for real-time
    applications.
- Hallucination: Another challenging factor in the LLMs is the
    hallucination issue, which might generate a factually incorrect or an
    unreliable prediction. In the context of service optimization less reliable
    predictions can cause errors in network configurations even leading to
    network failures. There might be model drift when the accuracy of a
    trained LLM degrades over time due to changes in traffic patterns, user
    behavior, or service types and needs fine-tuning. There are some
    strategies to adapt and to avoid hallucination which includes continuous
    learning, updating datasets with new data, and domain-specific fine-
    tuning to mitigate hallucination risks and also require careful control to
    avoid catastrophic forgetting or unintended behavior.
- Explainability: Though we know eXplainability is one of the emergent
    feature of the LLMs, it still remains a fundamental concern in the
    adoption of LLMs in the B5G networks. LLM predictions or
    recommendations for the optimizations sometimes are tough to interpret
    by the network operators due to the lack of transparent reasoning. This
    hindrance can reduce the confidence in the predictions especially in
    situations like real-time load balancing. Existing techniques like Chain-
    of-Thought prompting aim to provide a perception to the LLM behavior,
    and there arises a growing need for standardized interpretations of the


```
eXplainable solutions proposed by the LLMs and some visualization
tools for the same.
```
- Resource Constraints: The next major hurdle to implement LLMs widely
    across the B5G networks is the resource constraints to handle LLMs.
    Running LLMs incur large compute resources and energy which is
    always limited at the edge. Storage capacity of the network elements at
    the edge is another constraint while using models like GPT-4 or LLaMA
    3-70B reaching sizes of tens or even hundreds of gigabytes. Storage,
    compute, and energy constraints at the edge make it harder to deploy
    LLMs at the edge, but techniques like compression, quantization, and
    other hybrid deployment strategies make it possible. A fine balance is the
    need of the moment while integrating LLMs in the networks to maintain
    the model and the operation.
- Data Availability: The next pressing problem before widespread LLM
    adaptation with the B5G networks is the data availability for LLM
    training. Unlike the traditional LLMs that normally train with vast web-
    scale data, telecom-based domain-specific datasets are scarce, and the
    data is generally sensitive which makes less public availability. There are
    regulatory procedures and privacy concerns when we deal with telecom
    domain-specific data making it difficult to procure large and diverse data.
    Fine-tuning LLMs for telecom domain thereby might result in some
    misalignment and underfitting which can be overcome by some vendors,
    operators, and researchers by sharing datasets that can support safe and
    effective LLM adaptation.
- Scalability and Real-Time Adaptability: Scalability and Real-Time
    Adaptability are two critical concerns while deploying LLMs with the
    B5G networks. The B5G networks expect a dynamic growth and need to
    scale a millions of connected devices, and LLMs need to handle a
    diverse data traffic and types in real time. However with the existing
    LLMs issues like latency, context switching, and resource contention
    while operating in large-scale networks persist. Furthermore, LLMs
    require offline fine-tuning or batch updates making some limited
    adaptability to real-time updates making it difficult for real-time
    predictions. Though models like incremental learning, streaming
    inference, and fine-grained context management are in research, until
    they become operational, scalability and real-time adoption remain an
    open challenge.


- Energy Efficiency: Generally GenAI models are facing a growing energy
    consumption issues worldwide. Sustainability is one of the core
    principles of the B5G and the future 6G networks, and LLMs consume a
    lot of power regardless of their deployment at the edge or cloud. The
    performance of LLMs in the B5G networks along with the reduction of
    energy costs can be achieved by enabling inferences across a distributed
    set of base stations or MEC nodes and techniques like pruning and low-
    power hardware accelerators. The main challenge is to cater the varied
    traffic types like mMTC, uRLLC, in real time and also in resource-
    constrained scenarios.

**8.2 Future Outlook**

There are several emerging future developments that can reshape the
current LLMs to be more suitable for the B5G networks. For instance, the
progress in the multimodal LLMs can help in interpreting text, time-series
data, and even signal patterns in the B5G networks and enables advanced
decision-making. LLM-enabled service optimization can help the network
operators to plan and predict deployment schedules and policies also.
Furthermore LLM-based service optimization will take adaptive measures
for allocating bandwidth, compute, and spectrum based on real-time traffic
patterns and service demands. The conventional pipelines in the B5G
networks can overcome the shortcomings of traditional AI techniques with
LLM-enabled ML and can improve the adaptability, robustness, and
interpretability. Advancements in model compression can help deploy the
LLMs at the edge or even mobile devices for faster decisions reducing the
latency. Additionally pioneering techniques like Retrieval-Augmented
Generation (RAG) a hybrid AI architecture can extract contextual
information using real-time data. Future LLM tools can generate interactive
dashboards that allow network operators to understand, audit, and guide
model behavior easily. Finally the lightweight LLM development can
support widespread adoption even in resource-constraint environments like
IoT. Together with LLM as the building blocks the B5G networks can be
deployed as intelligent and autonomous suiting the future needs.

**9 Conclusion**


In conclusion, LLMs have emergent abilities to unlock a powerful new
layer of intelligence in the B5G networks for service optimization, but still
in the research stages. Addressing all the challenges discussed in the
previous section will be a stepping stone to deploy a trustworthy and
autonomous B5G networks. A continuous collaborative research work
between the GenAI researchers, network engineers, and operators can
unleash the full potential of GenAI in next-generation networks. The future
telecom systems cannot operate without the support of LLMs due to the fact
that the network traffic is increasingly dynamic and service demands more
heterogeneous. Intelligent decision-making empowered by the LLMS
across RAN, edge, and core layers creates self-organizing and optimizing
networks. Despite the fact that LLMs have many advantages, there are
hurdles like scalability, explainability, and reliability that need to be
overcome. The future B5G networks do not envision only speed and lower
latency and also enhance the network ability to think, learn, and act
proactively and can be made possible by fine-tuning LLMs and their
deployments.

**References**

1. Z. Zhang, Y. Xiao, Z. Ma, M. Xiao, Z. Ding, X. Lei, G.K. Karagiannidis, P. Fan, 6G wireless
    networks: vision, requirements, architecture, and key technologies. IEEE Veh. Technol. Mag.
    **14** (3), 28—41 (2019)
    [Crossref]
2.
    I. T. U. (ITU), IMT towards 2030 and beyond (2023). https:// [http://www.](http://www.) itu. int/ en/ ITU-R/ study-
    groups/ rsg5/ rwp5d/ imt-2030/ Pages/ default. aspx, last accessed 2025/05/01
3.
    J. Du, C. Jiang, J. Wang, Y. Ren, M. Debbah, Machine learning for 6G wireless networks:
    carrying forward enhanced bandwidth, massive access, and ultrareliable/low-latency service.
    IEEE Veh. Technol. Mag. **15** (4), 122–134 (2020)
    [Crossref]
4.
    H. Zhou, M. Erol-Kantarci, H.V. Poor, Knowledge transfer and reuse: a case study of AI-enabled
    resource management in RAN slicing. IEEE Wirel. Commun. **30** (5), 160–169 (2022)
    [Crossref]
5.
    I. Shayea, M. Ergen, M.H. Azmi, S.A. Çolak, R. Nordin, Y.I. Daradkeh, Key challenges, drivers
    and solutions for mobility management in 5G networks: A survey. IEEE Access **8** , 172534–
    172552 (2020)
    [Crossref]


6.
C. Luo, J. Ji, Q. Wang, X. Chen, P. Li, Channel state information prediction for 5G wireless
communications: A deep learning approach. IEEE Trans. Network Sci. Eng. **7** (1), 227–236
(2018)
[MathSciNet][Crossref]
7.
Z. Lin, G. Qu, Q. Chen, X. Chen, Z. Chen, K. Huang, Pushing large language models to the 6G
edge: vision, challenges, and opportunities. arXiv preprint arXiv:2309.16739 (2023)
8.
P. Popovski, K.F. Trillingsgaard, O. Simeone, G. Durisi, 5G wireless network slicing for eMBB,
URLLC, and mMTC: A communication-theoretic view. IEEE Access **6** , 55765–55779 (2018)
[Crossref]
9.
K. David, H. Berndt, 6G vision and requirements: is there any need for beyond 5G?. IEEE Veh.
Technol. Mag. **13** (3), 72–80 (2018)
[Crossref]
10.
H. Zhang, N. Liu, X. Chu, K. Long, A.H. Aghvami, V.C. Leung, Network slicing based 5G and
future mobile networks: mobility, resource management, and challenges. _IEEE Commun.
Mag._ **55** (8), 138–145 (2017)
[Crossref]
11.
A. Anand, G. De Veciana, S. Shakkottai, Joint scheduling of URLLC and eMBB traffic in 5G
wireless networks. IEEE/ACM Trans. Networking **28** (2), 477–490 (2020)
[Crossref]
12.
J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F.L. Aleman, D. Almeida, et al., GPT-4
technical report. arXiv preprint arXiv:2303.08774 (2023)
13.
J. Devlin, M.W. Chang, K. Lee, K. Toutanova, BERT: pre-training of deep bidirectional
transformers for language understanding. Proc. NAACL-HLT **1** , 4171–4186 (2019)
14.
A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, et al., PaLM:
scaling language modeling with pathways. J. Mach. Learn. Res. **24** (240), 1–113 (2023)
15.
H. Zhou, C. Hu, Y. Yuan, Y. Cui, Y. Jin, C. Chen, H. Wu et al., Large language model (LLM) for
telecommunications: a comprehensive survey on principles, key techniques, and opportunities, in
_IEEE Communications Surveys and Tutorials_ (2024)
16.
W.X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min et al., A survey of large language
models, vol. 1(2). arXiv preprint arXiv:2303.18223 (2023)
17.
L. Bariah, H. Zou, Q. Zhao, B. Mouhouche, F. Bader, M. Debbah, Understanding telecom
language through large language models, in _IEEE GLOBECOM_ (2023), pp. 6542–6547
18.
A.K. Bairagi, M.S. Munir, M. Alsenwi, N.H. Tran, S.S. Alshamrani, M. Masud, Z. Han, C.S.
Hong, Coexistence mechanism between eMBB and uRLLC in 5G wireless networks. IEEE
Trans. Commun. **69** (3), 1736–1749 (2020)
[Crossref]
19.
ITU-R, Minimum requirements related to technical performance for IMT-2020 radio interface(s),
in _Report ITU-R M.2410-0_ (2017)


20.
B.S. Khan, S. Jangsher, A. Ahmed, A. Al-Dweik, URLLC and eMBB in 5G industrial IoT: a
survey. IEEE Open J. Commun. Soc. **3** , 1134–1163 (2022)
[Crossref]
21.
N.A. Mohammed, A.M. Mansoor, R.B. Ahmad, Mission-critical machine-type communication:
An overview and perspectives towards 5G. IEEE Access **7** , 127198–127216 (2019)
[Crossref]
22.
S.F. Abedin, M.G. AlamR., S.A. Kazmi, N.H. Tran, D. Niyato, C.S. Hong, Resource allocation
for ultra-reliable and enhanced mobile broadband IoT applications in fog network. IEEE Trans.
Commun. **67** (1), 489–502 (2018)
23.
B. Da, M. Carugi, Representative use cases and key network requirements for network 2030, in
_Representative use cases and key network requirements for network, 2030_ (2020)
24.
X. You, C.X. Wang, J. Huang, X. Gao, Z. Zhang, M. Wang, Y. Huang, et al., Towards 6G
wireless communication networks: vision, enabling technologies, and new paradigm shifts. Sci.
China Inf. Sci. **64** , 1–74 (2021)
[Crossref]
25.
A.I. Salameh, M. El Tarhuni, From 5G to 6G—challenges, technologies, and applications. Future
Internet **14** (4), 117 (2022)
26.
H. Zhou, M. Erol-Kantarci, Y. Liu, H.V. Poor, A survey on model-based, heuristic, and machine
learning optimization approaches in RIS-aided wireless networks. IEEE Commun. Surv.
Tutorials **26** (2), 781–823 (2023)
[Crossref]
27.
Y. Du, H. Deng, S.C. Liew, K. Chen, Y. Shao, H. Chen, The power of large language models for
wireless communication system development: A case study on FPGA platforms. arXiv preprint
arXiv:2307.07319 (2023)
28.
M. Xu, D. Niyato, J. Kang, Z. Xiong, S. Mao, Z. Han, D.I. Kim, K.B. Letaief, When large
language model agents meet 6G networks: perception, grounding, and alignment. _IEEE Wirel.
Commun._ **31** (6), 63–71 (2024)
[Crossref]
29.
Y. Shen, J. Shao, X. Zhang, Z. Lin, H. Pan, D. Li, J. Zhang, K.B. Letaief, Large language models
empowered autonomous edge AI for connected intelligence. IEEE Commun. Mag. **62** (10), 140–
146 (2024)
[Crossref]
30.
S. Booth, W.B. Knox, J. Shah, S. Niekum, P. Stone A. Allievi, The perils of trial-and-error
reward design: misdesign through overfitting and invalid task specifications. AAAI Conference
Artif. Intell. **37** (5), 5920–5929 (2023)
[Crossref]
31.
S. Long, J. Tan, B. Mao, F. Tang, Y. Li, M. Zhao, N. Kato, A survey on intelligent network
operations and performance optimization based on large language models, in _IEEE
Communications Surveys & Tutorials_ (2025)


32.
F. Liu, B. Farkiani, P. Crowley, A survey on large language models for network operations &
management: applications, techniques, and opportunities, in _Authorea Preprints_ (2024)
33.
G. Charan, M. Alrabeiah, A. Alkhateeb, Vision-aided 6G wireless communications: Blockage
prediction and proactive handoff. IEEE Trans. Veh. Technol. **70** (10), 10193–10208 (2021)
[Crossref]
34.
M. Matsuura, Y.K. Jung, S.N. Lim, Visual-LLM zero-shot classification, in _CRC Technical
Report_ (2023)
35.
M.U. Hadi, R. Qureshi, A. Shah, M. Irfan, A. Zafar, M.B. Shaikh, N. Akhtar, J. Wu, S. Mirjalili,
A survey on large language models: applications, challenges, limitations, and practical usage, in
_Authorea Preprints_ (2023)
36.
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A.N. Gomez, Ł. Kaiser, et al.,
Attention is all you need, in _NeurIPS_ (2017)
37.
N. Shazeer, Fast transformer decoding: One write-head is all you need. arXiv:1911.02150 (2019)
38.
J. Ainslie, J. Lee-Thorp, M. de Jong, Y. Zemlyanskiy, F. Lebrón, S. Sanghai, GQA: training
generalized multi-query transformer models from multi-head checkpoints. arXiv:2305.13245
(2023)
39.
J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, BERT: pretraining of deep bidirectional
transformers for language understanding. arXiv:1810.04805 (2018)
40.
Y. Liu, M. Ott, N. Goyal, J. Du, D. Joshi, D. Chen, O. Levy, et al., RoBERTa: a robustly
optimized BERT pretraining approach. arXiv:1907.11692 (2019)
41.
Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, R. Soricut, ALBERT: A lite BERT for
self-supervised learning of language representations. arXiv:1909.11942 (2019)
42.
C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, et al., Exploring the
limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res. **21** (140),
1–67 (2020)
[MathSciNet]
43.
M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V. Stoyanov, L.
Zettlemoyer, BART: denoising sequence-to-sequence pre-training for natural language
generation, translation, and comprehension. arXiv:1910.13461 (2019)
44.
H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, et al.,
LLaMA: open and efficient foundation language models. arXiv:2302.13971 (2023)
45.
M. Guo, Z. Dai, D. Vrandečić, R. Al-Rfou, Wiki-40B: multilingual language model dataset, in
_LREC_ (2020), pp.2440–2452
46.
F.F. Xu, U. Alon, G. Neubig, V.J. Hellendoorn, A systematic evaluation of large language models
of code, in _ACM SIGPLAN MAPS_ (2022), pp.1–25


47.
H. Laurençon, L. Saulnier, T. Wang, C. Akiki, et al., The BigScience ROOTS corpus: A 1.6 TB
composite multilingual dataset. NeurIPS **35** , 31809–31826
48.
I. Beltagy, K. Lo, A. Cohan, SciBERT: a pretrained language model for scientific text.
arXiv:1903.10676 (2019)
49.
T. Kudo, Subword regularization: Improving neural network translation models with multiple
subword candidates. arXiv:1804.10959 (2018)
50.
Y. Huang, Y. Cheng, A. Bapna, O. Firat, D. Chen, M. Chen, H. Lee, et al., GPipe: efficient
training of giant neural networks using pipeline parallelism. NeurIPS **32** , 103–112 (2019)
51.
Z. Li, S. Zhuang, S. Guo, D. Zhuo, H. Zhang, D. Song, I. Stoica, TeraPipe: token-level pipeline
parallelism for training large-scale language models, IN _ICML_ (2021), pp. 6543–6552
52.
S. Rajbhandari, J. Rasley, O. Ruwase, Y. He, ZeRO: memory optimizations toward training
trillion parameter models, _SC20_ (2020), pp.1–16
53.
M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, B. Catanzaro, Megatron-LM: training
multi-billion parameter language models using model parallelism. arXiv:1909.08053 (2019)
54.
J. Wei, M. Bosma, V.Y. Zhao, et al., Finetuned language models are zero-shot learners.
_arXiv:2109.01652_ (2021)
55.
L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, et al., Training
language models to follow instructions with human feedback. NeurIPS, **35** , 27730–27744 (2022)
56.
OpenAI, GPT-4V(ision) system card, in _OpenAI Technical Report_ (2023)
57.
D.M. Ziegler, N. Stiennon, J. Wu, T.B. Brown, A. Radford, D. Amodei, P. Christiano, G. Irving,
Fine-tuning language models from human preferences. arXiv:1909.08593 (2019)
58.
H.W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, Y. Li, et al., Scaling instruction-
finetuned language models. arXiv:2210.11416 (2022)
59.
P.F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, D. Amodei, Deep reinforcement learning
from human preferences, in _NeurIPS_ , vol. 30 (2017)
60.
R. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C.D. Manning, C. Finn, Direct preference
optimization: your language model is secretly a reward model. arXiv:2305.18290 (2023)
61.
C. Zhou, P. Liu, P. Xu, S. Iyer, J. Sun, Y. Mao, X. Ma, et al., LIMA: less is more for alignment.
arXiv:2305.11206 (2023)
62.
M. Gao, X. Wan, DialSummEval: revisiting summarization evaluation for dialogues, in _NAACL_
(2022), pp. 5693–5709
63.
C.-Y. Lin, ROUGE: a package for automatic evaluation of summaries, in _ACL Workshop_ (2004),
pp. 74–81


64.
T. Zhang, V. Kishore, F. Wu, K.Q. Weinberger, Y. Artzi, BERTScore: evaluating text generation
with BERT. arXiv:1904.09675 (2019)
65.
S. Samsi, D. Zhao, J. McDonald, B. Li, A. Michaleas, M. Jones, W. Bergeron, et al., From words
to watts: Benchmarking the energy costs of large language model inference, in _IEEE HPEC_
(2023), pp. 1–9
66.
J. McDonald, B. Li, N. Frey, D. Tiwari, V. Gadepally, S. Samsi, Great power, great
responsibility: recommendations for reducing energy for training language models.
arXiv:2205.09646 (2022)
67.
A. Faiz, S. Kaneda, R. Wang, R. Osi, P. Sharma, F. Chen, L. Jiang, LLMCarbon: modeling the
end-to-end carbon footprint of large language models. arXiv:2309.14393 (2023)
68.
Y. Chang, X. Wang, J. Wang, Y. Wu, L. Yang, K. Zhu, H. Chen, et al., A survey on evaluation of
large language models, in _ACM TIST_ (2023)
69.
P. Liang, R. Bommasani, T. Lee, D. Tsipras, D. Soylu, M. Yasunaga, Y. Zhang, et al., Holistic
evaluation of language models. arXiv:2211.09110 (2022)
70.
C. Ziems, W. Held, O. Shaikh, J. Chen, Z. Zhang, D. Yang, Can large language models transform
computational social science? in _Computational Linguistics_ (2024), pp. 1–55
71.
C. Hu, H. Zhou, D. Wu, X. Chen, J. Yan, X. Liu, Self-refined generative foundation models for
wireless traffic prediction. arXiv:2408.10390 (2024)
72.
S.O. Semerikov, T.A. Vakaliuk, O.B. Kanevska, M.V. Moiseienko, I.I. Donchev, A.O. Kolhatin,
LLM on the edge: the new frontier, in _Proceedings of the 5th Edge Computing Workshop_ (2025),
pp.137–161
73.
M. Zhang, X. Shen, J. Cao, Z. Cui, S. Jiang, EdgeShard: efficient LLM inference via
collaborative edge computing. IEEE Internet Things J. **12** (10), 13119–13131 (2024)
[Crossref]
74.
G.O. Boateng, H. Sami, A. Alagha, H. Elmekki, A. Hammoud, R. Mizouni, A. Mourad, et al., A
survey on large language models for communication, network, and service management:
Application insights, challenges, and future directions, in _IEEE Communications Surveys &
Tutorials_ (2025)
75.
K.B. Kan, H. Mun, G. Cao, Y. Lee, Mobile-LLaMA: instruction fine-tuning open-source LLM
for network analysis in 5G networks. IEEE Netw. **38** (5), 76–83 (2024)
[Crossref]
76.
F. Jiang, C. Pan, L. Dong, K. Wang, M. Debbah, D. Niyato, Z. Han, A comprehensive survey of
large AI models for future communications: foundations, applications and challenges.
arXiv:2505.03556 (2025)
77.
L. Long, R. Wang, R. Xiao, J. Zhao, X. Ding, G. Chen, H. Wang, 2024. On LLMs-driven
synthetic data generation, curation, and evaluation: a survey. arXiv:2406.15126


78.
X. Guo, Y. Chen, Generative AI for synthetic data generation: methods, challenges and the
future. arXiv:2403.04190 (2024)
79.
A. Bilal, D. Ebert, B. Lin, LLMs for explainable AI: a comprehensive survey. arXiv:2504.00125
(2025)
80.
A.A. Abba Ari, F. Samafou, A. Ndam Njoya, A.C. Djedouboum, M. Aboubakar, A. Mohamadou,
IoT-5G and B5G/6G resource allocation and network slicing orchestration using learning
algorithms. IET Netw. **14** (1), e70002 (2025)
81.
S. Almakdi, A. Aqdus, R. Amin, M.S. Alshehri, An intelligent load balancing technique for
software defined networking based 5G using machine learning models. IEEE Access **11** ,
105082–105104 (2023)
[Crossref]
82.
X. Lin, W. Wang, Y. Li, S. Yang, F. Feng, Y. Wei, T.S. Chua, Data-efficient fine-tuning for LLM-
based recommendation, in _ACM SIGIR_ (2024), pp. 365–374
83.
M.V. Ngo, H.M. Yoo, Y.H. Pua, T.L. Le, X.L. Liang, B. Chen, E.K. Hong, T.Q. Quek, RAN
Intelligent Controller (RIC): from open-source implementation to real-world validation. ICT
Express **10** (3), 680–691 (2024)
[Crossref]
84.
J. Wang, L. Zhang, Y. Yang, Z. Zhuang, Q. Qi, H. Sun, L. Lu, J. Feng, J. Liao, Network meets
ChatGPT: intent autonomous management, control and operation. J. Commun. Inf. Netw. **8** (3),
239–255 (2023)
[Crossref]
85.
Y. Zhao, W. Zhai, J. Zhao, T. Zhang, S. Sun, D. Niyato, K.Y. Lam, A comprehensive survey of
6G wireless communications. arXiv:2101.03889 (2020)
86.
U. Kamath, K. Keenan, G. Somers, S. Sorenson, LLM challenges and solutions, in _Large
Language Models: A Deep Dive_ (2024), pp. 219–274
87.
H. Ghasemirahni, A. Farshin, M. Scazzariello, M. Chiesa, D. Kostić, Deploying stateful network
functions efficiently using large language models, in _Proceedings of the 4th Workshop on
Machine Learning and Systems_ (2024), pp. 28–38
_OceanofPDF.com_


(1)
(2)

(3)

(4)

© The Author(s), under exclusive license to Springer Nature Switzerland AG 2026
S. Singh et al. (eds.), _GenAI and LLMs for Beyond 5G Networks_
https://doi.org/10.1007/978-3-032-06418-9_16

##### AI-Based On-Device Traffic Classification

##### for Next Generation Mobile Network

Madhan Raj Kanagarathinam^1 ,^2 , Krishna M. Sivalingam^3 ,

Hyunwoo Choi^4 and JongMu Choi^4
MX Department, Samsung Electronics, Suwon, South Korea
Department of Computer Science and Engineering, Indian Institute of
Technology Madras, Chennai, India
Department of Computer Science and Engineering, Indian Institute of
Technology Madras, Chennai, India
MX Department, Samsung Electronics, Suwon, South Korea

```
Madhan Raj Kanagarathinam
Email: madhan.raj@samsung.com
```
```
Krishna M. Sivalingam (Corr esponding author)
Email: skrishnam@cse.iitm.ac.in
```
**Abstract**
Network traffic classification is essential to managing and optimizing
modern network environments. However, traditional methods like port-
based analysis and deep packet inspection often face scalability and
performance limitations challenges. This chapter proposes an innovative
context-aware eBPF-assisted AI (BAI) solution to address these challenges
and enable efficient and accurate smartphone traffic classification. Our
approach leverages the capabilities of the extended Berkeley Packet Filter
(eBPF) to reduce packet processing overhead and enhance performance. By
dynamically attaching BPF programs, our BAI solution enables targeted
and context-aware traffic classification in smartphones. We introduce a
power-efficient approach that selectively processes interested app traffic,


optimizing resource utilization and enhancing classification efficiency.
Experimental evaluations demonstrate the effectiveness of our method,
showing significant reductions in packet processing time (up to 93. 6 _%_
compared to the legacy approach) and CPU utilization. Integrating eBPF
and BPF maps enables real-time, low-latency traffic classification with
improved accuracy and scalability. Our proposed BAI solution offers a
robust and versatile approach to traffic classification, addressing the
limitations of traditional methods and providing a foundation for further
advancements in network monitoring and optimization.

**Keywords** Traffic classification – eBPF – Smartphones – Artificial
Intelligence

**1 Introduction**

With the rise of Internet-connected devices and the growing popularity of
bandwidth-intensive applications, network traffic has become more
complex and varied. To manage this diverse landscape, network analyzers
require practical tools and techniques. Traffic classification is a critical
process that involves identifying and categorizing network flows based on
their unique characteristics. Doing so helps network administrators
understand and manage this complex traffic landscape more effectively.
Traditional traffic classification methods rely on port-based analysis,
deep packet inspection (DPI), or protocol-based heuristics [ 1 ]. These
methods operate at higher network stack layers and involve complex
processing algorithms that inspect the packet payload or perform pattern
matching on packet headers. While these approaches have been helpful in
the past, they often need help with performance limitations and scalability
issues, especially in high-speed networks or environments with large traffic
volumes. The secured Domain Name System (DNS) and server name
indication (SNI) make these traditional approaches less efficient.
In recent years, the emergence of eBPF [ 2 ] has revolutionized network
monitoring and analysis by providing a flexible and efficient framework for
packet processing within the Linux kernel. Initially developed as an
extension to the Berkeley Packet Filter, eBPF allows for programmable and
efficient processing of network packets. It provides a safe and versatile


environment to execute custom packet filters and collect fine-grained
network data.
The versatility of eBPF has opened up new possibilities for traffic
classification. By leveraging the programmability of eBPF, it becomes
feasible to extract rich and contextual information from network packets,
enabling more accurate and robust traffic classification algorithms.
Additionally, eBPF’s low overhead and near-real-time capabilities make it
suitable for high-speed networks and real-time traffic analysis. BPF maps
[ 3 ] provide a shared data structure that allows efficient information storage
and retrieval during packet processing. By leveraging BPF maps, we can
eliminate the need for repeated processing of packets at higher layers,
significantly improving the performance and scalability of traffic
classification algorithms.
In this chapter, we propose eBPF-assisted AI (BAI), a novel method for
traffic classification that harnesses the power of eBPF for fast packet
processing and utilizes the BPF map for machine learning model input. One
key aspect of our proposed method is the dynamic attachment of the BPF
program based on framework information in smartphones. With the
increasing popularity of mobile devices and the diverse range of
applications available, it becomes crucial to accurately classify traffic based
on the specific framework or application used. For example, we observe
different traffic patterns for video call applications, gaming platforms, or
streaming services. By dynamically attaching the appropriate BPF program
based on this framework information, we can tailor the traffic classification
process to each application’s specific requirements and characteristics.
Our method uses BPF maps to store precomputed classification
information or metadata associated with specific network flows. During
packet processing, the eBPF program can quickly query the BPF map to
retrieve the relevant information and make classification decisions without
requiring expensive computations at higher layers. This approach reduces
the processing overhead and allows for real-time, low-latency traffic
classification.
Furthermore, our proposed method is robust, allowing for dynamic
updates to the classification rules and efficient adaptation to changing
network conditions. By leveraging the programmability of eBPF and the
versatility of BPF maps, we can easily modify and fine-tune the


classification rules without significant overhead or disruption to the overall
traffic processing pipeline.
This chapter will explain our proposed method, including the
architecture, design considerations, and implementation details. We will
compare our method with traditional packet classification approaches and
demonstrate the advantages of leveraging eBPF and BPF maps for fast and
efficient traffic classification. Additionally, we will present experimental
results and performance evaluations to showcase the effectiveness and
scalability of our method in real-world network environments. The main
contributions of the proposed work are the following:

```
Improved packet processing time: We propose eBPF-assisted packet
processing and input to the machine learning model to reduce the packet
processing overhead. The proposed approach drastically reduced the
packet processing time by 93.6%. The reduced packet processing
improves performance and reduces CPU utilization tenfold.
Event-driven BPF attaching: Unlike the network server, in
smartphones, we can infer the context of the app traffic with the
framework information. We use the framework information to trigger an
event and dynamically attach the BPF to collect the statistics of the
interested app.
Context-aware processing: We reduce the number of packets processed
by 80%, using the context information from the framework. The classic
traffic classification AI models poll to fetch and predict the traffic.
However, our AI model is event-driven with reduced input entities to
improve resource utilization. The context-aware BAI improved the
Geekbench score by 10.4
Real-time data and experiment: The proposed BAI is successfully
implemented and tested in the latest Samsung S23 device. We evaluated
BAI in a real-world experiment with the legacy approach. BAI is the first
real-time implementation of the eBPF-assisted and context-aware traffic
classification ML model. Unlike the legacy approach, which extracts the
data from Wireshark (pcap) or public datasets, our system can
dynamically collect and classify the network traffic.
```
**2 Background and Motivation**


In this section, we will discuss the background of traffic classification, the
literature study about traffic classification, and our motivation for the
proposal.

**2.1 Traffic Classification Techniques**

Traffic classification is a fundamental task in network management and
security, involving identifying and categorizing network flows based on
their characteristics. By understanding the nature of network traffic,
administrators and security professionals can gain insights into network
usage, detect anomalies, enforce policies, allocate resources effectively, and
protect against various threats.
Traditional traffic classification methods have relied on port-based
analysis, deep packet inspection (DPI), or protocol-based heuristics. Port-
based research involves examining the port numbers in packet headers to
identify the corresponding application or service. However, this approach
cannot work efficiently, as many modern applications and services use
nonstandard ports or dynamically assign them [ 4 ]. Deep packet inspection
involves analyzing the payload of network packets to extract information
about the application or service. While DPI can provide detailed insights, it
is computationally expensive and may raise privacy concerns due to the
inspection of packet contents [ 5 ].
Protocol-based heuristics involve analyzing packet header information
to make inferences about the underlying application or service. For
example, traffic over port 80 is often associated with HTTP web traffic,
while traffic over port 53 is associated with DNS queries. However, this
approach may lead to misclassifications due to the growing use of
nonstandard ports or encryption protocols.
To address the limitations of traditional methods, researchers have
explored more advanced techniques for traffic classification [ 6 ]. These
techniques leverage machine learning [ 7 ], statistical analysis, pattern
matching [ 8 ], and behavioral analysis for accurate and efficient
classification [ 9 , 10 ].
Machine learning approaches involve training models on labeled
datasets to learn patterns and characteristics of different applications or
services. These models can then classify unseen traffic based on learned
features. The widely used technique in traffic classification is supervised
learning algorithms such as decision trees, support vector machines (SVMs)


[ 11 ], and neural networks [ 12 ]. Unsupervised learning algorithms such as
clustering can also be applied when labeled datasets are unavailable [ 13 ].
Statistical analysis techniques examine statistical features of traffic flows,
such as packet inter-arrival times, packet size distributions, or flow
duration, to differentiate between different applications or services. These
methods exploit statistical variations that are inherent to different types of
traffic. Pattern-matching techniques involve the creation of signature-based
rules that match specific patterns or signatures associated with known
applications or services. The rules comprise the packet headers, payload
content, or both. We often use pattern-matching techniques in conjunction
with other methods to enhance classification accuracy. Behavioral analysis
techniques focus on capturing behavioral patterns and characteristics of
network traffic flows. This approach examines temporal and spatial
patterns, communication patterns, and flow dynamics to classify traffic.
Behavioral analysis can be instrumental in detecting and categorizing new
or unknown applications or services.
In recent years, the emergence of programmable packet processing
frameworks, such as eBPF (extended Berkeley Packet Filter), has opened
up new possibilities for networking [ 14 ]. eBPF allows efficient and
customizable processing of network packets within the Linux kernel [ 15 ],
providing a flexible platform for developing packet filtering techniques
[ 16 ]. However, the current research does not leverage eBPF’s
programmability to extract rich contextual information from network
packets and develop classification techniques tailored to specific network
environments.
Effective traffic classification is essential for network management,
security, and optimization [ 17 ]. Continued research and advancements in
traffic classification techniques, including utilizing emerging technologies
like eBPF, will play a vital role in addressing the challenges posed by
evolving network environments and the increasing complexity of network
traffic. The information from the higher layers can also be used to make the
BPF work more efficiently.

**2.2 Input for Traffic Classification**

The first step for traffic classification is obtaining network packets for
analysis. Various approaches and tools are available to capture network
packets, and two commonly used methods are libpcap and raw sockets.


**_2.2.1 Packet Capture Library_**

The libpcap [ 18 ] is a popular library for capturing network packets from
live network interfaces or reading packet capture (PCAP) files [ 19 ]. It
provides a portable and efficient interface for capturing packets at the link-
layer level. The libpcap operates by utilizing operating system mechanisms,
such as pcap (packet capture) or BPF (Berkeley Packet Filter). Using
libpcap, developers can specify filters to capture packets based on specific
criteria, such as source or destination IP addresses, port numbers, or
protocol types. This flexibility allows targeted packet capture, focusing on
traffic relevant to the classification task.
The libpcap supports various platforms and provides APIs for
programming languages like C, Python, and others [ 20 ]. It abstracts the
underlying operating system mechanisms, making it easier for developers to
access network packets and perform traffic analysis and classification. The
tcpdump [ 18 ], Tshark [ 21 ], and Wireshark [ 22 ] use libpcap for the packet
capture.

**_2.2.2 Raw Sockets_**

Raw sockets provide an alternative method for capturing packets directly
from the network stack [ 23 ]. With raw sockets, applications can access and
process packets at the network layer, bypassing the transport and
application layer protocols. The raw sockets enable more fine-grained
control and allow for in-depth analysis of packet headers and payload.
Developers can use raw sockets to implement custom packet capturing
mechanisms and perform traffic classification based on specific attributes.
Raw sockets provide access to the complete network packet, including the
IP header and payload, which can be helpful for advanced traffic
classification techniques that require detailed packet inspection.
However, working with raw sockets requires more expertise and
understanding of network protocols, as the application handles the complete
packet processing and protocol implementation. Raw socket-based traffic
classification implementations often involve parsing and analyzing the
packet headers and payload to extract relevant information for classification
purposes.

**_2.2.3 Packet Capturing for Machine Learning Input_**


Both libpcap and raw sockets provide potent mechanisms for capturing
network packets, and the choice between the two depends on the specific
requirements of the traffic classification task. The libpcap offers a higher
level abstraction and is suitable for most general-purpose traffic
classification needs [ 20 , 24 , 25 ]. On the other hand, raw sockets provide
greater control and flexibility, making them a preferred choice for advanced
classification techniques that require direct access to packet-level
information. The input source selection also depends on the platform and
operating system. Different operating systems may vary how libpcap or raw
sockets are implemented and accessed. Therefore, it is essential to consider
the platform compatibility and available resources when choosing the
appropriate input method for traffic classification.
By leveraging libpcap or raw sockets to capture network packets,
researchers and developers can gather the necessary input for traffic
classification algorithms. These input sources enable the analysis of packet-
level details, facilitating the development of accurate and effective traffic
classification techniques to understand, manage, and secure network traffic.
However, the major disadvantage is complex processing and computational
overhead if heavy traffic is involved. The monitoring or classification
application has to process the headers of the packets received from the
libpcap or raw sockets to feed to their machine learning approach. In
devices such as smartphones, if every packet has to be monitored and
processed, it would severely affect performance.
The potential advantages of the new and innovative eBPF technology
are overlooked in current ML-based traffic classification research [ 26 – 30 ],
which primarily relies on offline pcap files or pcap APIs for feature
collection and prediction. This work fills the gap by being the first to
propose a novel eBPF-assisted traffic classification in smartphones. By
leveraging the power of eBPF and BPF maps, traffic classification can be
achieved with higher accuracy, lower latency, and improved scalability,
outperforming traditional approaches. Additionally, we can use the app
context as one of the vital features in smartphone traffic classification. Our
proposed method presents a promising direction for traffic classification in
modern network environments and lays the foundation for further
advancements in network monitoring, security, and optimization. The
chapter organization consists of Sect. 3 , “BAI: Design and
Implementation,” which explains the eBPF-assisted AI (BAI) approach,


including integrating eBPF and the AI model. Section 4 , “Performance
Evaluation,” presents the experimental setup and the comparison between
the proposed BAI solution and the legacy approach. Finally, in Sect. 5 ,
“Conclusion,” the key contributions and findings of the chapter are
summarized.

**3 BAI: Design and Implementation**

In this section, we will explore the architecture of the proposed BAI system.
We will also discuss the data collection and model training using the BAI.

**3.1 System Design**

In this section, we will discuss the design of the proposed BAI system.
Figure 1 delineates the main modules of the BAI system. We also compared
it with the legacy traffic classification system for a better understanding.

**_Fig. 1_** System architecture of the proposed BAI system

**_3.1.1 BPF Programs_**


BPF program collects a set of statistics features that will be input
parameters to classify the network service categories. The BPF programs
process the IP packets and retrieve the required information. The BAI
consists of two BPF programs: one attached in the ingress and the other on
the egress of the network communication interface. In smartphones, only
one interface is used at a time by default. Though there are cases where
multiple communication interfaces can steer the traffic, it is rare. However,
our BPF programs can be attached to more than one interface. In the current
approach, we always attach the BPF program to the default network
communication interface. The ingress and egress program can use the same
BPF maps to update the per-unique-connection (PUC) information as
required. As discussed in the previous literature [ 31 ], we collect all the
parameters used to classify the traffic. We additionally use context
information to reduce the packet processing overhead.

**_3.1.2 BPF Maps_**

We use five different maps with different purposes for data handling in the
AI model. Table 1 consolidates the features collected from the BPF map.
The packet can either be processed coarse-grained to read the overall value
or processed fine-grained to parse the TCP/IP details. The higher the
network layer information we need, the greater the packet processing
overhead. In M1, we collect the overall interface statistics by coarse-
grained processing. The M2, M3, and M4 maps use fine-grained processing
of the interested traffic. In M2 and M3, we store the PUC inter-arrival time
(IAT) statistics. In M4, we store the other features of PUC used for AI
prediction. The BAI daemon can access the BPF map information directly.
Apart from the above maps passed from the kernel to the BAI daemon, we
also have M5, which provides the application user ID (UID) [ 32 ] bit vector
of the interested UID from the BAI daemon to the kernel. The BPF
programs access the M5 to understand whether the packets require fine-
grained or coarse-grained processing.

**_Table 1_** The features used for AI and their corresponding BPF map

```
Map Feature Description
M1 (BPF) accumulatedWakeTime Overall Wake Time
```
(^) maxTxInterPacketTime Max TX packet time


```
Map Feature Description
```
(^) minTxInterPacketTime Min TX packet time
(^) maxRxInterPacketTime2 Second Max RX packet time
(^) maxRxInterPacketTime Max RX packet time
(^) latestTxTime Latest TX packet time
(^) latestRxTime Latest RX packet time
**M2 (BPF)** maxRxPacketSize PUC Max RX packet size
(^) minRxPacketSize PUC Min RX packet size
(^) maxRxInterPacketTime2 PUC Second Max RX IAT
(^) maxRxInterPacketTime PUC Max RX IAT
(^) latestRxTime PUC RX latest time
**M3 (BPF)** maxTxPacketSize PUC Max TX packet size
(^) minTxPacketSize PUC Min TX packet size
(^) maxTxInterPacketTime2 PUC second Max TX IAT
(^) maxTxInterPacketTime PUC Max TX IAT
(^) minTxInterPacketTime PUC Min TX IAT
(^) latestTxTime PUC TX latest time
**M4 (BPF)** uid App User ID
(^) ipVersion IP Protocol version
(^) ipv4Addr V4 Address
(^) ipv6Addr V6 Address
(^) rxPackets RX packet count
(^) txPackets TX packet count
(^) tcpPackets TCP packet count
(^) udpPackets UDP packet count
(^) rxBytes RX cumulative bytes
(^) txBytes TX cumulative bytes
(^) firstTxTime First packet time
**M5 (BPF)** uidBitVector UID to be processed
**GAMU (FWK)** gamBitVector Context-Map with UID
**_3.1.3 BAI Framework_**


The BAI framework comprises three submodules: service detector, GAMU
framework, and AI service. The GAMU (Game, Audio, Media, UDP)
framework inputs the context of the current traffic. The game manager is
used to detect the category of the application. We set the game flag if the
game manager detects the app category as the game. We unset the flag if all
the processes associated with the game-marked UID are no longer active.
The audio and media states determine if there is an ongoing audio/video or
streaming season, respectively. An event is triggered if the audio or media
state is active. The event can also be triggered even if the in-mobile video is
viewed offline in the video streaming application. Hence, we also require
the network traffic statistics of the interface to identify if the audio, video,
or streaming is online or offline. We use the UDP tracker to determine the
overall UDP packets in the current system. The proc file system provides
the count of the cumulative UDP packets. The UDP tracker calculates the
relative difference to fetch the UDP packets per second (PPS). If the UDP
PPS crosses a specific threshold, we flag the UDP bit as true. If the GAMU
value is non-empty, it sends an event to the service detector to start the AI
predictions.
The Service Detector feeds the input parameters to the AI service and
queries to predict the category of the PUC list fetched from the BAI
daemon. The input parameter comprises both BPF map information and
GAMU information. The AI service uses the buffer of PUC x features as
input and predicts the category of each PUC.

**_3.1.4 BAI Daemon_**

The BAI daemon is the interface between the BPF and the BAI framework.
Figure 2 depicts the interaction between the BAI framework and the BAI
daemon. It further comprises three submodules: BPF controller, BPF UID
handler, and BPF dumper. The BPF controller will attach and detach the
BPF to the specific interface based on the event received from the service
detector. Figure 3 shows sample cases when the BPF is attached
dynamically. In case 1, there is no real-time context. In case 2, though the
video call app is active, the Audio flag is set to 0, as there is no active call.
In case 3, there is a video call, and the Audio bit is set to 1. In case 4, we set
the GAMU bit for multiple apps, as video call and gaming apps are active.
Therefore, we do not attach the BPF program in cases 1 and 2, but we
attach the BPF program in cases 3 and 4.


**_Fig. 2_** The interaction between BAI framework and BAI daemon


**_Fig. 3_** The sample cases when the BPF is attached
The BPF controller pins the ingress and egress BPF program to the
default interface using the traffic control (TC) utility. It also reads the BPF
map statistics (M1, M2, M3, and M4) and returns them to the BAI service
detector. More than one specific UID might be of interest to detect the
category. Hence, the BPF UID handler can mark/unmark multiple UIDs of
interest. If the service detector passes the specific UID, the M5 Map is
updated with the corresponding details. The BPF program uses the M5 Map
to differentiate the BAI selective application UID. Figure 4 illustrates the
overall packet processing in the BPF program. The BAI marked UID is
processed fine-grained to fetch the PUC value. However, the non-BAI UID
does not need to undergo header processing. The non-BAI packets add only
the coarse-grained detail in Map M1. The BPF dumper is a debugger that
dumps the map details and other debugging information.


**_Fig. 4_** The packet processing in the BPF maps. The information from the GAMU framework is used
to process only the interested UID
A high-level overview of BAI that processes all the packets and the
BAI, which can be enhanced using context awareness, is shown in Fig. 5.
By using context awareness, packet processing is significantly reduced.
Currently, the classic BPF programs do not use the context awareness.
Using our framework, we can enhance packet processing and improve
machine learning accuracy.


**_Fig. 5_** A high-level overview of BAI and BAI with context awareness Traffic Classification (TC)
system

**3.2 Data Collection and Model Accuracy**

This section provides a detailed account of the data collection process and
the training of AI models for traffic classification using Berkeley Packet
Filter (BPF). We aimed to develop an accurate and granular traffic
classification system by leveraging various network traffic parameters
obtained from BPF maps, such as transmission and reception inter-arrival
times, bytes, and packet count. Additionally, we introduced a novel
contextual input method based on the GAMU framework to enhance the
classification process’s accuracy and granularity.

**_3.2.1 Data Collection and Labeling_**

We want to train the model with various application categories. The BAI
daemon BPF program is attached from the boot-up to the network interface
to capture the application traffic characteristics. We check the application
UID in the framework, and the GAMU field corresponding to the PUC
entry is marked based on the UID. For example, if there are two UIDs, A
and B, we collect all the unique connections in A and B. If there is a video
call in UID A, then the GAMU field corresponding to A is marked as valid
with the Audio/UDP bit set as 1, and the GAMU field in B, by default, is
marked as 0. Hence, our dataset will have collated information on the traffic
characteristics and the context for each unique connection. Once the device
connects to the Internet, we attach the BPF program, and the BAI


framework collects data of all the required information. Finally, we store
and fetch the collated data in a specific location on the device.
In our user trail devices, we got 494,768 data points with 75 plus
different applications of categories (Fig. 6 ) audio/video calling, streaming,
browsing, chat, live streaming, mobile cloud gaming, mobile gaming real-
time, mobile gaming non-real-time, navigation, food service, delivery
service, file download/upload, bandwidth benchmarking services, hotel
booking, travel, and so on. We labeled the data as real-time and non-real-
time. We classified the data in real-time as mobile gaming, audio, and video
calls. We trained the ML models using 80 _%_ of the data points, and the rest
20 _%_ were used for testing the model’s efficacy.

**_Fig. 6_** The samples collected for AI training. We got 494,768 data points with 75 plus different
application categories

**_3.2.2 ML Models_**

For our classification task, we implement different ML models with varying
configurations. Primarily, we investigate the performance of RT and NRT
classification using supervised ML approaches like recurrent neural
network (RNN), XGBoost, random forest, and support vector machine
(SVM). All the methods use the data points to train the model to predict RT
and NRT classes.
RNN is a popular neural network method especially suited for time
series data. RNN, unlike other neural networks, is more suited for finding
patterns in a continuous stream of data. RNNs possess a crucial ability to
retain information from past inputs by utilizing hidden states. At each
sequence step, they produce an output and adjust the hidden state, which is


then transmitted to the subsequent step along with the current input. This
mechanism empowers RNNs to grasp dependencies and patterns within
sequential data. For our classification task, we use two configurations for
passing the data as input to the RNN models. First, we pass the data points
as single rows; in the second configuration, we pass it as time series data
having a window size of 30. Window size means that the input comprises
30 continuous data points passed as one input to the RNN model.
RNNs encounter a challenge known as the vanishing gradient problem,
where gradients diminish significantly as they propagate backward through
time during training. This phenomenon hampers the network’s ability to
learn long-term dependencies effectively. To overcome this obstacle,
advanced RNN architectures like Long Short-Term Memory (LSTM)
networks and Gated Recurrent Units (GRUs) have been developed [ 33 ].
These architectures excel in capturing extended dependencies and
alleviating the issue of vanishing gradients.
We implement LSTM and GRU for classification with and without time
series input. The training objective for both LSTM and GRU is to optimize
the model weights according to the loss function using backpropagation. We
use the following binary cross-entropy as a loss function for the RNN
models:

```
(1)
```
where _LossBCE_ denotes binary cross-entropy loss, _N_ is the number of
data points, _yi_ is the true label of the _i_ th sample (either RT or NRT), and _ŷi_ )
denotes the predicted label using the RNN model.
To make the RNN model training more robust, we also test the model
performances with binary focal cross-entropy (BFCE) loss function, a
variant of the weighted loss function. BFCE introduces a focusing
parameter, _γ_ , in the standard binary cross-entropy function. _γ_ acts as a
modulating factor that reduces the contribution of easily classified examples
to the overall loss. _γ_ controls the extent of this down-weighting, allowing
the model to focus more on complex examples. As a result, RNN model
weights are updated to become more robust to the class imbalance and can
better learn from the minority class, improving overall performance and
generalization. The implemented BFCE function is as follows:

```
(2)
```
# LossBCE =− N^1 ∑

```
N
```
```
i = 1 [ yi log( ŷi )+(^1 − yi )log(^1 − ŷi )]
```
# LBFCE =− N^1 ∑

```
N
```
```
i = 1 [(^1 − ŷi )
```
```
γy
i log( ŷi )+ ŷ
```
```
γ
i (^1 − yi )log(^1 − ŷi )]
```

where _LossBFCE_ denotes binary focal cross-entropy loss, _N_ is the
number of data points, _yi_ is the true label of the _i_ th sample (either RT or
NRT), and _ŷi_ ) denotes the predicted label using the RNN model. _γ_ is the
focusing parameter which makes BFCE a weighted loss function.
Neural networks, especially RNNs, need much processing power due to
their sizeable computational complexity. We investigate the classification
performance with low computational complexity models like SVM and
random forest. SVM seeks the best hyperplane to separate classes in feature
space, maximizing the margin between classes. Using kernel functions,
SVM can handle nonlinear boundaries by transforming data into higher
dimensions. During training, SVM minimizes classification error while
maximizing margin via optimization. We use the radial basis function
(RBF) as the kernel in our implementation. Also, different feature selection
methods like ANOVA, chi-square, and mutual information [ 34 ] were
explored to maximize the classification accuracy of the proposed SVM
model (Fig. 7 ).


**_Fig. 7_** The test application used for evaluation of BAI

**4 Performance Evaluation**

In this section, we will discuss the effectiveness and impact of the proposed
BAI solution. Android provides several tools for profiling and tracing per-
process usage of the system resources like CPU, GPU, and memory.
However, since the eBPF program directly handles the network socket in
the kernel space, which does not include any process and thread, its system
usage cannot be measured directly. We first discuss the tool used for our
evaluation. Then, we evaluate the processing time for parsing the packet
header to show the usefulness of BPF-based processing and to demonstrate
the advantage of context-aware BPF. Rather than measuring process-based
CPU usage, we evaluate the overall usage, which accounts for the user
space and kernel-space processing. In addition, we also measured the CPU
benchmark score during the traffic monitoring to show the robustness of our
approach.

**4.1 Experimental Setup**

We developed an Android application as an evaluation tool to evaluate the
proposed Conversational AI (BAI) solution. The tool provides two modes
of operation, emulation mode and live mode, allowing us to assess the
solution’s performance and effectiveness in different scenarios.

**_4.1.1 Emulation Mode_**

We collected the packet capture files for individual apps with different types
of application traffic. In the emulation mode, the tool reads pcap files of
various application traffic types. For instance, we collected traffic data from
multiple video calling apps (such as Meet, Zoom, Skype, and Teams) and
gaming apps (including Call of Duty, Brawl Stars, Xbox Cloud,
Battleground Mobile, Roboplex, Mobile Legends), as well as other types of
traffic like chatting, web surfing, downloads, and uploads. These pcap files
represent live packet captures obtained under different network conditions
and in various regions.
To ensure an accurate evaluation, we employ a replay program that
attaches to the test interface and generates data from the pcap file at the
same intervals as captured. Figure 8 depicts our emulation approach that


guarantees a replica of the exact traffic patterns and rates observed during
the original capture. It is important to note that live-air packets are always
uncertain due to factors affecting traffic rates and patterns. We developed
this emulation mode mainly to address the network traffic variability and
provide a fair comparison between our proposed solution and legacy
approaches.

**_Fig. 8_** The traffic emulation in the test interface to measure the performance

**_4.1.2 Live Mode_**

The evaluation tool also includes a live mode that captures packets directly
from the actual network communication interface of the smartphone. This
mode reflects the real-time traffic experienced by the device. The tool
provides predictions every second during the trial period and offers an
overall summary once the user clicks the pause or stop button to conclude
the evaluation.
The evaluation tool’s two modes allow us to thoroughly assess the
proposed BAI solution under controlled emulation conditions and real-
world live traffic scenarios. The tool functions as a system app in emulation
and live modes, granting access to additional metrics and system-level
information. These metrics provide valuable insights into the interpretation
and resource utilization of the BAI solution during evaluation. By
leveraging this tool, we can gather comprehensive performance data and
draw meaningful conclusions regarding the efficacy and efficiency of our
BAI solution.


**4.2 Evaluation of Packet Processing Time**

We evaluated the packet processing time by comparing the proposed BAI
solution with the legacy approach. This comparison involved utilizing raw
sockets for packet capture and parsing the required details used as input for
the AI model. We conducted 20 consecutive trials on the same device
(Samsung Galaxy S23 with Android 13 OS) to ensure consistency in our
experiments. During the evaluation, we found that the maximum observed
real-time traffic speed would be 10 Mbps. Based on experimental
observations, online gaming traffic typically operated within a range of 1
Mbps, with an average packet size or Maximum Segment Size (MSS) of
500 Bytes. Conversely, video call traffic exhibited higher speeds, ranging
from 2 to 5 Mbps, with an MSS of approximately 1400 bytes. We observed
that for specific applications involving multiparty group calls, such as
Zoom [ 35 ] and Skype [ 36 ], traffic speeds of up to 10 Mbps [ 37 , 38 ].

**_4.2.1 Overall Packet Processing Time of BPF Compared to_**

**_Legacy_**

The replay program of the emulation mode is fed pcap files with different
traffic characteristics to evaluate the overall packet processing and per-
packet processing time. The goal was to assess the efficiency of the packet
processing mechanisms in both the legacy and proposed BAI approaches on
the same traffic pattern.
The evaluation results were summarized using bar graphs, Fig. 9 ,
representing the packet processing times for each traffic patterning live-air
and emulation with IPERF traffic. We conducted separate tests on live-air
Brawl Stars gaming [ 39 ] and Skype video calling [ 36 ]. The results, depicted
in Fig. 9 , showed that the live-air results matched the emulation results and
yielded a 93. 6 _%_ improvement in processing time. Additionally, we
emulated IPERF [ 40 ] traffic with 2 Mbps with a small payload (500 B
MSS), 2 Mbps (no cap on payload size), 10 Mbps, 100 Mbps, 400 Mbps,
and 800 Mbps. In all the trials, the BAI with the BPF-based approach
outperformed the legacy method by up to 90 _%_. The packet processing time
in the user space is relatively less than expected in the 800 Mbps network.
The reduction is mainly due to dynamic CPU clock frequency [ 41 ]. Figure
10 shows that the CPU clock frequency increases with the packet bitrate.
Hence, in the legacy approach, we do not see an exponential increase in the


packet processing time due to an increased CPU frequency. However,
increased CPU frequency can lead to power issues in smartphones [ 42 ].

**_Fig. 9_** The comparison of the overall packet processing time with BPF

**_Fig. 10_** The CPU clock frequency of the eight cores with different download bitrate. As the
download increases, the CPU clock adapts and dynamically increases the clocking frequency
This evaluation highlights the superiority of the proposed BAI solution
over the legacy approach, demonstrating its capability to deliver enhanced
performance. The performance improvement is significant because instead
of copying the complete data to the user space and processing the IP header
in the user space, the BPF can fast process the packets in the kernel space


and provide the summary using the BPF map to the user space. The BAI
daemon directly reads the BPF map; hence, we avoid complete packet
capture and processing in the user space.

**_4.2.2 Unit Packet Processing Time Evaluation_**

The unit packet processing time for each traffic pattern is depicted in Fig.
11 , providing deeper insights into the efficiency of the packet processing
mechanisms. The proposed BAI solution leveraging BPF maps
demonstrated substantial savings in packet processing overhead compared
to the legacy approach. The results revealed a remarkable 86 _%_ reduction in
per-packet processing time when employing the proposed BAI solution
with BPF maps compared to the legacy approach.

**_Fig. 1 1_** Unit processing time comparison with BPF

The evaluation outcomes provide compelling evidence of the efficiency
and effectiveness of the proposed BAI solution. The results underscore the
superiority of the BAI approach over the legacy approach, showcasing
significant improvements in packet processing overhead and overall
performance in both emulation and live-air scenarios.

**4.3 Impact on Context-Aware Processing**


In our proposal, the GAMU framework helps the BAI BPF programs
selectively process certain apps by feeding the context. This section
evaluates the impact of BAI’s context-aware BPF processing approach. We
experimented with three devices: context-aware BAI, BAI that processes all
the packets, and the legacy approach that uses raw sockets. Figure 12 shows
the number of packets processed in each device. We can see that the
context-aware BAI process, with the help of GAMU information, processes
only the specific packets irrespective of the network traffic.

**_Fig. 12_** The comparison of the number of packets processed in BAI with the legacy approach

In the NRT-only (download) case, there will be no event to attach the
BPF program and load the AI model. Hence, there is 0 packet processed. If
we have only the RT-only (gaming) traffic, then the number of packets
processed by all three devices is the same. On the other hand, if there is
mixed traffic (gaming + download), then the context-aware BAI processes
only the packets from interested UID. As the packets from RT apps are not
more than 10 Mbps, we reach a constant packet processing with the
context-aware BAI solution irrespective of the overall packets in the


system. Figure 13 depicts that the overall packet processing time is less than
100 ms regardless of the system traffic due to the context information and
the characteristic of RT traffic.

**_Fig. 13_** The comparison of overall packet processing time with the context-aware information
The evaluation results support the robustness of the BAI solution, as it
effectively demonstrates its capability to manage packet processing at a
constant time. These findings further establish the suitability of the BAI
solution for real-time apps, where latency is paramount. Thanks to the
GAMU framework, the BPF is attached when a probable real-time
application is detected, conservatively utilizing the AI model only when
necessary. In contrast, traditional packet categorizing AI models framework
continuously polls for traffic patterns without considering packet context.

**4.4 Evaluation of the Other System Parameters**

In addition to evaluating packet processing time and CPU usage, we
assessed the overall system performance using third-party benchmarking
tools such as Geekbench [ 43 ] and 3DMark [ 44 ] app. We conducted various
trials to determine the impact of the proposed BAI solution on different
system parameters.

**_4.4.1 Geekbench Evaluation_**


To evaluate system performance using Geekbench, we employed our
emulation method to replicate the same traffic patterns and assess the
system’s performance. Geekbench scores rely on parameters that measure
CPU and memory performance, including single-core and multicore
processing, memory bandwidth, and latency.
Figure 14 summarizes the Geekbench scores obtained with BAI
(context-aware processing), BAI (all-packet processing), and legacy (with
raw socket) on the same device. The results indicate that the system
performance with the BAI solution is superior to that without the BAI
solution. We see a 4. 2 _%_ improvement in single-core processing and a
10. 4 _%_ improvement in multicore processing. Therefore, integrating the
context information with the BAI solution enhances the overall system
performance, as reflected in the improved Geekbench scores.

**_Fig. 14_** Geekbench score comparison of BAI with legacy

**_4.4.2 3DMark Evaluation_**

In addition to Geekbench, we utilized the 3DMark app to evaluate further
system parameters. While Geekbench primarily focuses on CPU and


memory performance, 3DMark measures graphics processing capabilities
and overall system performance during 3D rendering and gaming scenarios.
It provides insights into rendering speed and graphical effects. We want to
evaluate the BAI (context-aware) score with the legacy system with
different traffic flows.
Figure 15 compares system performance using the 3DMark tool of the
BAI with the legacy in different network conditions. The results highlight
the significant performance advantage of the BAI solution over the legacy
approach. This improvement can be attributed to the BAI solution
facilitating reduced CPU and GPU usage. The BAI solution optimizes
resource utilization and enhances system performance by reducing the input
size of the AI model. The score improves by 10 − 20 _%_ in different
conditions.

**_Fig. 15_** The 3DMark score, displaying the frames per second (FPS), comparison of BAI with legacy

These findings emphasize the effectiveness and versatility of the BAI
solution in optimizing system resources, reducing input size, and improving
overall system performance. By leveraging the context information
provided by the GAMU framework, the BAI solution operates in a more
targeted and efficient manner compared to traditional approaches.


**4.5 ML Model Performance**

Table 2 shows the performance of LSTM and GRU models with different
loss functions and time series input. We implement RNN models with two
RNN layers of 65 units each for LSTM and GRU. LSTM and GRU models
have two dense layers after the RNN layer for classification. The
penultimate dense layer has 32 nodes, and the last output layer has a single
node. ReLU activation function is used in the hidden layers, and the last
dense layer uses the sigmoid activation function. ADAM optimizer was
used with a learning rate of 0.001. The trained models were tested on the
test dataset. Weighted loss in Table 2 refers to BFCE as in Eq. 2 having _γ_
value of 5, and nonweighted loss is the standard binary cross-entropy. With
GRU, weighted loss without time series input gives the best classification
accuracy. AUC-ROC (Area Under the Receiver Operating Characteristic
Curve) is a metric used to evaluate the performance of binary classification
models. LSTM model with a standard binary cross-entropy loss function
and time series input is the best for our classification task in terms of
accuracy and AUC-ROC.

**_Table 2_** Performance of RNN models with different
configurations

```
RNN Configuration Performance Metric
Model Weighted Time series Accuracy AUC-ROC
```
(^) Loss Input (%)
**LSTM** NO NO 71.56 0.85
**GRU** NO NO 78.94 0.87
**LSTM** YES NO 79.52 0.73
**GRU** YES NO 79.50 0.71
**LSTM** NO YES 82.80 0.81
**GRU** NO YES 78.86 0.72
**LSTM** YES YES 78.14 0.74
**GRU** YES YES 77.67 0.69
Table 3 shows variation in the performance of different feature selection
algorithms and its effects on SVM classification accuracy. Mutual
information feature selection is the best in terms of accuracy and AUC-


ROC. Mutual information has been successfully used in tandem with SVM
to filter a group of features in classifying RT and NRT.

**_Table 3_** Evaluation of SVM with different feature
selection

**Feature selection Accuracy (%) AUC-ROC
ANOVA** 79.50 0.82
**Mutual information** 81.30 0.85
**Chi-square** 79.50 0.83
Table 4 shows the performance of XGBoost compared to other models
like LSTM, GRU, SVM, and random forest. Best configurations for LSTM
and GRU were used from Table 2. Similarly, the best configuration of SVM
was used. XGBoost can outperform other ML approaches in all three
performance metrics. XGBoost also has lower computational complexity
than the RNN models.

**_Table 4_** Evaluation of ML models

```
ML model Accuracy (%) F1 score AUC-ROC
GRU 79.50 0.79 0.71
LSTM 92.80 0.83 0.91
XGBoost 96.89 0.92 0.96
Random Forest 86.12 0.76 0.86
SVM 84.30 0.71 0.86
```
The highlighted values indicate the best value among the different methods
tried

We also study the role played by different features in affecting the
classification accuracy, F1 score, and AUC-ROC in the XGBoost model.
Table 5 shows the importance of each feature in the trained XGBoost
model. Features’ importance is reflected by the F score, which indicates
how much each feature contributes to the predictions made by the XGBoost
classification model. Feature importance helps in understanding which
features are most influential in classification.


**_Table 5_** Feature importance in XGBoost
classification

```
Map Feature F score
M1 latestRxTime 291
M4 txBytes 160
M4 rxBytes 98
M1 InterPacketTime 78
M5 isVOIP(UID bit vector) 64
M5 isLiveStreaming(UID bit vector) 60
M4 txPackets 43
M5 isGame(UID bit vector) 41
M4 UdpPackets 40
M2 rxPacketsSize 34
M5 isStreaming(UID bit vector) 29
M4 rxPackets 18
M3 txPacketsSize 13
```
**4.6 BPF-Assisted AI Model Versus Legacy AI Classification**

In this section, we compare the performance of different AI models for
traffic classification, mainly focusing on the legacy system and the
proposed eBPF-assisted AI (BAI) solution. For this comparison, we refer to
a recent research paper [ 31 ] that evaluates the accuracy of the XGBoost
model [ 45 ] for classifying traffic with two levels: layer-1 class (L1) and
layer-2 class (L2). The L1 comprises real-time (RT) and non-real-time
(NRT) traffic. The L2 further gives a fine-grained classification of L1 as
RT-Mobile gaming, RT-Voice call, RT-Video call, NRT-Streaming, NRT-
File download/upload, and NRT-Web browsing. We assessed the XGBoost
model’s accuracy and found it superior in accurately classifying traffic into
the l1 and l2 categories. Hence, we have collected a similar parameter used
in the previous raw socket-based AI model.
While the authors of [ 31 ] highlighted the benefits of XGBoost for traffic
classification, our results indicate that the BAI solution’s integration with
eBPF and BPF maps enables it to attain the same level of accuracy, if not
higher, in a real-world scenario for smartphone traffic classification. It is
essential to acknowledge that the performance and accuracy of an AI model


can vary based on the dataset used, the features extracted, and the specific
traffic characteristics. Moreover, the implementation details and parameter
tuning can significantly influence the model’s performance. Our
experiments ensured a fair and comprehensive evaluation of the legacy and
BAI approaches by utilizing the same dataset and meticulously fine-tuning
the model parameters. The results, as shown in Fig. 16 , showed promising
accuracy rates, indicating the potential of the XGBoost model for effective
traffic classification in the given context. However, we made an interesting
observation in our comparative analysis of the proposed BAI solution and
the legacy system. We discovered that the legacy approach, and the
proposed BAI solution achieved the same high accuracy level of 99. 5 _%_ for
traffic classification. The legacy approach misclassified a few thin-stream
traffic patterns NRT app traffic as RT. However, as the GAMU framework
introduces the context information of each app, the false positive cases of
the RT cases are reduced further in our approach. The observed accuracy
parity between the legacy and BAI approaches is a significant advantage for
the proposed solution. By offering comparable accuracy while leveraging
the power of eBPF to reduce processing overhead and enhance
performance, the BAI solution demonstrates its efficiency and practicality
in real-world smartphone environments.

**_Fig. 16_** The comparison of classification accuracy of BAI with legacy
In conclusion, the comparison between the AI model presented in [ 31 ]
and the proposed BAI solution reveals that both approaches achieve
exceptional accuracy in traffic classification. The BAI solution’s ability to
match the accuracy of XGBoost while utilizing eBPF’s advantages presents
a compelling case for adopting this novel approach in managing and


optimizing modern network environments, particularly in the context of
smartphone traffic classification.

**5 Conclusions**

This chapter presents a novel eBPF-assisted AI (BAI) method for efficient
and accurate traffic classification in smartphones. By dynamically attaching
BPF programs, our BAI solution achieves targeted and context-aware traffic
classification, addressing the limitations of traditional methods. The event-
driven approach and selective processing of interested app traffic optimize
resource utilization and improve classification efficiency.
Our experimental evaluations have demonstrated the superiority of the
proposed BAI solution. We observed significant reductions in packet
processing time (up to 93. 6 _%_ compared to the legacy approach) and CPU
utilization, highlighting the efficiency and effectiveness of our method. We
reduced packet processing overhead by leveraging BPF maps, allowing fast
and efficient packet processing in the kernel space.
One of the key contributions of our work is incorporating BPF maps
and context information for conservatively using the AI model. Unlike
traditional AI models that continuously poll for traffic patterns, our BAI
solution utilizes an event-driven approach based on context information
provided by the BAI framework. This approach enables us to selectively
process interested app traffic and avoid unnecessary computations,
improving efficiency and resource utilization.
In conclusion, our proposed BAI solution offers a novel and efficient
approach to traffic classification. We have significantly improved
performance and scalability by reducing packet processing overhead,
optimizing resource utilization, and leveraging context information. Our
method opens new network monitoring and optimization possibilities,
paving the way for future advancements in traffic classification techniques.
In future work, we want to classify live and on-demand videos using the
BAI framework. We will extend to the AR/VR and other real-time network
traffic categories.

**References**


1. El-Maghraby, Reham Taher and Abd Elazim, Nada Mostafa and Bahaa-Eldin, Ayman M., “A
    survey on deep packet inspection,” in _2017 12th International Conference on Computer_
    _Engineering and Systems (ICCES)_ , pp. 188–197, 2017.
2.
    eBPF—Introduction, Tutorials & Community Resources. https:// ebpf. io/. Accessed on Feb. 20,
    2024
3.
    What is eBPF? An Introduction and Deep Dive into the eBPF Technology. https:// ebpf. io/ what-
    is-ebpf/ #maps. Accessed on Feb. 20, 2024
4.
    T. Karagiannis, K. Papagiannaki, M. Faloutsos, BLINC: multilevel traffic classification in the
    dark, in _SIGCOMM ’05 (New York, NY, USA)_ (Association for Computing Machinery, New
    York, 2005), pp. 229–240
5.
    G. Aceto, A. Dainotti, W. de Donato, A. Pescape, PortLoad: taking the best of two worlds in
    traffic classification, in _2010 INFOCOM IEEE Conference on Computer Communications_
    _Workshops_ (2010), pp. 1–5
6.
    J. Zhang, X. Chen, Y. Xiang, W. Zhou, J. Wu, Robust network traffic classification. IEEE/ACM
    Trans. Netw. **23** , 1257–1270 (2015)
    [Crossref]
7.
    T.T. Nguyen, G. Armitage, A survey of techniques for internet traffic classification using
    machine learning. IEEE Commun. Surv. Tutorials **10** (4), 56–76 (2008)
    [Crossref]
8.
    A. Jain, R. Duin, J. Mao, Statistical pattern recognition: a review. IEEE Trans. Pattern Anal.
    Mach. Intell. **22** (1), 4–37 (2000)
    [Crossref]
9.
    R. Kamath, K.M. Sivalingam, Machine learning based flow classification in DCNs using p4
    switches, in _2021 International Conference on Computer Communications and Networks_
    _(ICCCN)_ (2021), pp. 1–10
10.
    K.M. Sivalingam, Applications of artificial intelligence, machine learning and related techniques
    for computer networking systems (2021)
11.
    I. Ahmad, M. Basheri, M.J. Iqbal, A. Rahim, Performance comparison of support vector
    machine, random forest, and extreme learning machine for intrusion detection. IEEE Access **6** ,
    33789–33795 (2018)
    [Crossref]
12.
    R. Xin, J. Zhang, Y. Shao, Complex network classification with convolutional neural network.
    Tsinghua Sci. Technol. **25** (4), 447–457 (2020)
    [Crossref]
13.
    J. Höchst, L. Baumgärtner, M. Hollick, B. Freisleben, Unsupervised traffic flow classification
    using a neural autoencoder, in _2017 IEEE 42nd Conference on Local Computer Networks (LCN)_
    (2017), pp. 523–526
14.
    S. Miano, M. Bertrone, F. Risso, M. Tumolo, M.V. Bernal, Creating complex network services
    with eBPF: experience and lessons learned, in _2018 IEEE 19th International Conference on High_


_Performance Switching and Routing (HPSR)_ (2018), pp. 1–8
15.
D. Scholz, D. Raumer, P. Emmerich, A. Kurtz, K. Lesiak, G. Carle, Performance implications of
packet filtering with Linux eBPF, in _2018 30th International Teletraffic Congress (ITC 30)_ , vol.
01 (2018), pp. 209–217
16.
S. Baidya, Y. Chen, M. Levorato, eBPF-based content and computation-aware communication
for real-time edge computing, in _IEEE INFOCOM 2018—IEEE Conference on Computer
Communications Workshops (INFOCOM WKSHPS)_ (2018), pp. 865–870
17.
M.S. Sheikh, Y. Peng, Procedures, criteria, and machine learning techniques for network traffic
classification: A survey. IEEE Access **10** , 61135–61158 (2022)
[Crossref]
18.
TCPDUMP and LIBPCAP. https:// [http://www.](http://www.) tcpdump. org/. Accessed on Jul. 20, 2023
19.
H. Xu, S. Li, Z. Cheng, R. Qin, J. Xie, P. Sun, TrafficGCN: mobile application encrypted traffic
classification based on GCN, in _GLOBECOM 2022–2022 IEEE Global Communications
Conference_ (2022), pp. 891–896
20.
V. Duarte, N. Farruca, Using libpcap for monitoring distributed applications, in _2010
International Conference on High Performance Computing & Simulation_ (2010), pp. 92–97
21.
tshark—Linux man page. https:// linux. die. net/ man/ 1/ tshark. Accessed on Jul. 20, 2023
22.
Wireshark—About. https:// [http://www.](http://www.) wireshark. org/ about. html. Accessed on Jul. 20, 2023
23.
RawCap—A Raw socket sniffer for Windows. https:// [http://www.](http://www.) netresec. com/? page= RawCap.
Accessed on Jul. 20, 2023
24.
M.P. Karpowicz, P. Arabas, Preliminary results on the Linux libpcap model identification, in
_2015 20th International Conference on Methods and Models in Automation and Robotics
(MMAR)_ (2015), pp. 1056–1061
25.
P. Orosz, T. Skopko, Software-based packet capturing with high precision timestamping for
Linux, in _2010 Fifth International Conference on Systems and Networks Communications_
(2010), pp. 381–386
26.
H. Tahaei, F. Afifi, A. Asemi, F. Zaki, N.B. Anuar, The rise of traffic classification in IoT
networks: a survey. J. Netw. Comput. Appl. **154** , 102538 (2020)
[Crossref]
27.
G. Aceto, D. Ciuonzo, A. Montieri, A. Pescapé, Mobile encrypted traffic classification using
deep learning: experimental evaluation, lessons learned, and challenges. IEEE Trans. Netw. Serv.
Manag. **16** (2), 445–458 (2019)
[Crossref]
28.
X. Wang, S. Chen, J. Su, App-net: a hybrid neural network for encrypted mobile traffic
classification, in _IEEE INFOCOM 2020—IEEE Conference on Computer Communications
Workshops (INFOCOM WKSHPS)_ (2020), pp. 424–429


29.
Y. Zhang, S. Zhao, J. Zhang, X. Ma, F. Huang, STNN: A novel TLS/SSL encrypted traffic
classification system based on stereo transform neural network, in _2019 IEEE 25th International
Conference on Parallel and Distributed Systems (ICPADS)_ (2019), pp. 907–910
30.
G. Aceto, D. Ciuonzo, A. Montieri, A. Pescape, Traffic classification of mobile apps through
multi-classification, in _GLOBECOM 2017—2017 IEEE Global Communications Conference_
(2017), pp. 1–6
31.
W. Qiu, G. Chen, K.N. Nguyen, A. Sehgal, P. Nayak, J. Choi, Category-based 802.11ax target
wake time solution. IEEE Access **9** , 100154–100172 (2021)
32.
Users for system developers. https:// android. googlesource. com/ platform/ frameworks/ base/ +/
master/ core/ java/ android/ os/ Users. md. Accessed on Jul. 20, 2023
33.
S. Kumar, V. Murgai, D. Singh, I. Kommeneni, Recurrent neural network architecture for
communication log analysis, in _2022 IEEE 33rd Annual International Symposium on Personal,
Indoor and Mobile Radio Communications (PIMRC)_ (2022), pp. 503–508
34.
K. Dissanayake, M.G. Md Johar, Comparative study on heart disease prediction using feature
selection techniques on classification algorithms, in _Applied Computational Intelligence and Soft
Computing_ , vol. 2021 (2021), pp. 1–17
35.
Google Play—Zoom. https:// play. google. com/ store/ apps/ details? id= us. zoom. videomeetings.
Accessed on Jul. 20, 2023
36.
Google Play—Skype. https:// play. google. com/ store/ apps/ details? id= com. skype. raider. Accessed
on Jul. 20, 2023
37.
How much bandwidth does Skype need?. https:// support. skype. com/ en/ faq/ FA1417/ how-much-
bandwidth-does-skype-need. Accessed on Jul. 20, 2023
38.
Google Meet hardware requirements—Minimum bandwidth required. https:// support. google.
com/ a/ answer/ 4541234? hl= en#zippy= %2Cminimum-bandwidth-required. Accessed on Jul. 20,
2023
39.
Google Play—Brawl Star. https:// play. google. com/ store/ apps/ details? id= com. supercell.
brawlstars. Accessed on Jul. 20, 2023
40.
iPerf—The ultimate speed test tool for TCP, UDP and SCTP. https:// iperf. fr/. Accessed on Jul.
20, 2023
41.
Y. Sato, M. Oguchi, S. Yamaguchi, Mobile application aware smartphone cpu clock frequency
optimization, in _2018 Sixth International Symposium on Computing and Networking Workshops
(CANDARW)_ (2018), pp. 564–566
42.
K. Kumakura, M. Oguchi, T. Kamiyama, S. Yamaguchi, Cpu usage trends in android
applications, in _2022 IEEE International Conference on Big Data (Big Data)_ (2022), pp. 6730–
6732
43.
Geekbench 6—Cross-Platform Benchmark. https:// [http://www.](http://www.) geekbench. com/. Accessed on Jul. 20,
2023


44.
Google Play—3DMark—The Gamer’s Benchmark. https:// play. google. com/ store/ apps/ details?
id= com. futuremark. dmandroid. application. Accessed on Jul. 20, 2023
45.
T. Chen, C. Guestrin, XGBoost: a scalable tree boosting system, in _Proceedings of the 22nd
ACM SIGKDD International Conference on Knowledge Discovery and Data Mining_ , KDD ’16,
(New York, NY, USA) (Association for Computing Machinery, New York, 2016), pp. 785–794
_OceanofPDF.com_


(1)

© The Author(s), under exclusive license to Springer Nature Switzerland AG 2026
S. Singh et al. (eds.), _GenAI and LLMs for Beyond 5G Networks_
https://doi.org/10.1007/978-3-032-06418-9_17

##### Advanced Data Management and

##### Analytics Using LLMs for B5G Networks

Pankaj Thorat^1
IBM Research, Bangalore, India

```
Pankaj Thorat
Email: pankaj.thorat@ibm.com
```
**Abstract**
The exponential growth in data complexity and network heterogeneity
within Beyond-5G (B5G) infrastructures demands a paradigm shift in data
analytics and network intelligence. This chapter explores the integration of
Large Language Models (LLMs) into predictive and real-time analytics for
B5G networks, leveraging the AI-as-a-Service (AIaaS) architectural pattern.
We outline methodologies involving corpus enrichment, LLM-ML teaming,
and real-time semantic interpretation to facilitate intelligent, explainable,
and risk-governed network operations. Practical use cases, implementation
guides, and future directions are presented along with seven figures
detailing technical architectures and workflows.

**1 Predictive Analytics with LLMs**

**1.1 From NWDAF to AIaaS in B5G**

The 3GPP Network Data Analytics Function (NWDAF) laid foundational
ground for integrating intelligence into core network slices. However, it
remains largely static, with predefined models and rigid data ingestion
pipelines. As B5G systems evolve into cloud-native, elastic infrastructures,
NWDAF is transforming into an AI-as-a-Service (AIaaS) layer-a modular,


microservice-based architecture that hosts dynamic analytics engines,
including LLM-driven components.
AIaaS enables seamless data access, model updates, and inference
delivery through standardized APIs. In this model, LLMs serve as adaptive
agents that process network logs, user behavior trends, and environmental
metadata to provide rich predictions. The modularity ensures that LLM
services-such as enrichment, summarization, and policy recommendations-
can be versioned and optimized independently (Fig. 1 ).

**_Fig. 1_** AIaaS architecture for predictive analytics

- Data Sources: telemetry, logs, topology maps.
- Corpus Enrichment Module: LLM-enhanced preprocessing.
- LLM-ML Engine: For semantic + numeric forecasting.
- Inference API Gateway: Serves predictions to orchestrators and policy
    engines.

**1.2 Traffic Forecasting Via Corpus Enrichment**

One of the biggest bottlenecks in real-time analytics is the _quality and
context of incoming data_. Raw telemetry may be incomplete, unstructured,
or unlabeled. Using corpus enrichment, LLMs synthetically generate
contextual metadata and fill missing data points based on learned patterns.
This mirrors techniques in Information Retrieval (IR) where documents are
enhanced for improved searchability and classification [ 1 ].
For example, if a telemetry stream lacks handover annotations, an LLM
trained on previous event logs can infer likely transitions and attach
semantically-rich tags such as “handover anomaly” or “paging collision.”

**1.3 Feature Extraction with LLM-ML Teaming**


As B5G networks transition toward intelligent, self-organizing
infrastructures, the need for accurate, adaptable, and explainable feature
extraction has grown increasingly critical. Traditional machine learning
(ML) pipelines often rely on handcrafted or purely statistical features
derived from traffic and telemetry data. However, in dynamic, high-
dimensional B5G environments, such approaches struggle with concept
drift, low interpretability, and limited responsiveness to novel situations.
To address these limitations, we introduce an LLM-ML teaming
paradigm that synergistically combines symbolic inference capabilities of
Large Language Models (LLMs) with numerical optimization strength of
gradient-based ML. Inspired by the teaming model proposed by Wang et al.
(arXiv:2506.09085), this architecture offers a bidirectional feedback loop
between symbolic reasoning and statistical learning, thereby improving
both accuracy and stability of the features used in predictive analytics.

**_1.3.1 Architectural Workflow_**

The teaming pipeline proceeds in three coordinated stages:

1. **Symbolic Pattern Generation Via LLMs**
    LLMs trained on enriched corpora of network logs, anomaly
    descriptions, and configuration metadata are used to generate candidate
    _semantic features_. These can include:
       Temporal anomalies (e.g., “burst traffic every 20 minutes”),
       Structural insights (e.g., “upstream delay correlated with neighbor
       node resets”),
       Rule-like constructs (e.g., “if TCP retries > 3 and RSSI < –85 dBm
       → path instability”).
2. **ML-Based Feature Optimization**
    These symbolic outputs are encoded into structured vectors and
    validated via a gradient-based ML backend (e.g., LSTM, GRU, or
    GNNs). The system applies statistical pruning to:
       eliminate unstable or redundant patterns,
       weight the feature vector according to contribution to predictive loss
       (e.g., RMSE in throughput estimation), and,
       normalize across time and topology.


3. **Feedback and Feature Looping**
    When the model’s error signal exceeds a defined threshold (e.g., due to
    concept drift or previously unseen topology behavior), a _new prompt_ is
    sent to the LLM to reframe the scenario. This “back-loop” enhances
    resilience and allows symbolic reasoning to adapt based on empirical
    feedback.

**_1.3.2 Benefits of LLM-ML Teaming_**

```
Interpretability : Each ML feature retains a semantic origin (i.e., LLM-
generated descriptor), making the model explainable to operators.
Stability : By pruning features that oscillate or degrade over time,
teaming ensures statistical reliability.
Adaptability : LLMs can quickly synthesize new features for novel
phenomena without retraining the entire ML model.
Security : LLM prompts can be guarded (see Sect. 2 ) to avoid generating
misleading or exploitable patterns.
```
**2 AIaaS for Real-Time Streams**

In Beyond-5G (B5G) networks, real-time decision-making is no longer a
luxury, it is a necessity. Rapid changes in traffic patterns, user mobility,
interference, and security threats demand systems that can analyze, infer,
and respond within milliseconds. The AI-as-a-Service (AIaaS) framework
offers a scalable and modular architecture to integrate real-time analytics
pipelines with LLM-powered decision engines.
In this architecture, LLMs are not just passive learners but active
interpreters of streaming telemetry. They can semantically decode patterns
from sequences of raw packet traces, error logs, or access records and
produce actionable insights such as:

Dynamic handover advisories,
Congestion avoidance policies,
Flow rerouting triggers based on semantic conditions (e.g., “handover +
latency spike”).
These models operate through **run-call APIs** that accept a semantic
query (e.g., “What’s the most probable cause of packet loss surge?”) and


return decision-ready insights within milliseconds (Fig. 2 ).

**_Fig. 2_** AIaaS architecture for real-time decisions

**Explanation**

```
Input: Live telemetry and alert streams from RAN, core, and application
layers
Stream Ingestion Module: Parses structured and unstructured events
LLM Contextual Decoder: Interprets semantic meaning from logs
Decision Layer: Applies threat scoring, prediction, or advisory
generation
Action API Gateway: Sends decisions to orchestrators (e.g., Open RAN
controller)
```
**2.1 Real-Time Security with LLMs**

Real-time security analytics in B5G must handle polymorphic attacks, zero-
day threats, and multilayer intrusion patterns. Traditional rule-based
intrusion detection systems (IDS) are often brittle and slow to adapt. LLMs
bring **semantic awareness** to the IDS pipeline-enabling dynamic translation
of packet logs, behavioral anomalies, and access patterns into natural-
language narratives and action prompts. For instance, given a burst of
authentication retries and high uplink usage, an LLM might infer: “Potential
credential-stuffing attack targeting IoT subnet.” Such semantic decoding
enables automatic policy synthesis—blacklisting sources, increasing
CAPTCHA frequency, or triggering re-authentication protocols (Fig. 3 ).


**_Fig. 3_** LLM-based security decision flow

**Explanation**

```
Input: Network event stream (logs, SNMP, NetFlow)
LLM Decoder: Translates sequences into event narratives
Anomaly Detection Layer: Scores severity using historical patterns
Policy Engine: Generates JSON-based rule updates (e.g., blocklists, QoS
shifts)
Enforcement Gateway: Communicates with firewall, controller, or user
plane
```
**2.2 Ensuring Safe AI Operations**

LLMs used in real-time decision systems must be hardened against
adversarial manipulation and policy drift. For instance, poorly constructed
prompts or unguarded instruction-following behaviors can lead to the
generation of overly permissive access policies or misclassification of
threats. To counter this, we integrate a **SafeStyle prompting layer** that
preconditions all LLM prompts with alignment constraints [ 2 ]. These
include:

Denial of action if insufficient evidence is available (“don’t guess”),
Enforcement of syntactic boundaries for rules (e.g., only generate within
valid firewall schema),
Prompt classifiers that reject ambiguous or speculative questions.
Real-time analytics powered by LLMs can drastically improve both the
responsiveness and intelligence of B5G networks. By combining symbolic


reasoning with modular, microservice-based inference, LLMs can drive
both secure and scalable operational strategies—given proper safeguards
are in place to mitigate misuse.

**3 Implementation Guides**

While conceptual architectures for LLM-powered analytics in B5G are
compelling, practical deployment requires modular design, resource-
awareness, and governance mechanisms. This section outlines how to
engineer and operationalize AIaaS systems featuring LLMs for scalable,
secure, and interpretable network operations.

**3.1 Modular AIaaS Deployments**

To support flexibility and upgradability, we recommend decomposing the
AIaaS system into six distinct, loosely-coupled components. Each module
is responsible for a well-defined function and can be deployed
independently, scaled elastically, or replaced without affecting other
modules.

**Component Breakdown**

1. **Ingestion Adapter**

```
Collects structured/unstructured telemetry
Normalizes and tags data for downstream processing
```
2. **Corpus Augmentor (LLM)**

```
Applies semantic enrichment
Fills missing annotations
Adds synthetic descriptors (e.g., “handover oscillation pattern”)
```
3. **Feature Engine (LLM-ML Teamed)**

```
Extracts symbolic features from LLM
Validates and refines via gradient-based ML
```
4. **Inference API**


```
Exposes predictions, scores, and advisory output to external
consumers
Supports both batch and stream endpoints
```
5. **Policy Enforcer**

```
Translates model output into actionable policy (e.g., firewall rule,
QoS command)
Enforces changes across SDN, RAN, or EPC layers
```
6. **Governance Layer**

```
Audits inference decisions
Logs prompt history and model response
Applies risk controls (e.g., deny hallucinated advisories) (Fig. 4 )
```
**_Fig. 4_** Modular AIaaS stack for LLM-based B5G analytics

**3.2 Scalability and Sustainability**


LLM integration in edge-centric B5G environments must be both **resource-
aware** and **carbon-efficient**. We propose two practical methods to address
this:

(a) **Small LLMs at the Edge**

```
Deploy distilled or quantized LLMs to handle lightweight
inference tasks
Offload heavy enrichment tasks to cloud-hosted counterparts
Use model switchers to adapt based on latency or capacity
```
(b) **Directive-Based Inference for Energy Control**

```
Apply generation directives to prompt LLMs for low-energy
decoding (e.g., restrict response size or beam width)
Implement carbon score estimators to evaluate compute cost of
each inference call [ 3 ]
```
**3.3 Governance**

Trust in LLM-backed decisions is essential, especially in automated
network control. The governance layer ensures traceability, fairness, and
policy compliance:

**Key Mechanisms**

```
Inference Audit Trails
```
- Logs include: prompt, context window, model output, timestamp,
    triggered action
**Explainability Interface**
- Translates LLM outputs into human-understandable summaries
- Example: “Drop in latency predicted due to reduced inter-eNB
handovers”
**Regulatory Alignment (e.g., GDPR, CCPA)**
- Personal data tokens can be anonymized before LLM access
- Access requests and deletions must propagate through enrichment and
ML pipelines


```
Fallback Layers
```
- If LLM response confidence < threshold, system falls back to known-
    good rulebase or human-in-the-loop alerts

LLM-based analytics pipelines can be made reliable and scalable through
modular decomposition, model compression, carbon-aware directives, and
robust governance overlays. This enables telcos and network operators to
safely and efficiently operationalize next-generation intelligence in their
B5G cores.

**4 Future Directions**

As B5G infrastructure continues to evolve, so too must the underlying
intelligence mechanisms that support its autonomy and resilience. The
integration of Large Language Models (LLMs) into network analytics
marks the beginning of a new cognitive networking paradigm. However,
sustaining this evolution demands innovation across architectural,
algorithmic, and ethical dimensions.
Here, we outline three promising future directions:

**4.1 Assembler-Model Stacks**

Rather than relying on large, monolithic language models, the trend is
shifting toward **assembler-style architectures** , where different micro-
models collaborate to perform specific functions in a modular fashion. This
decomposition is inspired by multi-agent LLM ecosystems and neural-
symbolic stacks.

**Examples**

**Planner Models** : Choose the inference task and delegate.
**Teacher Models** : Provide long-term guidance based on historical
context.
**Critic Models** : Evaluate outputs and reject hallucinations.
By designing such specialized roles for smaller LLMs (e.g., 3B or 7B
parameter scale), we can build **interpretability, adaptability, and
scalability** into AIaaS systems without centralized model dependency.


**4.2 Trustworthy AI in LLM Pipelines**

The more we automate decisions in B5G networks, the more critical it
becomes to ensure **fairness** , **robustness** , and **transparency** in LLM
pipelines.
Key Areas for Advancement:
**Adversarial Robustness** : Developing LLMs that resist prompt injections
or protocol manipulation.
**Bias Auditing Tools** : Building datasets and frameworks to identify
discriminatory behavior in LLM-based classifiers or policy advisors.
**Zero Trust AI Models** : Embedding runtime verifiers or explainers
alongside LLM outputs to qualify their trust level before enforcing
decisions.
Such approaches will be vital as LLMs increasingly shape mobility
management, access control, and congestion mitigation at the network core.

**4.3 Green AI and Directive-Aware Inference**

As generative models become more pervasive, their **carbon and resource
footprint** becomes a significant design concern. Techniques like _generation
directives_ enable precise control over the inference behavior of LLMs:

**Use Cases**

“Brief only” directives to limit verbosity,
“No speculations” instructions to eliminate unnecessary reasoning,
“Compute-budget capped” prompts to fit within energy envelopes.
When embedded into the orchestration logic, these controls allow for
**sustainable AI-as-a-Service deployments** , particularly in mobile edge
environments.

**5 Conclusion of Sect. 4**

The future of LLM-powered B5G analytics lies not in bigger models but in
**smarter architectures** —decentralized, trust-aware, and environmentally
conscious. By advancing assembler model stacks, embedding safety and
fairness into inference workflows, and minimizing compute footprints, we


can ensure that LLMs continue to play a transformative yet sustainable role
in next-generation networks.

**References**

1. Zhang et al. _On the Merits of LLM-Based Corpus Enrichment_ (2024).. arXiv: 2506. 06015
2.
    Li et al. _Defending Language Models Against Superficial Style Alignment_ (2024).. arXiv: 2506.
    07452
3.
    Kumar et al. _Toward Sustainable GenAI using Generation Directives_ (2024)_._. arXiv: 2403. 12900
       _OceanofPDF.com_


(1)

© The Author(s), under exclusive license to Springer Nature Switzerland AG 2026
S. Singh et al. (eds.), _GenAI and LLMs for Beyond 5G Networks_
https://doi.org/10.1007/978-3-032-06418-9_18

##### Conclusion

Sukhdeep Singh^1 , Madhan Raj Kanagarathinam^1 and Mohan Rao GNS^1
Samsung R&D Institute India – Bangalore (SRI-B), Bangalore, India

```
Sukhdeep Singh
Email: sukh.sandhu@samsung.com
```
In the heart of a futuristic metropolis, where skyscrapers pulse with digital
life and autonomous drones hum through the air, a Beyond 5G (B5G)
network orchestrates a symphony of connectivity. It’s 2025, and the city’s
infrastructure is alive with the intelligence of generative artificial
intelligence (GenAI) and large language models (LLMs), seamlessly
integrated into every node, from edge devices to core data centers. This is
no ordinary network, it’s an AI-native ecosystem, where algorithms don’t
just transmit data but anticipate needs, resolve conflicts, and fortify
defenses in real time. The streets below buzz with smart vehicles navigating
with split-second precision, while factories churn out goods optimized by
local AI inferences, and citizens interact with a metaverse powered by
ultralow-latency communications. This book has charted the technical
innovations driving this transformation, weaving together advancements in
edge computing, security, orchestration, and analytics to paint a vision of
B5G and 6G networks as the backbone of a smarter, more connected world.
Let us step into this landscape as a technical story, an epic of silicon,
algorithms, and human ingenuity converging to redefine the future of
telecommunications.
Deep within a smart factory on the city’s edge, a cluster of edge devices
hums with purpose, their neural processing units (NPUs) executing GenAI
models optimized for resource-constrained environments. These models,
distilled through techniques like quantization-aware training and model


compression, perform real-time inference on sensor data, enabling robotic
arms to adjust their movements with millisecond precision. LoRA-based
personalization ensures each device adapts to the factory’s unique
production patterns, while federated fine-tuning allows collaborative
learning across devices without compromising sensitive data. This edge-
driven intelligence, a cornerstone of B5G networks, eliminates the need for
constant cloud connectivity, slashing latency and preserving privacy. The
factory’s network operates like a living organism, its nodes communicating
seamlessly to optimize throughput, reduce energy consumption, and
maintain operational continuity, a testament to the power of localized AI in
transforming industrial workflows.
Across the city, a 6G radio access network (RAN) pulses with activity,
its antennas orchestrating beams with surgical precision. Here, the
mathematical foundations of AI and signal processing converge, with
matrix multiplications and dot products powering both neural networks and
MIMO beamforming. Vector processors, GPUs, and emerging data
processing units (DPUs) like NVIDIA’s BlueField-3 handle these
workloads, while domain-optimized system-on-chips (SoCs) like Marvell’s
Octeon 10 Fusion consolidate compute domains for efficiency. RISC-V
architectures, with their open and customizable designs, enable tailored
solutions for the unique demands of RAN and GenAI integration. This
convergence transforms the RAN into a programmable, intelligent platform,
where LLMs guide adaptive beam control and deliver real-time inference at
the edge. Telecom operators now offer not just connectivity but AI-native
services, from dynamic spectrum allocation to context-aware content
delivery, reshaping the mobile experience for millions.
Yet, this intelligent network operates in a world of constant threats. In a
fortified control room, engineers monitor a dashboard alive with telemetry,
where AI-driven anomaly detection systems scan for signs of cyberattacks,
be it a distributed denial-of-service (DDoS) assault or a sophisticated man-
in-the-middle exploit. GenAI models, trained on vast datasets of network
behavior, act as vigilant sentinels, identifying subtle deviations that signal
potential breaches. Robust encryption, rooted in secure-by-design
principles, protects user data and signaling, while distributed intelligence
across network nodes ensures resilience against disruptions. The network’s
self-healing capabilities, powered by LLM-driven agents, detect hardware
failures or congestion and dynamically reroute traffic, reconfiguring


topology to maintain service continuity. This duality of AI as both protector
and potential vulnerability underscores the need for rigorous security
frameworks, ensuring the network remains a bastion of trust in an
increasingly connected world.
In a nearby telecom lab, a digital twin hums to life on a high-
performance server, its virtual nodes simulating the chaos of a B5G network
under extreme conditions. This framework, powered by conditional
generative adversarial networks (cGANs), generates synthetic key
performance indicators (KPIs) that mirror real-world traffic patterns,
mobility profiles, and network topologies. Engineers use this digital
sandbox to stress-test AI models, validating their performance against edge
cases like sudden traffic spikes or hardware faults. The system’s ability to
synthesize realistic scenarios reduces operational risks, enabling confident
deployment of AI-driven features in production environments. By
integrating historical and real-time data, this digital twin framework
supports iterative validation and scenario-specific analysis, paving the way
for scalable, robust B5G networks that adapt to the unpredictable dynamics
of modern connectivity.
In a network operations center bathed in the glow of monitors, operators
issue high-level commands: “Optimize for low latency while ensuring
fairness across users.” LLMs, with their unparalleled ability to parse natural
language and reason through ambiguity, translate these intents into
actionable policies. Multi-agent systems, guided by these models, negotiate
competing demands, balancing latency, spectral efficiency, and energy
constraints in real time. Explainable AI techniques, such as SHAP and
counterfactual reasoning, provide transparent rationales for each decision,
enabling operators to audit and trust the system’s actions. This intent-based
networking paradigm, powered by LLMs, ushers in an era of zero-touch
orchestration, where the network autonomously adapts to changing
conditions, from reallocating resources in network slices to ensuring quality
of service (QoS) for mission-critical applications.
Data, the lifeblood of this ecosystem, flows through the network like a
digital river, its currents shaped by advanced analytics. LLMs, integrated
into AI-as-a-Service (AIaaS) architectures, process massive datasets to
predict traffic surges and optimize load balancing. In one scenario, a fleet of
autonomous vehicles generates a deluge of sensor data, but the network’s
predictive models dynamically redistribute resources to maintain QoS,


ensuring seamless navigation. Semantic interpretation and corpus
enrichment enable these systems to extract meaning from raw telemetry,
turning it into actionable intelligence. This data-driven approach supports
applications ranging from smart grid management to immersive media
streaming, making B5G networks not just faster but also smarter and more
user-centric.
On the streets below, a citizen’s smartphone processes network traffic
with unprecedented efficiency, thanks to an eBPF-assisted AI system. The
extended Berkeley Packet Filter (eBPF) reduces packet processing
overhead, enabling context-aware classification that cuts CPU utilization by
over 90%. This technology, now deployed in devices like the latest series of
the Samsung Galaxy devices like Galaxy Z Fold 7, ensures that even
resource-constrained devices can handle the demands of B5G connectivity.
By selectively processing relevant app traffic, the system optimizes battery
life and enhances user experience, demonstrating how AI can transform
even the smallest nodes in the network ecosystem.
The invisible highways of wireless channels, critical to this connected
world, are modeled with generative precision. Diffusion-based models,
leveraging U-Net architectures and noise-conditioned score networks,
simulate time-varying and frequency-selective channels with remarkable
accuracy. These generative approaches eliminate the need for costly
measurement campaigns, enabling engineers to design robust systems for
diverse environments, from dense urban grids to remote rural landscapes.
Applications like channel estimation, beam prediction, and interference
detection benefit from these models, ensuring reliable communications in
the face of fading and interference.
Data transmission itself is being redefined by GenAI, which moves
beyond traditional source and channel coding to prioritize semantic
relevance. Joint source-channel coding techniques, powered by LLMs,
enable ultrareliable, low-latency communications tailored to the needs of
IoT devices, autonomous systems, and immersive applications. By focusing
on task-oriented transmission, these methods optimize bandwidth and
reduce latency, supporting the data-intensive demands of B5G networks.
This technical epic is not without its challenges. Computational
constraints push the boundaries of hardware design, requiring innovative
solutions like RISC-V-based SoCs and domain-optimized accelerators.
Regulatory landscapes demand careful navigation, with frameworks like


GDPR and the EU’s AI Act setting strict standards for data privacy and
model accountability. Ethical considerations loom large, with guardrails like
prompt filtering, output validation, and reinforcement learning with human
feedback (RLHF) ensuring that AI systems remain safe, fair, and aligned
with human values. Global collaboration is essential to establish
interoperable standards, ensuring that the benefits of B5G networks, faster
speeds, smarter services, and equitable access, are shared worldwide.
As the city’s skyline glows against the twilight, a researcher in a lab
fine-tunes a causal reasoning model to improve network diagnostics, its
neurosymbolic architecture blending logic with neural power. An engineer
deploys a self-healing algorithm, watching as it reroutes traffic to bypass a
failing node. A policymaker, in a quiet office, drafts guidelines to protect
user privacy while fostering innovation. Together, they are the architects of
a new era, where B5G and 6G networks, infused with GenAI and LLMs,
become the foundation of a connected world. This book is their blueprint, a
technical saga of innovation, resilience, and responsibility. It invites you,
the reader, to join this journey, to design systems that are not just efficient
and intelligent but also trustworthy and inclusive. The future of connectivity
is unfolding, and its story is yours to shape.

```
OceanofPDF.com
```

